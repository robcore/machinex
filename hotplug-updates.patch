From cbf17b4b432bfe962ae330d9a86f8ded47d33b22 Mon Sep 17 00:00:00 2001
From: Tkkg1994 <luca.grifo@outlook.com>
Date: Fri, 25 Mar 2016 14:13:45 +0100
Subject: [PATCH] this adds back all cpugovs from Nova-Kernel, with features
 that are neeeded, please check Nova-Kernel github for history!

---
 arch/arm/configs/0StockKernel_SM-G901F_defconfig |   59 +
 arch/arm/mach-msm/alucard_hotplug.c              |  140 +-
 drivers/cpufreq/Kconfig                          |  600 ++
 drivers/cpufreq/Makefile                         |   56 +-
 drivers/cpufreq/cpufreq.c                        |   41 +-
 drivers/cpufreq/cpufreq_HYPER.c                  |  854 ++
 drivers/cpufreq/cpufreq_alucard.c                |  831 ++
 drivers/cpufreq/cpufreq_barry_allen.c            | 1539 ++++
 drivers/cpufreq/cpufreq_bioshock.c               |  590 ++
 drivers/cpufreq/cpufreq_blu_active.c             | 1206 +++
 drivers/cpufreq/cpufreq_cafactive.c              | 1851 +++++
 drivers/cpufreq/cpufreq_conservative_x.c         |  503 ++
 drivers/cpufreq/cpufreq_darkness.c               |  417 +
 drivers/cpufreq/cpufreq_despair.c                |  550 ++
 drivers/cpufreq/cpufreq_electroactive.c          | 1612 ++++
 drivers/cpufreq/cpufreq_electrodemand.c          | 1130 +++
 drivers/cpufreq/cpufreq_elementalx.c             |  589 ++
 drivers/cpufreq/cpufreq_governor.c               |   99 +-
 drivers/cpufreq/cpufreq_governor.h               |   36 +
 drivers/cpufreq/cpufreq_hellsactive.c            | 1548 ++++
 drivers/cpufreq/cpufreq_impulse.c                | 1653 ++++
 drivers/cpufreq/cpufreq_intelliactive.c          | 1439 ++++
 drivers/cpufreq/cpufreq_intellidemand.c          | 1405 ++++
 drivers/cpufreq/cpufreq_intellimm.c              | 1462 ++++
 drivers/cpufreq/cpufreq_interactive_pro.c        | 1562 ++++
 drivers/cpufreq/cpufreq_interactive_x.c          | 1388 ++++
 drivers/cpufreq/cpufreq_ironactive.c             | 1757 +++++
 drivers/cpufreq/cpufreq_lionheart.c              |  550 ++
 drivers/cpufreq/cpufreq_nightmare.c              |  726 ++
 drivers/cpufreq/cpufreq_ondemand.c               |   11 +-
 drivers/cpufreq/cpufreq_ondemand_x.c             |  895 +++
 drivers/cpufreq/cpufreq_ondemandplus.c           |  954 +++
 drivers/cpufreq/cpufreq_pegasusq.c               |  645 ++
 drivers/cpufreq/cpufreq_raccoon_city.c           |  176 +
 drivers/cpufreq/cpufreq_smartass2.c              |  925 +++
 drivers/cpufreq/cpufreq_smartmax.c               | 1282 +++
 drivers/cpufreq/cpufreq_smartmax_eps.c           | 1439 ++++
 drivers/cpufreq/cpufreq_umbrella_core.c          | 2088 +++++
 drivers/cpufreq/cpufreq_yankactive.c             | 1438 ++++
 drivers/cpufreq/cpufreq_zzmoove.c                | 9146 ++++++++++++++++++++++
 drivers/cpufreq/cpufreq_zzmoove_profiles.h       | 2332 ++++++
 drivers/gpu/msm/kgsl_pwrctrl.c                   |   16 +
 include/linux/cpufreq.h                          |  120 +-
 include/linux/msm_kgsl.h                         |    8 +
 include/trace/events/cpufreq_barry_allen.h       |  150 +
 include/trace/events/cpufreq_cafactive.h         |  124 +
 include/trace/events/cpufreq_electroactive.h     |   86 +
 include/trace/events/cpufreq_interactive.h       |   14 +
 include/trace/events/cpufreq_interactive_x.h     |  113 +
 include/trace/events/cpufreq_ironactive.h        |  138 +
 include/trace/events/cpufreq_ondemandplus.h      |   77 +
 include/trace/events/cpufreq_umbrella_core.h     |  150 +
 include/trace/events/cpufreq_yankactive.h        |  112 +
 53 files changed, 48501 insertions(+), 131 deletions(-)
 create mode 100755 drivers/cpufreq/cpufreq_HYPER.c
 create mode 100755 drivers/cpufreq/cpufreq_alucard.c
 create mode 100644 drivers/cpufreq/cpufreq_barry_allen.c
 create mode 100755 drivers/cpufreq/cpufreq_bioshock.c
 create mode 100644 drivers/cpufreq/cpufreq_blu_active.c
 create mode 100644 drivers/cpufreq/cpufreq_cafactive.c
 create mode 100644 drivers/cpufreq/cpufreq_conservative_x.c
 create mode 100644 drivers/cpufreq/cpufreq_darkness.c
 create mode 100644 drivers/cpufreq/cpufreq_despair.c
 create mode 100644 drivers/cpufreq/cpufreq_electroactive.c
 create mode 100644 drivers/cpufreq/cpufreq_electrodemand.c
 create mode 100644 drivers/cpufreq/cpufreq_elementalx.c
 create mode 100644 drivers/cpufreq/cpufreq_hellsactive.c
 create mode 100644 drivers/cpufreq/cpufreq_impulse.c
 create mode 100755 drivers/cpufreq/cpufreq_intelliactive.c
 create mode 100755 drivers/cpufreq/cpufreq_intellidemand.c
 create mode 100644 drivers/cpufreq/cpufreq_intellimm.c
 create mode 100644 drivers/cpufreq/cpufreq_interactive_pro.c
 create mode 100644 drivers/cpufreq/cpufreq_interactive_x.c
 create mode 100755 drivers/cpufreq/cpufreq_ironactive.c
 create mode 100755 drivers/cpufreq/cpufreq_lionheart.c
 create mode 100644 drivers/cpufreq/cpufreq_nightmare.c
 create mode 100755 drivers/cpufreq/cpufreq_ondemand_x.c
 create mode 100644 drivers/cpufreq/cpufreq_ondemandplus.c
 create mode 100755 drivers/cpufreq/cpufreq_pegasusq.c
 create mode 100755 drivers/cpufreq/cpufreq_raccoon_city.c
 create mode 100644 drivers/cpufreq/cpufreq_smartass2.c
 create mode 100644 drivers/cpufreq/cpufreq_smartmax.c
 create mode 100644 drivers/cpufreq/cpufreq_smartmax_eps.c
 create mode 100755 drivers/cpufreq/cpufreq_umbrella_core.c
 create mode 100644 drivers/cpufreq/cpufreq_yankactive.c
 create mode 100755 drivers/cpufreq/cpufreq_zzmoove.c
 create mode 100755 drivers/cpufreq/cpufreq_zzmoove_profiles.h
 create mode 100644 include/trace/events/cpufreq_barry_allen.h
 create mode 100644 include/trace/events/cpufreq_cafactive.h
 create mode 100644 include/trace/events/cpufreq_electroactive.h
 create mode 100755 include/trace/events/cpufreq_interactive_x.h
 create mode 100755 include/trace/events/cpufreq_ironactive.h
 create mode 100755 include/trace/events/cpufreq_ondemandplus.h
 create mode 100755 include/trace/events/cpufreq_umbrella_core.h
 create mode 100644 include/trace/events/cpufreq_yankactive.h

diff --git a/arch/arm/configs/0StockKernel_SM-G901F_defconfig b/arch/arm/configs/0StockKernel_SM-G901F_defconfig
index 96dfdd3..fbbd127 100644
--- a/arch/arm/configs/0StockKernel_SM-G901F_defconfig
+++ b/arch/arm/configs/0StockKernel_SM-G901F_defconfig
@@ -693,6 +693,35 @@ CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE=y
 # CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ZZMOOVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_BARRY_ALLEN is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_DARKNESS is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_YANKACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIMM is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_UMBRELLA_CORE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_BIOSHOCK is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_RACCOON_CITY is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_LIONHEART is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX_EPS is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ALUCARD is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ELEMENTALX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_BLU_ACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_CAFACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ELECTROACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ELECTRODEMAND is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND_X is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_DESPAIR is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_HELLSACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE_X is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE_X is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_ARTERACTIVE is not set
 CONFIG_CPU_FREQ_GOV_PERFORMANCE=y
 CONFIG_CPU_FREQ_GOV_POWERSAVE=y
@@ -700,6 +729,35 @@ CONFIG_CPU_FREQ_GOV_USERSPACE=y
 CONFIG_CPU_FREQ_GOV_ONDEMAND=y
 CONFIG_CPU_FREQ_GOV_INTERACTIVE=y
 CONFIG_CPU_FREQ_GOV_CONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_ZZMOOVE=y
+CONFIG_CPU_FREQ_GOV_BARRY_ALLEN=y
+CONFIG_CPU_FREQ_GOV_NIGHTMARE=y
+CONFIG_CPU_FREQ_GOV_DARKNESS=y
+CONFIG_CPU_FREQ_GOV_PEGASUSQ=y
+CONFIG_CPU_FREQ_GOV_YANKACTIVE=y
+CONFIG_CPU_FREQ_GOV_INTELLIACTIVE=y
+CONFIG_CPU_FREQ_GOV_ONDEMANDPLUS=y
+CONFIG_CPU_FREQ_GOV_INTELLIMM=y
+CONFIG_CPU_FREQ_GOV_UMBRELLA_CORE=y
+CONFIG_CPU_FREQ_GOV_BIOSHOCK=y
+# CONFIG_CPU_FREQ_GOV_RACCOON_CITY is not set
+CONFIG_CPU_FREQ_GOV_LIONHEART=y
+CONFIG_CPU_FREQ_GOV_SMARTMAX=y
+CONFIG_CPU_FREQ_GOV_SMARTASS2=y
+CONFIG_CPU_FREQ_GOV_SMARTMAX_EPS=y
+CONFIG_CPU_FREQ_GOV_ALUCARD=y
+CONFIG_CPU_FREQ_GOV_ELEMENTALX=y
+CONFIG_CPU_FREQ_GOV_BLU_ACTIVE=y
+CONFIG_CPU_FREQ_GOV_CAFACTIVE=y
+CONFIG_CPU_FREQ_GOV_ELECTROACTIVE=y
+CONFIG_CPU_FREQ_GOV_ELECTRODEMAND=y
+CONFIG_CPU_FREQ_GOV_ONDEMAND_X=y
+CONFIG_CPU_FREQ_GOV_HYPER=y
+CONFIG_CPU_FREQ_GOV_INTELLIDEMAND=y
+CONFIG_CPU_FREQ_GOV_DESPAIR=y
+# CONFIG_CPU_FREQ_GOV_HELLSACTIVE is not set
+CONFIG_CPU_FREQ_GOV_INTERACTIVE_X=y
+# CONFIG_CPU_FREQ_GOV_CONSERVATIVE_X is not set
 CONFIG_CPU_FREQ_GOV_ARTERACTIVE=y
 CONFIG_CPU_BOOST=y
 CONFIG_SUSPEND_BOOSTER=y
@@ -2060,6 +2118,7 @@ CONFIG_INPUT_TOUCHSCREEN=y
 # CONFIG_TOUCHSCREEN_SYNAPTICS_I2C_RMI4 is not set
 # CONFIG_TOUCHSCREEN_GT9XX is not set
 # CONFIG_SEC_STRING_ENABLE is not set
+CONFIG_TOUCHBOOST=y
 # CONFIG_INPUT_WACOM is not set
 # CONFIG_WACOM_LCD_FREQ_COMPENSATE is not set
 # CONFIG_TOUCHSCREEN_SYNAPTICS_I2C_RMI is not set
diff --git a/arch/arm/mach-msm/alucard_hotplug.c b/arch/arm/mach-msm/alucard_hotplug.c
index 55ad059..27f70ce 100644
--- a/arch/arm/mach-msm/alucard_hotplug.c
+++ b/arch/arm/mach-msm/alucard_hotplug.c
@@ -35,10 +35,10 @@
 static DEFINE_MUTEX(alucard_hotplug_mutex);
 
 struct hotplug_cpuinfo {
-	u64 prev_cpu_wall;
-	u64 prev_cpu_idle;
-	unsigned int prev_load;
-	ktime_t time_stamp;
+	unsigned int load;
+	unsigned int sampling_rate_us;
+	ktime_t now;
+	ktime_t pre;
 };
 
 static unsigned int last_online_cpus;
@@ -140,9 +140,7 @@ static void __ref hotplug_work_fn(struct work_struct *work)
 	unsigned int sum_load = 0, sum_freq = 0;
 	bool rq_avg_calc = true;
 	int online_cpus = 0, delay;
-	unsigned int sampling_rate =
-		hotplug_tuners_ins.hotplug_sampling_rate;
-	struct cpufreq_policy policy;
+	unsigned int sampling_rate = 0;
 
 	if (hotplug_tuners_ins.suspended) {
 		upmaxcoreslimit = hotplug_tuners_ins.maxcoreslimit_sleep;
@@ -152,79 +150,16 @@ static void __ref hotplug_work_fn(struct work_struct *work)
 	for_each_online_cpu(cpu) {
 		struct hotplug_cpuinfo *pcpu_info =
 			&per_cpu(ac_hp_cpuinfo, cpu);
-		u64 cur_wall_time, cur_idle_time;
-		unsigned int wall_time, idle_time;
 		unsigned int cur_load = 0;
 		unsigned int cur_freq = 0;
-		ktime_t time_now = ktime_get();
-		s64 delta_us;
 
-		delta_us = ktime_us_delta(time_now, pcpu_info->time_stamp);
-		/* Do nothing if cpu recently has become online */
-		if (delta_us < (s64)(sampling_rate / 2)) {
+		if (ktime_equal(pcpu_info->pre, pcpu_info->now)) {
 			continue;
 		}
-
-		cur_idle_time = get_cpu_idle_time(
-				cpu, &cur_wall_time,
-				0);
-
-		wall_time = (unsigned int)
-				(cur_wall_time -
-					pcpu_info->prev_cpu_wall);
-		pcpu_info->prev_cpu_wall = cur_wall_time;
-
-		idle_time = (unsigned int)
-				(cur_idle_time -
-					pcpu_info->prev_cpu_idle);
-		pcpu_info->prev_cpu_idle = cur_idle_time;
-
-		pcpu_info->time_stamp = time_now;
-
-		/* if wall_time < idle_time or wall_time == 0, evaluate cpu load next time */
-		if (unlikely(!wall_time || wall_time < idle_time))
-			continue;
-
-		/*
-		 * If the CPU had gone completely idle, and a task just woke up
-		 * on this CPU now, it would be unfair to calculate 'load' the
-		 * usual way for this elapsed time-window, because it will show
-		 * near-zero load, irrespective of how CPU intensive that task
-		 * actually is. This is undesirable for latency-sensitive bursty
-		 * workloads.
-		 *
-		 * To avoid this, we reuse the 'load' from the previous
-		 * time-window and give this task a chance to start with a
-		 * reasonably high CPU frequency. (However, we shouldn't over-do
-		 * this copy, lest we get stuck at a high load (high frequency)
-		 * for too long, even when the current system load has actually
-		 * dropped down. So we perform the copy only once, upon the
-		 * first wake-up from idle.)
-		 *
-		 * Detecting this situation is easy: the governor's deferrable
-		 * timer would not have fired during CPU-idle periods. Hence
-		 * an unusually large 'wall_time' (as compared to the sampling
-		 * rate) indicates this scenario.
-		 *
-		 * prev_load can be zero in two cases and we must recalculate it
-		 * for both cases:
-		 * - during long idle intervals
-		 * - explicitly set to zero
-		 */
-		if (unlikely(wall_time > (2 * sampling_rate) &&
-			     pcpu_info->prev_load)) {
-			cur_load = pcpu_info->prev_load;
-
-			/*
-			 * Perform a destructive copy, to ensure that we copy
-			 * the previous load only once, upon the first wake-up
-			 * from idle.
-			 */
-			pcpu_info->prev_load = 0;
-		} else {
-			cur_load = 100 * (wall_time - idle_time) / wall_time;
-			pcpu_info->prev_load = cur_load;
-		}
+		cur_load = pcpu_info->load;
+		if (pcpu_info->sampling_rate_us > sampling_rate)
+			sampling_rate = pcpu_info->sampling_rate_us;
+		pcpu_info->pre = pcpu_info->now;
 
 		/* get the cpu current frequency */
 		cur_freq = cpufreq_quick_get(cpu);
@@ -325,7 +260,10 @@ static void __ref hotplug_work_fn(struct work_struct *work)
 	}
 
 next_loop:
-	delay = msecs_to_jiffies(sampling_rate);
+	if (!sampling_rate)
+		delay = msecs_to_jiffies(hotplug_tuners_ins.hotplug_sampling_rate);
+	else
+		delay = usecs_to_jiffies(sampling_rate);
 	/*
 	 * We want hotplug governor to do sampling
 	 * just one jiffy later on cpu governor sampling
@@ -377,21 +315,15 @@ static int state_notifier_callback(struct notifier_block *this,
 static int alucard_hotplug_callback(struct notifier_block *nb,
 			unsigned long action, void *data)
 {
-	unsigned int cpu = (unsigned long)data;
 	struct hotplug_cpuinfo *pcpu_info = NULL;
-	unsigned int prev_load;
+	struct cpufreq_govinfo *govinfo = data;
 
 	switch (action) {
-	case CPU_ONLINE:
-		pcpu_info = &per_cpu(ac_hp_cpuinfo, cpu);
-		pcpu_info->prev_cpu_idle = get_cpu_idle_time(cpu,
-				&pcpu_info->prev_cpu_wall,
-				0);
-		pcpu_info->time_stamp = ktime_get();
-		prev_load = (unsigned int)
-				(pcpu_info->prev_cpu_wall - pcpu_info->prev_cpu_idle);
-		pcpu_info->prev_load = 100 * prev_load /
-				(unsigned int) pcpu_info->prev_cpu_wall;
+	case CPUFREQ_LOAD_CHANGE:
+		pcpu_info = &per_cpu(ac_hp_cpuinfo, govinfo->cpu);
+		pcpu_info->load = govinfo->load;
+		pcpu_info->sampling_rate_us = govinfo->sampling_rate_us;
+		pcpu_info->now = ktime_get();
 		break;
 	default:
 		break;
@@ -423,30 +355,23 @@ static int hotplug_start(void)
 		return -EINVAL;
 	}
 
-	get_online_cpus();
 	for_each_possible_cpu(cpu) {
-		struct hotplug_cpuinfo *pcpu_info = NULL;
+		struct hotplug_cpuinfo *pcpu_info =
+			&per_cpu(ac_hp_cpuinfo, cpu);
 		struct hotplug_cpuparm *pcpu_parm =
 			&per_cpu(ac_hp_cpuparm, cpu);
-		unsigned int prev_load;
-
-		if (cpu_online(cpu)) {
-			pcpu_info = &per_cpu(ac_hp_cpuinfo, cpu);
-			pcpu_info->prev_cpu_idle = get_cpu_idle_time(cpu,
-					&pcpu_info->prev_cpu_wall,
-					0);
-			pcpu_info->time_stamp = ktime_get();
-			prev_load = (unsigned int)
-				(pcpu_info->prev_cpu_wall - pcpu_info->prev_cpu_idle);
-			pcpu_info->prev_load = 100 * prev_load /
-					(unsigned int) pcpu_info->prev_cpu_wall;
-		}
+		ktime_t now = ktime_get();
+		pcpu_info = &per_cpu(ac_hp_cpuinfo, cpu);
+		pcpu_info->load = 0;
+		pcpu_info->sampling_rate_us = 0;
+		pcpu_info->now = now;
+		pcpu_info->pre = now;
 		pcpu_parm->cur_up_rate = 1;
 		pcpu_parm->cur_down_rate = 1;
 	}
 	last_online_cpus = num_online_cpus();
-	register_hotcpu_notifier(&alucard_hotplug_nb);
-	put_online_cpus();
+	cpufreq_register_notifier(&alucard_hotplug_nb,
+					CPUFREQ_GOVINFO_NOTIFIER);
 
 	delay = msecs_to_jiffies(hotplug_tuners_ins.hotplug_sampling_rate);
 	/*
@@ -478,9 +403,8 @@ static void hotplug_stop(void)
 	notif.notifier_call = NULL;
 #endif
 	cancel_delayed_work_sync(&alucard_hotplug_work);
-	get_online_cpus();
-	unregister_hotcpu_notifier(&alucard_hotplug_nb);
-	put_online_cpus();
+	cpufreq_unregister_notifier(&alucard_hotplug_nb,
+						CPUFREQ_GOVINFO_NOTIFIER);
 	destroy_workqueue(alucard_hp_wq);
 	mutex_unlock(&alucard_hotplug_mutex);
 }
diff --git a/drivers/cpufreq/Kconfig b/drivers/cpufreq/Kconfig
index 2558c45..d7ae364 100755
--- a/drivers/cpufreq/Kconfig
+++ b/drivers/cpufreq/Kconfig
@@ -114,6 +114,268 @@ config CPU_FREQ_DEFAULT_GOV_INTERACTIVE
 	  loading your cpufreq low-level hardware driver, using the
 	  'interactive' governor for latency-sensitive workloads.
 
+config CPU_FREQ_DEFAULT_GOV_ZZMOOVE
+	bool "zzmoove"
+	select CPU_FREQ_GOV_ZZMOOVE
+	select CPU_FREQ_GOV_PERFORMANCE
+	help
+	  Use the CPUFreq governor 'zzmoove' as default, using predefined
+	  frequency lookup tables and optimized scaling triggers instead of
+	  % frequency steps to get smooth up/downscaling dependant on CPU load.
+
+config CPU_FREQ_DEFAULT_GOV_IMPULSE
+	bool "impulse"
+	depends on SCHED_FREQ_INPUT
+	select CPU_FREQ_GOV_IMPULSE
+	help
+	  Use the CPUFreq governor 'impulse' as default. This allows
+	  you to get a full dynamic cpu frequency capable system by simply
+	  loading your cpufreq low-level hardware driver, using the
+	  'impulse' governor for latency-sensitive workloads.
+
+config CPU_FREQ_DEFAULT_GOV_BARRY_ALLEN
+	bool "barry_allen"
+	select CPU_FREQ_GOV_BARRY_ALLEN
+	help
+	  Use the CPUFreq governor 'barry_allen' as default. This allows
+	  you to get a full dynamic cpu frequency capable system by simply
+	  loading your cpufreq low-level hardware driver, using the
+	  'barry_allen' governor for latency-sensitive workloads.
+
+config CPU_FREQ_DEFAULT_GOV_NIGHTMARE
+	bool "nightmare"
+	select CPU_FREQ_GOV_NIGHTMARE
+	help
+	  Use the CPUFreq governor 'Nightmare' as default.
+
+config CPU_FREQ_DEFAULT_GOV_DARKNESS
+	bool "darkness"
+	select CPU_FREQ_GOV_DARKNESS
+	help
+	  Use the CPUFreq governor 'Darkness' as default.
+
+config CPU_FREQ_DEFAULT_GOV_PEGASUSQ
+	bool "pegasusq"
+	select CPU_FREQ_GOV_PEGASUSQ
+	help
+	  Use the CPUFreq governor 'pegasusq' as default.
+
+config CPU_FREQ_DEFAULT_GOV_YANKACTIVE
+	bool "yankactive"
+	select CPU_FREQ_GOV_YANKACTIVE
+	help
+	  Use the CPUFreq governor 'yankactive' as default. This allows
+	  you to get a full dynamic cpu frequency capable system by simply
+	  loading your cpufreq low-level hardware driver, using the
+	  'yankactive' governor for latency-sensitive workloads.
+
+config CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE
+	bool "intelliactive"
+	select CPU_FREQ_GOV_INTELLIACTIVE
+	help
+	  Use the CPUFreq governor 'intelliactive' as default. This allows
+	  you to get a full dynamic cpu frequency capable system by simply
+	  loading your cpufreq low-level hardware driver, using the
+	  'interactive' governor for latency-sensitive workloads.
+
+config CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS
+	bool "ondemandplus"
+	select CPU_FREQ_GOV_ONDEMANDPLUS
+	select CPU_FREQ_GOV_PERFORMANCE
+	help
+	  Use the CPUFreq governor 'ondemandplus' as default. This allows
+	  you to get a full dynamic frequency capable system by simply
+	  loading your cpufreq low-level hardware driver.
+	  Be aware that not all cpufreq drivers support the ondemandplus
+	  governor. If unsure have a look at the help section of the
+	  driver. Fallback governor will be the performance governor.
+
+config CPU_FREQ_DEFAULT_GOV_INTERACTIVE_PRO
+	bool "interactive_pro"
+	depends on SCHED_FREQ_INPUT
+	select CPU_FREQ_GOV_INTERACTIVE_PRO
+	help
+	  Use the CPUFreq governor 'interactive_pro' as default.
+
+config CPU_FREQ_DEFAULT_GOV_INTELLIMM
+       bool "intelliminmax"
+       select CPU_FREQ_GOV_INTELLIMM
+       help
+         Use the CPUFreq governor 'intelliminmax' as default. This is
+         governor will use only 3 different frequencies, idle, UI and max
+
+config CPU_FREQ_DEFAULT_GOV_UMBRELLA_CORE
+	bool "umbrella_core"
+	select CPU_FREQ_GOV_UMBRELLA_CORE
+	help
+	  Use the CPUFreq governor 'umbrella_core' as default. This allows
+	  you to get a full dynamic cpu frequency capable system by simply
+	  loading your cpufreq low-level hardware driver, using the
+	  'umbrella_core' governor for latency-sensitive workloads.
+
+config CPU_FREQ_DEFAULT_GOV_BIOSHOCK
+	bool "BioShock"
+	select CPU_FREQ_GOV_BIOSHOCK
+	help
+	  Default governor of BioShock kernel
+
+config CPU_FREQ_DEFAULT_GOV_RACCOON_CITY
+	bool "raccoon_city"
+	select CPU_FREQ_GOV_RACCOON_CITY
+	help
+	  Use the CPUFreq governor 'raccoon_city' as default. This sets
+	  the frequency statically to the lowest frequency supported by
+	  the CPU, while considering battery life to prevent unnecessary
+	  drain during suspend.
+
+config CPU_FREQ_DEFAULT_GOV_LIONHEART
+	bool "lionheart"
+	select CPU_FREQ_GOV_LIONHEART
+	help
+	  Use the CPUFreq governor 'lionheart' as default.
+
+config CPU_FREQ_DEFAULT_GOV_SMARTMAX
+	bool "smartmax"
+	select CPU_FREQ_GOV_SMARTMAX
+	help
+	  Use the CPUFreq governor 'smartmax' as default.
+
+config CPU_FREQ_DEFAULT_GOV_SMARTASS2
+	bool "smartassV2"
+	select CPU_FREQ_GOV_SMARTASS2
+	help
+	  Use the CPUFreq governor 'smartass2' as default.
+
+config CPU_FREQ_DEFAULT_GOV_IRONACTIVE
+	bool "ironactive"
+	depends on SCHED_FREQ_INPUT
+	select CPU_FREQ_GOV_IRONACTIVE
+	help
+	  Use the CPUFreq governor 'ironactive' as default. This cpugovernor
+	  is based on the 'interactive' governor with many changes made by
+	  CAF.
+
+config CPU_FREQ_DEFAULT_GOV_SMARTMAX_EPS
+	bool "smartmax_eps"
+	select CPU_FREQ_GOV_SMARTMAX_EPS
+	help
+	  Use the CPUFreq governor 'smartmax eps' as default
+
+config CPU_FREQ_DEFAULT_GOV_ALUCARD
+	bool "alucard"
+	select CPU_FREQ_GOV_ALUCARD
+	help
+
+config CPU_FREQ_DEFAULT_GOV_ELEMENTALX
+	bool "elementalx"
+	select CPU_FREQ_GOV_ELEMENTALX
+	select CPU_FREQ_GOV_PERFORMANCE
+	help
+	  Use the CPUFreq governor 'elementalx' as default.
+
+config CPU_FREQ_DEFAULT_GOV_BLU_ACTIVE
+	bool "blu_active"
+	select CPU_FREQ_GOV_BLU_ACTIVE
+	help
+	  Use the CPUFreq governor 'blu_active' as default. This allows
+	  you to get a full dynamic cpu frequency capable system by simply
+	  loading your cpufreq low-level hardware driver, using the
+	  'interactive' governor for latency-sensitive workloads.
+
+config CPU_FREQ_DEFAULT_GOV_CAFACTIVE
+	bool "cafactive"
+	select CPU_FREQ_GOV_CAFACTIVE
+	help
+	  Use the CPUFreq governor 'cafactive' as default. This allows
+	  you to get a full dynamic cpu frequency capable system by simply
+	  loading your cpufreq low-level hardware driver, using the
+	  'cafactive' governor for latency-sensitive workloads.
+
+config CPU_FREQ_DEFAULT_GOV_ELECTROACTIVE
+	bool "electroactive"
+	select CPU_FREQ_GOV_ELECTROACTIVE
+        select CPU_FREQ_GOV_PERFORMANCE
+	help
+	  Use the CPUFreq governor 'electroactive' as default. This allows
+	  you to get a full dynamic cpu frequency capable system by simply
+	  loading your cpufreq low-level hardware driver, using the
+	  'electroactive' governor for latency-sensitive workloads.
+
+
+config CPU_FREQ_DEFAULT_GOV_ELECTRODEMAND
+	bool "electrodemand"
+	select CPU_FREQ_GOV_ELECTRODEMAND
+        select CPU_FREQ_GOV_PERFORMANCE
+	help
+	  Use the CPUFreq governor 'electrodemand' as default. This allows
+	  you to get a full dynamic cpu frequency capable system by simply
+	  loading your cpufreq low-level hardware driver, using the
+	  'electrodemand' governor for latency-sensitive workloads.
+
+config CPU_FREQ_DEFAULT_GOV_ONDEMAND_X
+	bool "ondemand_x"
+	select CPU_FREQ_GOV_ONDEMAND_X
+	select CPU_FREQ_GOV_PERFORMANCE
+	help
+	  Use the CPUFreq governor 'ondemand_x' as default. This allows
+	  you to get a full dynamic frequency capable system by simply
+	  loading your cpufreq low-level hardware driver.
+	  Be aware that not all cpufreq drivers support the ondemand_x
+	  governor. If unsure have a look at the help section of the
+	  driver. Fallback governor will be the performance governor.
+
+config CPU_FREQ_DEFAULT_GOV_HYPER
+	bool "HYPER"
+	select CPU_FREQ_GOV_HYPER
+	---help---
+	  Use the CPUFreq governor 'HYPER' as default.
+
+config CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND
+	bool "intellidemand"
+	select CPU_FREQ_GOV_INTELLIDEMAND
+	help
+	  Use the CPUFreq governor 'intellidemand' as default. This is
+	  based on Ondemand with browsing detection based on GPU loading
+
+config CPU_FREQ_DEFAULT_GOV_DESPAIR
+	bool "despair"
+	depends on TOUCHBOOST
+	select CPU_FREQ_GOV_DESPAIR
+	help
+
+config CPU_FREQ_DEFAULT_GOV_HELLSACTIVE
+	bool "hellsactive"
+	depends on TOUCHBOOST
+	select CPU_FREQ_GOV_HELLSACTIVE
+	help
+	  Use the CPUFreq governor 'hellsactive' as default. This allows
+	  you to get a full dynamic cpu frequency capable system by simply
+	  loading your cpufreq low-level hardware driver, using the
+	  'interactive' governor for latency-sensitive workloads.
+
+config CPU_FREQ_DEFAULT_GOV_INTERACTIVE_X
+	bool "interactive_x"
+	depends on TOUCHBOOST
+	select CPU_FREQ_GOV_INTERACTIVE_X
+	help
+	  Use the CPUFreq governor 'interactive_x' as default. This allows
+	  you to get a full dynamic cpu frequency capable system by simply
+	  loading your cpufreq low-level hardware driver, using the
+	  'interactive' governor for latency-sensitive workloads.
+
+config CPU_FREQ_DEFAULT_GOV_CONSERVATIVE_X
+	bool "conservative_x"
+	depends on TOUCHBOOST
+	select CPU_FREQ_GOV_CONSERVATIVE_X
+	select CPU_FREQ_GOV_PERFORMANCE
+	help
+	  Use the CPUFreq governor 'conservative_x' as default. This allows
+	  you to get a full dynamic frequency capable system by simply
+	  loading your cpufreq low-level hardware driver.
+	  Be aware that not all cpufreq drivers support the conservative
+	  governor. If unsure have a look at the help section of the
+	  driver. Fallback governor will be the performance governor.
+
 config CPU_FREQ_DEFAULT_GOV_ARTERACTIVE
 	bool "arteractive"
 	select CPU_FREQ_GOV_ARTERACTIVE
@@ -221,6 +483,344 @@ config CPU_FREQ_GOV_CONSERVATIVE
 
 	  If in doubt, say N.
 
+config CPU_FREQ_GOV_ZZMOOVE
+	tristate "'zzmoove' cpufreq governor"
+	depends on CPU_FREQ
+	help
+	  'zzmoove' - based on cpufreq_conservative, using predefined
+	  frequency lookup tables and optimized scaling triggers instead of
+	  % frequency steps to get smooth up/downscaling dependant on CPU load.
+	  If in doubt, say N.
+
+config CPU_FREQ_GOV_IMPULSE
+	tristate "'interactive' cpufreq policy governor"
+	depends on SCHED_FREQ_INPUT
+	help
+	  'impulse' - This driver adds a dynamic cpufreq policy governor
+	  designed for latency-sensitive workloads.
+
+	  If in doubt, say N.
+
+config CPU_FREQ_GOV_BARRY_ALLEN
+	tristate "'barry_allen' cpufreq policy governor"
+	help
+	  'barry_allen' - This driver adds a dynamic cpufreq policy governor
+	  designed for latency-sensitive workloads.
+
+	  This governor attempts to reduce the latency of clock
+	  increases so that the system is more responsive to
+	  barry_allen workloads.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_barry_allen.
+
+	  For details, take a look at linux/Documentation/cpu-freq.
+
+	  If in doubt, say N.
+
+config CPU_FREQ_GOV_NIGHTMARE
+	tristate "'nightmare' cpufreq governor"
+	depends on CPU_FREQ
+
+config CPU_FREQ_GOV_DARKNESS
+	tristate "'darkness' cpufreq governor"
+	depends on CPU_FREQ
+
+config CPU_FREQ_GOV_PEGASUSQ
+	tristate "'pegasusq' cpufreq policy governor"
+
+config CPU_FREQ_GOV_YANKACTIVE
+	tristate "'yankactive' cpufreq policy governor"
+	help
+	  'yankactive' - This driver adds a dynamic cpufreq policy governor
+	  designed for latency-sensitive workloads.
+
+	  The governor is based on interactive with just different default settings
+	  and can be achieved using interactive with a script. This is added for
+	  simplification purposes for myself.
+
+config CPU_FREQ_GOV_INTELLIACTIVE
+	tristate "'intelliactive' cpufreq policy governor"
+	help
+	  'intelliactive' - This driver adds a dynamic cpufreq policy governor
+	  designed for latency-sensitive workloads.
+
+	  This governor attempts to reduce the latency of clock
+	  increases so that the system is more responsive to
+	  interactive workloads.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_interactive.
+
+	  For details, take a look at linux/Documentation/cpu-freq.
+
+	  If in doubt, say N.
+
+config CPU_FREQ_GOV_ONDEMANDPLUS
+	tristate "'ondemandplus' cpufreq policy governor"
+	select CPU_FREQ_TABLE
+	help
+	  'ondemandplus' - This driver adds a dynamic cpufreq policy
+	  governor. The governor does a periodic polling and
+	  changes frequency based on the CPU utilization.
+	  The support for this governor depends on CPU capability to
+	  do fast frequency switching (i.e, very low latency frequency
+	  transitions).
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_ondemandplus.
+	  For details, take a look at linux/Documentation/cpu-freq.
+	  If in doubt, say N.
+
+config CPU_FREQ_GOV_INTERACTIVE_PRO
+	tristate "'interactive_pro' cpufreq governor"
+	depends on SCHED_FREQ_INPUT
+	depends on CPU_FREQ
+
+config CPU_FREQ_GOV_INTELLIMM
+       tristate "'intelliminmax' cpufreq policy governor"
+       help
+         To compile this driver as a module, choose M here: the
+         module will be called cpufreq_interactive.
+
+         For details, take a look at linux/Documentation/cpu-freq.
+
+         If in doubt, say N.
+
+config CPU_FREQ_GOV_UMBRELLA_CORE
+	tristate "'umbrella_core' cpufreq policy governor"
+	help
+	  'umbrella_core' - This driver adds a dynamic cpufreq policy governor
+	  designed for latency-sensitive workloads.
+
+	  The governor is based on interactive to provide a resident controller
+	  which interprets unnecessary battery drain as evil.
+
+config CPU_FREQ_GOV_BIOSHOCK
+	tristate "'bioshock' cpufreq bioshock"
+	depends on CPU_FREQ
+	help
+	  'bioshock' - More aggressive version of conservative
+
+config CPU_FREQ_GOV_RACCOON_CITY
+	tristate "'raccoon_city' cpufreq policy governor"
+	help
+	  'raccoon_city' - This driver adds a dynamic cpufreq policy governor
+	  designed for latency-sensitive workloads.
+
+	  The governor is based on interactive to provide a resident controller
+	  which interprets unnecessary battery drain as evil.
+
+config CPU_FREQ_GOV_LIONHEART
+	tristate "'lionheart' cpufreq governor"
+	depends on CPU_FREQ
+	help
+	  lionheart' - a "smart" optimized governor!
+	  If in doubt, say N.
+
+config CPU_FREQ_GOV_SMARTMAX
+	tristate "'smartmax' cpufreq governor"
+	depends on CPU_FREQ
+	help
+	  'smartmax' - a "smart" optimized governor!
+	  If in doubt, say N.
+
+config CPU_FREQ_GOV_SMARTASS2
+	tristate "'smartassV2' cpufreq governor"
+	depends on CPU_FREQ
+	help
+	  'smartassV2' - a "smart" governor
+
+config CPU_FREQ_GOV_IRONACTIVE
+	tristate "'ironactive' cpufreq policy governor"
+	depends on SCHED_FREQ_INPUT
+	help
+	  'ironactive' - This driver adds a dynamic cpufreq policy governor
+
+	  This governor attempts to reduce the latency of clock
+	  increases so that the system is more responsive to
+	  ironactive workloads. Based on 'interactive' governor
+	  with many changes compared to our stock 'interactive'
+	  governor we have from samsung. Mainly CAF changes.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_ironactive.
+
+	  If in doubt, say N.
+
+config CPU_FREQ_GOV_SMARTMAX_EPS
+	tristate "'smartmax EPS' cpufreq policy governor"
+	select CPU_FREQ_TABLE
+	help
+	  'smartmax EPS' is the extreme powersaving version of smartmax
+
+config CPU_FREQ_GOV_ALUCARD
+	tristate "'alucard' cpufreq governor"
+	depends on CPU_FREQ
+
+config CPU_FREQ_GOV_ELEMENTALX
+	tristate "'elementalx' cpufreq policy governor"
+	select CPU_FREQ_TABLE
+	help
+	  'elementalx' - This driver adds a dynamic cpufreq policy governor.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_elementalx.
+
+	  For details, take a look at linux/Documentation/cpu-freq.
+
+	  If in doubt, say N.
+
+config CPU_FREQ_GOV_BLU_ACTIVE
+	tristate "'blu_active' cpufreq policy governor"
+	help
+	  'blu_active' - This driver adds a dynamic cpufreq policy governor
+	  interactive workloads.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_blu_active.
+	  designed for latency-sensitive workloads.
+
+config CPU_FREQ_GOV_CAFACTIVE
+	tristate "'cafactive' cpufreq policy governor"
+	help
+	  'cafactive' - CodeAurora(Qualcomm) version of interactive
+	  cpufreq policy governor.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_cafactive.
+
+	  If in doubt, say N.
+
+config CPU_FREQ_GOV_ELECTROACTIVE
+	tristate "'electroactive' cpufreq policy governor"
+        select CPU_FREQ_TABLE
+	help
+	  'electroactive' - A tweaked "hybrid" based smart and smooth optimized
+           governor designed for optimization in transition and a balance between
+           performance and power.
+	
+	   If in doubt, say N.
+
+config CPU_FREQ_GOV_ELECTRODEMAND
+	tristate "'electrodemand' cpufreq policy governor"
+        depends on CPU_FREQ
+	help
+	  'electrodemand' - A tweaked "ondemand" based smart and smooth optimized
+           governor designed for optimization in transition and a balance between
+           performance and power.
+	
+	   If in doubt, say N.
+
+config CPU_FREQ_GOV_ONDEMAND_X
+	tristate "'ondemand_x' cpufreq policy governor"
+	select CPU_FREQ_TABLE
+	help
+	  'ondemand_x' - This driver adds a dynamic cpufreq policy governor.
+	  The governor does a periodic polling and 
+	  changes frequency based on the CPU utilization.
+	  The support for this governor depends on CPU capability to
+	  do fast frequency switching (i.e, very low latency frequency
+	  transitions). 
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_ondemand_x.
+
+	  For details, take a look at linux/Documentation/cpu-freq.
+
+	  If in doubt, say N.
+
+config CPU_FREQ_GOV_HYPER
+	tristate "'HYPER' cpufreq governor"
+	depends on CPU_FREQ
+	---help---
+	  'HYPER' - a tweaked "ondemand" based smart and smooth optimized governor!
+
+	  If in doubt, say Y
+
+config CPU_FREQ_GOV_INTELLIDEMAND
+        tristate "'intellidemand' cpufreq policy governor"
+        select CPU_FREQ_TABLE
+        help
+          'intellidemand' - This driver adds a dynamic cpufreq policy governor.
+          The governor does a periodic polling and
+          changes frequency based on the CPU utilization.
+          The support for this governor depends on CPU capability to
+          do fast frequency switching (i.e, very low latency frequency
+          transitions). with browsing detection based on GPU loading
+
+          To compile this driver as a module, choose M here: the
+          module will be called cpufreq_ondemand.
+
+          For details, take a look at linux/Documentation/cpu-freq.
+
+          If in doubt, say N.
+
+config CPU_FREQ_GOV_DESPAIR
+	tristate "'despair' cpufreq governor"
+	depends on CPU_FREQ
+	depends on TOUCHBOOST
+
+config CPU_FREQ_GOV_HELLSACTIVE
+	tristate "'hellsactive' cpufreq policy governor"
+	depends on TOUCHBOOST
+	help
+	  'hellsactive' - This driver adds a dynamic cpufreq policy governor
+	  designed for latency-sensitive workloads.
+
+	  This governor attempts to reduce the latency of clock
+	  increases so that the system is more responsive to
+	  interactive workloads.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_interactive.
+
+	  For details, take a look at linux/Documentation/cpu-freq.
+
+	  If in doubt, say N.
+
+config CPU_FREQ_GOV_INTERACTIVE_X
+	tristate "'interactive_x' cpufreq policy governor"
+	depends on TOUCHBOOST
+	help
+	  'interactive_x' - This driver adds a dynamic cpufreq policy governor
+	  designed for latency-sensitive workloads.
+
+	  This governor attempts to reduce the latency of clock
+	  increases so that the system is more responsive to
+	  interactive workloads.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_interactive.
+
+	  For details, take a look at linux/Documentation/cpu-freq.
+
+	  If in doubt, say N.
+
+config CPU_FREQ_GOV_CONSERVATIVE_X
+	tristate "'conservative_x' cpufreq governor"
+	depends on CPU_FREQ
+	depends on TOUCHBOOST
+	select CPU_FREQ_GOV_COMMON
+	help
+	  'conservative_x' - this driver is rather similar to the 'ondemand'
+	  governor both in its source code and its purpose, the difference is
+	  its optimisation for better suitability in a battery powered
+	  environment.  The frequency is gracefully increased and decreased
+	  rather than jumping to 100% when speed is required.
+
+	  If you have a desktop machine then you should really be considering
+	  the 'ondemand' governor instead, however if you are using a laptop,
+	  PDA or even an AMD64 based computer (due to the unacceptable
+	  step-by-step latency issues between the minimum and maximum frequency
+	  transitions in the CPU) you will probably want to use this governor.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_conservative.
+
+	  For details, take a look at linux/Documentation/cpu-freq.
+
+	  If in doubt, say N.
+
 config CPU_FREQ_GOV_ARTERACTIVE
 	tristate "'arteractive' cpufreq policy governor"
 	help
diff --git a/drivers/cpufreq/Makefile b/drivers/cpufreq/Makefile
index 49e3efa..8ff725a 100755
--- a/drivers/cpufreq/Makefile
+++ b/drivers/cpufreq/Makefile
@@ -1,22 +1,54 @@
 # CPUfreq core
-obj-$(CONFIG_CPU_FREQ)			+= cpufreq.o freq_table.o
+obj-$(CONFIG_CPU_FREQ)				+= cpufreq.o freq_table.o
 # CPUfreq stats
-obj-$(CONFIG_CPU_FREQ_STAT)             += cpufreq_stats.o
+obj-$(CONFIG_CPU_FREQ_STAT)             	+= cpufreq_stats.o
 # CPUfreq limit
-obj-$(CONFIG_CPU_FREQ_LIMIT)		+= cpufreq_limit.o
+obj-$(CONFIG_CPU_FREQ_LIMIT)			+= cpufreq_limit.o
 
 # CPUfreq governors 
-obj-$(CONFIG_CPU_FREQ_GOV_PERFORMANCE)	+= cpufreq_performance.o
-obj-$(CONFIG_CPU_FREQ_GOV_POWERSAVE)	+= cpufreq_powersave.o
-obj-$(CONFIG_CPU_FREQ_GOV_USERSPACE)	+= cpufreq_userspace.o
-obj-$(CONFIG_CPU_FREQ_GOV_ONDEMAND)	+= cpufreq_ondemand.o
-obj-$(CONFIG_CPU_FREQ_GOV_CONSERVATIVE)	+= cpufreq_conservative.o
-obj-$(CONFIG_CPU_FREQ_GOV_INTERACTIVE)	+= cpufreq_interactive.o
+obj-$(CONFIG_CPU_FREQ_GOV_ALUCARD)		+= cpufreq_alucard.o
 obj-$(CONFIG_CPU_FREQ_GOV_ARTERACTIVE)	+= cpufreq_arteractive.o
-obj-$(CONFIG_CPU_FREQ_GOV_COMMON)	+= cpufreq_governor.o
-obj-$(CONFIG_CPU_BOOST)			+= cpu-boost.o
+obj-$(CONFIG_CPU_FREQ_GOV_BARRY_ALLEN)		+= cpufreq_barry_allen.o
+obj-$(CONFIG_CPU_FREQ_GOV_BIOSHOCK)		+= cpufreq_bioshock.o
+obj-$(CONFIG_CPU_FREQ_GOV_BLU_ACTIVE)		+= cpufreq_blu_active.o
+obj-$(CONFIG_CPU_FREQ_GOV_CAFACTIVE)		+= cpufreq_cafactive.o
+obj-$(CONFIG_CPU_FREQ_GOV_CONSERVATIVE)		+= cpufreq_conservative.o
+obj-$(CONFIG_CPU_FREQ_GOV_CONSERVATIVE_X)	+= cpufreq_conservative_x.o
+obj-$(CONFIG_CPU_FREQ_GOV_DARKNESS)		+= cpufreq_darkness.o
+obj-$(CONFIG_CPU_FREQ_GOV_DESPAIR)		+= cpufreq_despair.o
+obj-$(CONFIG_CPU_FREQ_GOV_ELECTROACTIVE)	+= cpufreq_electroactive.o
+obj-$(CONFIG_CPU_FREQ_GOV_ELECTRODEMAND)	+= cpufreq_electrodemand.o
+obj-$(CONFIG_CPU_FREQ_GOV_ELEMENTALX)		+= cpufreq_elementalx.o
+obj-$(CONFIG_CPU_FREQ_GOV_HELLSACTIVE)		+= cpufreq_hellsactive.o
+obj-$(CONFIG_CPU_FREQ_GOV_HYPER)		+= cpufreq_HYPER.o
+obj-$(CONFIG_CPU_FREQ_GOV_IMPULSE)		+= cpufreq_impulse.o
+obj-$(CONFIG_CPU_FREQ_GOV_INTELLIDEMAND)	+= cpufreq_intellidemand.o
+obj-$(CONFIG_CPU_FREQ_GOV_INTELLIACTIVE)	+= cpufreq_intelliactive.o
+obj-$(CONFIG_CPU_FREQ_GOV_INTERACTIVE)		+= cpufreq_interactive.o
+obj-$(CONFIG_CPU_FREQ_GOV_INTERACTIVE_PRO)	+= cpufreq_interactive_pro.o
+obj-$(CONFIG_CPU_FREQ_GOV_INTERACTIVE_X)	+= cpufreq_interactive_x.o
+obj-$(CONFIG_CPU_FREQ_GOV_INTELLIMM)	        += cpufreq_intellimm.o
+obj-$(CONFIG_CPU_FREQ_GOV_IRONACTIVE)		+= cpufreq_ironactive.o
+obj-$(CONFIG_CPU_FREQ_GOV_LIONHEART)		+= cpufreq_lionheart.o
+obj-$(CONFIG_CPU_FREQ_GOV_NIGHTMARE)		+= cpufreq_nightmare.o
+obj-$(CONFIG_CPU_FREQ_GOV_ONDEMAND)		+= cpufreq_ondemand.o
+obj-$(CONFIG_CPU_FREQ_GOV_ONDEMANDPLUS)		+= cpufreq_ondemandplus.o
+obj-$(CONFIG_CPU_FREQ_GOV_ONDEMAND_X)		+= cpufreq_ondemand_x.o
+obj-$(CONFIG_CPU_FREQ_GOV_PEGASUSQ)		+= cpufreq_pegasusq.o
+obj-$(CONFIG_CPU_FREQ_GOV_PERFORMANCE)		+= cpufreq_performance.o
+obj-$(CONFIG_CPU_FREQ_GOV_POWERSAVE)		+= cpufreq_powersave.o
+obj-$(CONFIG_CPU_FREQ_GOV_RACCOON_CITY)		+= cpufreq_raccoon_city.o
+obj-$(CONFIG_CPU_FREQ_GOV_SMARTASS2)		+= cpufreq_smartass2.o
+obj-$(CONFIG_CPU_FREQ_GOV_SMARTMAX)		+= cpufreq_smartmax.o
+obj-$(CONFIG_CPU_FREQ_GOV_SMARTMAX_EPS)		+= cpufreq_smartmax_eps.o
+obj-$(CONFIG_CPU_FREQ_GOV_UMBRELLA_CORE)	+= cpufreq_umbrella_core.o
+obj-$(CONFIG_CPU_FREQ_GOV_USERSPACE)		+= cpufreq_userspace.o
+obj-$(CONFIG_CPU_FREQ_GOV_YANKACTIVE)		+= cpufreq_yankactive.o
+obj-$(CONFIG_CPU_FREQ_GOV_ZZMOOVE)		+= cpufreq_zzmoove.o
+obj-$(CONFIG_CPU_FREQ_GOV_COMMON)		+= cpufreq_governor.o
+obj-$(CONFIG_CPU_BOOST)				+= cpu-boost.o
 
-obj-$(CONFIG_GENERIC_CPUFREQ_CPU0)	+= cpufreq-cpu0.o
+obj-$(CONFIG_GENERIC_CPUFREQ_CPU0)		+= cpufreq-cpu0.o
 
 ##################################################################################
 # x86 drivers.
diff --git a/drivers/cpufreq/cpufreq.c b/drivers/cpufreq/cpufreq.c
index 2740bbe..38c0c87 100755
--- a/drivers/cpufreq/cpufreq.c
+++ b/drivers/cpufreq/cpufreq.c
@@ -44,7 +44,7 @@ static struct cpufreq_driver *cpufreq_driver;
 static DEFINE_PER_CPU(struct cpufreq_policy *, cpufreq_cpu_data);
 static DEFINE_PER_CPU(struct cpufreq_policy *, cpufreq_cpu_data_fallback);
 static DEFINE_RWLOCK(cpufreq_driver_lock);
-static DEFINE_MUTEX(cpufreq_governor_lock);
+DEFINE_MUTEX(cpufreq_governor_lock);
 static LIST_HEAD(cpufreq_policy_list);
 
 #ifdef CONFIG_HOTPLUG_CPU
@@ -85,6 +85,7 @@ static void handle_update(struct work_struct *work);
  */
 static BLOCKING_NOTIFIER_HEAD(cpufreq_policy_notifier_list);
 static struct srcu_notifier_head cpufreq_transition_notifier_list;
+struct atomic_notifier_head cpufreq_govinfo_notifier_list;
 
 static bool init_cpufreq_transition_notifier_list_called;
 static int __init init_cpufreq_transition_notifier_list(void)
@@ -95,6 +96,15 @@ static int __init init_cpufreq_transition_notifier_list(void)
 }
 pure_initcall(init_cpufreq_transition_notifier_list);
 
+static bool init_cpufreq_govinfo_notifier_list_called;
+static int __init init_cpufreq_govinfo_notifier_list(void)
+{
+	ATOMIC_INIT_NOTIFIER_HEAD(&cpufreq_govinfo_notifier_list);
+	init_cpufreq_govinfo_notifier_list_called = true;
+	return 0;
+}
+pure_initcall(init_cpufreq_govinfo_notifier_list);
+
 static int off __read_mostly;
 static int cpufreq_disabled(void)
 {
@@ -1694,7 +1704,8 @@ int cpufreq_register_notifier(struct notifier_block *nb, unsigned int list)
 	if (cpufreq_disabled())
 		return -EINVAL;
 
-	WARN_ON(!init_cpufreq_transition_notifier_list_called);
+	WARN_ON(!init_cpufreq_transition_notifier_list_called ||
+		!init_cpufreq_govinfo_notifier_list_called);
 
 	switch (list) {
 	case CPUFREQ_TRANSITION_NOTIFIER:
@@ -1705,6 +1716,10 @@ int cpufreq_register_notifier(struct notifier_block *nb, unsigned int list)
 		ret = blocking_notifier_chain_register(
 				&cpufreq_policy_notifier_list, nb);
 		break;
+	case CPUFREQ_GOVINFO_NOTIFIER:
+		ret = atomic_notifier_chain_register(
+				&cpufreq_govinfo_notifier_list, nb);
+		break;
 	default:
 		ret = -EINVAL;
 	}
@@ -1739,6 +1754,10 @@ int cpufreq_unregister_notifier(struct notifier_block *nb, unsigned int list)
 		ret = blocking_notifier_chain_unregister(
 				&cpufreq_policy_notifier_list, nb);
 		break;
+	case CPUFREQ_GOVINFO_NOTIFIER:
+		ret = atomic_notifier_chain_unregister(
+				&cpufreq_govinfo_notifier_list, nb);
+		break;
 	default:
 		ret = -EINVAL;
 	}
@@ -1860,6 +1879,24 @@ int cpufreq_driver_target(struct cpufreq_policy *policy,
 }
 EXPORT_SYMBOL_GPL(cpufreq_driver_target);
 
+int __cpufreq_driver_getavg(struct cpufreq_policy *policy, unsigned int cpu)
+{
+	int ret = 0;
+
+	if (!(cpu_online(cpu) && cpufreq_driver->getavg))
+		return 0;
+
+	policy = cpufreq_cpu_get(policy->cpu);
+	if (!policy)
+		return -EINVAL;
+
+	ret = cpufreq_driver->getavg(policy, cpu);
+
+	cpufreq_cpu_put(policy);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(__cpufreq_driver_getavg);
+
 /*
  * when "event" is CPUFREQ_GOV_LIMITS
  */
diff --git a/drivers/cpufreq/cpufreq_HYPER.c b/drivers/cpufreq/cpufreq_HYPER.c
new file mode 100755
index 0000000..608c490
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_HYPER.c
@@ -0,0 +1,854 @@
+/*
+ *  drivers/cpufreq/cpufreq_HYPER.c
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *
+ *                 2012 Minor Edits by Sar Castillo <sar.castillo@gmail.com>
+ *                 2012 MAR heavy addons by DORIMANX <yuri@bynet.co.il>
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/input.h>
+#include <linux/slab.h>
+
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+#define DEF_FREQUENCY_DOWN_DIFFERENTIAL		(10)
+#define MIN_FREQUENCY_DOWN_DIFFERENTIAL		(1)
+#define DEF_FREQUENCY_UP_THRESHOLD		(70)
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define BOOSTED_SAMPLING_DOWN_FACTOR		(10)
+#define MAX_SAMPLING_DOWN_FACTOR		(3)
+#define MICRO_FREQUENCY_DOWN_DIFFERENTIAL	(5)
+#define MICRO_FREQUENCY_UP_THRESHOLD		(70)
+#define MICRO_FREQUENCY_MIN_SAMPLE_RATE		(5000)
+#define MIN_FREQUENCY_UP_THRESHOLD		(11)
+#define MAX_FREQUENCY_UP_THRESHOLD		(100)
+#define MIN_FREQUENCY_DOWN_DIFFERENTIAL		(1)
+#define FREQ_STEP				(50)
+#define DEFAULT_FREQ_BOOST_TIME			(500000)
+#define MAX_FREQ_BOOST_TIME			(5000000)
+#define UP_THRESHOLD_AT_MIN_FREQ		(40)
+
+static u64 hyper_freq_boosted_time;
+
+/*
+ * The polling frequency of this governor depends on the capability of
+ * the processor. Default polling frequency is 1000 times the transition
+ * latency of the processor. The governor will work on any processor with
+ * transition latency <= 10mS, using appropriate sampling
+ * rate.
+ * For CPUs with transition latency > 10mS (mostly drivers with CPUFREQ_ETERNAL)
+ * this governor will not work.
+ * All times here are in uS.
+ */
+#define MIN_SAMPLING_RATE_RATIO			(2)
+
+static unsigned int min_sampling_rate;
+#define LATENCY_MULTIPLIER			(1000)
+#define MIN_LATENCY_MULTIPLIER			(100)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+/* have the timer rate booted for this much time 4s*/
+#define TIMER_RATE_BOOST_TIME 4000000
+static int hyper_sampling_rate_boosted;
+static u64 hyper_sampling_rate_boosted_time;
+unsigned int hyper_current_sampling_rate;
+
+static void do_dbs_timer(struct work_struct *work);
+
+/* Sampling types */
+enum {DBS_NORMAL_SAMPLE, DBS_SUB_SAMPLE};
+
+struct cpu_dbs_info_s {
+	u64 prev_cpu_idle;
+	u64 prev_cpu_iowait;
+	u64 prev_cpu_wall;
+	unsigned int prev_cpu_wall_delta;
+	u64 prev_cpu_nice;
+	struct cpufreq_policy *cur_policy;
+	struct delayed_work work;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int freq_lo;
+	unsigned int freq_lo_jiffies;
+	unsigned int freq_hi_jiffies;
+	unsigned int rate_mult;
+	unsigned int load_at_prev_sample;
+	unsigned int cpu;
+	unsigned int sample_type:1;
+	unsigned int prev_load;
+	/*
+	 * percpu mutex that serializes governor limit change with
+	 * do_dbs_timer invocation. We do not want do_dbs_timer to run
+	 * when user is changing the governor or limits.
+	 */
+	struct mutex timer_mutex;
+};
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, od_cpu_dbs_info);
+
+static unsigned int dbs_enable;	/* number of CPUs using this policy */
+
+/*
+ * dbs_mutex protects dbs_enable in governor start/stop.
+ */
+static DEFINE_MUTEX(dbs_mutex);
+
+static struct workqueue_struct *dbs_wq;
+
+static struct dbs_tuners {
+	unsigned int sampling_rate;
+	unsigned int up_threshold;
+	unsigned int up_threshold_min_freq;
+	unsigned int down_differential;
+	unsigned int micro_freq_up_threshold;
+	unsigned int sampling_down_factor;
+	unsigned int boosted;
+	unsigned int freq_boost_time;
+	unsigned int boostfreq;
+	/*struct notifier_block dvfs_lat_qos_db;
+	unsigned int dvfs_lat_qos_wants;*/
+	unsigned int freq_step;
+} dbs_tuners_ins = {
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.up_threshold_min_freq = UP_THRESHOLD_AT_MIN_FREQ,
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.down_differential = DEF_FREQUENCY_DOWN_DIFFERENTIAL,
+	.micro_freq_up_threshold = MICRO_FREQUENCY_UP_THRESHOLD,
+	.freq_boost_time = DEFAULT_FREQ_BOOST_TIME,
+	.boostfreq = 1728000,
+	.freq_step = FREQ_STEP,
+	.sampling_rate = 20000,
+};
+
+static unsigned int dbs_enable = 0;	/* number of CPUs using this policy */
+
+static inline u64 get_cpu_iowait_time(unsigned int cpu, u64 *wall)
+{
+	u64 iowait_time = get_cpu_iowait_time_us(cpu, wall);
+
+	if (iowait_time == -1ULL)
+		return 0;
+
+	return iowait_time;
+}
+
+/************************** sysfs interface ************************/
+
+static ssize_t show_sampling_rate_min(struct kobject *kobj,
+				      struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", min_sampling_rate);
+}
+
+define_one_global_ro(sampling_rate_min);
+
+/* cpufreq_hyper_power Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)              \
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(up_threshold, up_threshold);
+show_one(up_threshold_min_freq, up_threshold_min_freq);
+show_one(sampling_down_factor, sampling_down_factor);
+show_one(micro_freq_up_threshold, micro_freq_up_threshold);
+show_one(down_differential, down_differential);
+show_one(boostpulse, boosted);
+show_one(boostfreq, boostfreq);
+show_one(freq_step, freq_step);
+
+static ssize_t store_boostpulse(struct kobject *kobj, struct attribute *attr,
+				const char *buf, size_t count)
+{
+	int ret;
+	unsigned int input;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret < 0)
+		return ret;
+
+	if (input > 1 && input <= MAX_FREQ_BOOST_TIME)
+		dbs_tuners_ins.freq_boost_time = input;
+	else
+		dbs_tuners_ins.freq_boost_time = DEFAULT_FREQ_BOOST_TIME;
+
+	dbs_tuners_ins.boosted = 1;
+	hyper_freq_boosted_time = ktime_to_us(ktime_get());
+
+	if (hyper_sampling_rate_boosted) {
+		hyper_sampling_rate_boosted = 0;
+		dbs_tuners_ins.sampling_rate = hyper_current_sampling_rate;
+	}
+	return count;
+}
+
+static ssize_t store_boostfreq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.boostfreq = input;
+	return count;
+}
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret = 0;
+	int mpd = strcmp(current->comm, "mpdecision");
+
+	if (mpd == 0)
+		return ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_rate = max(input, min_sampling_rate);
+	hyper_current_sampling_rate = dbs_tuners_ins.sampling_rate;
+
+	return count;
+}
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold = input;
+
+	return count;
+}
+
+static ssize_t store_micro_freq_up_threshold(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.micro_freq_up_threshold = input;
+	return count;
+}
+
+static ssize_t store_down_differential(struct kobject *a, struct attribute *b,
+				const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.down_differential = min(input, 100u);
+
+	return count;
+}
+
+static ssize_t store_up_threshold_min_freq(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold_min_freq = input;
+
+	return count;
+}
+
+static ssize_t store_sampling_down_factor(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input, j;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_down_factor = input;
+
+	/* Reset down sampling multiplier in case it was active */
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(od_cpu_dbs_info, j);
+		dbs_info->rate_mult = 1;
+	}
+	return count;
+}
+
+static ssize_t store_freq_step(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.freq_step = min(input, 100u);
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(up_threshold);
+define_one_global_rw(up_threshold_min_freq);
+define_one_global_rw(sampling_down_factor);
+define_one_global_rw(down_differential);
+define_one_global_rw(micro_freq_up_threshold);
+define_one_global_rw(boostpulse);
+define_one_global_rw(boostfreq);
+define_one_global_rw(freq_step);
+
+static struct attribute *dbs_attributes[] = {
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&up_threshold.attr,
+	&up_threshold_min_freq.attr,
+	&sampling_down_factor.attr,
+	&down_differential.attr,
+	&micro_freq_up_threshold.attr,
+	&boostpulse.attr,
+	&boostfreq.attr,
+	&freq_step.attr,
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "HYPER",
+};
+
+/************************** sysfs end ************************/
+
+static void dbs_freq_increase(struct cpufreq_policy *p, unsigned int freq)
+{
+	if (p->cur == p->max)
+		return;
+
+	__cpufreq_driver_target(p, freq, CPUFREQ_RELATION_L);
+}
+
+static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
+{
+	/* Extrapolated load of this CPU */
+	unsigned int load_at_max_freq = 0;
+	unsigned int avg_load_at_max_freq = 0;
+	unsigned int max_load_freq;
+	/* Current load across this CPU */
+	unsigned int cur_load = 0;
+	unsigned int max_load = 0;
+
+	struct cpufreq_policy *policy;
+	struct cpu_dbs_info_s *j_dbs_info;
+	unsigned int j = 0;
+	int up_threshold = dbs_tuners_ins.up_threshold;
+	unsigned int sampling_rate;
+
+	sampling_rate = dbs_tuners_ins.sampling_rate * this_dbs_info->rate_mult;
+	this_dbs_info->freq_lo = 0;
+
+	policy = this_dbs_info->cur_policy;
+	if (policy == NULL)
+		return;
+
+	/* Only core0 controls the boost */
+	if (dbs_tuners_ins.boosted && policy->cpu == 0) {
+		if (ktime_to_us(ktime_get()) - hyper_freq_boosted_time >=
+					dbs_tuners_ins.freq_boost_time) {
+			dbs_tuners_ins.boosted = 0;
+		}
+	}
+
+	/* Only core0 controls the timer_rate */
+	if (hyper_sampling_rate_boosted && policy->cpu == 0) {
+		if (ktime_to_us(ktime_get()) - hyper_sampling_rate_boosted_time >=
+					TIMER_RATE_BOOST_TIME) {
+
+			dbs_tuners_ins.sampling_rate = hyper_current_sampling_rate;
+			hyper_sampling_rate_boosted = 0;
+		}
+	}
+
+	/*
+	 * Every sampling_rate, we check, if current idle time is less
+	 * than 20% (default), then we try to increase frequency
+	 * Every sampling_rate, we look for a the lowest
+	 * frequency which can sustain the load while keeping idle time over
+	 * 30%. If such a frequency exist, we try to decrease to this frequency.
+	 *
+	 * Any frequency increase takes it to the maximum frequency.
+	 * Frequency reduction happens at minimum steps of
+	 * 5% (default) of current frequency
+	 */
+
+	/* Get Absolute Load - in terms of freq */
+	max_load_freq = 0;
+
+	for_each_cpu(j, policy->cpus) {
+		u64 cur_wall_time, cur_idle_time, cur_iowait_time;
+		unsigned int idle_time, wall_time, iowait_time;
+		unsigned int load_freq;
+		int freq_avg;
+		bool deep_sleep_detected = false;
+		/* the evil magic numbers, only 2 at least */
+		const unsigned int deep_sleep_backoff = 10;
+		const unsigned int deep_sleep_factor = 5;
+
+		j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time, 0);
+		cur_iowait_time = get_cpu_iowait_time(j, &cur_wall_time);
+
+		wall_time = (unsigned int)
+			(cur_wall_time - j_dbs_info->prev_cpu_wall);
+		j_dbs_info->prev_cpu_wall = cur_wall_time;
+
+		/*
+		 * Ignore wall delta jitters in both directions.  An
+		 * exceptionally long wall_time will likely result
+		 * idle but it was waken up to do work so the next
+		 * slice is less likely to want to run at low
+		 * frequency. Let's evaluate the next slice instead of
+		 * the idle long one that passed already and it's too
+		 * late to reduce in frequency.  As opposed an
+		 * exceptionally short slice that just run at low
+		 * frequency is unlikely to be idle, but we may go
+		 * back to idle pretty soon and that not idle slice
+		 * already passed. If short slices will keep coming
+		 * after a series of long slices the exponential
+		 * backoff will converge faster and we'll react faster
+		 * to high load. As opposed we'll decay slower
+		 * towards low load and long idle times.
+		 */
+		if (j_dbs_info->prev_cpu_wall_delta >
+		    wall_time * deep_sleep_factor ||
+		    j_dbs_info->prev_cpu_wall_delta * deep_sleep_factor <
+		    wall_time)
+			deep_sleep_detected = true;
+		j_dbs_info->prev_cpu_wall_delta =
+			(j_dbs_info->prev_cpu_wall_delta * deep_sleep_backoff
+			 + wall_time) / (deep_sleep_backoff+1);
+
+		idle_time = (unsigned int)
+			(cur_idle_time - j_dbs_info->prev_cpu_idle);
+		j_dbs_info->prev_cpu_idle = cur_idle_time;
+
+		iowait_time = (unsigned int)
+			(cur_iowait_time - j_dbs_info->prev_cpu_iowait);
+		j_dbs_info->prev_cpu_iowait = cur_iowait_time;
+
+		if (deep_sleep_detected)
+			continue;
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		/*
+		 * If the CPU had gone completely idle, and a task just woke up
+		 * on this CPU now, it would be unfair to calculate 'load' the
+		 * usual way for this elapsed time-window, because it will show
+		 * near-zero load, irrespective of how CPU intensive that task
+		 * actually is. This is undesirable for latency-sensitive bursty
+		 * workloads.
+		 *
+		 * To avoid this, we reuse the 'load' from the previous
+		 * time-window and give this task a chance to start with a
+		 * reasonably high CPU frequency. (However, we shouldn't over-do
+		 * this copy, lest we get stuck at a high load (high frequency)
+		 * for too long, even when the current system load has actually
+		 * dropped down. So we perform the copy only once, upon the
+		 * first wake-up from idle.)
+		 *
+		 * Detecting this situation is easy: the governor's deferrable
+		 * timer would not have fired during CPU-idle periods. Hence
+		 * an unusually large 'wall_time' (as compared to the sampling
+		 * rate) indicates this scenario.
+		 *
+		 * prev_load can be zero in two cases and we must recalculate it
+		 * for both cases:
+		 * - during long idle intervals
+		 * - explicitly set to zero
+		 */
+		if (unlikely(wall_time > (2 * sampling_rate) &&
+			     j_dbs_info->prev_load)) {
+			cur_load = j_dbs_info->prev_load;
+
+			/*
+			 * Perform a destructive copy, to ensure that we copy
+			 * the previous load only once, upon the first wake-up
+			 * from idle.
+			 */
+			j_dbs_info->prev_load = 0;
+		} else {
+			cur_load = 100 * (wall_time - idle_time) / wall_time;
+			j_dbs_info->prev_load = cur_load;
+		}
+
+		if (cur_load > max_load)
+			max_load = cur_load;
+
+		freq_avg = __cpufreq_driver_getavg(policy, j);
+		if (policy == NULL)
+			return;
+		if (freq_avg <= 0)
+			freq_avg = policy->cur;
+
+		load_freq = cur_load * freq_avg;
+		if (load_freq > max_load_freq)
+			max_load_freq = load_freq;
+
+		/* calculate the scaled load across CPU */
+		load_at_max_freq += (cur_load * policy->cur) /
+					policy->max;
+
+		avg_load_at_max_freq += ((load_at_max_freq +
+				j_dbs_info->load_at_prev_sample) / 2);
+
+		j_dbs_info->load_at_prev_sample = load_at_max_freq;
+	}
+
+	cpufreq_notify_utilization(policy, load_at_max_freq);
+
+	/* Check for frequency increase */
+	if (max_load_freq > up_threshold * policy->cur) {
+		int inc = (policy->max * dbs_tuners_ins.freq_step) / 100;
+		int target = min(policy->max, policy->cur + inc);
+
+		/* If switching to max speed, apply sampling_down_factor */
+		if (policy->cur < policy->max && target == policy->max) {
+			if (hyper_sampling_rate_boosted &&
+				(dbs_tuners_ins.sampling_down_factor <
+					BOOSTED_SAMPLING_DOWN_FACTOR)) {
+				this_dbs_info->rate_mult =
+					BOOSTED_SAMPLING_DOWN_FACTOR;
+			} else {
+				this_dbs_info->rate_mult =
+					dbs_tuners_ins.sampling_down_factor;
+			}
+		}
+		dbs_freq_increase(policy, target);
+		return;
+	}
+
+	/* check for frequency boost */
+	if (dbs_tuners_ins.boosted && policy->cur < dbs_tuners_ins.boostfreq) {
+		dbs_freq_increase(policy, dbs_tuners_ins.boostfreq);
+		dbs_tuners_ins.boostfreq = policy->cur;
+		return;
+	}
+
+	/* Check for frequency decrease */
+	/* if we cannot reduce the frequency anymore, break out early */
+	if (policy->cur == policy->min)
+		return;
+
+	/*
+	 * The optimal frequency is the frequency that is the lowest that
+	 * can support the current CPU usage without triggering the up
+	 * policy. To be safe, we focus 10 points under the threshold.
+	 */
+	if (max_load_freq <
+	    (dbs_tuners_ins.up_threshold - dbs_tuners_ins.down_differential) *
+			policy->cur) {
+		unsigned int freq_next;
+		unsigned int down_thres;
+
+		freq_next = max_load_freq /
+				(dbs_tuners_ins.up_threshold -
+					dbs_tuners_ins.down_differential);
+
+		if (dbs_tuners_ins.boosted &&
+				freq_next < dbs_tuners_ins.boostfreq) {
+			freq_next = dbs_tuners_ins.boostfreq;
+		}
+		/* No longer fully busy, reset rate_mult */
+		this_dbs_info->rate_mult = 1;
+
+		if (freq_next < policy->min)
+			freq_next = policy->min;
+
+		down_thres = dbs_tuners_ins.up_threshold_min_freq
+			- dbs_tuners_ins.down_differential;
+
+		__cpufreq_driver_target(policy, freq_next,
+			CPUFREQ_RELATION_L);
+	}
+}
+
+static void do_dbs_timer(struct work_struct *work)
+{
+	struct cpu_dbs_info_s *dbs_info =
+		container_of(work, struct cpu_dbs_info_s, work.work);
+	int sample_type = dbs_info->sample_type;
+
+	int delay;
+
+	if (unlikely(!cpu_online(dbs_info->cpu) || !dbs_info->cur_policy))
+		return;
+
+	mutex_lock(&dbs_info->timer_mutex);
+
+	/* Common NORMAL_SAMPLE setup */
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	if (sample_type == DBS_NORMAL_SAMPLE) {
+		dbs_check_cpu(dbs_info);
+		if (dbs_info->freq_lo) {
+			/* Setup timer for SUB_SAMPLE */
+			dbs_info->sample_type = DBS_SUB_SAMPLE;
+			delay = dbs_info->freq_hi_jiffies;
+		} else {
+			/* We want all CPUs to do sampling nearly on
+			 * same jiffy
+			 */
+			delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate
+				* dbs_info->rate_mult);
+
+			if (num_online_cpus() > 1)
+				delay -= jiffies % delay;
+		}
+	} else {
+		__cpufreq_driver_target(dbs_info->cur_policy,
+			dbs_info->freq_lo, CPUFREQ_RELATION_H);
+		delay = dbs_info->freq_lo_jiffies;
+	}
+	queue_delayed_work_on(dbs_info->cpu, dbs_wq, &dbs_info->work, delay);
+	mutex_unlock(&dbs_info->timer_mutex);
+}
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
+{
+	/* We want all CPUs to do sampling nearly on same jiffy */
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+	if (num_online_cpus() > 1)
+		delay -= jiffies % delay;
+
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	INIT_DEFERRABLE_WORK(&dbs_info->work, do_dbs_timer);
+	queue_delayed_work_on(dbs_info->cpu, dbs_wq, &dbs_info->work,
+				10 * delay);
+}
+
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
+{
+	cancel_delayed_work_sync(&dbs_info->work);
+}
+
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int rc;
+
+	this_dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy))
+			return -EINVAL;
+
+		mutex_lock(&dbs_mutex);
+
+		dbs_enable++;
+
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			unsigned int prev_load;
+
+			j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&j_dbs_info->prev_cpu_wall, 0);
+
+			prev_load = (unsigned int)
+				(j_dbs_info->prev_cpu_wall - j_dbs_info->prev_cpu_idle);
+			j_dbs_info->prev_load = 100 * prev_load /
+				(unsigned int) j_dbs_info->prev_cpu_wall;
+		}
+		cpu = policy->cpu;
+		this_dbs_info->cpu = cpu;
+		this_dbs_info->rate_mult = 1;
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 1) {
+			unsigned int latency;
+
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&dbs_attr_group);
+			if (rc) {
+				dbs_enable--;
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+
+			/* policy latency is in nS. Convert it to uS first */
+			latency = policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+			/* Bring kernel and HW constraints together */
+			min_sampling_rate = max(min_sampling_rate,
+					MIN_LATENCY_MULTIPLIER * latency);
+			if (latency != 1)
+				dbs_tuners_ins.sampling_rate =
+					max(dbs_tuners_ins.sampling_rate,
+						latency * LATENCY_MULTIPLIER);
+		}
+		mutex_unlock(&dbs_mutex);
+
+		mutex_init(&this_dbs_info->timer_mutex);
+
+		dbs_timer_init(this_dbs_info);
+
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		dbs_timer_exit(this_dbs_info);
+
+		mutex_lock(&dbs_mutex);
+		mutex_destroy(&this_dbs_info->timer_mutex);
+
+		dbs_enable--;
+		if (!dbs_enable)
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &dbs_attr_group);
+		mutex_unlock(&dbs_mutex);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		/* If device is being removed, skip set limits */
+		if (!this_dbs_info->cur_policy)
+			break;
+		mutex_lock(&this_dbs_info->timer_mutex);
+		__cpufreq_driver_target(this_dbs_info->cur_policy,
+				policy->cur, CPUFREQ_RELATION_L);
+		dbs_check_cpu(this_dbs_info);
+		mutex_unlock(&this_dbs_info->timer_mutex);
+
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER
+static
+#endif
+struct cpufreq_governor cpufreq_gov_HYPER = {
+	.name			= "HYPER",
+	.governor		= cpufreq_governor_dbs,
+	.max_transition_latency	= TRANSITION_LATENCY_LIMIT,
+	.owner			= THIS_MODULE,
+};
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	u64 wall;
+	u64 idle_time;
+	int cpu = get_cpu();
+	int err = 0;
+
+	idle_time = get_cpu_idle_time_us(cpu, &wall);
+	put_cpu();
+	if (idle_time != -1ULL) {
+		/* Idle micro accounting is supported. Use finer thresholds */
+		dbs_tuners_ins.up_threshold = dbs_tuners_ins.micro_freq_up_threshold;
+		dbs_tuners_ins.down_differential =
+					MICRO_FREQUENCY_DOWN_DIFFERENTIAL;
+		/*
+		 * In no_hz/micro accounting case we set the minimum frequency
+		 * not depending on HZ, but fixed (very low). The deferred
+		 * timer might skip some samples if idle/sleeping as needed.
+		*/
+		min_sampling_rate = MICRO_FREQUENCY_MIN_SAMPLE_RATE;
+	} else {
+		/* For correct statistics, we need 10 ticks for each measure */
+		min_sampling_rate =
+			MIN_SAMPLING_RATE_RATIO * jiffies_to_usecs(10);
+	}
+
+	dbs_wq = alloc_workqueue("HYPER_dbs_wq", WQ_HIGHPRI, 0);
+	if (!dbs_wq) {
+		printk(KERN_ERR "Failed to create HYPER_dbs_wq workqueue\n");
+		return -EFAULT;
+	}
+
+	err = cpufreq_register_governor(&cpufreq_gov_HYPER);
+	if (err)
+		goto error_reg;
+
+	return err;
+error_reg:
+	kfree(&dbs_tuners_ins);
+	return err;
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	/*pm_qos_remove_notifier(PM_QOS_DVFS_RESPONSE_LATENCY,
+			       &hyper_qos_dvfs_lat_nb);*/
+
+	cpufreq_unregister_governor(&cpufreq_gov_HYPER);
+	destroy_workqueue(dbs_wq);
+	kfree(&dbs_tuners_ins);
+}
+
+MODULE_AUTHOR("Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>");
+MODULE_AUTHOR("Alexey Starikovskiy <alexey.y.starikovskiy@intel.com>");
+MODULE_DESCRIPTION("'cpufreq_HYPER' - A dynamic cpufreq governor for "
+	"Low Latency Frequency Transition capable processors");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_alucard.c b/drivers/cpufreq/cpufreq_alucard.c
new file mode 100755
index 0000000..2aad141
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_alucard.c
@@ -0,0 +1,831 @@
+/*
+ *  drivers/cpufreq/cpufreq_alucard.c
+ *
+ *  Copyright (C)  2011 Samsung Electronics co. ltd
+ *    ByungChang Cha <bc.cha@samsung.com>
+ *
+ *  Based on ondemand governor
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * Created by Alucard_24@xda
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+/* Tuning Interface */
+#ifdef CONFIG_MACH_LGE
+#define FREQ_RESPONSIVENESS		2265600
+#else
+#define FREQ_RESPONSIVENESS		1134000
+#endif
+
+#define CPUS_DOWN_RATE			2
+#define CPUS_UP_RATE			1
+
+#define DEC_CPU_LOAD			70
+#define DEC_CPU_LOAD_AT_MIN_FREQ	60
+
+#define INC_CPU_LOAD			70
+#define INC_CPU_LOAD_AT_MIN_FREQ	60
+
+/* Pump Inc/Dec for all cores */
+#define PUMP_INC_STEP_AT_MIN_FREQ	2
+#define PUMP_INC_STEP			2
+#define PUMP_DEC_STEP_AT_MIN_FREQ	2
+#define PUMP_DEC_STEP			1
+
+/* sample rate */
+#define MIN_SAMPLING_RATE		10000
+#define SAMPLING_RATE			50000
+
+static void do_alucard_timer(struct work_struct *work);
+
+struct cpufreq_alucard_cpuinfo {
+	u64 prev_cpu_wall;
+	u64 prev_cpu_idle;
+	struct cpufreq_frequency_table *freq_table;
+	struct delayed_work work;
+	struct cpufreq_policy *cur_policy;
+	int pump_inc_step;
+	int pump_inc_step_at_min_freq;
+	int pump_dec_step;
+	int pump_dec_step_at_min_freq;
+	bool governor_enabled;
+	unsigned int up_rate;
+	unsigned int down_rate;
+	unsigned int cpu;
+	unsigned int min_index;
+	unsigned int max_index;
+	unsigned int prev_load;
+	/*
+	 * mutex that serializes governor limit change with
+	 * do_alucard_timer invocation. We do not want do_alucard_timer to run
+	 * when user is changing the governor or limits.
+	 */
+	struct mutex timer_mutex;
+};
+
+static DEFINE_PER_CPU(struct cpufreq_alucard_cpuinfo, od_alucard_cpuinfo);
+
+static unsigned int alucard_enable;	/* number of CPUs using this policy */
+/*
+ * alucard_mutex protects alucard_enable in governor start/stop.
+ */
+static DEFINE_MUTEX(alucard_mutex);
+
+static struct workqueue_struct *alucard_wq;
+
+/* alucard tuners */
+static struct alucard_tuners {
+	unsigned int sampling_rate;
+	int inc_cpu_load_at_min_freq;
+	int inc_cpu_load;
+	int dec_cpu_load_at_min_freq;
+	int dec_cpu_load;
+	int freq_responsiveness;
+	unsigned int cpus_up_rate;
+	unsigned int cpus_down_rate;
+} alucard_tuners_ins = {
+	.sampling_rate = SAMPLING_RATE,
+	.inc_cpu_load_at_min_freq = INC_CPU_LOAD_AT_MIN_FREQ,
+	.inc_cpu_load = INC_CPU_LOAD,
+	.dec_cpu_load_at_min_freq = DEC_CPU_LOAD_AT_MIN_FREQ,
+	.dec_cpu_load = DEC_CPU_LOAD,
+	.freq_responsiveness = FREQ_RESPONSIVENESS,
+	.cpus_up_rate = CPUS_UP_RATE,
+	.cpus_down_rate = CPUS_DOWN_RATE,
+};
+
+/************************** sysfs interface ************************/
+
+/* cpufreq_alucard Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return sprintf(buf, "%d\n", alucard_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(inc_cpu_load_at_min_freq, inc_cpu_load_at_min_freq);
+show_one(inc_cpu_load, inc_cpu_load);
+show_one(dec_cpu_load_at_min_freq, dec_cpu_load_at_min_freq);
+show_one(dec_cpu_load, dec_cpu_load);
+show_one(freq_responsiveness, freq_responsiveness);
+show_one(cpus_up_rate, cpus_up_rate);
+show_one(cpus_down_rate, cpus_down_rate);
+
+#define show_pcpu_param(file_name, num_core)		\
+static ssize_t show_##file_name##_##num_core		\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	struct cpufreq_alucard_cpuinfo *this_alucard_cpuinfo = &per_cpu(od_alucard_cpuinfo, num_core - 1); \
+	return sprintf(buf, "%d\n", \
+			this_alucard_cpuinfo->file_name);		\
+}
+
+show_pcpu_param(pump_inc_step_at_min_freq, 1);
+show_pcpu_param(pump_inc_step_at_min_freq, 2);
+show_pcpu_param(pump_inc_step_at_min_freq, 3);
+show_pcpu_param(pump_inc_step_at_min_freq, 4);
+show_pcpu_param(pump_inc_step, 1);
+show_pcpu_param(pump_inc_step, 2);
+show_pcpu_param(pump_inc_step, 3);
+show_pcpu_param(pump_inc_step, 4);
+show_pcpu_param(pump_dec_step_at_min_freq, 1);
+show_pcpu_param(pump_dec_step_at_min_freq, 2);
+show_pcpu_param(pump_dec_step_at_min_freq, 3);
+show_pcpu_param(pump_dec_step_at_min_freq, 4);
+show_pcpu_param(pump_dec_step, 1);
+show_pcpu_param(pump_dec_step, 2);
+show_pcpu_param(pump_dec_step, 3);
+show_pcpu_param(pump_dec_step, 4);
+
+#define store_pcpu_param(file_name, num_core)		\
+static ssize_t store_##file_name##_##num_core		\
+(struct kobject *kobj, struct attribute *attr,				\
+	const char *buf, size_t count)					\
+{									\
+	int input;						\
+	struct cpufreq_alucard_cpuinfo *this_alucard_cpuinfo; \
+	int ret;							\
+														\
+	ret = sscanf(buf, "%d", &input);					\
+	if (ret != 1)											\
+		return -EINVAL;										\
+														\
+	this_alucard_cpuinfo = &per_cpu(od_alucard_cpuinfo, num_core - 1); \
+														\
+	if (input == this_alucard_cpuinfo->file_name) {		\
+		return count;						\
+	}								\
+										\
+	this_alucard_cpuinfo->file_name = input;			\
+	return count;							\
+}
+
+
+#define store_pcpu_pump_param(file_name, num_core)		\
+static ssize_t store_##file_name##_##num_core		\
+(struct kobject *kobj, struct attribute *attr,				\
+	const char *buf, size_t count)					\
+{									\
+	int input;						\
+	struct cpufreq_alucard_cpuinfo *this_alucard_cpuinfo; \
+	int ret;							\
+														\
+	ret = sscanf(buf, "%d", &input);					\
+	if (ret != 1)											\
+		return -EINVAL;										\
+														\
+	input = min(max(1, input), 6);							\
+														\
+	this_alucard_cpuinfo = &per_cpu(od_alucard_cpuinfo, num_core - 1); \
+														\
+	if (input == this_alucard_cpuinfo->file_name) {		\
+		return count;						\
+	}								\
+										\
+	this_alucard_cpuinfo->file_name = input;			\
+	return count;							\
+}
+
+store_pcpu_pump_param(pump_inc_step_at_min_freq, 1);
+store_pcpu_pump_param(pump_inc_step_at_min_freq, 2);
+store_pcpu_pump_param(pump_inc_step_at_min_freq, 3);
+store_pcpu_pump_param(pump_inc_step_at_min_freq, 4);
+store_pcpu_pump_param(pump_inc_step, 1);
+store_pcpu_pump_param(pump_inc_step, 2);
+store_pcpu_pump_param(pump_inc_step, 3);
+store_pcpu_pump_param(pump_inc_step, 4);
+store_pcpu_pump_param(pump_dec_step_at_min_freq, 1);
+store_pcpu_pump_param(pump_dec_step_at_min_freq, 2);
+store_pcpu_pump_param(pump_dec_step_at_min_freq, 3);
+store_pcpu_pump_param(pump_dec_step_at_min_freq, 4);
+store_pcpu_pump_param(pump_dec_step, 1);
+store_pcpu_pump_param(pump_dec_step, 2);
+store_pcpu_pump_param(pump_dec_step, 3);
+store_pcpu_pump_param(pump_dec_step, 4);
+
+define_one_global_rw(pump_inc_step_at_min_freq_1);
+define_one_global_rw(pump_inc_step_at_min_freq_2);
+define_one_global_rw(pump_inc_step_at_min_freq_3);
+define_one_global_rw(pump_inc_step_at_min_freq_4);
+define_one_global_rw(pump_inc_step_1);
+define_one_global_rw(pump_inc_step_2);
+define_one_global_rw(pump_inc_step_3);
+define_one_global_rw(pump_inc_step_4);
+define_one_global_rw(pump_dec_step_at_min_freq_1);
+define_one_global_rw(pump_dec_step_at_min_freq_2);
+define_one_global_rw(pump_dec_step_at_min_freq_3);
+define_one_global_rw(pump_dec_step_at_min_freq_4);
+define_one_global_rw(pump_dec_step_1);
+define_one_global_rw(pump_dec_step_2);
+define_one_global_rw(pump_dec_step_3);
+define_one_global_rw(pump_dec_step_4);
+
+/* sampling_rate */
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret = 0;
+	int mpd = strcmp(current->comm, "mpdecision");
+
+	if (mpd == 0)
+		return ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(input, 10000);
+
+	if (input == alucard_tuners_ins.sampling_rate)
+		return count;
+
+	alucard_tuners_ins.sampling_rate = input;
+
+	return count;
+}
+
+/* inc_cpu_load_at_min_freq */
+static ssize_t store_inc_cpu_load_at_min_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1) {
+		return -EINVAL;
+	}
+
+	input = min(input, alucard_tuners_ins.inc_cpu_load);
+
+	if (input == alucard_tuners_ins.inc_cpu_load_at_min_freq)
+		return count;
+
+	alucard_tuners_ins.inc_cpu_load_at_min_freq = input;
+
+	return count;
+}
+
+/* inc_cpu_load */
+static ssize_t store_inc_cpu_load(struct kobject *a, struct attribute *b,
+					const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input, 100),0);
+
+	if (input == alucard_tuners_ins.inc_cpu_load)
+		return count;
+
+	alucard_tuners_ins.inc_cpu_load = input;
+
+	return count;
+}
+
+/* dec_cpu_load_at_min_freq */
+static ssize_t store_dec_cpu_load_at_min_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1) {
+		return -EINVAL;
+	}
+
+	input = min(input, alucard_tuners_ins.dec_cpu_load);
+
+	if (input == alucard_tuners_ins.dec_cpu_load_at_min_freq)
+		return count;
+
+	alucard_tuners_ins.dec_cpu_load_at_min_freq = input;
+
+	return count;
+}
+
+/* dec_cpu_load */
+static ssize_t store_dec_cpu_load(struct kobject *a, struct attribute *b,
+					const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input, 95),5);
+
+	if (input == alucard_tuners_ins.dec_cpu_load)
+		return count;
+
+	alucard_tuners_ins.dec_cpu_load = input;
+
+	return count;
+}
+
+/* freq_responsiveness */
+static ssize_t store_freq_responsiveness(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input == alucard_tuners_ins.freq_responsiveness)
+		return count;
+
+	alucard_tuners_ins.freq_responsiveness = input;
+
+	return count;
+}
+
+/* cpus_up_rate */
+static ssize_t store_cpus_up_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input == alucard_tuners_ins.cpus_up_rate)
+		return count;
+
+	alucard_tuners_ins.cpus_up_rate = input;
+
+	return count;
+}
+
+/* cpus_down_rate */
+static ssize_t store_cpus_down_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input == alucard_tuners_ins.cpus_down_rate)
+		return count;
+
+	alucard_tuners_ins.cpus_down_rate = input;
+
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(inc_cpu_load_at_min_freq);
+define_one_global_rw(inc_cpu_load);
+define_one_global_rw(dec_cpu_load_at_min_freq);
+define_one_global_rw(dec_cpu_load);
+define_one_global_rw(freq_responsiveness);
+define_one_global_rw(cpus_up_rate);
+define_one_global_rw(cpus_down_rate);
+
+static struct attribute *alucard_attributes[] = {
+	&sampling_rate.attr,
+	&inc_cpu_load_at_min_freq.attr,
+	&inc_cpu_load.attr,
+	&dec_cpu_load_at_min_freq.attr,
+	&dec_cpu_load.attr,
+	&freq_responsiveness.attr,
+	&pump_inc_step_at_min_freq_1.attr,
+	&pump_inc_step_at_min_freq_2.attr,
+	&pump_inc_step_at_min_freq_3.attr,
+	&pump_inc_step_at_min_freq_4.attr,
+	&pump_inc_step_1.attr,
+	&pump_inc_step_2.attr,
+	&pump_inc_step_3.attr,
+	&pump_inc_step_4.attr,
+	&pump_dec_step_at_min_freq_1.attr,
+	&pump_dec_step_at_min_freq_2.attr,
+	&pump_dec_step_at_min_freq_3.attr,
+	&pump_dec_step_at_min_freq_4.attr,
+	&pump_dec_step_1.attr,
+	&pump_dec_step_2.attr,
+	&pump_dec_step_3.attr,
+	&pump_dec_step_4.attr,
+	&cpus_up_rate.attr,
+	&cpus_down_rate.attr,
+	NULL
+};
+
+static struct attribute_group alucard_attr_group = {
+	.attrs = alucard_attributes,
+	.name = "alucard",
+};
+
+/************************** sysfs end ************************/
+
+static void cpufreq_frequency_table_policy_minmax_limits(struct cpufreq_policy *policy,
+					struct cpufreq_alucard_cpuinfo *this_alucard_cpuinfo)
+{
+	struct cpufreq_frequency_table *table = this_alucard_cpuinfo->freq_table;
+	unsigned int i = 0;
+
+	for (i = 0; (table[i].frequency != CPUFREQ_TABLE_END); i++) {
+		unsigned int freq = table[i].frequency;
+		if (freq == CPUFREQ_ENTRY_INVALID) {
+			continue;
+		}
+		if (freq == policy->min)
+			this_alucard_cpuinfo->min_index = i;
+		if (freq == policy->max)
+			this_alucard_cpuinfo->max_index = i;
+
+		if (freq >= policy->min &&
+			freq >= policy->max)
+			break;
+	}
+}
+
+static void cpufreq_frequency_table_policy_cur_limit(struct cpufreq_policy *policy,
+					struct cpufreq_frequency_table *table,
+					unsigned int *index)
+{
+	unsigned int i = 0;
+
+	for (i = 0; (table[i].frequency != CPUFREQ_TABLE_END); i++) {
+		unsigned int freq = table[i].frequency;
+		if (freq == CPUFREQ_ENTRY_INVALID) {
+			continue;
+		}
+		if (freq == policy->cur) {
+			*index = i;
+			break;
+		}
+	}
+}
+
+static void alucard_check_cpu(struct cpufreq_alucard_cpuinfo *this_alucard_cpuinfo)
+{
+	struct cpufreq_policy *policy;
+	unsigned int freq_responsiveness = alucard_tuners_ins.freq_responsiveness;
+	int dec_cpu_load = alucard_tuners_ins.dec_cpu_load;
+	int inc_cpu_load = alucard_tuners_ins.inc_cpu_load;
+	int pump_inc_step = this_alucard_cpuinfo->pump_inc_step;
+	int pump_dec_step = this_alucard_cpuinfo->pump_dec_step;
+	unsigned int cur_load = 0;
+	unsigned int max_load = 0;
+	unsigned int cpus_up_rate = alucard_tuners_ins.cpus_up_rate;
+	unsigned int cpus_down_rate = alucard_tuners_ins.cpus_down_rate;
+	unsigned int index = 0;
+	unsigned int j;
+	unsigned int sampling_rate = alucard_tuners_ins.sampling_rate;
+
+	policy = this_alucard_cpuinfo->cur_policy;
+	if (!policy)
+		return;
+
+	/* Get min, current, max indexes from current cpu policy */
+	cpufreq_frequency_table_policy_cur_limit(policy,
+				this_alucard_cpuinfo->freq_table,
+				&index);
+
+	for_each_cpu(j, policy->cpus) {
+		struct cpufreq_alucard_cpuinfo *j_alucard_cpuinfo = &per_cpu(od_alucard_cpuinfo, j);
+		u64 cur_wall_time, cur_idle_time;
+		unsigned int idle_time, wall_time;
+		
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time, 0);
+
+		wall_time = (unsigned int)
+			(cur_wall_time - j_alucard_cpuinfo->prev_cpu_wall);
+		j_alucard_cpuinfo->prev_cpu_wall = cur_wall_time;
+
+		idle_time = (unsigned int)
+			(cur_idle_time - j_alucard_cpuinfo->prev_cpu_idle);
+		j_alucard_cpuinfo->prev_cpu_idle = cur_idle_time;
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		/*
+		 * If the CPU had gone completely idle, and a task just woke up
+		 * on this CPU now, it would be unfair to calculate 'load' the
+		 * usual way for this elapsed time-window, because it will show
+		 * near-zero load, irrespective of how CPU intensive that task
+		 * actually is. This is undesirable for latency-sensitive bursty
+		 * workloads.
+		 *
+		 * To avoid this, we reuse the 'load' from the previous
+		 * time-window and give this task a chance to start with a
+		 * reasonably high CPU frequency. (However, we shouldn't over-do
+		 * this copy, lest we get stuck at a high load (high frequency)
+		 * for too long, even when the current system load has actually
+		 * dropped down. So we perform the copy only once, upon the
+		 * first wake-up from idle.)
+		 *
+		 * Detecting this situation is easy: the governor's deferrable
+		 * timer would not have fired during CPU-idle periods. Hence
+		 * an unusually large 'wall_time' (as compared to the sampling
+		 * rate) indicates this scenario.
+		 *
+		 * prev_load can be zero in two cases and we must recalculate it
+		 * for both cases:
+		 * - during long idle intervals
+		 * - explicitly set to zero
+		 */
+		if (unlikely(wall_time > (2 * sampling_rate) &&
+			     j_alucard_cpuinfo->prev_load)) {
+			cur_load = j_alucard_cpuinfo->prev_load;
+
+			/*
+			 * Perform a destructive copy, to ensure that we copy
+			 * the previous load only once, upon the first wake-up
+			 * from idle.
+			 */
+			j_alucard_cpuinfo->prev_load = 0;
+		} else {
+			cur_load = 100 * (wall_time - idle_time) / wall_time;
+			j_alucard_cpuinfo->prev_load = cur_load;
+		}
+
+		if (cur_load > max_load)
+			max_load = cur_load;
+	}
+
+	cpufreq_notify_utilization(policy, cur_load);
+
+	/* CPUs Online Scale Frequency*/
+	if (policy->cur < freq_responsiveness) {
+		inc_cpu_load = alucard_tuners_ins.inc_cpu_load_at_min_freq;
+		dec_cpu_load = alucard_tuners_ins.dec_cpu_load_at_min_freq;
+		pump_inc_step = this_alucard_cpuinfo->pump_inc_step_at_min_freq;
+		pump_dec_step = this_alucard_cpuinfo->pump_dec_step_at_min_freq;
+	}
+	/* check if policy is valid */
+	if (!policy)
+		return;
+	/* Check for frequency increase or for frequency decrease */
+	if (max_load >= inc_cpu_load
+		 && index < this_alucard_cpuinfo->max_index) {
+		if (this_alucard_cpuinfo->up_rate % cpus_up_rate == 0) {
+			if ((index + pump_inc_step) <= this_alucard_cpuinfo->max_index)
+				index += pump_inc_step;
+			else
+				index = this_alucard_cpuinfo->max_index;
+
+			this_alucard_cpuinfo->up_rate = 1;
+			this_alucard_cpuinfo->down_rate = 1;
+
+			if (this_alucard_cpuinfo->freq_table[index].frequency != CPUFREQ_ENTRY_INVALID)
+				__cpufreq_driver_target(policy,
+										this_alucard_cpuinfo->freq_table[index].frequency,
+										CPUFREQ_RELATION_L);
+		} else {
+			if (this_alucard_cpuinfo->up_rate < cpus_up_rate)
+				++this_alucard_cpuinfo->up_rate;
+			else
+				this_alucard_cpuinfo->up_rate = 1;
+		}
+	} else if (max_load < dec_cpu_load
+				 && index > this_alucard_cpuinfo->min_index) {
+		if (this_alucard_cpuinfo->down_rate % cpus_down_rate == 0) {
+			if ((index - this_alucard_cpuinfo->min_index) >= pump_dec_step)
+				index -= pump_dec_step;
+			else
+				index = this_alucard_cpuinfo->min_index;
+
+			this_alucard_cpuinfo->up_rate = 1;
+			this_alucard_cpuinfo->down_rate = 1;
+
+			if (this_alucard_cpuinfo->freq_table[index].frequency != CPUFREQ_ENTRY_INVALID)
+				__cpufreq_driver_target(policy,
+										this_alucard_cpuinfo->freq_table[index].frequency,
+										CPUFREQ_RELATION_L);
+		} else {
+			if (this_alucard_cpuinfo->down_rate < cpus_down_rate)
+				++this_alucard_cpuinfo->down_rate;
+			else
+				this_alucard_cpuinfo->down_rate = 1;
+		}
+	} else {
+		this_alucard_cpuinfo->up_rate = 1;
+		this_alucard_cpuinfo->down_rate = 1;
+	}
+}
+
+static void do_alucard_timer(struct work_struct *work)
+{
+	struct cpufreq_alucard_cpuinfo *this_alucard_cpuinfo = 
+		container_of(work, struct cpufreq_alucard_cpuinfo, work.work);
+	int delay;
+
+	if (unlikely(!cpu_online(this_alucard_cpuinfo->cpu) ||
+				!this_alucard_cpuinfo->cur_policy))
+		return;
+
+	mutex_lock(&this_alucard_cpuinfo->timer_mutex);
+
+	alucard_check_cpu(this_alucard_cpuinfo);
+
+	delay = usecs_to_jiffies(alucard_tuners_ins.sampling_rate);
+
+	/* We want all CPUs to do sampling nearly on same jiffy */
+	if (num_online_cpus() > 1) {
+		delay -= jiffies % delay;
+	}
+
+	queue_delayed_work_on(this_alucard_cpuinfo->cpu, alucard_wq,
+			&this_alucard_cpuinfo->work, delay);
+	mutex_unlock(&this_alucard_cpuinfo->timer_mutex);
+}
+
+static int cpufreq_governor_alucard(struct cpufreq_policy *policy,
+				unsigned int event)
+{
+	struct cpufreq_alucard_cpuinfo *this_alucard_cpuinfo;
+	unsigned int cpu = policy->cpu, j;
+	int rc, delay;
+
+	this_alucard_cpuinfo = &per_cpu(od_alucard_cpuinfo, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy))
+			return -EINVAL;
+
+		mutex_lock(&alucard_mutex);
+		this_alucard_cpuinfo->freq_table = cpufreq_frequency_get_table(cpu);
+		if (!this_alucard_cpuinfo->freq_table) {
+			mutex_unlock(&alucard_mutex);
+			return -EINVAL;
+		}
+		cpufreq_frequency_table_policy_minmax_limits(policy,
+				this_alucard_cpuinfo);	
+
+		for_each_cpu(j, policy->cpus) {
+			struct cpufreq_alucard_cpuinfo *j_alucard_cpuinfo = &per_cpu(od_alucard_cpuinfo, j);
+			unsigned int prev_load;
+
+			j_alucard_cpuinfo->prev_cpu_idle = get_cpu_idle_time(j,
+				&j_alucard_cpuinfo->prev_cpu_wall, 0);
+
+			prev_load = (unsigned int)
+				(j_alucard_cpuinfo->prev_cpu_wall -
+				j_alucard_cpuinfo->prev_cpu_idle);
+			j_alucard_cpuinfo->prev_load = 100 * prev_load /
+				(unsigned int) j_alucard_cpuinfo->prev_cpu_wall;
+		}
+
+		alucard_enable++;
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (alucard_enable == 1) {
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&alucard_attr_group);
+			if (rc) {
+				alucard_enable--;
+				mutex_unlock(&alucard_mutex);
+				return rc;
+			}
+		}
+		cpu = policy->cpu;
+		this_alucard_cpuinfo->cpu = cpu;
+		this_alucard_cpuinfo->cur_policy = policy;
+		this_alucard_cpuinfo->up_rate = 2;
+		this_alucard_cpuinfo->down_rate = 2;
+		this_alucard_cpuinfo->governor_enabled = true;
+		mutex_unlock(&alucard_mutex);
+
+		mutex_init(&this_alucard_cpuinfo->timer_mutex);
+
+		delay = usecs_to_jiffies(alucard_tuners_ins.sampling_rate);
+		/* We want all CPUs to do sampling nearly on same jiffy */
+		if (num_online_cpus() > 1) {
+			delay -= jiffies % delay;
+		}
+
+		INIT_DEFERRABLE_WORK(&this_alucard_cpuinfo->work, do_alucard_timer);
+		queue_delayed_work_on(cpu,
+			alucard_wq, &this_alucard_cpuinfo->work, delay);
+
+		break;
+	case CPUFREQ_GOV_STOP:
+		cancel_delayed_work_sync(&this_alucard_cpuinfo->work);
+
+		mutex_lock(&alucard_mutex);
+		mutex_destroy(&this_alucard_cpuinfo->timer_mutex);
+
+		this_alucard_cpuinfo->governor_enabled = false;
+
+		this_alucard_cpuinfo->cur_policy = NULL;
+
+		alucard_enable--;
+		if (!alucard_enable) {
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &alucard_attr_group);
+		}
+
+		mutex_unlock(&alucard_mutex);
+
+		break;
+	case CPUFREQ_GOV_LIMITS:
+		if (!this_alucard_cpuinfo->cur_policy
+			 || !policy) {
+			pr_debug("Unable to limit cpu freq due to cur_policy == NULL\n");
+			return -EPERM;
+		}
+		mutex_lock(&this_alucard_cpuinfo->timer_mutex);
+		__cpufreq_driver_target(this_alucard_cpuinfo->cur_policy,
+				policy->cur, CPUFREQ_RELATION_L);
+
+		cpufreq_frequency_table_policy_minmax_limits(policy,
+				this_alucard_cpuinfo);
+		mutex_unlock(&this_alucard_cpuinfo->timer_mutex);
+
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_ALUCARD
+static
+#endif
+struct cpufreq_governor cpufreq_gov_alucard = {
+	.name                   = "alucard",
+	.governor               = cpufreq_governor_alucard,
+	.owner                  = THIS_MODULE,
+};
+
+
+static int __init cpufreq_gov_alucard_init(void)
+{
+	unsigned int cpu;
+
+	for_each_possible_cpu(cpu) {
+		struct cpufreq_alucard_cpuinfo *this_alucard_cpuinfo = &per_cpu(od_alucard_cpuinfo, cpu);
+
+		this_alucard_cpuinfo->pump_inc_step_at_min_freq = PUMP_INC_STEP_AT_MIN_FREQ;
+		this_alucard_cpuinfo->pump_inc_step = PUMP_INC_STEP;
+		this_alucard_cpuinfo->pump_dec_step = PUMP_DEC_STEP;
+		this_alucard_cpuinfo->pump_dec_step_at_min_freq = PUMP_DEC_STEP_AT_MIN_FREQ;
+	}
+
+	alucard_wq = alloc_workqueue("alucard_wq", WQ_HIGHPRI, 0);
+	if (!alucard_wq) {
+		printk(KERN_ERR "Failed to create alucard_wq workqueue\n");
+		return -EFAULT;
+	}
+
+	return cpufreq_register_governor(&cpufreq_gov_alucard);
+}
+
+static void __exit cpufreq_gov_alucard_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_alucard);
+}
+
+MODULE_AUTHOR("Alucard24@XDA");
+MODULE_DESCRIPTION("'cpufreq_alucard' - A dynamic cpufreq governor v3.0 (SnapDragon)");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_ALUCARD
+fs_initcall(cpufreq_gov_alucard_init);
+#else
+module_init(cpufreq_gov_alucard_init);
+#endif
+module_exit(cpufreq_gov_alucard_exit);
diff --git a/drivers/cpufreq/cpufreq_barry_allen.c b/drivers/cpufreq/cpufreq_barry_allen.c
new file mode 100644
index 0000000..c33b7ee
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_barry_allen.c
@@ -0,0 +1,1539 @@
+/*
+ * drivers/cpufreq/cpufreq_barry_allen.c
+ *
+ * Copyright (C) 2010 Google, Inc.
+ * Copyright (C) 2015 Javier Sayago <admin@lonasdigital.com>
+ *
+ * Barry_Allen Version 1.0
+ * Last Update >> 13-06-2015
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ *
+ */
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/rwsem.h>
+#include <linux/sched.h>
+#include <linux/sched/rt.h>
+#include <linux/tick.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/kernel_stat.h>
+#include <asm/cputime.h>
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/cpufreq_barry_allen.h>
+
+static int active_count;
+
+static bool ba_locked = false;
+
+struct cpufreq_barry_allen_cpuinfo {
+	struct timer_list cpu_timer;
+	struct timer_list cpu_slack_timer;
+	spinlock_t load_lock; /* protects the next 4 fields */
+	u64 time_in_idle;
+	u64 time_in_idle_timestamp;
+	u64 cputime_speedadj;
+	u64 cputime_speedadj_timestamp;
+	struct cpufreq_policy *policy;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int target_freq;
+	unsigned int floor_freq;
+	u64 floor_validate_time;
+	u64 hispeed_validate_time;
+	struct rw_semaphore enable_sem;
+	int governor_enabled;
+	int prev_load;
+	bool limits_changed;
+	unsigned int nr_timer_resched;
+};
+
+#define MIN_TIMER_JIFFIES 1UL
+
+static DEFINE_PER_CPU(struct cpufreq_barry_allen_cpuinfo, cpuinfo);
+
+/* realtime thread handles frequency scaling */
+static struct task_struct *speedchange_task;
+static cpumask_t speedchange_cpumask;
+static spinlock_t speedchange_cpumask_lock;
+static struct mutex gov_lock;
+
+/* Hi speed to bump to from lo speed when load burst (default max) */
+static unsigned int hispeed_freq;
+
+/* Go to hi speed when CPU load at or above this value. */
+#define DEFAULT_GO_HISPEED_LOAD 90
+static unsigned long go_hispeed_load = DEFAULT_GO_HISPEED_LOAD;
+
+/* Sampling down factor to be applied to min_sample_time at max freq */
+static unsigned int sampling_down_factor;
+
+/* Target load.  Lower values result in higher CPU speeds. */
+#define DEFAULT_TARGET_LOAD 80
+static unsigned int default_target_loads[] = {DEFAULT_TARGET_LOAD};
+static spinlock_t target_loads_lock;
+static unsigned int *target_loads = default_target_loads;
+static int ntarget_loads = ARRAY_SIZE(default_target_loads);
+
+/*
+ * The minimum amount of time to spend at a frequency before we can ramp down.
+ */
+#define DEFAULT_MIN_SAMPLE_TIME (80 * USEC_PER_MSEC)
+static unsigned long min_sample_time = DEFAULT_MIN_SAMPLE_TIME;
+
+/*
+ * The sample rate of the timer used to increase frequency
+ */
+#define DEFAULT_TIMER_RATE (20 * USEC_PER_MSEC)
+static unsigned long timer_rate = DEFAULT_TIMER_RATE;
+
+/* Busy SDF parameters*/
+#define MIN_BUSY_TIME (100 * USEC_PER_MSEC)
+
+/*
+ * Wait this long before raising speed above hispeed, by default a single
+ * timer interval.
+ */
+#define DEFAULT_ABOVE_HISPEED_DELAY DEFAULT_TIMER_RATE
+static unsigned int default_above_hispeed_delay[] = {
+	DEFAULT_ABOVE_HISPEED_DELAY };
+static spinlock_t above_hispeed_delay_lock;
+static unsigned int *above_hispeed_delay = default_above_hispeed_delay;
+static int nabove_hispeed_delay = ARRAY_SIZE(default_above_hispeed_delay);
+
+/* Non-zero means indefinite speed boost active */
+static int boost_val;
+/* Duration of a boot pulse in usecs */
+static int boostpulse_duration_val = DEFAULT_MIN_SAMPLE_TIME;
+/* End time of boost pulse in ktime converted to usecs */
+static u64 boostpulse_endtime;
+
+static bool boosted;
+
+/*
+ * Max additional time to wait in idle, beyond timer_rate, at speeds above
+ * minimum before wakeup to reduce speed, or -1 if unnecessary.
+ */
+#define DEFAULT_TIMER_SLACK (4 * DEFAULT_TIMER_RATE)
+static int timer_slack_val = DEFAULT_TIMER_SLACK;
+
+// Stock S5+ = 2457600
+// Stock Note4 = 2649600
+#define TOP_STOCK_FREQ 2496000
+
+static bool io_is_busy;
+
+/*
+ * If the max load among other CPUs is higher than up_threshold_any_cpu_load
+ * and if the highest frequency among the other CPUs is higher than
+ * up_threshold_any_cpu_freq then do not let the frequency to drop below
+ * sync_freq
+ */
+static unsigned int up_threshold_any_cpu_load;
+static unsigned int sync_freq;
+static unsigned int up_threshold_any_cpu_freq;
+
+static int cpufreq_governor_barry_allen(struct cpufreq_policy *policy,
+		unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_BARRY_ALLEN
+static
+#endif
+struct cpufreq_governor cpufreq_gov_barry_allen = {
+	.name = "barry_allen",
+	.governor = cpufreq_governor_barry_allen,
+	.max_transition_latency = 10000000,
+	.owner = THIS_MODULE,
+};
+
+static void cpufreq_barry_allen_timer_resched(
+	struct cpufreq_barry_allen_cpuinfo *pcpu)
+{
+	unsigned long expires;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->time_in_idle =
+		get_cpu_idle_time(smp_processor_id(),
+				  &pcpu->time_in_idle_timestamp, io_is_busy);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	expires = jiffies + usecs_to_jiffies(timer_rate);
+	mod_timer_pinned(&pcpu->cpu_timer, expires);
+
+	if (timer_slack_val >= 0 && pcpu->target_freq > pcpu->policy->min) {
+		expires += usecs_to_jiffies(timer_slack_val);
+		mod_timer_pinned(&pcpu->cpu_slack_timer, expires);
+	}
+
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+/* The caller shall take enable_sem write semaphore to avoid any timer race.
+ * The cpu_timer and cpu_slack_timer must be deactivated when calling this
+ * function.
+ */
+static void cpufreq_barry_allen_timer_start(int cpu, int time_override)
+{
+	struct cpufreq_barry_allen_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	unsigned long flags;
+	unsigned long expires;
+	if (time_override)
+		expires = jiffies + time_override;
+	else
+		expires = jiffies + usecs_to_jiffies(timer_rate);
+
+	pcpu->cpu_timer.expires = expires;
+	add_timer_on(&pcpu->cpu_timer, cpu);
+	if (timer_slack_val >= 0 && pcpu->target_freq > pcpu->policy->min) {
+		expires += usecs_to_jiffies(timer_slack_val);
+		pcpu->cpu_slack_timer.expires = expires;
+		add_timer_on(&pcpu->cpu_slack_timer, cpu);
+	}
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->time_in_idle =
+		get_cpu_idle_time(cpu, &pcpu->time_in_idle_timestamp,
+				  io_is_busy);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+static unsigned int freq_to_above_hispeed_delay(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay - 1 &&
+			freq >= above_hispeed_delay[i+1]; i += 2)
+		;
+
+	ret = above_hispeed_delay[i];
+	ret = (ret > (1 * USEC_PER_MSEC)) ? (ret - (1 * USEC_PER_MSEC)) : ret;
+
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static unsigned int freq_to_targetload(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+
+	for (i = 0; i < ntarget_loads - 1 && freq >= target_loads[i+1]; i += 2)
+		;
+
+	ret = target_loads[i];
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+/*
+ * If increasing frequencies never map to a lower target load then
+ * choose_freq() will find the minimum frequency that does not exceed its
+ * target load given the current load.
+ */
+
+static unsigned int choose_freq(
+	struct cpufreq_barry_allen_cpuinfo *pcpu, unsigned int loadadjfreq)
+{
+	unsigned int freq = pcpu->policy->cur;
+	unsigned int prevfreq, freqmin, freqmax;
+	unsigned int tl;
+	int index;
+
+	freqmin = 0;
+	freqmax = UINT_MAX;
+
+	do {
+		prevfreq = freq;
+		tl = freq_to_targetload(freq);
+
+		/*
+		 * Find the lowest frequency where the computed load is less
+		 * than or equal to the target load.
+		 */
+
+		if (cpufreq_frequency_table_target(
+			    pcpu->policy, pcpu->freq_table, loadadjfreq / tl,
+			    CPUFREQ_RELATION_C, &index))
+			break;
+		freq = pcpu->freq_table[index].frequency;
+
+		if (freq > prevfreq) {
+			/* The previous frequency is too low. */
+			freqmin = prevfreq;
+
+			if (freq >= freqmax) {
+				/*
+				 * Find the highest frequency that is less
+				 * than freqmax.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmax - 1, CPUFREQ_RELATION_H,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				if (freq == freqmin) {
+					/*
+					 * The first frequency below freqmax
+					 * has already been found to be too
+					 * low.  freqmax is the lowest speed
+					 * we found that is fast enough.
+					 */
+					freq = freqmax;
+					break;
+				}
+			}
+		} else if (freq < prevfreq) {
+			/* The previous frequency is high enough. */
+			freqmax = prevfreq;
+
+			if (freq <= freqmin) {
+				/*
+				 * Find the lowest frequency that is higher
+				 * than freqmin.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmin + 1, CPUFREQ_RELATION_C,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				/*
+				 * If freqmax is the first frequency above
+				 * freqmin then we have already found that
+				 * this speed is fast enough.
+				 */
+				if (freq == freqmax)
+					break;
+			}
+		}
+
+		/* If same frequency chosen as previous then done. */
+	} while (freq != prevfreq);
+
+	return freq;
+}
+
+static u64 update_load(int cpu)
+{
+	struct cpufreq_barry_allen_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 now;
+	u64 now_idle;
+	unsigned int delta_idle;
+	unsigned int delta_time;
+	u64 active_time;
+
+	now_idle = get_cpu_idle_time(cpu, &now, io_is_busy);
+	delta_idle = (unsigned int)(now_idle - pcpu->time_in_idle);
+	delta_time = (unsigned int)(now - pcpu->time_in_idle_timestamp);
+
+	if (delta_time <= delta_idle)
+		active_time = 0;
+	else
+		active_time = delta_time - delta_idle;
+
+	pcpu->cputime_speedadj += active_time * pcpu->policy->cur;
+
+	pcpu->time_in_idle = now_idle;
+	pcpu->time_in_idle_timestamp = now;
+	return now;
+}
+
+#define MAX_LOCAL_LOAD 100
+static void cpufreq_barry_allen_timer(unsigned long data)
+{
+	u64 now;
+	unsigned int delta_time;
+	u64 cputime_speedadj;
+	int cpu_load;
+	struct cpufreq_barry_allen_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, data);
+	unsigned int new_freq;
+	unsigned int loadadjfreq;
+	unsigned int index;
+	unsigned long flags;
+	unsigned long mod_min_sample_time;
+	int i, max_load;
+	unsigned int max_freq;
+	struct cpufreq_barry_allen_cpuinfo *picpu;
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled)
+		goto exit;
+
+	pcpu->nr_timer_resched = 0;
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	now = update_load(data);
+	delta_time = (unsigned int)(now - pcpu->cputime_speedadj_timestamp);
+	cputime_speedadj = pcpu->cputime_speedadj;
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+
+	if (WARN_ON_ONCE(!delta_time))
+		goto rearm;
+
+	do_div(cputime_speedadj, delta_time);
+	loadadjfreq = (unsigned int)cputime_speedadj * 100;
+	cpu_load = loadadjfreq / pcpu->policy->cur;
+	pcpu->prev_load = cpu_load;
+	boosted = boost_val || now < boostpulse_endtime;
+
+	if (cpu_load >= go_hispeed_load || boosted) {
+		if (pcpu->policy->cur < hispeed_freq &&
+		    cpu_load <= MAX_LOCAL_LOAD) {
+			new_freq = hispeed_freq;
+		} else {
+			new_freq = choose_freq(pcpu, loadadjfreq);
+
+			if (new_freq < hispeed_freq)
+				new_freq = hispeed_freq;
+		}
+		if (new_freq > TOP_STOCK_FREQ && cpu_load < 99)
+			new_freq = TOP_STOCK_FREQ;
+	} else {
+		new_freq = choose_freq(pcpu, loadadjfreq);
+
+		if (sync_freq && new_freq < sync_freq) {
+
+			max_load = 0;
+			max_freq = 0;
+
+			for_each_online_cpu(i) {
+				picpu = &per_cpu(cpuinfo, i);
+
+				if (i == data || picpu->prev_load <
+						up_threshold_any_cpu_load)
+					continue;
+
+				max_load = max(max_load, picpu->prev_load);
+				max_freq = max(max_freq, picpu->target_freq);
+			}
+
+			if (max_freq > up_threshold_any_cpu_freq &&
+				max_load >= up_threshold_any_cpu_load)
+				new_freq = sync_freq;
+		}
+	}
+
+	if (cpu_load <= MAX_LOCAL_LOAD &&
+	    pcpu->policy->cur >= hispeed_freq &&
+	    new_freq > pcpu->policy->cur &&
+	    now - pcpu->hispeed_validate_time <
+	    freq_to_above_hispeed_delay(pcpu->policy->cur)) {
+		trace_cpufreq_barry_allen_notyet(
+			data, cpu_load, pcpu->target_freq,
+			pcpu->policy->cur, new_freq);
+		goto rearm;
+	}
+
+	if (cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table,
+					   new_freq, CPUFREQ_RELATION_C,
+					   &index))
+		goto rearm;
+
+	new_freq = pcpu->freq_table[index].frequency;
+
+	/*
+	 * Do not scale below floor_freq unless we have been at or above the
+	 * floor frequency for the minimum sample time since last validated.
+	 */
+	if (sampling_down_factor && pcpu->policy->cur == pcpu->policy->max)
+		mod_min_sample_time = sampling_down_factor;
+	else
+		mod_min_sample_time = min_sample_time;
+
+	if (pcpu->limits_changed) {
+		if (sampling_down_factor &&
+			(pcpu->policy->cur != pcpu->policy->max))
+			mod_min_sample_time = 0;
+
+		pcpu->limits_changed = false;
+	}
+
+	if (new_freq < pcpu->floor_freq) {
+		if (now - pcpu->floor_validate_time < mod_min_sample_time) {
+			trace_cpufreq_barry_allen_notyet(
+				data, cpu_load, pcpu->target_freq,
+				pcpu->policy->cur, new_freq);
+			goto rearm;
+		}
+	}
+
+	/*
+	 * Update the timestamp for checking whether speed has been held at
+	 * or above the selected frequency for a minimum of min_sample_time,
+	 * if not boosted to hispeed_freq.  If boosted to hispeed_freq then we
+	 * allow the speed to drop as soon as the boostpulse duration expires
+	 * (or the indefinite boost is turned off).
+	 */
+
+	if (!boosted || new_freq > hispeed_freq) {
+		pcpu->floor_freq = new_freq;
+		pcpu->floor_validate_time = now;
+	}
+
+	if (pcpu->target_freq == new_freq &&
+			pcpu->target_freq <= pcpu->policy->cur) {
+		trace_cpufreq_barry_allen_already(
+			data, cpu_load, pcpu->target_freq,
+			pcpu->policy->cur, new_freq);
+		goto rearm_if_notmax;
+	}
+
+	trace_cpufreq_barry_allen_target(data, cpu_load, pcpu->target_freq,
+					 pcpu->policy->cur, new_freq);
+
+	pcpu->target_freq = new_freq;
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+	cpumask_set_cpu(data, &speedchange_cpumask);
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+	wake_up_process(speedchange_task);
+
+rearm_if_notmax:
+	/*
+	 * Already set max speed and don't see a need to change that,
+	 * wait until next idle to re-evaluate, don't need timer.
+	 */
+	if (pcpu->target_freq == pcpu->policy->max)
+		goto exit;
+
+rearm:
+	if (!timer_pending(&pcpu->cpu_timer))
+		cpufreq_barry_allen_timer_resched(pcpu);
+
+exit:
+	up_read(&pcpu->enable_sem);
+	return;
+}
+
+static void cpufreq_barry_allen_idle_start(void)
+{
+	struct cpufreq_barry_allen_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+	int pending;
+	u64 now;
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled) {
+		up_read(&pcpu->enable_sem);
+		return;
+	}
+
+	pending = timer_pending(&pcpu->cpu_timer);
+
+	if (pcpu->target_freq != pcpu->policy->min) {
+		/*
+		 * Entering idle while not at lowest speed.  On some
+		 * platforms this can hold the other CPU(s) at that speed
+		 * even though the CPU is idle. Set a timer to re-evaluate
+		 * speed so this idle CPU doesn't hold the other CPUs above
+		 * min indefinitely.  This should probably be a quirk of
+		 * the CPUFreq driver.
+		 */
+		if (!pending) {
+			cpufreq_barry_allen_timer_resched(pcpu);
+
+			now = ktime_to_us(ktime_get());
+			if ((pcpu->policy->cur == pcpu->policy->max) &&
+				(now - pcpu->hispeed_validate_time) >
+							MIN_BUSY_TIME) {
+				pcpu->floor_validate_time = now;
+			}
+
+		}
+	}
+
+	up_read(&pcpu->enable_sem);
+}
+
+static void cpufreq_barry_allen_idle_end(void)
+{
+	struct cpufreq_barry_allen_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled) {
+		up_read(&pcpu->enable_sem);
+		return;
+	}
+
+	/* Arm the timer for 1-2 ticks later if not already. */
+	if (!timer_pending(&pcpu->cpu_timer)) {
+		cpufreq_barry_allen_timer_resched(pcpu);
+	} else if (time_after_eq(jiffies, pcpu->cpu_timer.expires)) {
+		del_timer(&pcpu->cpu_timer);
+		del_timer(&pcpu->cpu_slack_timer);
+		cpufreq_barry_allen_timer(smp_processor_id());
+	}
+
+	up_read(&pcpu->enable_sem);
+}
+
+static int cpufreq_barry_allen_speedchange_task(void *data)
+{
+	unsigned int cpu;
+	cpumask_t tmp_mask;
+	unsigned long flags;
+	struct cpufreq_barry_allen_cpuinfo *pcpu;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+		if (cpumask_empty(&speedchange_cpumask)) {
+			spin_unlock_irqrestore(&speedchange_cpumask_lock,
+					       flags);
+			schedule();
+
+			if (kthread_should_stop())
+				break;
+
+			spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+		}
+
+		set_current_state(TASK_RUNNING);
+		tmp_mask = speedchange_cpumask;
+		cpumask_clear(&speedchange_cpumask);
+		spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+		for_each_cpu(cpu, &tmp_mask) {
+			unsigned int j;
+			unsigned int max_freq = 0;
+			struct cpufreq_barry_allen_cpuinfo *pjcpu;
+
+			pcpu = &per_cpu(cpuinfo, cpu);
+			if (!down_read_trylock(&pcpu->enable_sem))
+				continue;
+			if (!pcpu->governor_enabled) {
+				up_read(&pcpu->enable_sem);
+				continue;
+			}
+
+			for_each_cpu(j, pcpu->policy->cpus) {
+				pjcpu = &per_cpu(cpuinfo, j);
+
+				if (pjcpu->target_freq > max_freq)
+					max_freq = pjcpu->target_freq;
+			}
+
+			if (max_freq != pcpu->policy->cur) {
+				u64 now;
+				__cpufreq_driver_target(pcpu->policy,
+							max_freq,
+							CPUFREQ_RELATION_H);
+				now = ktime_to_us(ktime_get());
+				for_each_cpu(j, pcpu->policy->cpus) {
+					pjcpu = &per_cpu(cpuinfo, j);
+					pjcpu->hispeed_validate_time = now;
+				}
+			}
+			trace_cpufreq_barry_allen_setspeed(cpu,
+						     pcpu->target_freq,
+						     pcpu->policy->cur);
+
+			up_read(&pcpu->enable_sem);
+		}
+	}
+
+	return 0;
+}
+
+static void cpufreq_barry_allen_boost(void)
+{
+	int i;
+	int anyboost = 0;
+	unsigned long flags;
+	struct cpufreq_barry_allen_cpuinfo *pcpu;
+
+	boosted = true;
+	
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+	for_each_online_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+
+		if (pcpu->target_freq < hispeed_freq) {
+			pcpu->target_freq = hispeed_freq;
+			cpumask_set_cpu(i, &speedchange_cpumask);
+			pcpu->hispeed_validate_time =
+				ktime_to_us(ktime_get());
+			anyboost = 1;
+		}
+
+		/*
+		 * Set floor freq and (re)start timer for when last
+		 * validated.
+		 */
+
+		pcpu->floor_freq = hispeed_freq;
+		pcpu->floor_validate_time = ktime_to_us(ktime_get());
+	}
+
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+	if (anyboost)
+		wake_up_process(speedchange_task);
+}
+
+static int cpufreq_barry_allen_notifier(
+	struct notifier_block *nb, unsigned long val, void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpufreq_barry_allen_cpuinfo *pcpu;
+	int cpu;
+	unsigned long flags;
+
+	if (val == CPUFREQ_PRECHANGE) {
+		pcpu = &per_cpu(cpuinfo, freq->cpu);
+		if (!down_read_trylock(&pcpu->enable_sem))
+			return 0;
+		if (!pcpu->governor_enabled) {
+			up_read(&pcpu->enable_sem);
+			return 0;
+		}
+
+		for_each_cpu(cpu, pcpu->policy->cpus) {
+			struct cpufreq_barry_allen_cpuinfo *pjcpu =
+				&per_cpu(cpuinfo, cpu);
+			if (cpu != freq->cpu) {
+				if (!down_read_trylock(&pjcpu->enable_sem))
+					continue;
+				if (!pjcpu->governor_enabled) {
+					up_read(&pjcpu->enable_sem);
+					continue;
+				}
+			}
+			spin_lock_irqsave(&pjcpu->load_lock, flags);
+			update_load(cpu);
+			spin_unlock_irqrestore(&pjcpu->load_lock, flags);
+			if (cpu != freq->cpu)
+				up_read(&pjcpu->enable_sem);
+		}
+
+		up_read(&pcpu->enable_sem);
+	}
+	return 0;
+}
+
+static struct notifier_block cpufreq_notifier_block = {
+	.notifier_call = cpufreq_barry_allen_notifier,
+};
+
+static unsigned int *get_tokenized_data(const char *buf, int *num_tokens)
+{
+	const char *cp;
+	int i;
+	int ntokens = 1;
+	unsigned int *tokenized_data;
+	int err = -EINVAL;
+
+	cp = buf;
+	while ((cp = strpbrk(cp + 1, " :")))
+		ntokens++;
+
+	if (!(ntokens & 0x1))
+		goto err;
+
+	tokenized_data = kmalloc(ntokens * sizeof(unsigned int), GFP_KERNEL);
+	if (!tokenized_data) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	cp = buf;
+	i = 0;
+	while (i < ntokens) {
+		if (sscanf(cp, "%u", &tokenized_data[i++]) != 1)
+			goto err_kfree;
+
+		cp = strpbrk(cp, " :");
+		if (!cp)
+			break;
+		cp++;
+	}
+
+	if (i != ntokens)
+		goto err_kfree;
+
+	*num_tokens = ntokens;
+	return tokenized_data;
+
+err_kfree:
+	kfree(tokenized_data);
+err:
+	return ERR_PTR(err);
+}
+
+static ssize_t show_target_loads(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+
+	for (i = 0; i < ntarget_loads; i++)
+		ret += sprintf(buf + ret, "%u%s", target_loads[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+static ssize_t store_target_loads(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_target_loads = NULL;
+	unsigned long flags;
+
+	if (ba_locked)
+		return count;
+
+	new_target_loads = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_target_loads))
+		return PTR_RET(new_target_loads);
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+	if (target_loads != default_target_loads)
+		kfree(target_loads);
+	target_loads = new_target_loads;
+	ntarget_loads = ntokens;
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return count;
+}
+
+static struct global_attr target_loads_attr =
+	__ATTR(target_loads, S_IRUGO | S_IWUSR,
+		show_target_loads, store_target_loads);
+
+static ssize_t show_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay; i++)
+		ret += sprintf(buf + ret, "%u%s", above_hispeed_delay[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static ssize_t store_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_above_hispeed_delay = NULL;
+	unsigned long flags;
+
+	new_above_hispeed_delay = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_above_hispeed_delay))
+		return PTR_RET(new_above_hispeed_delay);
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+	if (above_hispeed_delay != default_above_hispeed_delay)
+		kfree(above_hispeed_delay);
+	above_hispeed_delay = new_above_hispeed_delay;
+	nabove_hispeed_delay = ntokens;
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return count;
+
+}
+
+static struct global_attr above_hispeed_delay_attr =
+	__ATTR(above_hispeed_delay, S_IRUGO | S_IWUSR,
+		show_above_hispeed_delay, store_above_hispeed_delay);
+
+static ssize_t show_hispeed_freq(struct kobject *kobj,
+				 struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", hispeed_freq);
+}
+
+static ssize_t store_hispeed_freq(struct kobject *kobj,
+				  struct attribute *attr, const char *buf,
+				  size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+	if (ba_locked)
+		return count;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	hispeed_freq = val;
+	return count;
+}
+
+static struct global_attr hispeed_freq_attr = __ATTR(hispeed_freq, 0644,
+		show_hispeed_freq, store_hispeed_freq);
+
+static ssize_t show_sampling_down_factor(struct kobject *kobj,
+				struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sampling_down_factor);
+}
+
+static ssize_t store_sampling_down_factor(struct kobject *kobj,
+				struct attribute *attr, const char *buf,
+				size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+	if (ba_locked)
+		return count;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	sampling_down_factor = val;
+	return count;
+}
+
+static struct global_attr sampling_down_factor_attr =
+				__ATTR(sampling_down_factor, 0644,
+		show_sampling_down_factor, store_sampling_down_factor);
+
+static ssize_t show_go_hispeed_load(struct kobject *kobj,
+				     struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", go_hispeed_load);
+}
+
+static ssize_t store_go_hispeed_load(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	if (ba_locked)
+		return count;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	go_hispeed_load = val;
+	return count;
+}
+
+static struct global_attr go_hispeed_load_attr = __ATTR(go_hispeed_load, 0644,
+		show_go_hispeed_load, store_go_hispeed_load);
+
+static ssize_t show_min_sample_time(struct kobject *kobj,
+				struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", min_sample_time);
+}
+
+static ssize_t store_min_sample_time(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	if (ba_locked)
+		return count;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	min_sample_time = val;
+	return count;
+}
+
+static struct global_attr min_sample_time_attr = __ATTR(min_sample_time, 0644,
+		show_min_sample_time, store_min_sample_time);
+
+static ssize_t show_timer_rate(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", timer_rate);
+}
+
+static ssize_t store_timer_rate(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val, val_round;
+
+	if (ba_locked)
+		return count;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	val_round = jiffies_to_usecs(usecs_to_jiffies(val));
+	if (val != val_round)
+		pr_warn("timer_rate not aligned to jiffy. Rounded up to %lu\n",
+				val_round);
+
+	timer_rate = val_round;
+	return count;
+}
+
+static struct global_attr timer_rate_attr = __ATTR(timer_rate, 0644,
+		show_timer_rate, store_timer_rate);
+
+static ssize_t show_timer_slack(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", timer_slack_val);
+}
+
+static ssize_t store_timer_slack(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	if (ba_locked)
+		return count;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret < 0)
+		return ret;
+
+	timer_slack_val = val;
+	return count;
+}
+
+static ssize_t show_ba_locked(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	if (ba_locked)
+		return snprintf(buf, PAGE_SIZE, "1\n");
+	
+	return snprintf(buf, PAGE_SIZE, "0\n");
+}
+
+static ssize_t store_ba_locked(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret < 0)
+		return ret;
+
+	if (val == 1)
+		ba_locked = true;
+	else
+		ba_locked = false;
+	
+	return count;
+}
+
+static struct global_attr ba_locked_attr = __ATTR(ba_locked, 0644,
+		show_ba_locked, store_ba_locked);
+
+define_one_global_rw(timer_slack);
+
+static ssize_t show_boost(struct kobject *kobj, struct attribute *attr,
+			  char *buf)
+{
+	return sprintf(buf, "%d\n", boost_val);
+}
+
+static ssize_t store_boost(struct kobject *kobj, struct attribute *attr,
+			   const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	if (ba_locked)
+		return count;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boost_val = val;
+
+	if (boost_val) {
+		trace_cpufreq_barry_allen_boost("on");
+		if (!boosted)
+			cpufreq_barry_allen_boost();
+	} else {
+		trace_cpufreq_barry_allen_unboost("off");
+	}
+
+	return count;
+}
+
+define_one_global_rw(boost);
+
+static ssize_t store_boostpulse(struct kobject *kobj, struct attribute *attr,
+				const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	if (ba_locked)
+		return count;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_endtime = ktime_to_us(ktime_get()) + boostpulse_duration_val;
+	trace_cpufreq_barry_allen_boost("pulse");
+	if (!boosted)
+		cpufreq_barry_allen_boost();
+	return count;
+}
+
+static struct global_attr boostpulse =
+	__ATTR(boostpulse, 0200, NULL, store_boostpulse);
+
+static ssize_t show_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", boostpulse_duration_val);
+}
+
+static ssize_t store_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	if (ba_locked)
+		return count;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_duration_val = val;
+	return count;
+}
+
+define_one_global_rw(boostpulse_duration);
+
+static ssize_t show_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	if (ba_locked)
+		return count;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	io_is_busy = val;
+	return count;
+}
+
+static struct global_attr io_is_busy_attr = __ATTR(io_is_busy, 0644,
+		show_io_is_busy, store_io_is_busy);
+
+static ssize_t show_sync_freq(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sync_freq);
+}
+
+static ssize_t store_sync_freq(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	if (ba_locked)
+		return count;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	sync_freq = val;
+	return count;
+}
+
+static struct global_attr sync_freq_attr = __ATTR(sync_freq, 0644,
+		show_sync_freq, store_sync_freq);
+
+static ssize_t show_up_threshold_any_cpu_load(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", up_threshold_any_cpu_load);
+}
+
+static ssize_t store_up_threshold_any_cpu_load(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	if (ba_locked)
+		return count;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	up_threshold_any_cpu_load = val;
+	return count;
+}
+
+static struct global_attr up_threshold_any_cpu_load_attr =
+		__ATTR(up_threshold_any_cpu_load, 0644,
+		show_up_threshold_any_cpu_load,
+				store_up_threshold_any_cpu_load);
+
+static ssize_t show_up_threshold_any_cpu_freq(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", up_threshold_any_cpu_freq);
+}
+
+static ssize_t store_up_threshold_any_cpu_freq(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	if (ba_locked)
+		return count;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	up_threshold_any_cpu_freq = val;
+	return count;
+}
+
+static struct global_attr up_threshold_any_cpu_freq_attr =
+		__ATTR(up_threshold_any_cpu_freq, 0644,
+		show_up_threshold_any_cpu_freq,
+				store_up_threshold_any_cpu_freq);
+
+static struct attribute *barry_allen_attributes[] = {
+	&ba_locked_attr.attr,
+	&target_loads_attr.attr,
+	&above_hispeed_delay_attr.attr,
+	&hispeed_freq_attr.attr,
+	&go_hispeed_load_attr.attr,
+	&min_sample_time_attr.attr,
+	&timer_rate_attr.attr,
+	&timer_slack.attr,
+	&boost.attr,
+	&boostpulse.attr,
+	&boostpulse_duration.attr,
+	&io_is_busy_attr.attr,
+	&sampling_down_factor_attr.attr,
+	&sync_freq_attr.attr,
+	&up_threshold_any_cpu_load_attr.attr,
+	&up_threshold_any_cpu_freq_attr.attr,
+	NULL,
+};
+
+static struct attribute_group barry_allen_attr_group = {
+	.attrs = barry_allen_attributes,
+	.name = "barry_allen",
+};
+
+static int cpufreq_barry_allen_idle_notifier(struct notifier_block *nb,
+					     unsigned long val,
+					     void *data)
+{
+	switch (val) {
+	case IDLE_START:
+		cpufreq_barry_allen_idle_start();
+		break;
+	case IDLE_END:
+		cpufreq_barry_allen_idle_end();
+		break;
+	}
+
+	return 0;
+}
+
+static struct notifier_block cpufreq_barry_allen_idle_nb = {
+	.notifier_call = cpufreq_barry_allen_idle_notifier,
+};
+
+static int cpufreq_governor_barry_allen(struct cpufreq_policy *policy,
+		unsigned int event)
+{
+	int rc;
+	unsigned int j;
+	struct cpufreq_barry_allen_cpuinfo *pcpu;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned long expire_time;
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+
+		mutex_lock(&gov_lock);
+
+		freq_table =
+			cpufreq_frequency_get_table(policy->cpu);
+		if (!hispeed_freq)
+			hispeed_freq = policy->max;
+
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			pcpu->policy = policy;
+			pcpu->target_freq = policy->cur;
+			pcpu->freq_table = freq_table;
+			pcpu->floor_freq = pcpu->target_freq;
+			pcpu->floor_validate_time =
+				ktime_to_us(ktime_get());
+			pcpu->hispeed_validate_time =
+				pcpu->floor_validate_time;
+			down_write(&pcpu->enable_sem);
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			cpufreq_barry_allen_timer_start(j, 0);
+			pcpu->governor_enabled = 1;
+			up_write(&pcpu->enable_sem);
+		}
+
+		/*
+		 * Do not register the idle hook and create sysfs
+		 * entries if we have already done so.
+		 */
+		if (++active_count > 1) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		if (!have_governor_per_policy())
+			WARN_ON(cpufreq_get_global_kobject());
+
+		rc = sysfs_create_group(get_governor_parent_kobj(policy),
+				&barry_allen_attr_group);
+		if (rc) {
+			mutex_unlock(&gov_lock);
+			return rc;
+		}
+
+		idle_notifier_register(&cpufreq_barry_allen_idle_nb);
+		cpufreq_register_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		mutex_unlock(&gov_lock);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		mutex_lock(&gov_lock);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			down_write(&pcpu->enable_sem);
+			pcpu->governor_enabled = 0;
+			pcpu->target_freq = 0;
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			up_write(&pcpu->enable_sem);
+		}
+
+		if (--active_count > 0) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		cpufreq_unregister_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		idle_notifier_unregister(&cpufreq_barry_allen_idle_nb);
+		sysfs_remove_group(get_governor_parent_kobj(policy),
+				&barry_allen_attr_group);
+		if (!have_governor_per_policy())
+			cpufreq_put_global_kobject();
+		mutex_unlock(&gov_lock);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		if (policy->max < policy->cur)
+			__cpufreq_driver_target(policy,
+					policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > policy->cur)
+			__cpufreq_driver_target(policy,
+					policy->min, CPUFREQ_RELATION_C);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+
+			/* hold write semaphore to avoid race */
+			down_write(&pcpu->enable_sem);
+			if (pcpu->governor_enabled == 0) {
+				up_write(&pcpu->enable_sem);
+				continue;
+			}
+
+			/* update target_freq firstly */
+			if (policy->max < pcpu->target_freq)
+				pcpu->target_freq = policy->max;
+			/*
+			 * Delete and reschedule timer.
+			 * Else the timer callback may return without
+			 * re-arming the timer when it fails to acquire
+			 * the semaphore. This race condition may cause the
+			 * timer to stop unexpectedly.
+			 */
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+
+			if (pcpu->nr_timer_resched) {
+				if (pcpu->policy->min >= pcpu->target_freq)
+					pcpu->target_freq = pcpu->policy->min;
+				/*
+				 * To avoid deferring load evaluation for a
+				 * long time rearm the timer for the same jiffy
+				 * as it was supposed to fire at, if it has
+				 * already been rescheduled once. The timer
+				 * start and rescheduling functions aren't used
+				 * here so that the timestamps used for load
+				 * calculations do not get reset.
+				 */
+				add_timer_on(&pcpu->cpu_timer, j);
+				if (timer_slack_val >= 0 && pcpu->target_freq >
+							pcpu->policy->min)
+					add_timer_on(&pcpu->cpu_slack_timer, j);
+			} else if (policy->min >= pcpu->target_freq) {
+				pcpu->target_freq = policy->min;
+				/*
+				 * Reschedule timer.
+				 * The governor needs more time to evaluate
+				 * the load after changing policy parameters.
+				 */
+				cpufreq_barry_allen_timer_start(j, 0);
+				pcpu->nr_timer_resched++;
+			} else {
+				/*
+				 * Reschedule timer with variable duration.
+				 * No boost was applied so the governor
+				 * doesn't need extra time to evaluate load.
+				 * The timer can be set to fire quicker if it
+				 * was already going to expire soon.
+				 */
+				expire_time = pcpu->cpu_timer.expires - jiffies;
+				expire_time = min(usecs_to_jiffies(timer_rate),
+						  expire_time);
+				expire_time = max(MIN_TIMER_JIFFIES,
+						  expire_time);
+
+				cpufreq_barry_allen_timer_start(j, expire_time);
+				pcpu->nr_timer_resched++;
+			}
+			pcpu->limits_changed = true;
+			up_write(&pcpu->enable_sem);
+		}
+		break;
+	}
+	return 0;
+}
+
+static void cpufreq_barry_allen_nop_timer(unsigned long data)
+{
+}
+
+static int __init cpufreq_barry_allen_init(void)
+{
+	unsigned int i;
+	struct cpufreq_barry_allen_cpuinfo *pcpu;
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+
+	/* Initalize per-cpu timers */
+	for_each_possible_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+		init_timer_deferrable(&pcpu->cpu_timer);
+		pcpu->cpu_timer.function = cpufreq_barry_allen_timer;
+		pcpu->cpu_timer.data = i;
+		init_timer(&pcpu->cpu_slack_timer);
+		pcpu->cpu_slack_timer.function = cpufreq_barry_allen_nop_timer;
+		spin_lock_init(&pcpu->load_lock);
+		init_rwsem(&pcpu->enable_sem);
+	}
+
+	spin_lock_init(&target_loads_lock);
+	spin_lock_init(&speedchange_cpumask_lock);
+	spin_lock_init(&above_hispeed_delay_lock);
+	mutex_init(&gov_lock);
+	speedchange_task =
+		kthread_create(cpufreq_barry_allen_speedchange_task, NULL,
+			       "cfbarry_allen");
+	if (IS_ERR(speedchange_task))
+		return PTR_ERR(speedchange_task);
+
+	sched_setscheduler_nocheck(speedchange_task, SCHED_FIFO, &param);
+	get_task_struct(speedchange_task);
+
+	/* NB: wake up so the thread does not look hung to the freezer */
+	wake_up_process(speedchange_task);
+
+	return cpufreq_register_governor(&cpufreq_gov_barry_allen);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_BARRY_ALLEN
+fs_initcall(cpufreq_barry_allen_init);
+#else
+module_init(cpufreq_barry_allen_init);
+#endif
+
+static void __exit cpufreq_barry_allen_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_barry_allen);
+	kthread_stop(speedchange_task);
+	put_task_struct(speedchange_task);
+}
+
+module_exit(cpufreq_barry_allen_exit);
+
+MODULE_AUTHOR("Javier Sayago <admin@lonasdigital.com>");
+MODULE_DESCRIPTION("'cpufreq_barry_allen' - A cpufreq governor for "
+	"Latency sensitive workloads");
+MODULE_LICENSE("GPL");
diff --git a/drivers/cpufreq/cpufreq_bioshock.c b/drivers/cpufreq/cpufreq_bioshock.c
new file mode 100755
index 0000000..e424123
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_bioshock.c
@@ -0,0 +1,590 @@
+/*
+ * drivers/cpufreq/cpufreq_bioshock.c
+ *
+ * Based on the Conservative governor by:
+ *
+ *    Copyright (C)  2001 Russell King
+ *              (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                        Jun Nakajima <jun.nakajima@intel.com>
+ *              (C)  2009 Alexander Clouter <alex@digriz.org.uk>
+ *              (C)  2014 Jamison904 <infamousprollc@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <asm/cputime.h>
+#include <linux/kthread.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/cpumask.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/input.h>
+#include <linux/workqueue.h>
+#include <linux/slab.h>
+#ifdef CONFIG_HAS_EARLYSUSPEND
+#include <linux/earlysuspend.h>
+#endif
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+#define DEF_FREQUENCY_UP_THRESHOLD		(70)
+#define DEF_FREQUENCY_DOWN_THRESHOLD		(30)
+
+/*
+ * The polling frequency of this governor depends on the capability of
+ * the processor. Default polling frequency is 1000 times the transition
+ * latency of the processor. The governor will work on any processor with
+ * transition latency <= 10mS, using appropriate sampling
+ * rate.
+ * For CPUs with transition latency > 10mS (mostly drivers with CPUFREQ_ETERNAL)
+ * this governor will not work.
+ * All times here are in uS.
+ */
+#define MIN_SAMPLING_RATE_RATIO			(2)
+
+static unsigned int min_sampling_rate;
+
+#define LATENCY_MULTIPLIER			(1000)
+#define MIN_LATENCY_MULTIPLIER			(100)
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define MAX_SAMPLING_DOWN_FACTOR		(10)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+static void do_dbs_timer(struct work_struct *work);
+
+struct cpu_dbs_info_s {
+	cputime64_t prev_cpu_idle;
+	cputime64_t prev_cpu_wall;
+	cputime64_t prev_cpu_nice;
+	struct cpufreq_policy *cur_policy;
+	struct delayed_work work;
+	unsigned int down_skip;
+	unsigned int requested_freq;
+	int cpu;
+	unsigned int enable:1;
+	/*
+	 * percpu mutex that serializes governor limit change with
+	 * do_dbs_timer invocation. We do not want do_dbs_timer to run
+	 * when user is changing the governor or limits.
+	 */
+	struct mutex timer_mutex;
+};
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, cs_cpu_dbs_info);
+
+static unsigned int dbs_enable;	/* number of CPUs using this policy */
+
+/*
+ * dbs_mutex protects dbs_enable in governor start/stop.
+ */
+static DEFINE_MUTEX(dbs_mutex);
+
+static struct dbs_tuners {
+	unsigned int sampling_rate;
+	unsigned int sampling_down_factor;
+	unsigned int up_threshold;
+	unsigned int down_threshold;
+	unsigned int ignore_nice;
+	unsigned int freq_step;
+} dbs_tuners_ins = {
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.down_threshold = DEF_FREQUENCY_DOWN_THRESHOLD,
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.ignore_nice = 0,
+	.freq_step = 5,
+};
+
+/* keep track of frequency transitions */
+static int
+dbs_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
+		     void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpu_dbs_info_s *this_dbs_info = &per_cpu(cs_cpu_dbs_info,
+							freq->cpu);
+
+	struct cpufreq_policy *policy;
+
+	if (!this_dbs_info->enable)
+		return 0;
+
+	policy = this_dbs_info->cur_policy;
+
+	/*
+	 * we only care if our internally tracked freq moves outside
+	 * the 'valid' ranges of freqency available to us otherwise
+	 * we do not change it
+	*/
+	if (this_dbs_info->requested_freq > policy->max
+			|| this_dbs_info->requested_freq < policy->min)
+		this_dbs_info->requested_freq = freq->new;
+
+	return 0;
+}
+
+static struct notifier_block dbs_cpufreq_notifier_block = {
+	.notifier_call = dbs_cpufreq_notifier
+};
+
+/************************** sysfs interface ************************/
+static ssize_t show_sampling_rate_min(struct kobject *kobj,
+				      struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", min_sampling_rate);
+}
+
+define_one_global_ro(sampling_rate_min);
+
+/* cpufreq_conservativex Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(sampling_down_factor, sampling_down_factor);
+show_one(up_threshold, up_threshold);
+show_one(down_threshold, down_threshold);
+show_one(ignore_nice_load, ignore_nice);
+show_one(freq_step, freq_step);
+
+static ssize_t store_sampling_down_factor(struct kobject *a,
+					  struct attribute *b,
+					  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_down_factor = input;
+	return count;
+}
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_rate = max(input, min_sampling_rate);
+	return count;
+}
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 ||
+			input <= dbs_tuners_ins.down_threshold)
+		return -EINVAL;
+
+	dbs_tuners_ins.up_threshold = input;
+	return count;
+}
+
+static ssize_t store_down_threshold(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	/* cannot be lower than 11 otherwise freq will not fall */
+	if (ret != 1 || input < 11 || input > 100 ||
+			input >= dbs_tuners_ins.up_threshold)
+		return -EINVAL;
+
+	dbs_tuners_ins.down_threshold = input;
+	return count;
+}
+
+static ssize_t store_ignore_nice_load(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	unsigned int j;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == dbs_tuners_ins.ignore_nice) /* nothing to do */
+		return count;
+
+	dbs_tuners_ins.ignore_nice = input;
+
+	/* we need to re-evaluate prev_cpu_idle */
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+		dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&dbs_info->prev_cpu_wall, 0);
+		if (dbs_tuners_ins.ignore_nice)
+			dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+	}
+	return count;
+}
+
+static ssize_t store_freq_step(struct kobject *a, struct attribute *b,
+			       const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 100)
+		input = 100;
+
+	/* no need to test here if freq_step is zero as the user might actually
+	 * want this, they would be crazy though :) */
+	dbs_tuners_ins.freq_step = input;
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(sampling_down_factor);
+define_one_global_rw(up_threshold);
+define_one_global_rw(down_threshold);
+define_one_global_rw(ignore_nice_load);
+define_one_global_rw(freq_step);
+
+static struct attribute *dbs_attributes[] = {
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&sampling_down_factor.attr,
+	&up_threshold.attr,
+	&down_threshold.attr,
+	&ignore_nice_load.attr,
+	&freq_step.attr,
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "bioshock",
+};
+
+/************************** sysfs end ************************/
+
+static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
+{
+	unsigned int load = 0;
+	unsigned int max_load = 0;
+	unsigned int freq_target;
+
+	struct cpufreq_policy *policy;
+	unsigned int j;
+
+	policy = this_dbs_info->cur_policy;
+
+	/*
+	 * Every sampling_rate, we check, if current idle time is less
+	 * than 20% (default), then we try to increase frequency
+	 * Every sampling_rate*sampling_down_factor, we check, if current
+	 * idle time is more than 80%, then we try to decrease frequency
+	 *
+	 * Any frequency increase takes it to the maximum frequency.
+	 * Frequency reduction happens at minimum steps of
+	 * 5% (default) of maximum frequency
+	 */
+
+	/* Get Absolute Load */
+	for_each_cpu(j, policy->cpus) {
+		struct cpu_dbs_info_s *j_dbs_info;
+		cputime64_t cur_wall_time, cur_idle_time;
+		unsigned int idle_time, wall_time;
+
+		j_dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time, 0);
+
+		wall_time = (unsigned int)
+			(cur_wall_time - j_dbs_info->prev_cpu_wall);
+		j_dbs_info->prev_cpu_wall = cur_wall_time;
+
+		idle_time = (unsigned int)
+			(cur_idle_time - j_dbs_info->prev_cpu_idle);
+		j_dbs_info->prev_cpu_idle = cur_idle_time;
+
+		if (dbs_tuners_ins.ignore_nice) {
+			cputime64_t cur_nice;
+			unsigned long cur_nice_jiffies;
+
+			cur_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE] -
+					 j_dbs_info->prev_cpu_nice;
+			/*
+			 * Assumption: nice time between sampling periods will
+			 * be less than 2^32 jiffies for 32 bit sys
+			 */
+			cur_nice_jiffies = (unsigned long)
+					cputime64_to_jiffies64(cur_nice);
+
+			j_dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			idle_time += jiffies_to_usecs(cur_nice_jiffies);
+		}
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		load = 100 * (wall_time - idle_time) / wall_time;
+
+		if (load > max_load)
+			max_load = load;
+	}
+
+	/*
+	 * break out if we 'cannot' reduce the speed as the user might
+	 * want freq_step to be zero
+	 */
+	if (dbs_tuners_ins.freq_step == 0)
+		return;
+
+	/* Check for frequency increase */
+	if (max_load > dbs_tuners_ins.up_threshold) {
+		this_dbs_info->down_skip = 0;
+
+		/* if we are already at full speed then break out early */
+		if (this_dbs_info->requested_freq == policy->max)
+			return;
+
+		freq_target = (dbs_tuners_ins.freq_step * policy->max) / 100;
+
+		/* max freq cannot be less than 100. But who knows.... */
+		if (unlikely(freq_target == 0))
+			freq_target = 5;
+
+		this_dbs_info->requested_freq += freq_target;
+		if (this_dbs_info->requested_freq > policy->max)
+			this_dbs_info->requested_freq = policy->max;
+
+		__cpufreq_driver_target(policy, this_dbs_info->requested_freq,
+			CPUFREQ_RELATION_H);
+		return;
+	}
+
+	/*
+	 * The optimal frequency is the frequency that is the lowest that
+	 * can support the current CPU usage without triggering the up
+	 * policy. To be safe, we focus 10 points under the threshold.
+	 */
+	if (max_load < (dbs_tuners_ins.down_threshold - 10)) {
+		freq_target = (dbs_tuners_ins.freq_step * policy->max) / 100;
+
+		this_dbs_info->requested_freq -= freq_target;
+		if (this_dbs_info->requested_freq < policy->min)
+			this_dbs_info->requested_freq = policy->min;
+
+		/*
+		 * if we cannot reduce the frequency anymore, break out early
+		 */
+		if (policy->cur == policy->min)
+			return;
+
+		__cpufreq_driver_target(policy, this_dbs_info->requested_freq,
+				CPUFREQ_RELATION_H);
+		return;
+	}
+}
+
+static void do_dbs_timer(struct work_struct *work)
+{
+	struct cpu_dbs_info_s *dbs_info =
+		container_of(work, struct cpu_dbs_info_s, work.work);
+	unsigned int cpu = dbs_info->cpu;
+
+	/* We want all CPUs to do sampling nearly on same jiffy */
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+	// delay -= jiffies % delay;
+
+	mutex_lock(&dbs_info->timer_mutex);
+
+	dbs_check_cpu(dbs_info);
+
+	schedule_delayed_work_on(cpu, &dbs_info->work, delay);
+	mutex_unlock(&dbs_info->timer_mutex);
+}
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
+{
+	/* We want all CPUs to do sampling nearly on same jiffy */
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+	// delay -= jiffies % delay;
+
+	dbs_info->enable = 1;
+	INIT_DEFERRABLE_WORK(&dbs_info->work, do_dbs_timer);
+	schedule_delayed_work_on(dbs_info->cpu, &dbs_info->work, delay);
+}
+
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
+{
+	dbs_info->enable = 0;
+	cancel_delayed_work_sync(&dbs_info->work);
+}
+
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int rc;
+
+	this_dbs_info = &per_cpu(cs_cpu_dbs_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy->cur))
+			return -EINVAL;
+
+		mutex_lock(&dbs_mutex);
+
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			j_dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&j_dbs_info->prev_cpu_wall, 0);
+			if (dbs_tuners_ins.ignore_nice) {
+				j_dbs_info->prev_cpu_nice =
+						kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			}
+		}
+		this_dbs_info->down_skip = 0;
+		this_dbs_info->requested_freq = policy->cur;
+
+		mutex_init(&this_dbs_info->timer_mutex);
+		dbs_enable++;
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 1) {
+			unsigned int latency;
+			/* policy latency is in nS. Convert it to uS first */
+			latency = policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&dbs_attr_group);
+			if (rc) {
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+
+			min_sampling_rate = 10000;
+			dbs_tuners_ins.sampling_rate = 10000;
+
+			cpufreq_register_notifier(
+					&dbs_cpufreq_notifier_block,
+					CPUFREQ_TRANSITION_NOTIFIER);
+		}
+		mutex_unlock(&dbs_mutex);
+
+		dbs_timer_init(this_dbs_info);
+
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		dbs_timer_exit(this_dbs_info);
+
+		mutex_lock(&dbs_mutex);
+		dbs_enable--;
+		mutex_destroy(&this_dbs_info->timer_mutex);
+
+		/*
+		 * Stop the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 0)
+			cpufreq_unregister_notifier(
+					&dbs_cpufreq_notifier_block,
+					CPUFREQ_TRANSITION_NOTIFIER);
+
+		mutex_unlock(&dbs_mutex);
+		if (!dbs_enable)
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &dbs_attr_group);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&this_dbs_info->timer_mutex);
+		if (policy->max < this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(
+					this_dbs_info->cur_policy,
+					policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(
+					this_dbs_info->cur_policy,
+					policy->min, CPUFREQ_RELATION_C);
+		mutex_unlock(&this_dbs_info->timer_mutex);
+
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_BIOSHOCK
+static
+#endif
+struct cpufreq_governor cpufreq_gov_bioshock = {
+	.name			= "bioshock",
+	.governor		= cpufreq_governor_dbs,
+	.max_transition_latency	= TRANSITION_LATENCY_LIMIT,
+	.owner			= THIS_MODULE,
+};
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	return cpufreq_register_governor(&cpufreq_gov_bioshock);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_bioshock);
+}
+
+MODULE_AUTHOR("James Jamison");
+MODULE_AUTHOR("Jamison904 <tinfamousprollc@gmail.com>");
+MODULE_DESCRIPTION("'cpufreq_bioshock' - The Infmous conservative-based governor.");
+MODULE_LICENSE("GPL");
+
+fs_initcall(cpufreq_gov_dbs_init);
+module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_blu_active.c b/drivers/cpufreq/cpufreq_blu_active.c
new file mode 100644
index 0000000..e706454
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_blu_active.c
@@ -0,0 +1,1206 @@
+/*
+ * drivers/cpufreq/cpufreq_blu_active.c
+ *
+ * Copyright (C) 2010 Google, Inc.
+ * Copyright (C) 2015 engstk (changes for blu_active)
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * Author: Mike Chan (mike@android.com)
+ * Author: engstk (eng.stk@sapo.pt)
+ */
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/rwsem.h>
+#include <linux/sched.h>
+#include <linux/sched/rt.h>
+#include <linux/tick.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/kernel_stat.h>
+#include <asm/cputime.h>
+#include <linux/input.h>
+
+static int active_count;
+
+struct cpufreq_interactive_cpuinfo {
+	struct timer_list cpu_timer;
+	struct timer_list cpu_slack_timer;
+	spinlock_t load_lock; /* protects the next 4 fields */
+	u64 time_in_idle;
+	u64 time_in_idle_timestamp;
+	u64 cputime_speedadj;
+	u64 cputime_speedadj_timestamp;
+	u64 last_evaluated_jiffy;
+	struct cpufreq_policy *policy;
+	struct cpufreq_frequency_table *freq_table;
+	spinlock_t target_freq_lock; /*protects target freq */
+	unsigned int target_freq;
+	unsigned int floor_freq;
+	unsigned int max_freq;
+	unsigned int min_freq;
+	u64 floor_validate_time;
+	u64 hispeed_validate_time; /* cluster hispeed_validate_time */
+	u64 local_hvtime; /* per-cpu hispeed_validate_time */
+	u64 max_freq_hyst_start_time;
+	struct rw_semaphore enable_sem;
+	int governor_enabled;
+};
+
+static DEFINE_PER_CPU(struct cpufreq_interactive_cpuinfo, cpuinfo);
+
+/* realtime thread handles frequency scaling */
+static struct task_struct *speedchange_task;
+static cpumask_t speedchange_cpumask;
+static spinlock_t speedchange_cpumask_lock;
+static struct mutex gov_lock;
+
+/* Hi speed to bump to from lo speed when load burst (default max) */
+static unsigned int hispeed_freq;
+
+/* Go to hi speed when CPU load at or above this value. */
+#define DEFAULT_GO_HISPEED_LOAD 99
+static unsigned long go_hispeed_load = DEFAULT_GO_HISPEED_LOAD;
+
+/*
+ * The minimum amount of time to spend at a frequency before we can ramp down.
+ */
+#define DEFAULT_MIN_SAMPLE_TIME (80 * USEC_PER_MSEC)
+static unsigned long min_sample_time = DEFAULT_MIN_SAMPLE_TIME;
+
+/*
+ * The sample rate of the timer used to increase frequency
+ */
+#define DEFAULT_TIMER_RATE (20 * USEC_PER_MSEC)
+static unsigned long timer_rate = DEFAULT_TIMER_RATE;
+
+/*
+ * Wait this long before raising speed above hispeed, by default a single
+ * timer interval.
+ */
+#define DEFAULT_ABOVE_HISPEED_DELAY DEFAULT_TIMER_RATE
+static unsigned int default_above_hispeed_delay[] = {
+	DEFAULT_ABOVE_HISPEED_DELAY };
+static spinlock_t above_hispeed_delay_lock;
+static unsigned int *above_hispeed_delay = default_above_hispeed_delay;
+static int nabove_hispeed_delay = ARRAY_SIZE(default_above_hispeed_delay);
+
+/* Duration of a boot pulse in usecs */
+static int boostpulse_duration_val = DEFAULT_MIN_SAMPLE_TIME;
+/* End time of boost pulse in ktime converted to usecs */
+static u64 boostpulse_endtime;
+/* input boost frequency */
+static unsigned int input_boost_freq;
+
+/*
+ * Max additional time to wait in idle, beyond timer_rate, at speeds above
+ * minimum before wakeup to reduce speed, or -1 if unnecessary.
+ */
+#define DEFAULT_TIMER_SLACK (4 * DEFAULT_TIMER_RATE)
+static int timer_slack_val = DEFAULT_TIMER_SLACK;
+
+/*
+ * Stay at max freq for at least max_freq_hysteresis before dropping
+ * frequency.
+ */
+static unsigned int max_freq_hysteresis;
+
+static bool io_is_busy;
+
+/* Don't scale frequency if load is bellow threshold */
+static unsigned int low_load_down_threshold = 10;
+
+/* Round to starting jiffy of next evaluation window */
+static u64 round_to_nw_start(u64 jif)
+{
+	unsigned long step = usecs_to_jiffies(timer_rate);
+
+	do_div(jif, step);
+	return (jif + 1) * step;
+}
+
+static void cpufreq_interactive_timer_resched(unsigned long cpu, bool slack_only)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 expires;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	expires = round_to_nw_start(pcpu->last_evaluated_jiffy);
+
+	if (!slack_only) {
+		pcpu->time_in_idle =
+			get_cpu_idle_time(smp_processor_id(),
+				  &pcpu->time_in_idle_timestamp, io_is_busy);
+		pcpu->cputime_speedadj = 0;
+		pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+		del_timer(&pcpu->cpu_timer);
+		pcpu->cpu_timer.expires = expires;
+		add_timer_on(&pcpu->cpu_timer, cpu);
+	}
+
+	if (timer_slack_val >= 0 && pcpu->target_freq > pcpu->policy->min) {
+		expires += usecs_to_jiffies(timer_slack_val);
+		del_timer(&pcpu->cpu_slack_timer);
+		pcpu->cpu_slack_timer.expires = expires;
+		add_timer_on(&pcpu->cpu_slack_timer, cpu);
+	}
+
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+/* The caller shall take enable_sem write semaphore to avoid any timer race.
+ * The cpu_timer and cpu_slack_timer must be deactivated when calling this
+ * function.
+ */
+static void cpufreq_interactive_timer_start(int cpu)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 expires = round_to_nw_start(pcpu->last_evaluated_jiffy);
+	unsigned long flags;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->cpu_timer.expires = expires;
+	add_timer_on(&pcpu->cpu_timer, cpu);
+	if (timer_slack_val >= 0 && pcpu->target_freq > pcpu->policy->min) {
+		expires += usecs_to_jiffies(timer_slack_val);
+		pcpu->cpu_slack_timer.expires = expires;
+		add_timer_on(&pcpu->cpu_slack_timer, cpu);
+	}
+
+	pcpu->time_in_idle =
+		get_cpu_idle_time(cpu, &pcpu->time_in_idle_timestamp,
+				  io_is_busy);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+static unsigned int freq_to_above_hispeed_delay(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay - 1 &&
+			freq >= above_hispeed_delay[i+1]; i += 2)
+		;
+
+	ret = above_hispeed_delay[i];
+
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static u64 update_load(int cpu)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 now;
+	u64 now_idle;
+	unsigned int delta_idle;
+	unsigned int delta_time;
+	u64 active_time;
+
+	now_idle = get_cpu_idle_time(cpu, &now, io_is_busy);
+	delta_idle = (unsigned int)(now_idle - pcpu->time_in_idle);
+	delta_time = (unsigned int)(now - pcpu->time_in_idle_timestamp);
+
+	if (delta_time <= delta_idle)
+		active_time = 0;
+	else
+		active_time = delta_time - delta_idle;
+
+	pcpu->cputime_speedadj += active_time * pcpu->policy->cur;
+
+	pcpu->time_in_idle = now_idle;
+	pcpu->time_in_idle_timestamp = now;
+	return now;
+}
+
+static void cpufreq_interactive_timer(unsigned long data)
+{
+	u64 now;
+	unsigned int delta_time;
+	u64 cputime_speedadj;
+	int cpu_load;
+	struct cpufreq_interactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, data);
+	unsigned int new_freq;
+	unsigned int loadadjfreq;
+	unsigned int index;
+	unsigned long flags;
+	bool boosted;
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled)
+		goto exit;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	now = update_load(data);
+	delta_time = (unsigned int)(now - pcpu->cputime_speedadj_timestamp);
+	cputime_speedadj = pcpu->cputime_speedadj;
+	pcpu->last_evaluated_jiffy = get_jiffies_64();
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+
+	if (WARN_ON_ONCE(!delta_time))
+		goto rearm;
+
+	spin_lock_irqsave(&pcpu->target_freq_lock, flags);
+	do_div(cputime_speedadj, delta_time);
+	loadadjfreq = (unsigned int)cputime_speedadj * 100;
+	cpu_load = loadadjfreq / pcpu->policy->cur;
+	boosted = now < boostpulse_endtime;
+
+	cpufreq_notify_utilization(pcpu->policy, cpu_load);
+
+	if (cpu_load >= go_hispeed_load) {
+		if (pcpu->policy->cur < hispeed_freq) {
+			new_freq = hispeed_freq;
+		} else {
+			new_freq = pcpu->policy->min + cpu_load * (pcpu->policy->max - pcpu->policy->min) / 100;
+
+			if (new_freq < hispeed_freq)
+				new_freq = hispeed_freq;
+		}
+	} else if (cpu_load <= low_load_down_threshold) {
+		new_freq = pcpu->policy->cpuinfo.min_freq;
+	} else {
+		new_freq = pcpu->policy->min + cpu_load * (pcpu->policy->max - pcpu->policy->min) / 100;
+	}
+	
+	if (boosted && new_freq < input_boost_freq)
+		new_freq = input_boost_freq;
+
+	if (pcpu->policy->cur >= hispeed_freq &&
+	    new_freq > pcpu->policy->cur &&
+	    now - pcpu->hispeed_validate_time <
+	    freq_to_above_hispeed_delay(pcpu->policy->cur)) {
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	pcpu->local_hvtime = now;
+
+	if (cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table,
+					   new_freq, CPUFREQ_RELATION_L,
+					   &index)) {
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	new_freq = pcpu->freq_table[index].frequency;
+
+	if (new_freq < pcpu->target_freq &&
+	    now - pcpu->max_freq_hyst_start_time < max_freq_hysteresis) {
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	/*
+	 * Do not scale below floor_freq unless we have been at or above the
+	 * floor frequency for the minimum sample time since last validated.
+	 */
+	if (new_freq < pcpu->floor_freq) {
+		if (now - pcpu->floor_validate_time < min_sample_time) {
+			spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+			goto rearm;
+		}
+	}
+
+	/*
+	 * Update the timestamp for checking whether speed has been held at
+	 * or above the selected frequency for a minimum of min_sample_time,
+	 * if not boosted to hispeed_freq.  If boosted to hispeed_freq then we
+	 * allow the speed to drop as soon as the boostpulse duration expires
+	 * (or the indefinite boost is turned off).
+	 */
+
+	if (!boosted || new_freq > hispeed_freq) {
+		pcpu->floor_freq = new_freq;
+		pcpu->floor_validate_time = now;
+	}
+
+	if (new_freq == pcpu->policy->max)
+		pcpu->max_freq_hyst_start_time = now;
+	
+	if (pcpu->target_freq == new_freq) {
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	pcpu->target_freq = new_freq;
+	spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+	cpumask_set_cpu(data, &speedchange_cpumask);
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+	wake_up_process(speedchange_task);
+
+rearm:
+	if (!timer_pending(&pcpu->cpu_timer))
+		cpufreq_interactive_timer_resched(data, false);
+
+exit:
+	up_read(&pcpu->enable_sem);
+	return;
+}
+
+static void cpufreq_interactive_idle_end(void)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled) {
+		up_read(&pcpu->enable_sem);
+		return;
+	}
+
+	/* Arm the timer for 1-2 ticks later if not already. */
+	if (!timer_pending(&pcpu->cpu_timer)) {
+		cpufreq_interactive_timer_resched(smp_processor_id(), false);
+	} else if (time_after_eq(jiffies, pcpu->cpu_timer.expires)) {
+		del_timer(&pcpu->cpu_timer);
+		del_timer(&pcpu->cpu_slack_timer);
+		cpufreq_interactive_timer(smp_processor_id());
+	}
+
+	up_read(&pcpu->enable_sem);
+}
+
+static int cpufreq_interactive_speedchange_task(void *data)
+{
+	unsigned int cpu;
+	cpumask_t tmp_mask;
+	unsigned long flags;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+		if (cpumask_empty(&speedchange_cpumask)) {
+			spin_unlock_irqrestore(&speedchange_cpumask_lock,
+					       flags);
+			schedule();
+
+			if (kthread_should_stop())
+				break;
+
+			spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+		}
+
+		set_current_state(TASK_RUNNING);
+		tmp_mask = speedchange_cpumask;
+		cpumask_clear(&speedchange_cpumask);
+		spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+		for_each_cpu(cpu, &tmp_mask) {
+			unsigned int j;
+			unsigned int max_freq = 0;
+			struct cpufreq_interactive_cpuinfo *pjcpu;
+			u64 hvt;
+
+			pcpu = &per_cpu(cpuinfo, cpu);
+			if (!down_read_trylock(&pcpu->enable_sem))
+				continue;
+			if (!pcpu->governor_enabled) {
+				up_read(&pcpu->enable_sem);
+				continue;
+			}
+
+			for_each_cpu(j, pcpu->policy->cpus) {
+				pjcpu = &per_cpu(cpuinfo, j);
+
+				if (pjcpu->target_freq > max_freq) {
+					max_freq = pjcpu->target_freq;
+					hvt = pjcpu->local_hvtime;
+				} else if (pjcpu->target_freq == max_freq) {
+					hvt = min(hvt, pjcpu->local_hvtime);
+				}
+			}
+
+			if (max_freq != pcpu->policy->cur) {
+				__cpufreq_driver_target(pcpu->policy,
+							max_freq,
+							CPUFREQ_RELATION_H);
+				for_each_cpu(j, pcpu->policy->cpus) {
+					pjcpu = &per_cpu(cpuinfo, j);
+					pjcpu->hispeed_validate_time = hvt;
+				}
+			}
+
+			up_read(&pcpu->enable_sem);
+		}
+	}
+
+	return 0;
+}
+
+static void cpufreq_interactive_boost(void)
+{
+	int i;
+	int anyboost = 0;
+	unsigned long flags[2];
+	struct cpufreq_interactive_cpuinfo *pcpu;
+
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags[0]);
+
+	for_each_online_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+		spin_lock_irqsave(&pcpu->target_freq_lock, flags[1]);
+		if (pcpu->target_freq < hispeed_freq) {
+			pcpu->target_freq = hispeed_freq;
+			cpumask_set_cpu(i, &speedchange_cpumask);
+			pcpu->hispeed_validate_time =
+				ktime_to_us(ktime_get());
+			anyboost = 1;
+		}
+
+		/*
+		 * Set floor freq and (re)start timer for when last
+		 * validated.
+		 */
+
+		pcpu->floor_freq = hispeed_freq;
+		pcpu->floor_validate_time = ktime_to_us(ktime_get());
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags[1]);
+	}
+
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags[0]);
+
+	if (anyboost)
+		wake_up_process(speedchange_task);
+}
+
+static int cpufreq_interactive_notifier(
+	struct notifier_block *nb, unsigned long val, void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+	int cpu;
+	unsigned long flags;
+
+	if (val == CPUFREQ_PRECHANGE) {
+		pcpu = &per_cpu(cpuinfo, freq->cpu);
+		if (!down_read_trylock(&pcpu->enable_sem))
+			return 0;
+		if (!pcpu->governor_enabled) {
+			up_read(&pcpu->enable_sem);
+			return 0;
+		}
+
+		for_each_cpu(cpu, pcpu->policy->cpus) {
+			struct cpufreq_interactive_cpuinfo *pjcpu =
+				&per_cpu(cpuinfo, cpu);
+			if (cpu != freq->cpu) {
+				if (!down_read_trylock(&pjcpu->enable_sem))
+					continue;
+				if (!pjcpu->governor_enabled) {
+					up_read(&pjcpu->enable_sem);
+					continue;
+				}
+			}
+			spin_lock_irqsave(&pjcpu->load_lock, flags);
+			update_load(cpu);
+			spin_unlock_irqrestore(&pjcpu->load_lock, flags);
+			if (cpu != freq->cpu)
+				up_read(&pjcpu->enable_sem);
+		}
+
+		up_read(&pcpu->enable_sem);
+	}
+	return 0;
+}
+
+static struct notifier_block cpufreq_notifier_block = {
+	.notifier_call = cpufreq_interactive_notifier,
+};
+
+static unsigned int *get_tokenized_data(const char *buf, int *num_tokens)
+{
+	const char *cp;
+	int i;
+	int ntokens = 1;
+	unsigned int *tokenized_data;
+	int err = -EINVAL;
+
+	cp = buf;
+	while ((cp = strpbrk(cp + 1, " :")))
+		ntokens++;
+
+	if (!(ntokens & 0x1))
+		goto err;
+
+	tokenized_data = kmalloc(ntokens * sizeof(unsigned int), GFP_KERNEL);
+	if (!tokenized_data) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	cp = buf;
+	i = 0;
+	while (i < ntokens) {
+		if (sscanf(cp, "%u", &tokenized_data[i++]) != 1)
+			goto err_kfree;
+
+		cp = strpbrk(cp, " :");
+		if (!cp)
+			break;
+		cp++;
+	}
+
+	if (i != ntokens)
+		goto err_kfree;
+
+	*num_tokens = ntokens;
+	return tokenized_data;
+
+err_kfree:
+	kfree(tokenized_data);
+err:
+	return ERR_PTR(err);
+}
+
+static ssize_t show_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay; i++)
+		ret += sprintf(buf + ret, "%u%s", above_hispeed_delay[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static ssize_t store_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_above_hispeed_delay = NULL;
+	unsigned long flags;
+
+	new_above_hispeed_delay = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_above_hispeed_delay))
+		return PTR_RET(new_above_hispeed_delay);
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+	if (above_hispeed_delay != default_above_hispeed_delay)
+		kfree(above_hispeed_delay);
+	above_hispeed_delay = new_above_hispeed_delay;
+	nabove_hispeed_delay = ntokens;
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return count;
+
+}
+
+static struct global_attr above_hispeed_delay_attr =
+	__ATTR(above_hispeed_delay, S_IRUGO | S_IWUSR,
+		show_above_hispeed_delay, store_above_hispeed_delay);
+
+static ssize_t show_hispeed_freq(struct kobject *kobj,
+				 struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", hispeed_freq);
+}
+
+static ssize_t store_hispeed_freq(struct kobject *kobj,
+				  struct attribute *attr, const char *buf,
+				  size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	hispeed_freq = val;
+	return count;
+}
+
+static struct global_attr hispeed_freq_attr = __ATTR(hispeed_freq, 0644,
+		show_hispeed_freq, store_hispeed_freq);
+
+static ssize_t show_max_freq_hysteresis(struct kobject *kobj,
+				     struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", max_freq_hysteresis);
+}
+
+static ssize_t store_max_freq_hysteresis(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	max_freq_hysteresis = val;
+	return count;
+}
+
+static struct global_attr max_freq_hysteresis_attr =
+	__ATTR(max_freq_hysteresis, 0644, show_max_freq_hysteresis,
+		store_max_freq_hysteresis);
+
+static ssize_t show_go_hispeed_load(struct kobject *kobj,
+				     struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", go_hispeed_load);
+}
+
+static ssize_t store_go_hispeed_load(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	go_hispeed_load = val;
+	return count;
+}
+
+static struct global_attr go_hispeed_load_attr = __ATTR(go_hispeed_load, 0644,
+		show_go_hispeed_load, store_go_hispeed_load);
+
+static ssize_t show_min_sample_time(struct kobject *kobj,
+				struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", min_sample_time);
+}
+
+static ssize_t store_min_sample_time(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	min_sample_time = val;
+	return count;
+}
+
+static struct global_attr min_sample_time_attr = __ATTR(min_sample_time, 0644,
+		show_min_sample_time, store_min_sample_time);
+
+static ssize_t show_timer_rate(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", timer_rate);
+}
+
+static ssize_t store_timer_rate(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val, val_round;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	val_round = jiffies_to_usecs(usecs_to_jiffies(val));
+	if (val != val_round)
+		pr_warn("timer_rate not aligned to jiffy. Rounded up to %lu\n",
+				val_round);
+
+	timer_rate = val_round;
+	return count;
+}
+
+static struct global_attr timer_rate_attr = __ATTR(timer_rate, 0644,
+		show_timer_rate, store_timer_rate);
+
+static ssize_t show_input_boost_freq(struct kobject *kobj, struct attribute *attr,
+                                     char *buf)
+{
+	return sprintf(buf, "%d\n", input_boost_freq);
+}
+
+static ssize_t store_input_boost_freq(struct kobject *kobj, struct attribute *attr,
+                                      const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	input_boost_freq = val;
+	return count;
+
+}
+
+static struct global_attr input_boost_freq_attr = __ATTR(input_boost_freq, 0644,
+		show_input_boost_freq, store_input_boost_freq);
+
+static ssize_t show_timer_slack(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", timer_slack_val);
+}
+
+static ssize_t store_timer_slack(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret < 0)
+		return ret;
+
+	timer_slack_val = val;
+	return count;
+}
+
+define_one_global_rw(timer_slack);
+
+static ssize_t store_boostpulse(struct kobject *kobj, struct attribute *attr,
+				const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_endtime = ktime_to_us(ktime_get()) + boostpulse_duration_val;
+	cpufreq_interactive_boost();
+	return count;
+}
+
+static struct global_attr boostpulse =
+	__ATTR(boostpulse, 0200, NULL, store_boostpulse);
+
+static ssize_t show_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", boostpulse_duration_val);
+}
+
+static ssize_t store_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_duration_val = val;
+	return count;
+}
+
+define_one_global_rw(boostpulse_duration);
+
+static ssize_t show_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	io_is_busy = val;
+	return count;
+}
+
+static struct global_attr io_is_busy_attr = __ATTR(io_is_busy, 0644,
+		show_io_is_busy, store_io_is_busy);
+
+static ssize_t show_low_load_down_threshold(struct kobject *kobj, struct attribute *attr,
+                                     char *buf)
+{
+	return sprintf(buf, "%d\n", low_load_down_threshold);
+}
+
+static ssize_t store_low_load_down_threshold(struct kobject *kobj, struct attribute *attr,
+                                      const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	if (val < 0 || val > 100)
+		low_load_down_threshold = low_load_down_threshold;
+	else
+		low_load_down_threshold = val;
+	
+	return count;
+}
+
+static struct global_attr low_load_down_threshold_attr = __ATTR(low_load_down_threshold, 0644,
+		show_low_load_down_threshold, store_low_load_down_threshold);
+
+static struct attribute *interactive_attributes[] = {
+	&above_hispeed_delay_attr.attr,
+	&hispeed_freq_attr.attr,
+	&go_hispeed_load_attr.attr,
+	&min_sample_time_attr.attr,
+	&timer_rate_attr.attr,
+	&timer_slack.attr,
+	&boostpulse.attr,
+	&boostpulse_duration.attr,
+	&input_boost_freq_attr.attr,
+	&io_is_busy_attr.attr,
+	&max_freq_hysteresis_attr.attr,
+	&low_load_down_threshold_attr.attr,
+	NULL,
+};
+
+static void interactive_input_event(struct input_handle *handle,
+		unsigned int type,
+		unsigned int code, int value)
+{
+	if (type == EV_SYN && code == SYN_REPORT) {
+		boostpulse_endtime = ktime_to_us(ktime_get()) + boostpulse_duration_val;
+	}
+}
+
+static int interactive_input_connect(struct input_handler *handler,
+		struct input_dev *dev, const struct input_device_id *id)
+{
+	struct input_handle *handle;
+	int error;
+
+	handle = kzalloc(sizeof(struct input_handle), GFP_KERNEL);
+	if (!handle)
+		return -ENOMEM;
+
+	handle->dev = dev;
+	handle->handler = handler;
+	handle->name = "cpufreq";
+
+	error = input_register_handle(handle);
+	if (error)
+		goto err2;
+
+	error = input_open_device(handle);
+	if (error)
+		goto err1;
+
+	return 0;
+err1:
+	input_unregister_handle(handle);
+err2:
+	kfree(handle);
+	return error;
+}
+
+static void interactive_input_disconnect(struct input_handle *handle)
+{
+	input_close_device(handle);
+	input_unregister_handle(handle);
+	kfree(handle);
+}
+
+static const struct input_device_id interactive_ids[] = {
+	{
+		.flags = INPUT_DEVICE_ID_MATCH_EVBIT |
+			 INPUT_DEVICE_ID_MATCH_ABSBIT,
+		.evbit = { BIT_MASK(EV_ABS) },
+		.absbit = { [BIT_WORD(ABS_MT_POSITION_X)] =
+			    BIT_MASK(ABS_MT_POSITION_X) |
+			    BIT_MASK(ABS_MT_POSITION_Y) },
+	}, /* multi-touch touchscreen */
+	{
+		.flags = INPUT_DEVICE_ID_MATCH_KEYBIT |
+			 INPUT_DEVICE_ID_MATCH_ABSBIT,
+		.keybit = { [BIT_WORD(BTN_TOUCH)] = BIT_MASK(BTN_TOUCH) },
+		.absbit = { [BIT_WORD(ABS_X)] =
+			    BIT_MASK(ABS_X) | BIT_MASK(ABS_Y) },
+	}, /* touchpad */
+};
+
+static struct input_handler interactive_input_handler = {
+	.event		= interactive_input_event,
+	.connect	= interactive_input_connect,
+	.disconnect	= interactive_input_disconnect,
+	.name		= "blu_active",
+	.id_table	= interactive_ids,
+};
+
+static struct attribute_group interactive_attr_group = {
+	.attrs = interactive_attributes,
+	.name = "blu_active",
+};
+
+
+static int cpufreq_interactive_idle_notifier(struct notifier_block *nb,
+					     unsigned long val,
+					     void *data)
+{
+	if (val == IDLE_END)
+		cpufreq_interactive_idle_end();
+
+	return 0;
+}
+
+static struct notifier_block cpufreq_interactive_idle_nb = {
+	.notifier_call = cpufreq_interactive_idle_notifier,
+};
+
+static int cpufreq_governor_blu_active(struct cpufreq_policy *policy,
+		unsigned int event)
+{
+	int rc;
+	unsigned int j;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned long flags;
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if (!cpu_online(policy->cpu))
+			return -EINVAL;
+
+		mutex_lock(&gov_lock);
+
+		freq_table =
+			cpufreq_frequency_get_table(policy->cpu);
+		if (!hispeed_freq)
+			hispeed_freq = policy->max;
+
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			pcpu->policy = policy;
+			pcpu->target_freq = policy->cur;
+			pcpu->freq_table = freq_table;
+			pcpu->floor_freq = pcpu->target_freq;
+			pcpu->floor_validate_time =
+				ktime_to_us(ktime_get());
+			pcpu->hispeed_validate_time =
+				pcpu->floor_validate_time;
+			pcpu->local_hvtime = pcpu->floor_validate_time;
+			pcpu->max_freq = policy->max;
+			pcpu->min_freq = policy->min;
+			down_write(&pcpu->enable_sem);
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			pcpu->last_evaluated_jiffy = get_jiffies_64();
+			cpufreq_interactive_timer_start(j);
+			pcpu->governor_enabled = 1;
+			up_write(&pcpu->enable_sem);
+		}
+
+		/*
+		 * Do not register the idle hook and create sysfs
+		 * entries if we have already done so.
+		 */
+		if (++active_count > 1) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		rc = sysfs_create_group(cpufreq_global_kobject,
+				&interactive_attr_group);
+		if (rc) {
+			mutex_unlock(&gov_lock);
+			return rc;
+		}
+		
+		idle_notifier_register(&cpufreq_interactive_idle_nb);
+		cpufreq_register_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		mutex_unlock(&gov_lock);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		mutex_lock(&gov_lock);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			down_write(&pcpu->enable_sem);
+			pcpu->governor_enabled = 0;
+			pcpu->target_freq = 0;
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			up_write(&pcpu->enable_sem);
+		}
+
+		if (--active_count > 0) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+		
+		cpufreq_unregister_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		idle_notifier_unregister(&cpufreq_interactive_idle_nb);
+		sysfs_remove_group(cpufreq_global_kobject,
+				&interactive_attr_group);
+		mutex_unlock(&gov_lock);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		__cpufreq_driver_target(policy,
+			policy->cur, CPUFREQ_RELATION_L);
+		
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+
+			down_read(&pcpu->enable_sem);
+			if (pcpu->governor_enabled == 0) {
+				up_read(&pcpu->enable_sem);
+				continue;
+			}
+
+			spin_lock_irqsave(&pcpu->target_freq_lock, flags);
+			if (policy->max < pcpu->target_freq)
+				pcpu->target_freq = policy->max;
+			else if (policy->min > pcpu->target_freq)
+				pcpu->target_freq = policy->min;
+
+			spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+			
+			if (policy->min < pcpu->min_freq)
+				cpufreq_interactive_timer_resched(j, true);
+			pcpu->min_freq = policy->min;
+			
+			up_read(&pcpu->enable_sem);
+
+			/* Reschedule timer.
+			 * Delete the timers, else the timer callback may
+			 * return without re-arm the timer when failed
+			 * acquire the semaphore. This race may cause timer
+			 * stopped unexpectedly.
+			 */
+			if (policy->max > pcpu->max_freq) {
+				down_write(&pcpu->enable_sem);
+				del_timer_sync(&pcpu->cpu_timer);
+				del_timer_sync(&pcpu->cpu_slack_timer);
+				cpufreq_interactive_timer_resched(j, false);
+				up_write(&pcpu->enable_sem);
+			}
+
+			pcpu->max_freq = policy->max;
+		}
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_BLU_ACTIVE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_blu_active = {
+	.name = "blu_active",
+	.governor = cpufreq_governor_blu_active,
+	.max_transition_latency = 10000000,
+	.owner = THIS_MODULE,
+};
+
+static void cpufreq_interactive_nop_timer(unsigned long data)
+{
+}
+
+static int __init cpufreq_blu_active_init(void)
+{
+	unsigned int i, rc;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+
+	/* Initalize per-cpu timers */
+	for_each_possible_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+		init_timer_deferrable(&pcpu->cpu_timer);
+		pcpu->cpu_timer.function = cpufreq_interactive_timer;
+		pcpu->cpu_timer.data = i;
+		init_timer(&pcpu->cpu_slack_timer);
+		pcpu->cpu_slack_timer.function = cpufreq_interactive_nop_timer;
+		spin_lock_init(&pcpu->load_lock);
+		spin_lock_init(&pcpu->target_freq_lock);
+		init_rwsem(&pcpu->enable_sem);
+		if (!i)
+			rc = input_register_handler(&interactive_input_handler);
+	}
+
+	spin_lock_init(&speedchange_cpumask_lock);
+	spin_lock_init(&above_hispeed_delay_lock);
+	mutex_init(&gov_lock);
+	speedchange_task =
+		kthread_create(cpufreq_interactive_speedchange_task, NULL,
+			       "cfblu_active");
+	if (IS_ERR(speedchange_task))
+		return PTR_ERR(speedchange_task);
+
+	sched_setscheduler_nocheck(speedchange_task, SCHED_FIFO, &param);
+	get_task_struct(speedchange_task);
+
+	/* NB: wake up so the thread does not look hung to the freezer */
+	wake_up_process(speedchange_task);
+
+	return cpufreq_register_governor(&cpufreq_gov_blu_active);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_BLU_ACTIVE
+fs_initcall(cpufreq_blu_active_init);
+#else
+module_init(cpufreq_blu_active_init);
+#endif
+
+static void __exit cpufreq_interactive_exit(void)
+{
+	unsigned int cpu;
+	
+	cpufreq_unregister_governor(&cpufreq_gov_blu_active);
+	for_each_possible_cpu(cpu) {
+		if(!cpu)
+			input_unregister_handler(&interactive_input_handler);
+	}
+	kthread_stop(speedchange_task);
+	put_task_struct(speedchange_task);
+}
+
+module_exit(cpufreq_interactive_exit);
+
+MODULE_AUTHOR("Mike Chan <mike@android.com>");
+MODULE_AUTHOR("engstk <eng.stk@sapo.pt>");
+MODULE_DESCRIPTION("'cpufreq_blu_active' - A cpufreq governor for "
+	"Latency sensitive workloads based on Google & CM Interactive");
+MODULE_LICENSE("GPLv2");
diff --git a/drivers/cpufreq/cpufreq_cafactive.c b/drivers/cpufreq/cpufreq_cafactive.c
new file mode 100644
index 0000000..70f8902
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_cafactive.c
@@ -0,0 +1,1851 @@
+/*
+ * drivers/cpufreq/cpufreq_cafactive.c
+ *
+ * Copyright (C) 2010 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * Author: Mike Chan (mike@android.com)
+ *
+ */
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/ipa.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/rwsem.h>
+#include <linux/sched.h>
+#include <linux/sched/rt.h>
+#include <linux/tick.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/kernel_stat.h>
+#include <asm/cputime.h>
+
+#ifdef CONFIG_PMU_COREMEM_RATIO
+#include "pmu_func.h"
+#endif
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/cpufreq_cafactive.h>
+
+struct cpufreq_cafactive_cpuinfo {
+	struct timer_list cpu_timer;
+	struct timer_list cpu_slack_timer;
+	spinlock_t load_lock; /* protects the next 4 fields */
+	u64 time_in_idle;
+	u64 time_in_idle_timestamp;
+	u64 cputime_speedadj;
+	u64 cputime_speedadj_timestamp;
+	u64 last_evaluated_jiffy;
+	struct cpufreq_policy *policy;
+	struct cpufreq_frequency_table *freq_table;
+	spinlock_t target_freq_lock; /*protects target freq */
+	unsigned int target_freq;
+	unsigned int floor_freq;
+	unsigned int max_freq;
+	unsigned int min_freq;
+	u64 floor_validate_time;
+	u64 local_fvtime; /* per-cpu floor_validate_time */
+	u64 hispeed_validate_time; /* cluster hispeed_validate_time */
+	u64 local_hvtime; /* per-cpu hispeed_validate_time */
+	u64 max_freq_hyst_start_time;
+	struct rw_semaphore enable_sem;
+	bool reject_notification;
+	int governor_enabled;
+	struct cpufreq_cafactive_tunables *cached_tunables;
+	int first_cpu;
+#ifdef CONFIG_PMU_COREMEM_RATIO
+	int region;
+	int prev_region;
+#endif
+};
+
+static DEFINE_PER_CPU(struct cpufreq_cafactive_cpuinfo, cpuinfo);
+
+/* realtime thread handles frequency scaling */
+static struct task_struct *speedchange_task;
+static cpumask_t speedchange_cpumask;
+static spinlock_t speedchange_cpumask_lock;
+static struct mutex gov_lock;
+
+static int set_window_count;
+static int migration_register_count;
+static struct mutex sched_lock;
+
+#ifdef CONFIG_PMU_COREMEM_RATIO
+static cpumask_t regionchange_cpumask;
+static spinlock_t regionchange_cpumask_lock;
+#endif
+
+/* Target load.  Lower values result in higher CPU speeds. */
+#define DEFAULT_TARGET_LOAD 90
+static unsigned int default_target_loads[] = {DEFAULT_TARGET_LOAD};
+
+#define DEFAULT_TIMER_RATE (20 * USEC_PER_MSEC)
+#define DEFAULT_ABOVE_HISPEED_DELAY DEFAULT_TIMER_RATE
+static unsigned int default_above_hispeed_delay[] = {
+	DEFAULT_ABOVE_HISPEED_DELAY };
+
+struct cpufreq_cafactive_tunables {
+	int usage_count;
+	/* Hi speed to bump to from lo speed when load burst (default max) */
+	unsigned int hispeed_freq;
+	/* Go to hi speed when CPU load at or above this value. */
+#define DEFAULT_GO_HISPEED_LOAD 99
+	unsigned long go_hispeed_load;
+	/* Target load. Lower values result in higher CPU speeds. */
+	spinlock_t target_loads_lock;
+	unsigned int *target_loads;
+	int ntarget_loads;
+	/*
+	 * The minimum amount of time to spend at a frequency before we can ramp
+	 * down.
+	 */
+#define DEFAULT_MIN_SAMPLE_TIME (80 * USEC_PER_MSEC)
+	unsigned long min_sample_time;
+	/*
+	 * The sample rate of the timer used to increase frequency
+	 */
+	unsigned long timer_rate;
+	/*
+	 * Wait this long before raising speed above hispeed, by default a
+	 * single timer interval.
+	 */
+	spinlock_t above_hispeed_delay_lock;
+	unsigned int *above_hispeed_delay;
+	int nabove_hispeed_delay;
+	/* Non-zero means indefinite speed boost active */
+	int boost_val;
+	/* Duration of a boot pulse in usecs */
+	int boostpulse_duration_val;
+	/* End time of boost pulse in ktime converted to usecs */
+	u64 boostpulse_endtime;
+	bool boosted;
+	/*
+	 * Max additional time to wait in idle, beyond timer_rate, at speeds
+	 * above minimum before wakeup to reduce speed, or -1 if unnecessary.
+	 */
+#define DEFAULT_TIMER_SLACK (4 * DEFAULT_TIMER_RATE)
+	int timer_slack_val;
+	bool io_is_busy;
+
+#ifdef CONFIG_PMU_COREMEM_RATIO
+	struct task_struct *regionchange_task;
+	unsigned int prev_max_region;
+	unsigned int max_region;
+	u64 region_time_in_state[6];
+	u64 region_last_stat;
+#endif
+
+	/* scheduler input related flags */
+	bool use_sched_load;
+	bool use_migration_notif;
+
+	/*
+	 * Whether to align timer windows across all CPUs. When
+	 * use_sched_load is true, this flag is ignored and windows
+	 * will always be aligned.
+	 */
+	bool align_windows;
+
+	/*
+	 * Stay at max freq for at least max_freq_hysteresis before dropping
+	 * frequency.
+	 */
+	unsigned int max_freq_hysteresis;
+};
+
+/* For cases where we have single governor instance for system */
+static struct cpufreq_cafactive_tunables *common_tunables;
+
+static struct attribute_group *get_sysfs_attr(void);
+
+/* Round to starting jiffy of next evaluation window */
+static u64 round_to_nw_start(u64 jif,
+			     struct cpufreq_cafactive_tunables *tunables)
+{
+	unsigned long step = usecs_to_jiffies(tunables->timer_rate);
+	u64 ret;
+
+	if (tunables->use_sched_load || tunables->align_windows) {
+		do_div(jif, step);
+		ret = (jif + 1) * step;
+	} else {
+		ret = jiffies + usecs_to_jiffies(tunables->timer_rate);
+	}
+
+	return ret;
+}
+
+static void cpufreq_cafactive_timer_resched(unsigned long cpu,
+					      bool slack_only)
+{
+	struct cpufreq_cafactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	struct cpufreq_cafactive_tunables *tunables =
+		pcpu->policy->governor_data;
+	u64 expires;
+	unsigned long flags;
+
+#ifdef CONFIG_PMU_COREMEM_RATIO
+	if (!tunables->regionchange_task)
+		return;
+#endif
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	expires = round_to_nw_start(pcpu->last_evaluated_jiffy, tunables);
+	if (!slack_only) {
+		pcpu->time_in_idle =
+			get_cpu_idle_time(smp_processor_id(),
+				  &pcpu->time_in_idle_timestamp,
+				  tunables->io_is_busy);
+		pcpu->cputime_speedadj = 0;
+		pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+		del_timer(&pcpu->cpu_timer);
+		pcpu->cpu_timer.expires = expires;
+		add_timer_on(&pcpu->cpu_timer, cpu);
+	}
+
+	if (tunables->timer_slack_val >= 0 &&
+	    pcpu->target_freq > pcpu->policy->min) {
+		expires += usecs_to_jiffies(tunables->timer_slack_val);
+		del_timer(&pcpu->cpu_slack_timer);
+		pcpu->cpu_slack_timer.expires = expires;
+		add_timer_on(&pcpu->cpu_slack_timer, cpu);
+	}
+
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+/* The caller shall take enable_sem write semaphore to avoid any timer race.
+ * The cpu_timer and cpu_slack_timer must be deactivated when calling this
+ * function.
+ */
+static void cpufreq_cafactive_timer_start(
+	struct cpufreq_cafactive_tunables *tunables, int cpu)
+{
+	struct cpufreq_cafactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 expires = round_to_nw_start(pcpu->last_evaluated_jiffy, tunables);
+	unsigned long flags;
+
+#ifdef CONFIG_PMU_COREMEM_RATIO
+	if (!tunables->regionchange_task)
+		return;
+#endif
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->cpu_timer.expires = expires;
+	add_timer_on(&pcpu->cpu_timer, cpu);
+	if (tunables->timer_slack_val >= 0 &&
+	    pcpu->target_freq > pcpu->policy->min) {
+		expires += usecs_to_jiffies(tunables->timer_slack_val);
+		pcpu->cpu_slack_timer.expires = expires;
+		add_timer_on(&pcpu->cpu_slack_timer, cpu);
+	}
+
+	pcpu->time_in_idle =
+		get_cpu_idle_time(cpu, &pcpu->time_in_idle_timestamp,
+				  tunables->io_is_busy);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+static unsigned int freq_to_above_hispeed_delay(
+	struct cpufreq_cafactive_tunables *tunables,
+	unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&tunables->above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < tunables->nabove_hispeed_delay - 1 &&
+			freq >= tunables->above_hispeed_delay[i+1]; i += 2)
+		;
+
+	ret = tunables->above_hispeed_delay[i];
+	spin_unlock_irqrestore(&tunables->above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static unsigned int freq_to_targetload(
+	struct cpufreq_cafactive_tunables *tunables, unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&tunables->target_loads_lock, flags);
+
+	for (i = 0; i < tunables->ntarget_loads - 1 &&
+		    freq >= tunables->target_loads[i+1]; i += 2)
+		;
+
+	ret = tunables->target_loads[i];
+	spin_unlock_irqrestore(&tunables->target_loads_lock, flags);
+	return ret;
+}
+
+/*
+ * If increasing frequencies never map to a lower target load then
+ * choose_freq() will find the minimum frequency that does not exceed its
+ * target load given the current load.
+ */
+static unsigned int choose_freq(struct cpufreq_cafactive_cpuinfo *pcpu,
+		unsigned int loadadjfreq)
+{
+	unsigned int freq = pcpu->policy->cur;
+	unsigned int prevfreq, freqmin, freqmax;
+	unsigned int tl;
+	int index;
+
+	freqmin = 0;
+	freqmax = UINT_MAX;
+
+	do {
+		prevfreq = freq;
+		tl = freq_to_targetload(pcpu->policy->governor_data, freq);
+
+		/*
+		 * Find the lowest frequency where the computed load is less
+		 * than or equal to the target load.
+		 */
+
+		if (cpufreq_frequency_table_target(
+			    pcpu->policy, pcpu->freq_table, loadadjfreq / tl,
+			    CPUFREQ_RELATION_L, &index))
+			break;
+		freq = pcpu->freq_table[index].frequency;
+
+		if (freq > prevfreq) {
+			/* The previous frequency is too low. */
+			freqmin = prevfreq;
+
+			if (freq >= freqmax) {
+				/*
+				 * Find the highest frequency that is less
+				 * than freqmax.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmax - 1, CPUFREQ_RELATION_H,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				if (freq == freqmin) {
+					/*
+					 * The first frequency below freqmax
+					 * has already been found to be too
+					 * low.  freqmax is the lowest speed
+					 * we found that is fast enough.
+					 */
+					freq = freqmax;
+					break;
+				}
+			}
+		} else if (freq < prevfreq) {
+			/* The previous frequency is high enough. */
+			freqmax = prevfreq;
+
+			if (freq <= freqmin) {
+				/*
+				 * Find the lowest frequency that is higher
+				 * than freqmin.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmin + 1, CPUFREQ_RELATION_L,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				/*
+				 * If freqmax is the first frequency above
+				 * freqmin then we have already found that
+				 * this speed is fast enough.
+				 */
+				if (freq == freqmax)
+					break;
+			}
+		}
+
+		/* If same frequency chosen as previous then done. */
+	} while (freq != prevfreq);
+
+	return freq;
+}
+
+static u64 update_load(int cpu)
+{
+	struct cpufreq_cafactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	struct cpufreq_cafactive_tunables *tunables =
+		pcpu->policy->governor_data;
+	u64 now;
+	u64 now_idle;
+	unsigned int delta_idle;
+	unsigned int delta_time;
+	u64 active_time;
+
+	now_idle = get_cpu_idle_time(cpu, &now, tunables->io_is_busy);
+	delta_idle = (unsigned int)(now_idle - pcpu->time_in_idle);
+	delta_time = (unsigned int)(now - pcpu->time_in_idle_timestamp);
+
+	if (delta_time <= delta_idle)
+		active_time = 0;
+	else
+		active_time = delta_time - delta_idle;
+
+	pcpu->cputime_speedadj += active_time * pcpu->policy->cur;
+
+	pcpu->time_in_idle = now_idle;
+	pcpu->time_in_idle_timestamp = now;
+	return now;
+}
+
+#define MAX_LOCAL_LOAD 100
+static void __cpufreq_cafactive_timer(unsigned long data, bool is_notif)
+{
+	u64 now;
+	unsigned int delta_time;
+	u64 cputime_speedadj;
+	int cpu_load;
+	struct cpufreq_cafactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, data);
+	struct cpufreq_cafactive_tunables *tunables =
+		pcpu->policy->governor_data;
+	unsigned int new_freq;
+	unsigned int loadadjfreq;
+	unsigned int index;
+	unsigned long flags;
+	struct cpufreq_govinfo int_info;
+	u64 max_fvtime;
+#ifdef CONFIG_PMU_COREMEM_RATIO
+	struct pmu_count_value pmu_data;
+	int region = 0;
+#endif
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled)
+		goto exit;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	now = update_load(data);
+	delta_time = (unsigned int)(now - pcpu->cputime_speedadj_timestamp);
+	cputime_speedadj = pcpu->cputime_speedadj;
+	pcpu->last_evaluated_jiffy = get_jiffies_64();
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+
+	if (WARN_ON_ONCE(!delta_time))
+		goto rearm;
+
+	do_div(cputime_speedadj, delta_time);
+	loadadjfreq = (unsigned int)cputime_speedadj * 100;
+
+	int_info.cpu = data;
+	int_info.load = loadadjfreq / pcpu->policy->max;
+	int_info.sampling_rate_us = tunables->timer_rate;
+	atomic_notifier_call_chain(&cpufreq_govinfo_notifier_list,
+					CPUFREQ_LOAD_CHANGE, &int_info);
+
+	spin_lock_irqsave(&pcpu->target_freq_lock, flags);
+	cpu_load = loadadjfreq / pcpu->policy->cur;
+	tunables->boosted = tunables->boost_val || now < tunables->boostpulse_endtime;
+
+#ifdef CONFIG_PMU_COREMEM_RATIO
+	/* Get crypto load information from PMU */
+	pmu_data.core_num = data;
+	pmu_data.valid = 0;
+
+	read_pmu_one(&pmu_data);
+
+	pcpu->prev_region = pcpu->region;
+	region = coremem_ratio(pmu_data.pmnc1, pmu_data.pmnc2);
+	if (region != pcpu->prev_region) {
+		pcpu->region = region;
+		spin_lock_irqsave(&regionchange_cpumask_lock, flags);
+		cpumask_set_cpu(data, &regionchange_cpumask);
+		spin_unlock_irqrestore(&regionchange_cpumask_lock, flags);
+		wake_up_process(tunables->regionchange_task);
+	}
+#endif
+
+	if (cpu_load >= tunables->go_hispeed_load || tunables->boosted) {
+		if (pcpu->policy->cur < tunables->hispeed_freq &&
+		    cpu_load <= MAX_LOCAL_LOAD) {
+			new_freq = tunables->hispeed_freq;
+		} else {
+			new_freq = choose_freq(pcpu, loadadjfreq);
+
+			if (new_freq < tunables->hispeed_freq)
+				new_freq = tunables->hispeed_freq;
+		}
+	} else {
+		new_freq = choose_freq(pcpu, loadadjfreq);
+	}
+
+	if (cpu_load <= MAX_LOCAL_LOAD &&
+	    pcpu->policy->cur >= tunables->hispeed_freq &&
+	    new_freq > pcpu->policy->cur &&
+	    now - pcpu->hispeed_validate_time <
+	    freq_to_above_hispeed_delay(tunables, pcpu->policy->cur)) {
+		trace_cpufreq_cafactive_notyet(
+			data, cpu_load, pcpu->target_freq,
+			pcpu->policy->cur, new_freq);
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	pcpu->local_hvtime = now;
+
+	if (cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table,
+					   new_freq, CPUFREQ_RELATION_L,
+					   &index)) {
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	new_freq = pcpu->freq_table[index].frequency;
+
+	if (!is_notif && new_freq < pcpu->target_freq &&
+	    now - pcpu->max_freq_hyst_start_time <
+	    tunables->max_freq_hysteresis) {
+		trace_cpufreq_cafactive_notyet(data, cpu_load,
+			pcpu->target_freq, pcpu->policy->cur, new_freq);
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	/*
+	 * Do not scale below floor_freq unless we have been at or above the
+	 * floor frequency for the minimum sample time since last validated.
+	 */
+	max_fvtime = max(pcpu->floor_validate_time, pcpu->local_fvtime);
+	if (!is_notif && new_freq < pcpu->floor_freq &&
+	    pcpu->target_freq >= pcpu->policy->cur) {
+		if (now - max_fvtime < tunables->min_sample_time) {
+			trace_cpufreq_cafactive_notyet(
+				data, cpu_load, pcpu->target_freq,
+				pcpu->policy->cur, new_freq);
+			spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+			goto rearm;
+		}
+	}
+
+	/*
+	 * Update the timestamp for checking whether speed has been held at
+	 * or above the selected frequency for a minimum of min_sample_time,
+	 * if not boosted to hispeed_freq.  If boosted to hispeed_freq then we
+	 * allow the speed to drop as soon as the boostpulse duration expires
+	 * (or the indefinite boost is turned off).
+	 */
+
+	if (!tunables->boosted || new_freq > tunables->hispeed_freq) {
+		pcpu->floor_freq = new_freq;
+		if (pcpu->target_freq >= pcpu->policy->cur ||
+		    new_freq >= pcpu->policy->cur)
+			pcpu->local_fvtime = now;
+	}
+
+	if (new_freq == pcpu->policy->max)
+		pcpu->max_freq_hyst_start_time = now;
+
+	if (pcpu->target_freq == new_freq) {
+		trace_cpufreq_cafactive_already(
+			data, cpu_load, pcpu->target_freq,
+			pcpu->policy->cur, new_freq);
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	trace_cpufreq_cafactive_target(data, cpu_load, pcpu->target_freq,
+					 pcpu->policy->cur, new_freq);
+
+	pcpu->target_freq = new_freq;
+	spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+	cpumask_set_cpu(data, &speedchange_cpumask);
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+	wake_up_process(speedchange_task);
+
+rearm:
+	if (!timer_pending(&pcpu->cpu_timer))
+		cpufreq_cafactive_timer_resched(data, false);
+
+exit:
+	up_read(&pcpu->enable_sem);
+	return;
+}
+
+static void cpufreq_cafactive_timer(unsigned long data)
+{
+	__cpufreq_cafactive_timer(data, false);
+}
+
+static void cpufreq_cafactive_idle_end(void)
+{
+	struct cpufreq_cafactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled) {
+		up_read(&pcpu->enable_sem);
+		return;
+	}
+
+	/* Arm the timer for 1-2 ticks later if not already. */
+	if (!timer_pending(&pcpu->cpu_timer)) {
+		cpufreq_cafactive_timer_resched(smp_processor_id(), false);
+	} else if (time_after_eq(jiffies, pcpu->cpu_timer.expires)) {
+		del_timer(&pcpu->cpu_timer);
+		del_timer(&pcpu->cpu_slack_timer);
+		cpufreq_cafactive_timer(smp_processor_id());
+	}
+
+	up_read(&pcpu->enable_sem);
+}
+
+#ifdef CONFIG_PMU_COREMEM_RATIO
+static void region_update_status(struct cpufreq_cafactive_tunables *tunables)
+{
+	unsigned long cur_time;
+
+	cur_time = jiffies;
+	tunables->region_time_in_state[tunables->prev_max_region] +=
+			cur_time - tunables->region_last_stat;
+	tunables->region_last_stat = cur_time;
+}
+
+static int cpufreq_cafactive_regionchange_task(void *data)
+{
+	unsigned int cpu;
+	cpumask_t tmp_mask;
+	cpumask_t policy_mask;
+	unsigned long flags;
+	struct cpufreq_cafactive_cpuinfo *pcpu;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		spin_lock_irqsave(&regionchange_cpumask_lock, flags);
+
+		pcpu = &per_cpu(cpuinfo, smp_processor_id());
+		cpumask_and(&policy_mask, &regionchange_cpumask, pcpu->policy->related_cpus);
+		if (cpumask_empty(&policy_mask)) {
+			spin_unlock_irqrestore(&regionchange_cpumask_lock, flags);
+
+			if (kthread_should_stop())
+				break;
+
+			schedule();
+
+			if (kthread_should_stop())
+				break;
+
+			spin_lock_irqsave(&regionchange_cpumask_lock, flags);
+		}
+
+		set_current_state(TASK_RUNNING);
+		cpumask_and(&tmp_mask, &regionchange_cpumask, pcpu->policy->related_cpus);
+		cpumask_andnot(&regionchange_cpumask, &regionchange_cpumask, pcpu->policy->related_cpus);
+		spin_unlock_irqrestore(&regionchange_cpumask_lock, flags);
+
+		for_each_cpu(cpu, &tmp_mask) {
+			unsigned int j;
+			unsigned int max_region = 0;
+			struct cpufreq_cafactive_tunables *tunables;
+
+			pcpu = &per_cpu(cpuinfo, cpu);
+			tunables = pcpu->policy->governor_data;
+
+			if (!down_read_trylock(&pcpu->enable_sem))
+				continue;
+			if (!pcpu->governor_enabled) {
+				up_read(&pcpu->enable_sem);
+				continue;
+			}
+
+			tunables->prev_max_region = tunables->max_region;
+			for_each_cpu(j, pcpu->policy->cpus) {
+				struct cpufreq_cafactive_cpuinfo *pjcpu =
+					&per_cpu(cpuinfo, j);
+
+				if (pjcpu->region > max_region)
+					max_region = pjcpu->region;
+			}
+
+			if (tunables->prev_max_region != max_region) {
+				tunables->max_region = max_region;
+				coremem_region_bus_lock(max_region, pcpu->policy);
+				region_update_status(tunables);
+			}
+
+			up_read(&pcpu->enable_sem);
+		}
+	}
+
+	return 0;
+}
+#endif
+
+static int cpufreq_cafactive_speedchange_task(void *data)
+{
+	unsigned int cpu;
+	cpumask_t tmp_mask;
+	unsigned long flags;
+	struct cpufreq_cafactive_cpuinfo *pcpu;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+		if (cpumask_empty(&speedchange_cpumask)) {
+			spin_unlock_irqrestore(&speedchange_cpumask_lock,
+					       flags);
+			schedule();
+
+			if (kthread_should_stop())
+				break;
+
+			spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+		}
+
+		set_current_state(TASK_RUNNING);
+		tmp_mask = speedchange_cpumask;
+		cpumask_clear(&speedchange_cpumask);
+		spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+		for_each_cpu(cpu, &tmp_mask) {
+			unsigned int j;
+			unsigned int max_freq = 0;
+			struct cpufreq_cafactive_cpuinfo *pjcpu;
+			u64 hvt = ~0ULL, fvt = 0;
+
+			pcpu = &per_cpu(cpuinfo, cpu);
+			if (!down_read_trylock(&pcpu->enable_sem))
+				continue;
+			if (!pcpu->governor_enabled) {
+				up_read(&pcpu->enable_sem);
+				continue;
+			}
+
+			for_each_cpu(j, pcpu->policy->cpus) {
+				pjcpu = &per_cpu(cpuinfo, j);
+
+				fvt = max(fvt, pjcpu->local_fvtime);
+				if (pjcpu->target_freq > max_freq) {
+					max_freq = pjcpu->target_freq;
+					hvt = pjcpu->local_hvtime;
+				} else if (pjcpu->target_freq == max_freq) {
+					hvt = min(hvt, pjcpu->local_hvtime);
+				}
+			}
+			for_each_cpu(j, pcpu->policy->cpus) {
+				pjcpu = &per_cpu(cpuinfo, j);
+				pjcpu->floor_validate_time = fvt;
+			}
+
+			if (max_freq != pcpu->policy->cur) {
+				__cpufreq_driver_target(pcpu->policy,
+							max_freq,
+							CPUFREQ_RELATION_H);
+				for_each_cpu(j, pcpu->policy->cpus) {
+					pjcpu = &per_cpu(cpuinfo, j);
+					pjcpu->hispeed_validate_time = hvt;
+				}
+			}
+
+#if defined(CONFIG_CPU_THERMAL_IPA)
+			ipa_cpufreq_requested(pcpu->policy, max_freq);
+#endif
+
+			trace_cpufreq_cafactive_setspeed(cpu,
+						     pcpu->target_freq,
+						     pcpu->policy->cur);
+
+			up_read(&pcpu->enable_sem);
+		}
+	}
+
+	return 0;
+}
+
+static void cpufreq_cafactive_boost(struct cpufreq_cafactive_tunables *tunables)
+{
+	int i;
+	int anyboost = 0;
+	unsigned long flags[2];
+	struct cpufreq_cafactive_cpuinfo *pcpu;
+
+	tunables->boosted = true;
+
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags[0]);
+
+	for_each_online_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+		if (tunables != pcpu->policy->governor_data)
+			continue;
+
+		spin_lock_irqsave(&pcpu->target_freq_lock, flags[1]);
+		if (pcpu->target_freq < tunables->hispeed_freq) {
+			pcpu->target_freq = tunables->hispeed_freq;
+			cpumask_set_cpu(i, &speedchange_cpumask);
+			pcpu->hispeed_validate_time =
+				ktime_to_us(ktime_get());
+			anyboost = 1;
+		}
+
+		/*
+		 * Set floor freq and (re)start timer for when last
+		 * validated.
+		 */
+
+		pcpu->floor_freq = tunables->hispeed_freq;
+		pcpu->floor_validate_time = ktime_to_us(ktime_get());
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags[1]);
+	}
+
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags[0]);
+
+	if (anyboost)
+		wake_up_process(speedchange_task);
+}
+
+static int cpufreq_cafactive_notifier(
+	struct notifier_block *nb, unsigned long val, void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpufreq_cafactive_cpuinfo *pcpu;
+	int cpu;
+	unsigned long flags;
+
+	if (val == CPUFREQ_PRECHANGE) {
+		pcpu = &per_cpu(cpuinfo, freq->cpu);
+		if (!down_read_trylock(&pcpu->enable_sem))
+			return 0;
+		if (!pcpu->governor_enabled) {
+			up_read(&pcpu->enable_sem);
+			return 0;
+		}
+
+		for_each_cpu(cpu, pcpu->policy->cpus) {
+			struct cpufreq_cafactive_cpuinfo *pjcpu =
+				&per_cpu(cpuinfo, cpu);
+			if (cpu != freq->cpu) {
+				if (!down_read_trylock(&pjcpu->enable_sem))
+					continue;
+				if (!pjcpu->governor_enabled) {
+					up_read(&pjcpu->enable_sem);
+					continue;
+				}
+			}
+			spin_lock_irqsave(&pjcpu->load_lock, flags);
+			update_load(cpu);
+			spin_unlock_irqrestore(&pjcpu->load_lock, flags);
+			if (cpu != freq->cpu)
+				up_read(&pjcpu->enable_sem);
+		}
+
+		up_read(&pcpu->enable_sem);
+	}
+	return 0;
+}
+
+static struct notifier_block cpufreq_notifier_block = {
+	.notifier_call = cpufreq_cafactive_notifier,
+};
+
+static unsigned int *get_tokenized_data(const char *buf, int *num_tokens)
+{
+	const char *cp;
+	int i;
+	int ntokens = 1;
+	unsigned int *tokenized_data;
+	int err = -EINVAL;
+
+	cp = buf;
+	while ((cp = strpbrk(cp + 1, " :")))
+		ntokens++;
+
+	if (!(ntokens & 0x1))
+		goto err;
+
+	tokenized_data = kmalloc(ntokens * sizeof(unsigned int), GFP_KERNEL);
+	if (!tokenized_data) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	cp = buf;
+	i = 0;
+	while (i < ntokens) {
+		if (sscanf(cp, "%u", &tokenized_data[i++]) != 1)
+			goto err_kfree;
+
+		cp = strpbrk(cp, " :");
+		if (!cp)
+			break;
+		cp++;
+	}
+
+	if (i != ntokens)
+		goto err_kfree;
+
+	*num_tokens = ntokens;
+	return tokenized_data;
+
+err_kfree:
+	kfree(tokenized_data);
+err:
+	return ERR_PTR(err);
+}
+
+void cafactive_boost_ondemand(int cpu, s64 miliseconds, bool static_switch)
+{
+	struct cpufreq_cafactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	struct cpufreq_cafactive_tunables *tunables;
+
+	if(pcpu && pcpu->policy)
+		tunables = pcpu->policy->governor_data;
+	else
+		return;
+
+	if (!tunables)
+		return;
+
+	if (!miliseconds) {
+		 if (static_switch) {
+			trace_cpufreq_cafactive_boost("on");
+			if (!tunables->boosted)
+				cpufreq_cafactive_boost(tunables);
+		 } else {
+			tunables->boostpulse_endtime = ktime_to_us(ktime_get());
+			trace_cpufreq_cafactive_unboost("off");
+		 }
+	} else {
+		 tunables->boostpulse_endtime = ktime_to_us(ktime_get()) +
+			(miliseconds * 1000);
+		 trace_cpufreq_cafactive_boost("pulse");
+		 if (!tunables->boosted)
+			cpufreq_cafactive_boost(tunables);
+	}
+}
+
+static ssize_t show_target_loads(
+	struct cpufreq_cafactive_tunables *tunables,
+	char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&tunables->target_loads_lock, flags);
+
+	for (i = 0; i < tunables->ntarget_loads; i++)
+		ret += sprintf(buf + ret, "%u%s", tunables->target_loads[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&tunables->target_loads_lock, flags);
+	return ret;
+}
+
+static ssize_t store_target_loads(
+	struct cpufreq_cafactive_tunables *tunables,
+	const char *buf, size_t count)
+{
+	int ntokens;
+	unsigned int *new_target_loads = NULL;
+	unsigned long flags;
+
+	new_target_loads = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_target_loads))
+		return PTR_RET(new_target_loads);
+
+	spin_lock_irqsave(&tunables->target_loads_lock, flags);
+	if (tunables->target_loads != default_target_loads)
+		kfree(tunables->target_loads);
+	tunables->target_loads = new_target_loads;
+	tunables->ntarget_loads = ntokens;
+	spin_unlock_irqrestore(&tunables->target_loads_lock, flags);
+	return count;
+}
+
+static ssize_t show_above_hispeed_delay(
+	struct cpufreq_cafactive_tunables *tunables, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&tunables->above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < tunables->nabove_hispeed_delay; i++)
+		ret += sprintf(buf + ret, "%u%s",
+			       tunables->above_hispeed_delay[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&tunables->above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static ssize_t store_above_hispeed_delay(
+	struct cpufreq_cafactive_tunables *tunables,
+	const char *buf, size_t count)
+{
+	int ntokens;
+	unsigned int *new_above_hispeed_delay = NULL;
+	unsigned long flags;
+
+	new_above_hispeed_delay = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_above_hispeed_delay))
+		return PTR_RET(new_above_hispeed_delay);
+
+	spin_lock_irqsave(&tunables->above_hispeed_delay_lock, flags);
+	if (tunables->above_hispeed_delay != default_above_hispeed_delay)
+		kfree(tunables->above_hispeed_delay);
+	tunables->above_hispeed_delay = new_above_hispeed_delay;
+	tunables->nabove_hispeed_delay = ntokens;
+	spin_unlock_irqrestore(&tunables->above_hispeed_delay_lock, flags);
+	return count;
+
+}
+
+static ssize_t show_hispeed_freq(struct cpufreq_cafactive_tunables *tunables,
+		char *buf)
+{
+	return sprintf(buf, "%u\n", tunables->hispeed_freq);
+}
+
+static ssize_t store_hispeed_freq(struct cpufreq_cafactive_tunables *tunables,
+		const char *buf, size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	tunables->hispeed_freq = val;
+	return count;
+}
+
+#define show_store_one(file_name)					\
+static ssize_t show_##file_name(					\
+	struct cpufreq_cafactive_tunables *tunables, char *buf)	\
+{									\
+	return snprintf(buf, PAGE_SIZE, "%u\n", tunables->file_name);	\
+}									\
+static ssize_t store_##file_name(					\
+		struct cpufreq_cafactive_tunables *tunables,		\
+		const char *buf, size_t count)				\
+{									\
+	int ret;							\
+	long unsigned int val;						\
+									\
+	ret = kstrtoul(buf, 0, &val);				\
+	if (ret < 0)							\
+		return ret;						\
+	tunables->file_name = val;					\
+	return count;							\
+}
+show_store_one(max_freq_hysteresis);
+show_store_one(align_windows);
+
+static ssize_t show_go_hispeed_load(struct cpufreq_cafactive_tunables
+		*tunables, char *buf)
+{
+	return sprintf(buf, "%lu\n", tunables->go_hispeed_load);
+}
+
+static ssize_t store_go_hispeed_load(struct cpufreq_cafactive_tunables
+		*tunables, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	tunables->go_hispeed_load = val;
+	return count;
+}
+
+static ssize_t show_min_sample_time(struct cpufreq_cafactive_tunables
+		*tunables, char *buf)
+{
+	return sprintf(buf, "%lu\n", tunables->min_sample_time);
+}
+
+static ssize_t store_min_sample_time(struct cpufreq_cafactive_tunables
+		*tunables, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	tunables->min_sample_time = val;
+	return count;
+}
+
+static ssize_t show_timer_rate(struct cpufreq_cafactive_tunables *tunables,
+		char *buf)
+{
+	return sprintf(buf, "%lu\n", tunables->timer_rate);
+}
+
+static ssize_t store_timer_rate(struct cpufreq_cafactive_tunables *tunables,
+		const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val, val_round;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	val_round = jiffies_to_usecs(usecs_to_jiffies(val));
+	if (val != val_round)
+		pr_warn("timer_rate not aligned to jiffy. Rounded up to %lu\n",
+			val_round);
+
+	tunables->timer_rate = val_round;
+	return count;
+}
+
+static ssize_t show_timer_slack(struct cpufreq_cafactive_tunables *tunables,
+		char *buf)
+{
+	return sprintf(buf, "%d\n", tunables->timer_slack_val);
+}
+
+static ssize_t store_timer_slack(struct cpufreq_cafactive_tunables *tunables,
+		const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret < 0)
+		return ret;
+
+	tunables->timer_slack_val = val;
+	return count;
+}
+
+static ssize_t show_boost(struct cpufreq_cafactive_tunables *tunables,
+			  char *buf)
+{
+	return sprintf(buf, "%d\n", tunables->boost_val);
+}
+
+static ssize_t store_boost(struct cpufreq_cafactive_tunables *tunables,
+			   const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	tunables->boost_val = val;
+
+	if (tunables->boost_val) {
+		trace_cpufreq_cafactive_boost("on");
+		if (!tunables->boosted)
+			cpufreq_cafactive_boost(tunables);
+	} else {
+		tunables->boostpulse_endtime = ktime_to_us(ktime_get());
+		trace_cpufreq_cafactive_unboost("off");
+	}
+
+	return count;
+}
+
+static ssize_t store_boostpulse(struct cpufreq_cafactive_tunables *tunables,
+				const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	tunables->boostpulse_endtime = ktime_to_us(ktime_get()) +
+		tunables->boostpulse_duration_val;
+	trace_cpufreq_cafactive_boost("pulse");
+	if (!tunables->boosted)
+		cpufreq_cafactive_boost(tunables);
+	return count;
+}
+
+static ssize_t show_boostpulse_duration(struct cpufreq_cafactive_tunables
+		*tunables, char *buf)
+{
+	return sprintf(buf, "%d\n", tunables->boostpulse_duration_val);
+}
+
+static ssize_t store_boostpulse_duration(struct cpufreq_cafactive_tunables
+		*tunables, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	tunables->boostpulse_duration_val = val;
+	return count;
+}
+
+static ssize_t show_io_is_busy(struct cpufreq_cafactive_tunables *tunables,
+		char *buf)
+{
+	return sprintf(buf, "%u\n", tunables->io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct cpufreq_cafactive_tunables *tunables,
+		const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	tunables->io_is_busy = val;
+	return count;
+}
+
+#ifdef CONFIG_PMU_COREMEM_RATIO
+static ssize_t show_region_time_in_state(struct cpufreq_cafactive_tunables *tunables,
+			  char *buf)
+{
+	int i;
+	ssize_t len = 0;
+
+	region_update_status(tunables);
+
+	for (i = 0; i < 6; i++)
+		len += snprintf(buf + len, PAGE_SIZE, "region %2d: %20llu\n",
+			i, tunables->region_time_in_state[i]);
+
+	return len;
+}
+#endif
+/*
+ * Create show/store routines
+ * - sys: One governor instance for complete SYSTEM
+ * - pol: One governor instance per struct cpufreq_policy
+ */
+#define show_gov_pol_sys(file_name)					\
+static ssize_t show_##file_name##_gov_sys				\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return show_##file_name(common_tunables, buf);			\
+}									\
+									\
+static ssize_t show_##file_name##_gov_pol				\
+(struct cpufreq_policy *policy, char *buf)				\
+{									\
+	return show_##file_name(policy->governor_data, buf);		\
+}
+
+#define store_gov_pol_sys(file_name)					\
+static ssize_t store_##file_name##_gov_sys				\
+(struct kobject *kobj, struct attribute *attr, const char *buf,		\
+	size_t count)							\
+{									\
+	return store_##file_name(common_tunables, buf, count);		\
+}									\
+									\
+static ssize_t store_##file_name##_gov_pol				\
+(struct cpufreq_policy *policy, const char *buf, size_t count)		\
+{									\
+	return store_##file_name(policy->governor_data, buf, count);	\
+}
+
+#define show_store_gov_pol_sys(file_name)				\
+show_gov_pol_sys(file_name);						\
+store_gov_pol_sys(file_name)
+
+show_store_gov_pol_sys(target_loads);
+show_store_gov_pol_sys(above_hispeed_delay);
+show_store_gov_pol_sys(hispeed_freq);
+show_store_gov_pol_sys(go_hispeed_load);
+show_store_gov_pol_sys(min_sample_time);
+show_store_gov_pol_sys(timer_rate);
+show_store_gov_pol_sys(timer_slack);
+show_store_gov_pol_sys(boost);
+store_gov_pol_sys(boostpulse);
+show_store_gov_pol_sys(boostpulse_duration);
+show_store_gov_pol_sys(io_is_busy);
+show_store_gov_pol_sys(max_freq_hysteresis);
+show_store_gov_pol_sys(align_windows);
+#ifdef CONFIG_PMU_COREMEM_RATIO
+show_gov_pol_sys(region_time_in_state);
+#endif
+#define gov_sys_attr_rw(_name)						\
+static struct global_attr _name##_gov_sys =				\
+__ATTR(_name, 0644, show_##_name##_gov_sys, store_##_name##_gov_sys)
+
+#define gov_pol_attr_rw(_name)						\
+static struct freq_attr _name##_gov_pol =				\
+__ATTR(_name, 0644, show_##_name##_gov_pol, store_##_name##_gov_pol)
+
+#define gov_sys_pol_attr_rw(_name)					\
+	gov_sys_attr_rw(_name);						\
+	gov_pol_attr_rw(_name)
+
+gov_sys_pol_attr_rw(target_loads);
+gov_sys_pol_attr_rw(above_hispeed_delay);
+gov_sys_pol_attr_rw(hispeed_freq);
+gov_sys_pol_attr_rw(go_hispeed_load);
+gov_sys_pol_attr_rw(min_sample_time);
+gov_sys_pol_attr_rw(timer_rate);
+gov_sys_pol_attr_rw(timer_slack);
+gov_sys_pol_attr_rw(boost);
+gov_sys_pol_attr_rw(boostpulse_duration);
+gov_sys_pol_attr_rw(io_is_busy);
+gov_sys_pol_attr_rw(max_freq_hysteresis);
+gov_sys_pol_attr_rw(align_windows);
+
+static struct global_attr boostpulse_gov_sys =
+	__ATTR(boostpulse, 0200, NULL, store_boostpulse_gov_sys);
+
+static struct freq_attr boostpulse_gov_pol =
+	__ATTR(boostpulse, 0200, NULL, store_boostpulse_gov_pol);
+
+#ifdef CONFIG_PMU_COREMEM_RATIO
+static struct global_attr region_time_in_state_gov_sys =
+	__ATTR(region_time_in_state, 0440, show_region_time_in_state_gov_sys, NULL);
+
+static struct freq_attr region_time_in_state_gov_pol =
+	__ATTR(region_time_in_state, 0440, show_region_time_in_state_gov_pol, NULL);
+#endif
+
+/* One Governor instance for entire system */
+static struct attribute *cafactive_attributes_gov_sys[] = {
+	&target_loads_gov_sys.attr,
+	&above_hispeed_delay_gov_sys.attr,
+	&hispeed_freq_gov_sys.attr,
+	&go_hispeed_load_gov_sys.attr,
+	&min_sample_time_gov_sys.attr,
+	&timer_rate_gov_sys.attr,
+	&timer_slack_gov_sys.attr,
+	&boost_gov_sys.attr,
+	&boostpulse_gov_sys.attr,
+	&boostpulse_duration_gov_sys.attr,
+	&io_is_busy_gov_sys.attr,
+	&max_freq_hysteresis_gov_sys.attr,
+	&align_windows_gov_sys.attr,
+#ifdef CONFIG_PMU_COREMEM_RATIO
+	&region_time_in_state_gov_sys.attr,
+#endif
+	NULL,
+};
+
+static struct attribute_group cafactive_attr_group_gov_sys = {
+	.attrs = cafactive_attributes_gov_sys,
+	.name = "cafactive",
+};
+
+/* Per policy governor instance */
+static struct attribute *cafactive_attributes_gov_pol[] = {
+	&target_loads_gov_pol.attr,
+	&above_hispeed_delay_gov_pol.attr,
+	&hispeed_freq_gov_pol.attr,
+	&go_hispeed_load_gov_pol.attr,
+	&min_sample_time_gov_pol.attr,
+	&timer_rate_gov_pol.attr,
+	&timer_slack_gov_pol.attr,
+	&boost_gov_pol.attr,
+	&boostpulse_gov_pol.attr,
+	&boostpulse_duration_gov_pol.attr,
+	&io_is_busy_gov_pol.attr,
+	&max_freq_hysteresis_gov_pol.attr,
+	&align_windows_gov_pol.attr,
+#ifdef CONFIG_PMU_COREMEM_RATIO
+	&region_time_in_state_gov_pol.attr,
+#endif
+	NULL,
+};
+
+static struct attribute_group cafactive_attr_group_gov_pol = {
+	.attrs = cafactive_attributes_gov_pol,
+	.name = "cafactive",
+};
+
+static struct attribute_group *get_sysfs_attr(void)
+{
+	if (have_governor_per_policy())
+		return &cafactive_attr_group_gov_pol;
+	else
+		return &cafactive_attr_group_gov_sys;
+}
+
+#ifdef CONFIG_PMU_COREMEM_RATIO
+void pmu_suspend_cpu(int cpu)
+{
+	int ret;
+
+	ret = stop_counter_cpu(cpu);
+	if (ret)
+		pr_err("%s: Fail to stop counter. cpu: %d\n", __func__, cpu);
+}
+
+void pmu_resume_cpu(int cpu)
+{
+	int ret;
+
+	ret = start_counter_cpu(cpu);
+	if (ret)
+		pr_err("%s: Fail to start counter. cpu: %d\n", __func__, cpu);
+}
+#endif
+
+static int cpufreq_cafactive_idle_notifier(struct notifier_block *nb,
+					     unsigned long val,
+					     void *data)
+{
+#ifdef CONFIG_PMU_COREMEM_RATIO
+	int cpu = smp_processor_id();
+#endif
+
+	switch (val) {
+	case IDLE_START:
+#ifdef CONFIG_PMU_COREMEM_RATIO
+		pmu_suspend_cpu(cpu);
+#endif
+		break;
+	case IDLE_END:
+		cpufreq_cafactive_idle_end();
+#ifdef CONFIG_PMU_COREMEM_RATIO
+		pmu_resume_cpu(cpu);
+#endif
+		break;
+	}
+
+	return 0;
+}
+
+static struct notifier_block cpufreq_cafactive_idle_nb = {
+	.notifier_call = cpufreq_cafactive_idle_notifier,
+};
+
+#ifdef CONFIG_PMU_COREMEM_RATIO
+static int __cpuinit exynos_pmu_cpu_notifier(struct notifier_block *nfb,
+		unsigned long action, void *hcpu)
+{
+	int cpu = (unsigned long)hcpu;
+
+	switch (action & ~CPU_TASKS_FROZEN) {
+	case CPU_STARTING:
+	case CPU_DOWN_FAILED:
+		pmu_resume_cpu(cpu);
+		break;
+	case CPU_DYING:
+		pmu_suspend_cpu(cpu);
+		break;
+	}
+	return NOTIFY_OK;
+}
+
+static struct notifier_block __cpuinitdata exynos_pmu_cpu_notifier_block = {
+	.notifier_call = exynos_pmu_cpu_notifier,
+};
+#endif
+
+static void save_tunables(struct cpufreq_policy *policy,
+			  struct cpufreq_cafactive_tunables *tunables)
+{
+	int cpu;
+	struct cpufreq_cafactive_cpuinfo *pcpu;
+
+	if (have_governor_per_policy())
+		cpu = cpumask_first(policy->related_cpus);
+	else
+		cpu = 0;
+
+	pcpu = &per_cpu(cpuinfo, cpu);
+	WARN_ON(pcpu->cached_tunables && pcpu->cached_tunables != tunables);
+	pcpu->cached_tunables = tunables;
+}
+
+static struct cpufreq_cafactive_tunables *alloc_tunable(
+					struct cpufreq_policy *policy)
+{
+	struct cpufreq_cafactive_tunables *tunables;
+
+	tunables = kzalloc(sizeof(*tunables), GFP_KERNEL);
+	if (!tunables) {
+		pr_err("%s: POLICY_INIT: kzalloc failed\n", __func__);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	tunables->above_hispeed_delay = default_above_hispeed_delay;
+	tunables->nabove_hispeed_delay =
+		ARRAY_SIZE(default_above_hispeed_delay);
+	tunables->go_hispeed_load = DEFAULT_GO_HISPEED_LOAD;
+	tunables->target_loads = default_target_loads;
+	tunables->ntarget_loads = ARRAY_SIZE(default_target_loads);
+	tunables->min_sample_time = DEFAULT_MIN_SAMPLE_TIME;
+	tunables->timer_rate = DEFAULT_TIMER_RATE;
+	tunables->boostpulse_duration_val = DEFAULT_MIN_SAMPLE_TIME;
+	tunables->timer_slack_val = DEFAULT_TIMER_SLACK;
+
+	spin_lock_init(&tunables->target_loads_lock);
+	spin_lock_init(&tunables->above_hispeed_delay_lock);
+
+	save_tunables(policy, tunables);
+	return tunables;
+}
+
+static struct cpufreq_cafactive_tunables *restore_tunables(
+						struct cpufreq_policy *policy)
+{
+	int cpu;
+
+	if (have_governor_per_policy())
+		cpu = cpumask_first(policy->related_cpus);
+	else
+		cpu = 0;
+
+	return per_cpu(cpuinfo, cpu).cached_tunables;
+}
+
+static int cpufreq_governor_cafactive(struct cpufreq_policy *policy,
+		unsigned int event)
+{
+	int rc;
+	unsigned int j;
+	struct cpufreq_cafactive_cpuinfo *pcpu;
+	struct cpufreq_frequency_table *freq_table;
+	struct cpufreq_cafactive_tunables *tunables;
+	unsigned long flags;
+#ifdef CONFIG_PMU_COREMEM_RATIO
+	char regionchange_task_name[TASK_NAME_LEN];
+#endif
+
+	if (have_governor_per_policy())
+		tunables = policy->governor_data;
+	else
+		tunables = common_tunables;
+
+	BUG_ON(!tunables && (event != CPUFREQ_GOV_POLICY_INIT));
+
+	switch (event) {
+	case CPUFREQ_GOV_POLICY_INIT:
+		if (have_governor_per_policy()) {
+			WARN_ON(tunables);
+		} else if (tunables) {
+			tunables->usage_count++;
+			policy->governor_data = tunables;
+			return 0;
+		}
+
+		tunables = restore_tunables(policy);
+		if (!tunables) {
+			tunables = alloc_tunable(policy);
+			if (IS_ERR(tunables))
+				return PTR_ERR(tunables);
+		}
+
+#ifdef CONFIG_PMU_COREMEM_RATIO
+		tunables->region_last_stat = jiffies;
+#endif
+		tunables->usage_count = 1;
+		policy->governor_data = tunables;
+		if (!have_governor_per_policy())
+			common_tunables = tunables;
+
+		rc = sysfs_create_group(get_governor_parent_kobj(policy),
+				get_sysfs_attr());
+		if (rc) {
+			kfree(tunables);
+			policy->governor_data = NULL;
+			if (!have_governor_per_policy())
+				common_tunables = NULL;
+			return rc;
+		}
+
+		if (!policy->governor->initialized) {
+			idle_notifier_register(&cpufreq_cafactive_idle_nb);
+#ifdef CONFIG_PMU_COREMEM_RATIO
+			register_cpu_notifier(&exynos_pmu_cpu_notifier_block);
+#endif
+			cpufreq_register_notifier(&cpufreq_notifier_block,
+					CPUFREQ_TRANSITION_NOTIFIER);
+		}
+
+		break;
+
+	case CPUFREQ_GOV_POLICY_EXIT:
+		if (!--tunables->usage_count) {
+			if (policy->governor->initialized == 1) {
+				cpufreq_unregister_notifier(&cpufreq_notifier_block,
+						CPUFREQ_TRANSITION_NOTIFIER);
+#ifdef CONFIG_PMU_COREMEM_RATIO
+				unregister_cpu_notifier(&exynos_pmu_cpu_notifier_block);
+#endif
+				idle_notifier_unregister(&cpufreq_cafactive_idle_nb);
+			}
+
+			sysfs_remove_group(get_governor_parent_kobj(policy),
+					get_sysfs_attr());
+			common_tunables = NULL;
+		}
+
+		policy->governor_data = NULL;
+		break;
+
+	case CPUFREQ_GOV_START:
+		mutex_lock(&gov_lock);
+
+		freq_table = cpufreq_frequency_get_table(policy->cpu);
+		if (!tunables->hispeed_freq)
+			tunables->hispeed_freq = policy->max;
+
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			pcpu->policy = policy;
+			pcpu->target_freq = policy->cur;
+			pcpu->freq_table = freq_table;
+			pcpu->floor_freq = pcpu->target_freq;
+			pcpu->floor_validate_time =
+				ktime_to_us(ktime_get());
+			pcpu->local_fvtime = pcpu->floor_validate_time;
+			pcpu->hispeed_validate_time =
+				pcpu->floor_validate_time;
+			pcpu->local_hvtime = pcpu->floor_validate_time;
+			pcpu->max_freq = policy->max;
+			pcpu->min_freq = policy->min;
+			pcpu->reject_notification = true;
+			down_write(&pcpu->enable_sem);
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			pcpu->last_evaluated_jiffy = get_jiffies_64();
+			cpufreq_cafactive_timer_start(tunables, j);
+			pcpu->governor_enabled = 1;
+			up_write(&pcpu->enable_sem);
+			pcpu->reject_notification = false;
+		}
+
+#ifdef CONFIG_PMU_COREMEM_RATIO
+		snprintf(regionchange_task_name, TASK_NAME_LEN, "region%d",
+					policy->cpu);
+
+		tunables->regionchange_task =
+			kthread_create(cpufreq_cafactive_regionchange_task, NULL,
+					regionchange_task_name);
+		if (IS_ERR(tunables->regionchange_task)) {
+			kthread_stop(tunables->regionchange_task);
+			mutex_unlock(&gov_lock);
+			return PTR_ERR(tunables->regionchange_task);
+		}
+
+		sched_setscheduler_nocheck(tunables->regionchange_task, SCHED_FIFO, &param);
+		get_task_struct(tunables->regionchange_task);
+#endif
+
+#ifdef CONFIG_PMU_COREMEM_RATIO
+		kthread_bind(tunables->regionchange_task, policy->cpu);
+#endif
+#ifdef CONFIG_PMU_COREMEM_RATIO
+		wake_up_process(tunables->regionchange_task);
+#endif
+
+		mutex_unlock(&gov_lock);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		mutex_lock(&gov_lock);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			pcpu->reject_notification = true;
+			down_write(&pcpu->enable_sem);
+			pcpu->governor_enabled = 0;
+			pcpu->target_freq = 0;
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			up_write(&pcpu->enable_sem);
+			pcpu->reject_notification = false;
+		}
+
+#ifdef CONFIG_PMU_COREMEM_RATIO
+		kthread_stop(tunables->regionchange_task);
+		put_task_struct(tunables->regionchange_task);
+		tunables->regionchange_task = NULL;
+#endif
+
+		mutex_unlock(&gov_lock);
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		__cpufreq_driver_target(policy,
+				policy->cur, CPUFREQ_RELATION_L);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+
+			down_read(&pcpu->enable_sem);
+			if (pcpu->governor_enabled == 0) {
+				up_read(&pcpu->enable_sem);
+				continue;
+			}
+
+			spin_lock_irqsave(&pcpu->target_freq_lock, flags);
+			if (policy->max < pcpu->target_freq)
+				pcpu->target_freq = policy->max;
+			else if (policy->min > pcpu->target_freq)
+				pcpu->target_freq = policy->min;
+
+			spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+
+			if (policy->min < pcpu->min_freq)
+				cpufreq_cafactive_timer_resched(j, true);
+			pcpu->min_freq = policy->min;
+
+			up_read(&pcpu->enable_sem);
+
+			/* Reschedule timer only if policy->max is raised.
+			 * Delete the timers, else the timer callback may
+			 * return without re-arm the timer when failed
+			 * acquire the semaphore. This race may cause timer
+			 * stopped unexpectedly.
+			 */
+
+			if (policy->max > pcpu->max_freq) {
+				pcpu->reject_notification = true;
+				down_write(&pcpu->enable_sem);
+				del_timer_sync(&pcpu->cpu_timer);
+				del_timer_sync(&pcpu->cpu_slack_timer);
+				cpufreq_cafactive_timer_resched(j, false);
+				up_write(&pcpu->enable_sem);
+				pcpu->reject_notification = false;
+			}
+
+			pcpu->max_freq = policy->max;
+		}
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_CAFACTIVE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_cafactive = {
+	.name = "cafactive",
+	.governor = cpufreq_governor_cafactive,
+	.max_transition_latency = 10000000,
+	.owner = THIS_MODULE,
+};
+
+static void cpufreq_cafactive_nop_timer(unsigned long data)
+{
+}
+
+unsigned int cpufreq_cafactive_get_hispeed_freq(int cpu)
+{
+	struct cpufreq_cafactive_cpuinfo *pcpu =
+			&per_cpu(cpuinfo, cpu);
+	struct cpufreq_cafactive_tunables *tunables;
+
+	if (pcpu && pcpu->policy)
+		tunables = pcpu->policy->governor_data;
+	else
+		return 0;
+
+	if (!tunables)
+		return 0;
+
+	return tunables->hispeed_freq;
+}
+
+static int __init cpufreq_cafactive_init(void)
+{
+	unsigned int i;
+	struct cpufreq_cafactive_cpuinfo *pcpu;
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+
+	/* Initalize per-cpu timers */
+	for_each_possible_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+		init_timer_deferrable(&pcpu->cpu_timer);
+		pcpu->cpu_timer.function = cpufreq_cafactive_timer;
+		pcpu->cpu_timer.data = i;
+		init_timer(&pcpu->cpu_slack_timer);
+		pcpu->cpu_slack_timer.function = cpufreq_cafactive_nop_timer;
+		spin_lock_init(&pcpu->load_lock);
+		spin_lock_init(&pcpu->target_freq_lock);
+		init_rwsem(&pcpu->enable_sem);
+	}
+
+	spin_lock_init(&speedchange_cpumask_lock);
+#ifdef CONFIG_PMU_COREMEM_RATIO
+	spin_lock_init(&regionchange_cpumask_lock);
+#endif
+	mutex_init(&gov_lock);
+	speedchange_task =
+		kthread_create(cpufreq_cafactive_speedchange_task, NULL,
+			       "cfcafactive");
+	if (IS_ERR(speedchange_task))
+		return PTR_ERR(speedchange_task);
+
+	sched_setscheduler_nocheck(speedchange_task, SCHED_FIFO, &param);
+	get_task_struct(speedchange_task);
+
+	/* NB: wake up so the thread does not look hung to the freezer */
+	wake_up_process(speedchange_task);
+
+	return cpufreq_register_governor(&cpufreq_gov_cafactive);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_CAFACTIVE
+fs_initcall(cpufreq_cafactive_init);
+#else
+module_init(cpufreq_cafactive_init);
+#endif
+
+static void __exit cpufreq_cafactive_exit(void)
+{
+	int cpu;
+	struct cpufreq_cafactive_cpuinfo *pcpu;
+
+	cpufreq_unregister_governor(&cpufreq_gov_cafactive);
+	kthread_stop(speedchange_task);
+	put_task_struct(speedchange_task);
+
+	for_each_possible_cpu(cpu) {
+		pcpu = &per_cpu(cpuinfo, cpu);
+		kfree(pcpu->cached_tunables);
+		pcpu->cached_tunables = NULL;
+	}
+}
+
+module_exit(cpufreq_cafactive_exit);
+
+MODULE_AUTHOR("Mike Chan <mike@android.com>");
+MODULE_DESCRIPTION("'cpufreq_cafactive' - A cpufreq governor for "
+	"Latency sensitive workloads");
+MODULE_LICENSE("GPL");
diff --git a/drivers/cpufreq/cpufreq_conservative_x.c b/drivers/cpufreq/cpufreq_conservative_x.c
new file mode 100644
index 0000000..f60f661
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_conservative_x.c
@@ -0,0 +1,503 @@
+/*
+ *  drivers/cpufreq/cpufreq_conservative_x.c
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *            (C)  2009 Alexander Clouter <alex@digriz.org.uk>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/slab.h>
+#include "cpufreq_governor.h"
+#include <linux/touchboost.h>
+
+/* Conservative governor macros */
+#define DEF_FREQUENCY_UP_THRESHOLD		(95)
+#define DEF_FREQUENCY_DOWN_THRESHOLD		(30)
+#define DEF_FREQUENCY_TWOSTEP_THRESHOLD		(60)
+#define DEF_FREQUENCY_STEP			(5)
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define MAX_SAMPLING_DOWN_FACTOR		(10)
+#define MICRO_FREQUENCY_MIN_SAMPLE_RATE		(20000)
+#define BOOST_DURATION_US			(40000)
+#define BOOST_FREQ_VAL				(1497600)
+#define DEFAULT_MIN_LOAD			(5)
+
+static DEFINE_PER_CPU(struct cs_cpu_dbs_info_s, cs_cpu_dbs_info);
+
+static inline unsigned int get_freq_target(struct cs_dbs_tuners *cs_tuners,
+					   unsigned int freq_mult)
+{
+	unsigned int freq_target = (cs_tuners->freq_step * freq_mult) / 100;
+
+	/* max freq cannot be less than 100. But who knows... */
+	if (unlikely(freq_target == 0))
+		freq_target = DEF_FREQUENCY_STEP;
+
+	return freq_target;
+}
+
+/*
+ * Every sampling_rate, we check, if current idle time is less than 20%
+ * (default), then we try to increase frequency. Every sampling_rate *
+ * sampling_down_factor, we check, if current idle time is more than 80%
+ * (default), then we try to decrease frequency
+ *
+ * Any frequency increase takes it to the maximum frequency. Frequency reduction
+ * happens at minimum steps of 5% (default) of maximum frequency
+ */
+static void cs_check_cpu(int cpu, unsigned int load)
+{
+	struct cs_cpu_dbs_info_s *dbs_info = &per_cpu(cs_cpu_dbs_info, cpu);
+	struct cpufreq_policy *policy = dbs_info->cdbs.cur_policy;
+	struct dbs_data *dbs_data = policy->governor_data;
+	struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+	bool boosted;
+	u64 now;
+
+	cpufreq_notify_utilization(policy, load);
+
+	/*
+	 * break out if we 'cannot' reduce the speed as the user might
+	 * want freq_step to be zero
+	 */
+	if (cs_tuners->freq_step == 0)
+		return;
+
+	now = ktime_to_us(ktime_get());
+	boosted = now < (get_input_time() + cs_tuners->input_boost_duration);
+
+	/* Check for frequency increase */
+	if (load > DEF_FREQUENCY_TWOSTEP_THRESHOLD) {
+		if (load >= cs_tuners->up_threshold)
+			dbs_info->down_skip = 0;
+
+		/* if we are already at full speed then break out early */
+		if (policy->cur == policy->max)
+			return;
+
+		if (load < cs_tuners->up_threshold && dbs_info->twostep_counter++ < 2) {
+			dbs_info->twostep_time = now;
+			dbs_info->requested_freq += get_freq_target(cs_tuners, policy->max >> 1);
+		} else {
+			if (load >= cs_tuners->up_threshold)
+				dbs_info->requested_freq += get_freq_target(cs_tuners, policy->max);
+
+			dbs_info->twostep_counter = 0;
+		}
+
+		if (dbs_info->requested_freq > policy->max)
+			dbs_info->requested_freq = policy->max;
+
+		if (boosted)
+                        dbs_info->requested_freq
+                                = max(cs_tuners->input_boost_freq,
+                                        dbs_info->requested_freq);
+
+		__cpufreq_driver_target(policy, dbs_info->requested_freq,
+			CPUFREQ_RELATION_H);
+		return;
+	}
+
+	/* if sampling_down_factor is active break out early */
+	if (++dbs_info->down_skip < cs_tuners->sampling_down_factor)
+		return;
+	dbs_info->down_skip = 0;
+
+	/* Check for frequency decrease */
+	if (load < cs_tuners->down_threshold) {
+		unsigned int freq_target;
+
+		/*
+		 * we're scaling down, so reset the counter if
+		 * the conditions are met
+		 */
+		if (dbs_info->twostep_counter) {
+			/* 150ms*/
+			if ((now - dbs_info->twostep_time) >= 150000)
+                		dbs_info->twostep_counter = 0;
+		}
+
+		/*
+		 * if we cannot reduce the frequency anymore, break out early
+		 */
+		if (policy->cur == policy->min)
+			return;
+
+		if (load < DEFAULT_MIN_LOAD) {
+			dbs_info->requested_freq = policy->min;
+			goto scale_down;
+		}
+
+		freq_target = get_freq_target(cs_tuners, policy->max);
+		if (dbs_info->requested_freq > freq_target)
+			dbs_info->requested_freq -= freq_target;
+		else
+			dbs_info->requested_freq = policy->min;
+
+scale_down:
+
+		if (boosted)
+                        dbs_info->requested_freq
+				= max(cs_tuners->input_boost_freq,
+					dbs_info->requested_freq);
+
+		__cpufreq_driver_target(policy, dbs_info->requested_freq,
+				CPUFREQ_RELATION_L);
+		return;
+	}
+}
+
+static void cs_dbs_timer(struct work_struct *work)
+{
+	struct cs_cpu_dbs_info_s *dbs_info = container_of(work,
+			struct cs_cpu_dbs_info_s, cdbs.work.work);
+	unsigned int cpu = dbs_info->cdbs.cur_policy->cpu;
+	struct cs_cpu_dbs_info_s *core_dbs_info = &per_cpu(cs_cpu_dbs_info,
+			cpu);
+	struct dbs_data *dbs_data = dbs_info->cdbs.cur_policy->governor_data;
+	struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+	int delay = delay_for_sampling_rate(cs_tuners->sampling_rate);
+	bool modify_all = true;
+
+	mutex_lock(&core_dbs_info->cdbs.timer_mutex);
+	if (!need_load_eval(&core_dbs_info->cdbs, cs_tuners->sampling_rate))
+		modify_all = false;
+	else
+		dbs_check_cpu(dbs_data, cpu);
+
+	gov_queue_work(dbs_data, dbs_info->cdbs.cur_policy, delay, modify_all);
+	mutex_unlock(&core_dbs_info->cdbs.timer_mutex);
+}
+
+static int dbs_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
+		void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cs_cpu_dbs_info_s *dbs_info =
+					&per_cpu(cs_cpu_dbs_info, freq->cpu);
+	struct cpufreq_policy *policy;
+
+	if (!dbs_info->enable)
+		return 0;
+
+	policy = dbs_info->cdbs.cur_policy;
+
+	/*
+	 * we only care if our internally tracked freq moves outside the 'valid'
+	 * ranges of frequency available to us otherwise we do not change it
+	*/
+	if (dbs_info->requested_freq > policy->max
+			|| dbs_info->requested_freq < policy->min)
+		dbs_info->requested_freq = freq->new;
+
+	return 0;
+}
+
+/************************** sysfs interface ************************/
+static struct common_dbs_data cs_dbs_cdata;
+
+static ssize_t store_sampling_down_factor(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+
+	cs_tuners->sampling_down_factor = input;
+	return count;
+}
+
+static ssize_t store_sampling_rate(struct dbs_data *dbs_data, const char *buf,
+		size_t count)
+{
+	struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	cs_tuners->sampling_rate = max(input, dbs_data->min_sampling_rate);
+	return count;
+}
+
+static ssize_t store_up_threshold(struct dbs_data *dbs_data, const char *buf,
+		size_t count)
+{
+	struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 || input <= cs_tuners->down_threshold)
+		return -EINVAL;
+
+	cs_tuners->up_threshold = input;
+	return count;
+}
+
+static ssize_t store_down_threshold(struct dbs_data *dbs_data, const char *buf,
+		size_t count)
+{
+	struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	/* cannot be lower than 11 otherwise freq will not fall */
+	if (ret != 1 || input < 11 || input > 100 ||
+			input >= cs_tuners->up_threshold)
+		return -EINVAL;
+
+	cs_tuners->down_threshold = input;
+	return count;
+}
+
+static ssize_t store_ignore_nice_load(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+	unsigned int input, j;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == cs_tuners->ignore_nice_load) /* nothing to do */
+		return count;
+
+	cs_tuners->ignore_nice_load = input;
+
+	/* we need to re-evaluate prev_cpu_idle */
+	for_each_online_cpu(j) {
+		struct cs_cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+		dbs_info->cdbs.prev_cpu_idle = get_cpu_idle_time(j,
+					&dbs_info->cdbs.prev_cpu_wall, 0);
+		if (cs_tuners->ignore_nice_load)
+			dbs_info->cdbs.prev_cpu_nice =
+				kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+	}
+	return count;
+}
+
+static ssize_t store_freq_step(struct dbs_data *dbs_data, const char *buf,
+		size_t count)
+{
+	struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 100)
+		input = 100;
+
+	/*
+	 * no need to test here if freq_step is zero as the user might actually
+	 * want this, they would be crazy though :)
+	 */
+	cs_tuners->freq_step = input;
+	return count;
+}
+
+static ssize_t store_input_boost_freq(struct dbs_data *dbs_data, const char *buf,
+                size_t count)
+{
+        struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+        unsigned int input;
+        int ret;
+        ret = sscanf(buf, "%u", &input);
+
+        if (ret != 1)
+                return -EINVAL;
+
+        if (input < 0)
+                input = 0;
+
+        cs_tuners->input_boost_freq = input;
+        return count;
+}
+
+static ssize_t store_input_boost_duration(struct dbs_data *dbs_data, const char *buf,
+                size_t count)
+{
+        struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+        unsigned int input;
+        int ret;
+        ret = sscanf(buf, "%u", &input);
+
+        if (ret != 1)
+                return -EINVAL;
+
+        if (input < 0)
+                input = 0;
+
+        cs_tuners->input_boost_duration = input;
+        return count;
+}
+
+show_store_one(cs, sampling_rate);
+show_store_one(cs, sampling_down_factor);
+show_store_one(cs, up_threshold);
+show_store_one(cs, down_threshold);
+show_store_one(cs, ignore_nice_load);
+show_store_one(cs, freq_step);
+declare_show_sampling_rate_min(cs);
+show_store_one(cs, input_boost_freq);
+show_store_one(cs, input_boost_duration);
+
+gov_sys_pol_attr_rw(sampling_rate);
+gov_sys_pol_attr_rw(sampling_down_factor);
+gov_sys_pol_attr_rw(up_threshold);
+gov_sys_pol_attr_rw(down_threshold);
+gov_sys_pol_attr_rw(ignore_nice_load);
+gov_sys_pol_attr_rw(freq_step);
+gov_sys_pol_attr_ro(sampling_rate_min);
+gov_sys_pol_attr_rw(input_boost_freq);
+gov_sys_pol_attr_rw(input_boost_duration);
+
+static struct attribute *dbs_attributes_gov_sys[] = {
+	&sampling_rate_min_gov_sys.attr,
+	&sampling_rate_gov_sys.attr,
+	&sampling_down_factor_gov_sys.attr,
+	&up_threshold_gov_sys.attr,
+	&down_threshold_gov_sys.attr,
+	&ignore_nice_load_gov_sys.attr,
+	&freq_step_gov_sys.attr,
+	&input_boost_freq_gov_sys.attr,
+	&input_boost_duration_gov_sys.attr,
+	NULL
+};
+
+static struct attribute_group cs_attr_group_gov_sys = {
+	.attrs = dbs_attributes_gov_sys,
+	.name = "conservative_x",
+};
+
+static struct attribute *dbs_attributes_gov_pol[] = {
+	&sampling_rate_min_gov_pol.attr,
+	&sampling_rate_gov_pol.attr,
+	&sampling_down_factor_gov_pol.attr,
+	&up_threshold_gov_pol.attr,
+	&down_threshold_gov_pol.attr,
+	&ignore_nice_load_gov_pol.attr,
+	&freq_step_gov_pol.attr,
+	&input_boost_freq_gov_pol.attr,
+	&input_boost_duration_gov_pol.attr,
+	NULL
+};
+
+static struct attribute_group cs_attr_group_gov_pol = {
+	.attrs = dbs_attributes_gov_pol,
+	.name = "conservative_x",
+};
+
+/************************** sysfs end ************************/
+
+static int cs_init(struct dbs_data *dbs_data)
+{
+	struct cs_dbs_tuners *tuners;
+
+	tuners = kzalloc(sizeof(*tuners), GFP_KERNEL);
+	if (!tuners) {
+		pr_err("%s: kzalloc failed\n", __func__);
+		return -ENOMEM;
+	}
+
+	tuners->up_threshold = DEF_FREQUENCY_UP_THRESHOLD;
+	tuners->down_threshold = DEF_FREQUENCY_DOWN_THRESHOLD;
+	tuners->sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR;
+	tuners->ignore_nice_load = 0;
+	tuners->freq_step = DEF_FREQUENCY_STEP;
+	tuners->input_boost_freq = BOOST_FREQ_VAL;
+        tuners->input_boost_duration = BOOST_DURATION_US;
+
+	dbs_data->tuners = tuners;
+	dbs_data->min_sampling_rate = MICRO_FREQUENCY_MIN_SAMPLE_RATE;
+	mutex_init(&dbs_data->mutex);
+	return 0;
+}
+
+static void cs_exit(struct dbs_data *dbs_data)
+{
+	kfree(dbs_data->tuners);
+}
+
+define_get_cpu_dbs_routines(cs_cpu_dbs_info);
+
+static struct notifier_block cs_cpufreq_notifier_block = {
+	.notifier_call = dbs_cpufreq_notifier,
+};
+
+static struct cs_ops cs_ops = {
+	.notifier_block = &cs_cpufreq_notifier_block,
+};
+
+static struct common_dbs_data cs_dbs_cdata = {
+	.governor = GOV_CONSERVATIVE_X,
+	.attr_group_gov_sys = &cs_attr_group_gov_sys,
+	.attr_group_gov_pol = &cs_attr_group_gov_pol,
+	.get_cpu_cdbs = get_cpu_cdbs,
+	.get_cpu_dbs_info_s = get_cpu_dbs_info_s,
+	.gov_dbs_timer = cs_dbs_timer,
+	.gov_check_cpu = cs_check_cpu,
+	.gov_ops = &cs_ops,
+	.init = cs_init,
+	.exit = cs_exit,
+};
+
+static int cs_cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	return cpufreq_governor_dbs(policy, &cs_dbs_cdata, event);
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE_X
+static
+#endif
+struct cpufreq_governor cpufreq_gov_conservative_x = {
+	.name			= "conservative_x",
+	.governor		= cs_cpufreq_governor_dbs,
+	.max_transition_latency	= TRANSITION_LATENCY_LIMIT,
+	.owner			= THIS_MODULE,
+};
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	return cpufreq_register_governor(&cpufreq_gov_conservative_x);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_conservative_x);
+}
+
+MODULE_AUTHOR("Alexander Clouter <alex@digriz.org.uk>");
+MODULE_DESCRIPTION("'cpufreq_conservative_x' - A dynamic cpufreq governor for "
+		"Low Latency Frequency Transition capable processors "
+		"optimised for use in a battery environment");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE_X
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_darkness.c b/drivers/cpufreq/cpufreq_darkness.c
new file mode 100644
index 0000000..340385d
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_darkness.c
@@ -0,0 +1,417 @@
+/*
+ *  drivers/cpufreq/cpufreq_darkness.c
+ *
+ *  Copyright (C)  2011 Samsung Electronics co. ltd
+ *    ByungChang Cha <bc.cha@samsung.com>
+ *
+ *  Based on ondemand governor
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * Created by Alucard_24@xda
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+
+#define MIN_SAMPLING_RATE	10000
+
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+static void do_darkness_timer(struct work_struct *work);
+
+struct cpufreq_darkness_cpuinfo {
+	u64 prev_cpu_wall;
+	u64 prev_cpu_idle;
+	struct cpufreq_frequency_table *freq_table;
+	struct delayed_work work;
+	struct cpufreq_policy *cur_policy;
+	bool governor_enabled;
+	unsigned int cpu;
+	unsigned int prev_load;
+	/*
+	 * percpu mutex that serializes governor limit change with
+	 * do_dbs_timer invocation. We do not want do_dbs_timer to run
+	 * when user is changing the governor or limits.
+	 */
+	struct mutex timer_mutex;
+};
+/*
+ * mutex that serializes governor limit change with
+ * do_darkness_timer invocation. We do not want do_darkness_timer to run
+ * when user is changing the governor or limits.
+ */
+static DEFINE_PER_CPU(struct cpufreq_darkness_cpuinfo, od_darkness_cpuinfo);
+
+static unsigned int darkness_enable;	/* number of CPUs using this policy */
+/*
+ * darkness_mutex protects darkness_enable in governor start/stop.
+ */
+static DEFINE_MUTEX(darkness_mutex);
+
+static struct workqueue_struct *darkness_wq;
+
+/* darkness tuners */
+static struct darkness_tuners {
+	unsigned int sampling_rate;
+} darkness_tuners_ins = {
+	.sampling_rate = 60000,
+};
+
+/************************** sysfs interface ************************/
+
+/* cpufreq_darkness Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return sprintf(buf, "%d\n", darkness_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+
+/* sampling_rate */
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret = 0;
+	int mpd = strcmp(current->comm, "mpdecision");
+
+	if (mpd == 0)
+		return ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(input,10000);
+
+	if (input == darkness_tuners_ins.sampling_rate)
+		return count;
+
+	darkness_tuners_ins.sampling_rate = input;
+
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+
+static struct attribute *darkness_attributes[] = {
+	&sampling_rate.attr,
+	NULL
+};
+
+static struct attribute_group darkness_attr_group = {
+	.attrs = darkness_attributes,
+	.name = "darkness",
+};
+
+/************************** sysfs end ************************/
+
+static unsigned int adjust_cpufreq_frequency_target(struct cpufreq_policy *policy,
+					struct cpufreq_frequency_table *table,
+					unsigned int tmp_freq)
+{
+	unsigned int i = 0, l_freq = 0, h_freq = 0, target_freq = 0;
+
+	if (tmp_freq < policy->min)
+		tmp_freq = policy->min;
+	if (tmp_freq > policy->max)
+		tmp_freq = policy->max;
+
+	for (i = 0; (table[i].frequency != CPUFREQ_TABLE_END); i++) {
+		unsigned int freq = table[i].frequency;
+		if (freq == CPUFREQ_ENTRY_INVALID) {
+			continue;
+		}
+		if (freq < tmp_freq) {
+			h_freq = freq;
+		}
+		if (freq == tmp_freq) {
+			target_freq = freq;
+			break;
+		}
+		if (freq > tmp_freq) {
+			l_freq = freq;
+			break;
+		}
+	}
+	if (!target_freq) {
+		if (policy->cur >= h_freq
+			 && policy->cur <= l_freq)
+			target_freq = policy->cur;
+		else
+			target_freq = l_freq;
+	}
+
+	return target_freq;
+}
+
+static void darkness_check_cpu(struct cpufreq_darkness_cpuinfo *this_darkness_cpuinfo)
+{
+	struct cpufreq_policy *policy;
+	unsigned int max_load = 0;
+	unsigned int next_freq = 0;
+	unsigned int j;
+	unsigned int sampling_rate = darkness_tuners_ins.sampling_rate;
+
+	policy = this_darkness_cpuinfo->cur_policy;
+	if (!policy)
+		return;
+
+	for_each_cpu(j, policy->cpus) {
+		struct cpufreq_darkness_cpuinfo *j_darkness_cpuinfo = &per_cpu(od_darkness_cpuinfo, j);
+		u64 cur_wall_time, cur_idle_time;
+		unsigned int idle_time, wall_time;
+		unsigned int cur_load;
+		
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time, 0);
+
+		wall_time = (unsigned int)
+			(cur_wall_time - j_darkness_cpuinfo->prev_cpu_wall);
+		j_darkness_cpuinfo->prev_cpu_wall = cur_wall_time;
+
+		idle_time = (unsigned int)
+			(cur_idle_time - j_darkness_cpuinfo->prev_cpu_idle);
+		j_darkness_cpuinfo->prev_cpu_idle = cur_idle_time;
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		/*
+		 * If the CPU had gone completely idle, and a task just woke up
+		 * on this CPU now, it would be unfair to calculate 'load' the
+		 * usual way for this elapsed time-window, because it will show
+		 * near-zero load, irrespective of how CPU intensive that task
+		 * actually is. This is undesirable for latency-sensitive bursty
+		 * workloads.
+		 *
+		 * To avoid this, we reuse the 'load' from the previous
+		 * time-window and give this task a chance to start with a
+		 * reasonably high CPU frequency. (However, we shouldn't over-do
+		 * this copy, lest we get stuck at a high load (high frequency)
+		 * for too long, even when the current system load has actually
+		 * dropped down. So we perform the copy only once, upon the
+		 * first wake-up from idle.)
+		 *
+		 * Detecting this situation is easy: the governor's deferrable
+		 * timer would not have fired during CPU-idle periods. Hence
+		 * an unusually large 'wall_time' (as compared to the sampling
+		 * rate) indicates this scenario.
+		 *
+		 * prev_load can be zero in two cases and we must recalculate it
+		 * for both cases:
+		 * - during long idle intervals
+		 * - explicitly set to zero
+		 */
+		if (unlikely(wall_time > (2 * sampling_rate) &&
+			     j_darkness_cpuinfo->prev_load)) {
+			cur_load = j_darkness_cpuinfo->prev_load;
+
+			/*
+			 * Perform a destructive copy, to ensure that we copy
+			 * the previous load only once, upon the first wake-up
+			 * from idle.
+			 */
+			j_darkness_cpuinfo->prev_load = 0;
+		} else {
+			cur_load = 100 * (wall_time - idle_time) / wall_time;
+			j_darkness_cpuinfo->prev_load = cur_load;
+		}
+
+		if (cur_load > max_load)
+			max_load = cur_load;
+	}
+
+	cpufreq_notify_utilization(policy, max_load);
+
+	/* CPUs Online Scale Frequency*/
+	next_freq = adjust_cpufreq_frequency_target(policy, this_darkness_cpuinfo->freq_table, 
+												max_load * (policy->max / 100));
+	if (next_freq != policy->cur && next_freq > 0)
+		__cpufreq_driver_target(policy, next_freq, CPUFREQ_RELATION_L);
+}
+
+static void do_darkness_timer(struct work_struct *work)
+{
+	struct cpufreq_darkness_cpuinfo *this_darkness_cpuinfo = 
+		container_of(work, struct cpufreq_darkness_cpuinfo, work.work);
+	int delay;
+
+	if (unlikely(!cpu_online(this_darkness_cpuinfo->cpu) ||
+				!this_darkness_cpuinfo->cur_policy))
+		return;
+
+	mutex_lock(&this_darkness_cpuinfo->timer_mutex);
+
+	darkness_check_cpu(this_darkness_cpuinfo);
+
+	delay = usecs_to_jiffies(darkness_tuners_ins.sampling_rate);
+	/* We want all CPUs to do sampling nearly on
+	 * same jiffy
+	 */
+	if (num_online_cpus() > 1) {
+		delay -= jiffies % delay;
+	}
+
+	queue_delayed_work_on(this_darkness_cpuinfo->cpu, darkness_wq,
+			&this_darkness_cpuinfo->work, delay);
+	mutex_unlock(&this_darkness_cpuinfo->timer_mutex);
+}
+
+static int cpufreq_governor_darkness(struct cpufreq_policy *policy,
+				unsigned int event)
+{
+	struct cpufreq_darkness_cpuinfo *this_darkness_cpuinfo;
+	unsigned int cpu = policy->cpu, j;
+	int rc, delay;
+
+	this_darkness_cpuinfo = &per_cpu(od_darkness_cpuinfo, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy))
+			return -EINVAL;
+
+		mutex_lock(&darkness_mutex);
+		this_darkness_cpuinfo->freq_table = cpufreq_frequency_get_table(cpu);
+		if (!this_darkness_cpuinfo->freq_table) {
+			mutex_unlock(&darkness_mutex);
+			return -EINVAL;
+		}
+
+		for_each_cpu(j, policy->cpus) {
+			struct cpufreq_darkness_cpuinfo *j_darkness_cpuinfo = &per_cpu(od_darkness_cpuinfo, j);
+			unsigned int prev_load;
+
+			j_darkness_cpuinfo->prev_cpu_idle = get_cpu_idle_time(j,
+				&j_darkness_cpuinfo->prev_cpu_wall, 0);
+
+			prev_load = (unsigned int)
+				(j_darkness_cpuinfo->prev_cpu_wall -
+				j_darkness_cpuinfo->prev_cpu_idle);
+			j_darkness_cpuinfo->prev_load = 100 * prev_load /
+				(unsigned int) j_darkness_cpuinfo->prev_cpu_wall;
+		}
+
+		darkness_enable++;
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (darkness_enable == 1) {
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&darkness_attr_group);
+			if (rc) {
+				darkness_enable--;
+				mutex_unlock(&darkness_mutex);
+				return rc;
+			}
+		}
+		cpu = policy->cpu;
+		this_darkness_cpuinfo->cpu = cpu;
+		this_darkness_cpuinfo->cur_policy = policy;
+		this_darkness_cpuinfo->governor_enabled = true;
+		mutex_unlock(&darkness_mutex);
+
+		mutex_init(&this_darkness_cpuinfo->timer_mutex);
+
+		delay = usecs_to_jiffies(darkness_tuners_ins.sampling_rate);
+		/* We want all CPUs to do sampling nearly on same jiffy */
+		if (num_online_cpus() > 1) {
+			delay -= jiffies % delay;
+		}
+
+		INIT_DEFERRABLE_WORK(&this_darkness_cpuinfo->work, do_darkness_timer);
+		queue_delayed_work_on(cpu,
+			darkness_wq, &this_darkness_cpuinfo->work, delay);
+
+		break;
+	case CPUFREQ_GOV_STOP:
+		cancel_delayed_work_sync(&this_darkness_cpuinfo->work);
+
+		mutex_lock(&darkness_mutex);
+		mutex_destroy(&this_darkness_cpuinfo->timer_mutex);
+
+		this_darkness_cpuinfo->governor_enabled = false;
+
+		this_darkness_cpuinfo->cur_policy = NULL;
+
+		darkness_enable--;
+		if (!darkness_enable) {
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &darkness_attr_group);
+		}
+		mutex_unlock(&darkness_mutex);
+
+		break;
+	case CPUFREQ_GOV_LIMITS:
+		if (!this_darkness_cpuinfo->cur_policy
+			 || !policy) {
+			pr_debug("Unable to limit cpu freq due to cur_policy == NULL\n");
+			return -EPERM;
+		}
+		mutex_lock(&this_darkness_cpuinfo->timer_mutex);
+		__cpufreq_driver_target(this_darkness_cpuinfo->cur_policy,
+				policy->cur, CPUFREQ_RELATION_L);
+		mutex_unlock(&this_darkness_cpuinfo->timer_mutex);
+
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_DARKNESS
+static
+#endif
+struct cpufreq_governor cpufreq_gov_darkness = {
+	.name                   = "darkness",
+	.governor               = cpufreq_governor_darkness,
+	.owner                  = THIS_MODULE,
+};
+
+static int __init cpufreq_gov_darkness_init(void)
+{
+	darkness_wq = alloc_workqueue("darkness_wq", WQ_HIGHPRI, 0);
+	if (!darkness_wq) {
+		printk(KERN_ERR "Failed to create darkness_wq workqueue\n");
+		return -EFAULT;
+	}
+
+	return cpufreq_register_governor(&cpufreq_gov_darkness);
+}
+
+static void __exit cpufreq_gov_darkness_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_darkness);
+}
+
+MODULE_AUTHOR("Alucard24@XDA");
+MODULE_DESCRIPTION("'cpufreq_darkness' - A dynamic cpufreq/cpuhotplug governor v5.0 (SnapDragon)");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_DARKNESS
+fs_initcall(cpufreq_gov_darkness_init);
+#else
+module_init(cpufreq_gov_darkness_init);
+#endif
+module_exit(cpufreq_gov_darkness_exit);
diff --git a/drivers/cpufreq/cpufreq_despair.c b/drivers/cpufreq/cpufreq_despair.c
new file mode 100644
index 0000000..6865017
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_despair.c
@@ -0,0 +1,550 @@
+/*
+ *  drivers/cpufreq/cpufreq_despair.c
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *            (C)  2009 Alexander Clouter <alex@digriz.org.uk>
+ *				   2015 Matthew Alex <matthewalex@outlook.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/slab.h>
+#include "cpufreq_governor.h"
+#include <linux/touchboost.h>
+
+/* Conservative governor macros */
+#define DEF_FREQUENCY_UP_THRESHOLD		(95)
+#define DEF_FREQUENCY_DOWN_THRESHOLD		(35)
+#define DEF_FREQUENCY_TWOSTEP_THRESHOLD		(65)
+#define DEF_FREQUENCY_STEP			(5)
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define MAX_SAMPLING_DOWN_FACTOR		(10)
+#define MICRO_FREQUENCY_MIN_SAMPLE_RATE		(10000)
+#define BOOST_DURATION_US			(40000)
+#define BOOST_FREQ_VAL				(960000)
+#define DEFAULT_MIN_LOAD			(15)
+
+static DEFINE_PER_CPU(struct cs_cpu_dbs_info_s, cs_cpu_dbs_info);
+
+static inline unsigned int get_freq_target(struct cs_dbs_tuners *cs_tuners,
+					   unsigned int freq_mult)
+{
+	unsigned int freq_target = (cs_tuners->freq_step * freq_mult) / 100;
+
+	/* max freq cannot be less than 100. But who knows... */
+	if (unlikely(freq_target == 0))
+		freq_target = DEF_FREQUENCY_STEP;
+
+	return freq_target;
+}
+
+/*
+ * Every sampling_rate, we check, if current idle time is less than 20%
+ * (default), then we try to increase frequency. Every sampling_rate *
+ * sampling_down_factor, we check, if current idle time is more than 80%
+ * (default), then we try to decrease frequency
+ *
+ * Any frequency increase takes it to the maximum frequency. Frequency reduction
+ * happens at minimum steps of 5% (default) of maximum frequency
+ */
+static void cs_check_cpu(int cpu, unsigned int load)
+{
+	struct cs_cpu_dbs_info_s *dbs_info = &per_cpu(cs_cpu_dbs_info, cpu);
+	struct cpufreq_policy *policy = dbs_info->cdbs.cur_policy;
+	struct dbs_data *dbs_data = policy->governor_data;
+	struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+	bool boosted;
+	u64 now;
+
+	cpufreq_notify_utilization(policy, load);
+
+	/*
+	 * break out if we 'cannot' reduce the speed as the user might
+	 * want freq_step to be zero
+	 */
+	if (cs_tuners->freq_step == 0)
+		return;
+
+	now = ktime_to_us(ktime_get());
+	boosted = now < (get_input_time() + cs_tuners->input_boost_duration);
+
+	/* Check for frequency increase */
+	if (load > DEF_FREQUENCY_TWOSTEP_THRESHOLD) {
+		if (load >= cs_tuners->up_threshold)
+			dbs_info->down_skip = 0;
+
+		/* if we are already at full speed then break out early */
+		if (policy->cur == policy->max)
+			return;
+
+		if (load < cs_tuners->up_threshold && dbs_info->twostep_counter++ < 2) {
+			dbs_info->twostep_time = now;
+			dbs_info->requested_freq += get_freq_target(cs_tuners, policy->max >> 1);
+		} else {
+			if (load >= cs_tuners->up_threshold)
+				dbs_info->requested_freq += get_freq_target(cs_tuners, policy->max);
+
+			dbs_info->twostep_counter = 0;
+		}
+
+		if (dbs_info->requested_freq > policy->max)
+			dbs_info->requested_freq = policy->max;
+
+		if (boosted)
+                        dbs_info->requested_freq
+                                = max(cs_tuners->input_boost_freq,
+                                        dbs_info->requested_freq);
+
+		__cpufreq_driver_target(policy, dbs_info->requested_freq,
+			CPUFREQ_RELATION_C);
+		return;
+	}
+
+	/* if sampling_down_factor is active break out early */
+	if (++dbs_info->down_skip < cs_tuners->sampling_down_factor)
+		return;
+	dbs_info->down_skip = 0;
+
+	/* Check for frequency decrease */
+	if (load < cs_tuners->down_threshold) {
+		unsigned int freq_target;
+
+		/*
+		 * we're scaling down, so reset the counter if
+		 * the conditions are met
+		 */
+		if (dbs_info->twostep_counter) {
+			/* 150ms*/
+			if ((now - dbs_info->twostep_time) >= 150000)
+                		dbs_info->twostep_counter = 0;
+		}
+
+		/*
+		 * if we cannot reduce the frequency anymore, break out early
+		 */
+		if (policy->cur == policy->min)
+			return;
+
+		if (load < DEFAULT_MIN_LOAD) {
+			dbs_info->requested_freq = policy->min;
+			goto scale_down;
+		}
+
+		freq_target = get_freq_target(cs_tuners, policy->max);
+		if (dbs_info->requested_freq > freq_target)
+			dbs_info->requested_freq -= freq_target;
+		else
+			dbs_info->requested_freq = policy->min;
+
+scale_down:
+
+		if (boosted)
+                        dbs_info->requested_freq
+				= max(cs_tuners->input_boost_freq,
+					dbs_info->requested_freq);
+
+		__cpufreq_driver_target(policy, dbs_info->requested_freq,
+				CPUFREQ_RELATION_L);
+		return;
+	}
+}
+
+static void cs_dbs_timer(struct work_struct *work)
+{
+	struct cs_cpu_dbs_info_s *dbs_info = container_of(work,
+			struct cs_cpu_dbs_info_s, cdbs.work.work);
+	unsigned int cpu = dbs_info->cdbs.cur_policy->cpu;
+	struct cs_cpu_dbs_info_s *core_dbs_info = &per_cpu(cs_cpu_dbs_info,
+			cpu);
+	struct dbs_data *dbs_data = dbs_info->cdbs.cur_policy->governor_data;
+	struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+	int delay = delay_for_sampling_rate(cs_tuners->sampling_rate);
+	bool modify_all = true;
+
+	mutex_lock(&core_dbs_info->cdbs.timer_mutex);
+	if (!need_load_eval(&core_dbs_info->cdbs, cs_tuners->sampling_rate))
+		modify_all = false;
+	else
+		dbs_check_cpu(dbs_data, cpu);
+
+	gov_queue_work(dbs_data, dbs_info->cdbs.cur_policy, delay, modify_all);
+	mutex_unlock(&core_dbs_info->cdbs.timer_mutex);
+}
+
+static int dbs_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
+		void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cs_cpu_dbs_info_s *dbs_info =
+					&per_cpu(cs_cpu_dbs_info, freq->cpu);
+	struct cpufreq_policy *policy;
+
+	if (!dbs_info->enable)
+		return 0;
+
+	policy = dbs_info->cdbs.cur_policy;
+
+	/*
+	 * we only care if our internally tracked freq moves outside the 'valid'
+	 * ranges of frequency available to us otherwise we do not change it
+	*/
+	if (dbs_info->requested_freq > policy->max
+			|| dbs_info->requested_freq < policy->min)
+		dbs_info->requested_freq = freq->new;
+
+	return 0;
+}
+
+/************************** sysfs interface ************************/
+static struct common_dbs_data cs_dbs_cdata;
+
+static ssize_t store_sampling_down_factor(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+
+	cs_tuners->sampling_down_factor = input;
+	return count;
+}
+
+static ssize_t store_sampling_rate(struct dbs_data *dbs_data, const char *buf,
+		size_t count)
+{
+	struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	cs_tuners->sampling_rate = max(input, dbs_data->min_sampling_rate);
+	return count;
+}
+
+static ssize_t store_up_threshold(struct dbs_data *dbs_data, const char *buf,
+		size_t count)
+{
+	struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 || input <= cs_tuners->down_threshold)
+		return -EINVAL;
+
+	cs_tuners->up_threshold = input;
+	return count;
+}
+
+static ssize_t store_down_threshold(struct dbs_data *dbs_data, const char *buf,
+		size_t count)
+{
+	struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	/* cannot be lower than 11 otherwise freq will not fall */
+	if (ret != 1 || input < 11 || input > 100 ||
+			input >= cs_tuners->up_threshold)
+		return -EINVAL;
+
+	cs_tuners->down_threshold = input;
+	return count;
+}
+
+static ssize_t store_ignore_nice_load(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+	unsigned int input, j;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == cs_tuners->ignore_nice_load) /* nothing to do */
+		return count;
+
+	cs_tuners->ignore_nice_load = input;
+
+	/* we need to re-evaluate prev_cpu_idle */
+	for_each_online_cpu(j) {
+		struct cs_cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+		dbs_info->cdbs.prev_cpu_idle = get_cpu_idle_time(j,
+					&dbs_info->cdbs.prev_cpu_wall, 0);
+		if (cs_tuners->ignore_nice_load)
+			dbs_info->cdbs.prev_cpu_nice =
+				kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+	}
+	return count;
+}
+
+static ssize_t store_freq_step(struct dbs_data *dbs_data, const char *buf,
+		size_t count)
+{
+	struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 100)
+		input = 100;
+
+	/*
+	 * no need to test here if freq_step is zero as the user might actually
+	 * want this, they would be crazy though :)
+	 */
+	cs_tuners->freq_step = input;
+	return count;
+}
+
+static ssize_t store_input_boost_freq(struct dbs_data *dbs_data, const char *buf,
+                size_t count)
+{
+        struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+        unsigned int input;
+        int ret;
+        ret = sscanf(buf, "%u", &input);
+
+        if (ret != 1)
+                return -EINVAL;
+
+        if (input < 0)
+                input = 0;
+
+        cs_tuners->input_boost_freq = input;
+        return count;
+}
+
+static ssize_t store_input_boost_duration(struct dbs_data *dbs_data, const char *buf,
+                size_t count)
+{
+        struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+        unsigned int input;
+        int ret;
+        ret = sscanf(buf, "%u", &input);
+
+        if (ret != 1)
+                return -EINVAL;
+
+        if (input < 0)
+                input = 0;
+
+        cs_tuners->input_boost_duration = input;
+        return count;
+}
+
+static ssize_t store_min_load(struct dbs_data *dbs_data, const char *buf,
+                size_t count)
+{
+        struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+        unsigned int input;
+        int ret;
+        ret = sscanf(buf, "%u", &input);
+
+        if (ret != 1)
+                return -EINVAL;
+
+        if (input < 0)
+                input = 0;
+
+        cs_tuners->min_load = input;
+        return count;
+}
+
+static ssize_t store_twostep_threshold(struct dbs_data *dbs_data, const char *buf,
+                size_t count)
+{
+        struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+        unsigned int input;
+        int ret;
+        ret = sscanf(buf, "%u", &input);
+
+        if (ret != 1)
+                return -EINVAL;
+
+        if (input < 0)
+                input = 0;
+
+        cs_tuners->twostep_threshold = input;
+        return count;
+}
+
+show_store_one(cs, sampling_rate);
+show_store_one(cs, sampling_down_factor);
+show_store_one(cs, up_threshold);
+show_store_one(cs, down_threshold);
+show_store_one(cs, ignore_nice_load);
+show_store_one(cs, freq_step);
+declare_show_sampling_rate_min(cs);
+show_store_one(cs, input_boost_freq);
+show_store_one(cs, input_boost_duration);
+show_store_one(cs, min_load);
+show_store_one(cs, twostep_threshold);
+
+gov_sys_pol_attr_rw(sampling_rate);
+gov_sys_pol_attr_rw(sampling_down_factor);
+gov_sys_pol_attr_rw(up_threshold);
+gov_sys_pol_attr_rw(down_threshold);
+gov_sys_pol_attr_rw(ignore_nice_load);
+gov_sys_pol_attr_rw(freq_step);
+gov_sys_pol_attr_ro(sampling_rate_min);
+gov_sys_pol_attr_rw(input_boost_freq);
+gov_sys_pol_attr_rw(input_boost_duration);
+gov_sys_pol_attr_rw(min_load);
+gov_sys_pol_attr_rw(twostep_threshold);
+
+static struct attribute *dbs_attributes_gov_sys[] = {
+	&sampling_rate_min_gov_sys.attr,
+	&sampling_rate_gov_sys.attr,
+	&sampling_down_factor_gov_sys.attr,
+	&up_threshold_gov_sys.attr,
+	&down_threshold_gov_sys.attr,
+	&ignore_nice_load_gov_sys.attr,
+	&freq_step_gov_sys.attr,
+	&input_boost_freq_gov_sys.attr,
+	&input_boost_duration_gov_sys.attr,
+	&min_load_gov_sys.attr,
+	&twostep_threshold_gov_sys.attr,
+	NULL
+};
+
+static struct attribute_group cs_attr_group_gov_sys = {
+	.attrs = dbs_attributes_gov_sys,
+	.name = "despair",
+};
+
+static struct attribute *dbs_attributes_gov_pol[] = {
+	&sampling_rate_min_gov_pol.attr,
+	&sampling_rate_gov_pol.attr,
+	&sampling_down_factor_gov_pol.attr,
+	&up_threshold_gov_pol.attr,
+	&down_threshold_gov_pol.attr,
+	&ignore_nice_load_gov_pol.attr,
+	&freq_step_gov_pol.attr,
+	&input_boost_freq_gov_pol.attr,
+	&input_boost_duration_gov_pol.attr,
+	&min_load_gov_pol.attr,
+	&twostep_threshold_gov_pol.attr,
+	NULL
+};
+
+static struct attribute_group cs_attr_group_gov_pol = {
+	.attrs = dbs_attributes_gov_pol,
+	.name = "despair",
+};
+
+/************************** sysfs end ************************/
+
+static int cs_init(struct dbs_data *dbs_data)
+{
+	struct cs_dbs_tuners *tuners;
+
+	tuners = kzalloc(sizeof(*tuners), GFP_KERNEL);
+	if (!tuners) {
+		pr_err("%s: kzalloc failed\n", __func__);
+		return -ENOMEM;
+	}
+
+	tuners->up_threshold = DEF_FREQUENCY_UP_THRESHOLD;
+	tuners->down_threshold = DEF_FREQUENCY_DOWN_THRESHOLD;
+	tuners->sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR;
+	tuners->ignore_nice_load = 0;
+	tuners->freq_step = DEF_FREQUENCY_STEP;
+	tuners->input_boost_freq = BOOST_FREQ_VAL;
+    tuners->input_boost_duration = BOOST_DURATION_US;
+	tuners->min_load=DEFAULT_MIN_LOAD;	
+	tuners->twostep_threshold=DEF_FREQUENCY_TWOSTEP_THRESHOLD;	
+
+	dbs_data->tuners = tuners;
+	dbs_data->min_sampling_rate = MICRO_FREQUENCY_MIN_SAMPLE_RATE;
+	mutex_init(&dbs_data->mutex);
+	return 0;
+}
+
+static void cs_exit(struct dbs_data *dbs_data)
+{
+	kfree(dbs_data->tuners);
+}
+
+define_get_cpu_dbs_routines(cs_cpu_dbs_info);
+
+static struct notifier_block cs_cpufreq_notifier_block = {
+	.notifier_call = dbs_cpufreq_notifier,
+};
+
+static struct cs_ops cs_ops = {
+	.notifier_block = &cs_cpufreq_notifier_block,
+};
+
+static struct common_dbs_data cs_dbs_cdata = {
+	.governor = GOV_CONSERVATIVE,
+	.attr_group_gov_sys = &cs_attr_group_gov_sys,
+	.attr_group_gov_pol = &cs_attr_group_gov_pol,
+	.get_cpu_cdbs = get_cpu_cdbs,
+	.get_cpu_dbs_info_s = get_cpu_dbs_info_s,
+	.gov_dbs_timer = cs_dbs_timer,
+	.gov_check_cpu = cs_check_cpu,
+	.gov_ops = &cs_ops,
+	.init = cs_init,
+	.exit = cs_exit,
+};
+
+static int cs_cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	return cpufreq_governor_dbs(policy, &cs_dbs_cdata, event);
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_DESPAIR
+static
+#endif
+struct cpufreq_governor cpufreq_gov_despair = {
+	.name			= "despair",
+	.governor		= cs_cpufreq_governor_dbs,
+	.max_transition_latency	= TRANSITION_LATENCY_LIMIT,
+	.owner			= THIS_MODULE,
+};
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	return cpufreq_register_governor(&cpufreq_gov_despair);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_despair);
+}
+
+MODULE_AUTHOR("Alexander Clouter <alex@digriz.org.uk>");
+MODULE_DESCRIPTION("'cpufreq_despair' - A dynamic cpufreq governor for "
+		"Low Latency Frequency Transition capable processors "
+		"optimised for use in a battery environment");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_DESPAIR
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_electroactive.c b/drivers/cpufreq/cpufreq_electroactive.c
new file mode 100644
index 0000000..9580af3
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_electroactive.c
@@ -0,0 +1,1612 @@
+/*
+ *  drivers/cpufreq/cpufreq_electroative.c
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *            (C)  2015 Yusuf Mostafa <ymostafa30@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/sched/rt.h>
+#include <linux/input.h>
+#include <linux/workqueue.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/msm_kgsl.h>
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/cpufreq_electroactive.h>
+
+
+static int orig_up_threshold = 90;
+static int g_count = 0;
+
+#define DEF_SAMPLING_RATE			(30000)
+#define DEF_FREQUENCY_DOWN_DIFFERENTIAL		(10)
+#define DEF_FREQUENCY_UP_THRESHOLD		(90)
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define MAX_SAMPLING_DOWN_FACTOR		(100000)
+#define MICRO_FREQUENCY_DOWN_DIFFERENTIAL	(3)
+#define MICRO_FREQUENCY_UP_THRESHOLD		(95)
+#define MICRO_FREQUENCY_MIN_SAMPLE_RATE		(10000)
+#define MIN_FREQUENCY_UP_THRESHOLD		(11)
+#define MAX_FREQUENCY_UP_THRESHOLD		(100)
+#define MIN_FREQUENCY_DOWN_DIFFERENTIAL		(1)
+#define UI_DYNAMIC_SAMPLING_RATE		(15000)
+#define DBS_SWITCH_MODE_TIMEOUT			(1000)
+#define INPUT_EVENT_MIN_TIMEOUT 		(0)
+#define INPUT_EVENT_MAX_TIMEOUT 		(3000)
+#define INPUT_EVENT_TIMEOUT			(500)
+#define MIN_SAMPLING_RATE_RATIO			(2)
+
+static unsigned int min_sampling_rate;
+static unsigned int skip_electroactive = 0;
+
+#define LATENCY_MULTIPLIER			(1000)
+#define MIN_LATENCY_MULTIPLIER			(100)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+#define POWERSAVE_BIAS_MAXLEVEL			(1000)
+#define POWERSAVE_BIAS_MINLEVEL			(-1000)
+
+static void do_dbs_timer(struct work_struct *work);
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_ELECTROACTIVE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_electroactive = {
+       .name                   = "electroactive",
+       .governor               = cpufreq_governor_dbs,
+       .max_transition_latency = TRANSITION_LATENCY_LIMIT,
+       .owner                  = THIS_MODULE,
+};
+
+enum {DBS_NORMAL_SAMPLE, DBS_SUB_SAMPLE};
+
+struct cpu_dbs_info_s {
+	cputime64_t prev_cpu_idle;
+	cputime64_t prev_cpu_iowait;
+	cputime64_t prev_cpu_wall;
+	cputime64_t prev_cpu_nice;
+	struct cpufreq_policy *cur_policy;
+	struct delayed_work work;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int freq_lo;
+	unsigned int freq_lo_jiffies;
+	unsigned int freq_hi_jiffies;
+	unsigned int rate_mult;
+	unsigned int prev_load;
+	unsigned int max_load;
+	int input_event_freq;
+	int cpu;
+	unsigned int sample_type:1;
+	struct mutex timer_mutex;
+};
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, od_cpu_dbs_info);
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info);
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info);
+
+static unsigned int dbs_enable;	/* number of CPUs using this policy */
+
+/*
+ * dbs_mutex protects dbs_enable and dbs_info during start/stop.
+ */
+static DEFINE_PER_CPU(struct task_struct *, up_task);
+static spinlock_t input_boost_lock;
+static bool input_event_boost = false;
+static unsigned long input_event_boost_expired = 0;
+
+#define TABLE_SIZE			5
+#define MAX(x,y)			(x > y ? x : y)
+#define MIN(x,y)			(x < y ? x : y)
+#define FREQ_NEED_BURST(x)		(x < 600000 ? 1 : 0)
+
+static	struct cpufreq_frequency_table *tbl = NULL;
+static unsigned int *tblmap[TABLE_SIZE] __read_mostly;
+static unsigned int tbl_select[4];
+static unsigned int up_threshold_level[2] __read_mostly = {95, 85};
+static int input_event_counter = 0;
+struct timer_list freq_mode_timer;
+
+static inline void switch_turbo_mode(unsigned);
+static inline void switch_normal_mode(void);
+
+static DEFINE_MUTEX(dbs_mutex);
+
+static struct dbs_tuners {
+	unsigned int sampling_rate;
+	unsigned int up_threshold;
+	unsigned int up_threshold_multi_core;
+	unsigned int down_differential;
+	unsigned int down_differential_multi_core;
+	unsigned int optimal_freq;
+	unsigned int up_threshold_any_cpu_load;
+	unsigned int sync_freq;
+	unsigned int ignore_nice;
+	unsigned int sampling_down_factor;
+	int           powersave_bias;
+	unsigned int io_is_busy;
+	unsigned int two_phase_freq;
+	unsigned int origin_sampling_rate;
+	unsigned int ui_sampling_rate;
+	unsigned int input_event_timeout;
+	int gboost;
+} dbs_tuners_ins = {
+	.up_threshold_multi_core = DEF_FREQUENCY_UP_THRESHOLD,
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.down_differential = DEF_FREQUENCY_DOWN_DIFFERENTIAL,
+	.down_differential_multi_core = MICRO_FREQUENCY_DOWN_DIFFERENTIAL,
+	.up_threshold_any_cpu_load = DEF_FREQUENCY_UP_THRESHOLD,
+	.ignore_nice = 0,
+	.powersave_bias = 0,
+	.sync_freq = 0,
+	.optimal_freq = 0,
+	.io_is_busy = 1,
+	.two_phase_freq = 0,
+	.ui_sampling_rate = UI_DYNAMIC_SAMPLING_RATE,
+	.input_event_timeout = INPUT_EVENT_TIMEOUT,
+	.gboost = 1,
+};
+
+static inline cputime64_t get_cpu_iowait_time(unsigned int cpu, cputime64_t *wall)
+{
+	u64 iowait_time = get_cpu_iowait_time_us(cpu, wall);
+
+	if (iowait_time == -1ULL)
+		return 0;
+
+	return iowait_time;
+}
+
+static unsigned int powersave_bias_target(struct cpufreq_policy *policy,
+					  unsigned int freq_next,
+					  unsigned int relation)
+{
+	unsigned int freq_req, freq_avg;
+	unsigned int freq_hi, freq_lo;
+	unsigned int index = 0;
+	unsigned int jiffies_total, jiffies_hi, jiffies_lo;
+	int freq_reduc;
+	struct cpu_dbs_info_s *dbs_info = &per_cpu(od_cpu_dbs_info,
+						   policy->cpu);
+
+	if (!dbs_info->freq_table) {
+		dbs_info->freq_lo = 0;
+		dbs_info->freq_lo_jiffies = 0;
+		return freq_next;
+	}
+
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_next,
+			relation, &index);
+	freq_req = dbs_info->freq_table[index].frequency;
+	freq_reduc = freq_req * dbs_tuners_ins.powersave_bias / 1000;
+	freq_avg = freq_req - freq_reduc;
+
+
+	index = 0;
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_avg,
+			CPUFREQ_RELATION_H, &index);
+	freq_lo = dbs_info->freq_table[index].frequency;
+	index = 0;
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_avg,
+			CPUFREQ_RELATION_L, &index);
+	freq_hi = dbs_info->freq_table[index].frequency;
+
+
+	if (freq_hi == freq_lo) {
+		dbs_info->freq_lo = 0;
+		dbs_info->freq_lo_jiffies = 0;
+		return freq_lo;
+	}
+	jiffies_total = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+	jiffies_hi = (freq_avg - freq_lo) * jiffies_total;
+	jiffies_hi += ((freq_hi - freq_lo) / 2);
+	jiffies_hi /= (freq_hi - freq_lo);
+	jiffies_lo = jiffies_total - jiffies_hi;
+	dbs_info->freq_lo = freq_lo;
+	dbs_info->freq_lo_jiffies = jiffies_lo;
+	dbs_info->freq_hi_jiffies = jiffies_hi;
+	return freq_hi;
+}
+
+static int electroactive_powersave_bias_setspeed(struct cpufreq_policy *policy,
+					    struct cpufreq_policy *altpolicy,
+					    int level)
+{
+	if (level == POWERSAVE_BIAS_MAXLEVEL) {
+
+		__cpufreq_driver_target(policy,
+			(altpolicy) ? altpolicy->min : policy->min,
+			CPUFREQ_RELATION_L);
+		return 1;
+	} else if (level == POWERSAVE_BIAS_MINLEVEL) {
+
+		__cpufreq_driver_target(policy,
+			(altpolicy) ? altpolicy->max : policy->max,
+			CPUFREQ_RELATION_H);
+		return 1;
+	}
+	return 0;
+}
+
+static void electroactive_powersave_bias_init_cpu(int cpu)
+{
+	struct cpu_dbs_info_s *dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+	dbs_info->freq_table = cpufreq_frequency_get_table(cpu);
+	dbs_info->freq_lo = 0;
+}
+
+static void electroactive_powersave_bias_init(void)
+{
+	int i;
+	for_each_online_cpu(i) {
+		electroactive_powersave_bias_init_cpu(i);
+	}
+}
+
+static ssize_t show_sampling_rate_min(struct kobject *kobj,
+				      struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", min_sampling_rate);
+}
+
+define_one_global_ro(sampling_rate_min);
+
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)              \
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(up_threshold, up_threshold);
+show_one(up_threshold_multi_core, up_threshold_multi_core);
+show_one(down_differential, down_differential);
+show_one(sampling_down_factor, sampling_down_factor);
+show_one(ignore_nice_load, ignore_nice);
+show_one(optimal_freq, optimal_freq);
+show_one(up_threshold_any_cpu_load, up_threshold_any_cpu_load);
+show_one(sync_freq, sync_freq);
+show_one(gboost, gboost);
+
+static ssize_t show_powersave_bias
+(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%d\n", dbs_tuners_ins.powersave_bias);
+}
+
+static void update_sampling_rate(unsigned int new_rate)
+{
+	int cpu;
+
+	dbs_tuners_ins.sampling_rate = new_rate
+				     = max(new_rate, min_sampling_rate);
+
+	for_each_online_cpu(cpu) {
+		struct cpufreq_policy *policy;
+		struct cpu_dbs_info_s *dbs_info;
+		unsigned long next_sampling, appointed_at;
+
+		policy = cpufreq_cpu_get(cpu);
+		if (!policy)
+			continue;
+		dbs_info = &per_cpu(od_cpu_dbs_info, policy->cpu);
+		cpufreq_cpu_put(policy);
+
+		mutex_lock(&dbs_info->timer_mutex);
+
+		if (!delayed_work_pending(&dbs_info->work)) {
+			mutex_unlock(&dbs_info->timer_mutex);
+			continue;
+		}
+
+		next_sampling  = jiffies + usecs_to_jiffies(new_rate);
+		appointed_at = dbs_info->work.timer.expires;
+
+		if (time_before(next_sampling, appointed_at)) {
+
+			mutex_unlock(&dbs_info->timer_mutex);
+			cancel_delayed_work_sync(&dbs_info->work);
+			mutex_lock(&dbs_info->timer_mutex);
+
+			schedule_delayed_work_on(dbs_info->cpu, &dbs_info->work,
+						 usecs_to_jiffies(new_rate));
+
+		}
+		mutex_unlock(&dbs_info->timer_mutex);
+	}
+}
+
+show_one(input_event_timeout, input_event_timeout);
+
+static ssize_t store_input_event_timeout(struct kobject *a, struct attribute *b,
+					const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+	return -EINVAL;
+
+	input = max(input, (unsigned int)INPUT_EVENT_MIN_TIMEOUT);
+	dbs_tuners_ins.input_event_timeout = min(input, (unsigned int)INPUT_EVENT_MAX_TIMEOUT);
+
+	return count;
+}
+
+static int two_phase_freq_array[NR_CPUS] = {[0 ... NR_CPUS-1] = 1728000} ;
+
+static ssize_t show_two_phase_freq
+(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i = 0 ;
+	int shift = 0 ;
+	char *buf_pos = buf;
+	for ( i = 0 ; i < NR_CPUS; i++) {
+		shift = sprintf(buf_pos,"%d,",two_phase_freq_array[i]);
+		buf_pos += shift;
+	}
+	*(buf_pos-1) = '\0';
+	return strlen(buf);
+}
+
+static ssize_t store_two_phase_freq(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count)
+{
+	int ret = 0;
+	if (NR_CPUS == 1)
+		ret = sscanf(buf,"%u",&two_phase_freq_array[0]);
+	else if (NR_CPUS == 2)
+		ret = sscanf(buf,"%u,%u",&two_phase_freq_array[0],
+				&two_phase_freq_array[1]);
+	else if (NR_CPUS == 4)
+		ret = sscanf(buf, "%u,%u,%u,%u", &two_phase_freq_array[0],
+				&two_phase_freq_array[1],
+				&two_phase_freq_array[2],
+				&two_phase_freq_array[3]);
+	if (ret < NR_CPUS)
+		return -EINVAL;
+
+	return count;
+}
+
+static int input_event_min_freq_array[NR_CPUS] = {1728000, 1267200, 1267200, 1267200} ;
+
+static ssize_t show_input_event_min_freq
+(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i = 0 ;
+	int shift = 0 ;
+	char *buf_pos = buf;
+	for ( i = 0 ; i < NR_CPUS; i++) {
+		shift = sprintf(buf_pos,"%d,",input_event_min_freq_array[i]);
+		buf_pos += shift;
+	}
+	*(buf_pos-1) = '\0';
+	return strlen(buf);
+}
+
+static ssize_t store_input_event_min_freq(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count)
+{
+	int ret = 0;
+	if (NR_CPUS == 1)
+		ret = sscanf(buf,"%u",&input_event_min_freq_array[0]);
+	else if (NR_CPUS == 2)
+		ret = sscanf(buf,"%u,%u",&input_event_min_freq_array[0],
+				&input_event_min_freq_array[1]);
+	else if (NR_CPUS == 4)
+		ret = sscanf(buf, "%u,%u,%u,%u", &input_event_min_freq_array[0],
+				&input_event_min_freq_array[1],
+				&input_event_min_freq_array[2],
+				&input_event_min_freq_array[3]);
+	if (ret < NR_CPUS)
+		return -EINVAL;
+
+	return count;
+}
+
+show_one(ui_sampling_rate, ui_sampling_rate);
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	if (input == dbs_tuners_ins.origin_sampling_rate)
+		return count;
+	update_sampling_rate(input);
+	dbs_tuners_ins.origin_sampling_rate = dbs_tuners_ins.sampling_rate;
+	return count;
+}
+
+static ssize_t store_ui_sampling_rate(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.ui_sampling_rate = max(input, min_sampling_rate);
+
+	return count;
+}
+
+static ssize_t store_sync_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.sync_freq = input;
+	return count;
+}
+
+static ssize_t store_optimal_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.optimal_freq = input;
+	return count;
+}
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold = input;
+	orig_up_threshold = dbs_tuners_ins.up_threshold;
+	return count;
+}
+
+static ssize_t store_up_threshold_multi_core(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold_multi_core = input;
+	return count;
+}
+
+static ssize_t store_up_threshold_any_cpu_load(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold_any_cpu_load = input;
+	return count;
+}
+
+static ssize_t store_down_differential(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input >= dbs_tuners_ins.up_threshold ||
+			input < MIN_FREQUENCY_DOWN_DIFFERENTIAL) {
+		return -EINVAL;
+	}
+
+	dbs_tuners_ins.down_differential = input;
+
+	return count;
+}
+
+static ssize_t store_sampling_down_factor(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input, j;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+	dbs_tuners_ins.sampling_down_factor = input;
+
+
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(od_cpu_dbs_info, j);
+		dbs_info->rate_mult = 1;
+	}
+	return count;
+}
+
+static ssize_t store_ignore_nice_load(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	unsigned int j;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == dbs_tuners_ins.ignore_nice) {
+		return count;
+	}
+	dbs_tuners_ins.ignore_nice = input;
+
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(od_cpu_dbs_info, j);
+		dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&dbs_info->prev_cpu_wall, 0);
+		if (dbs_tuners_ins.ignore_nice)
+			dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+	}
+	return count;
+}
+
+static ssize_t store_powersave_bias(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	int input  = 0;
+	int bypass = 0;
+	int ret, cpu, reenable_timer, j;
+	struct cpu_dbs_info_s *dbs_info;
+	struct cpufreq_policy *policy;
+
+	struct cpumask cpus_timer_done;
+	cpumask_clear(&cpus_timer_done);
+
+	ret = sscanf(buf, "%d", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input >= POWERSAVE_BIAS_MAXLEVEL) {
+		input  = POWERSAVE_BIAS_MAXLEVEL;
+		bypass = 1;
+	} else if (input <= POWERSAVE_BIAS_MINLEVEL) {
+		input  = POWERSAVE_BIAS_MINLEVEL;
+		bypass = 1;
+	}
+
+	if (input == dbs_tuners_ins.powersave_bias) {
+
+		return count;
+	}
+
+	reenable_timer = ((dbs_tuners_ins.powersave_bias ==
+				POWERSAVE_BIAS_MAXLEVEL) ||
+				(dbs_tuners_ins.powersave_bias ==
+				POWERSAVE_BIAS_MINLEVEL));
+
+	dbs_tuners_ins.powersave_bias = input;
+
+        mutex_lock(&dbs_mutex);
+        get_online_cpus();
+
+	if (!bypass) {
+		if (reenable_timer) {
+                        /* reinstate dbs timer */
+			for_each_online_cpu(cpu) {
+//				if (lock_policy_rwsem_write(cpu) < 0)
+					continue;
+
+				dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+
+				for_each_cpu(j, &cpus_timer_done) {
+					if (!dbs_info->cur_policy) {
+						pr_err("Dbs policy is NULL\n");
+						goto skip_this_cpu;
+					}
+					if (cpumask_test_cpu(j, dbs_info->
+							cur_policy->cpus))
+						goto skip_this_cpu;
+				}
+
+				cpumask_set_cpu(cpu, &cpus_timer_done);
+				if (dbs_info->cur_policy) {
+
+					dbs_timer_init(dbs_info);
+				}
+skip_this_cpu:
+				up_write(&policy->rwsem);
+			}
+		}
+		electroactive_powersave_bias_init();
+	} else {
+                /* running at maximum or minimum frequencies; cancel
+ 		   dbs timer as periodic load sampling is not necessary */
+		for_each_online_cpu(cpu) {
+//			if (lock_policy_rwsem_write(cpu) < 0)
+				continue;
+
+			dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+
+			for_each_cpu(j, &cpus_timer_done) {
+				if (!dbs_info->cur_policy) {
+					pr_err("Dbs policy is NULL\n");
+					goto skip_this_cpu_bypass;
+				}
+				if (cpumask_test_cpu(j, dbs_info->
+							cur_policy->cpus))
+					goto skip_this_cpu_bypass;
+			}
+
+			cpumask_set_cpu(cpu, &cpus_timer_done);
+
+			if (dbs_info->cur_policy) {
+                                /* cpu using electroactive, cancel dbs timer */
+				dbs_timer_exit(dbs_info);
+
+                                mutex_lock(&dbs_info->timer_mutex);
+				electroactive_powersave_bias_setspeed(
+					dbs_info->cur_policy,
+					NULL,
+					input);
+				mutex_unlock(&dbs_info->timer_mutex);
+
+			}
+skip_this_cpu_bypass:
+			up_write(&policy->rwsem);
+		}
+	}
+
+        put_online_cpus();
+        mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_gboost(struct kobject *a, struct attribute *b,
+				const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if(ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.gboost = (input > 0 ? input : 0);
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(up_threshold);
+define_one_global_rw(down_differential);
+define_one_global_rw(sampling_down_factor);
+define_one_global_rw(ignore_nice_load);
+define_one_global_rw(powersave_bias);
+define_one_global_rw(up_threshold_multi_core);
+define_one_global_rw(optimal_freq);
+define_one_global_rw(up_threshold_any_cpu_load);
+define_one_global_rw(sync_freq);
+define_one_global_rw(two_phase_freq);
+define_one_global_rw(input_event_min_freq);
+define_one_global_rw(ui_sampling_rate);
+define_one_global_rw(input_event_timeout);
+define_one_global_rw(gboost);
+
+static struct attribute *dbs_attributes[] = {
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&up_threshold.attr,
+	&down_differential.attr,
+	&sampling_down_factor.attr,
+	&ignore_nice_load.attr,
+	&powersave_bias.attr,
+	&up_threshold_multi_core.attr,
+	&optimal_freq.attr,
+	&up_threshold_any_cpu_load.attr,
+	&sync_freq.attr,
+	&two_phase_freq.attr,
+	&input_event_min_freq.attr,
+	&ui_sampling_rate.attr,
+	&input_event_timeout.attr,
+	&gboost.attr,
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "electroactive",
+};
+
+
+static inline void switch_turbo_mode(unsigned timeout)
+{
+	if (timeout > 0)
+		mod_timer(&freq_mode_timer, jiffies + msecs_to_jiffies(timeout));
+	tbl_select[0] = 2;
+	tbl_select[1] = 3;
+	tbl_select[2] = 4;
+	tbl_select[3] = 4;
+}
+
+static inline void switch_normal_mode(void)
+{
+	if (input_event_counter > 0)
+		return;
+	tbl_select[0] = 0;
+	tbl_select[1] = 1;
+	tbl_select[2] = 2;
+	tbl_select[3] = 4;
+}
+
+static void switch_mode_timer(unsigned long data)
+{
+	switch_normal_mode();
+}
+
+static void dbs_init_freq_map_table(struct cpufreq_policy *policy)
+{
+	unsigned int min_diff, top1, top2;
+	int cnt, i, j;
+
+	tbl = cpufreq_frequency_get_table(0);
+	min_diff = policy->cpuinfo.max_freq;
+
+
+	for (cnt = 0; (tbl[cnt].frequency != CPUFREQ_TABLE_END); cnt++) {
+		if (cnt > 0)
+			min_diff = MIN(tbl[cnt].frequency - tbl[cnt-1].frequency, min_diff);
+	}
+
+	top1 = (policy->cpuinfo.max_freq + policy->cpuinfo.min_freq) / 2;
+	top2 = (policy->cpuinfo.max_freq + top1) / 2;
+
+	for (i = 0; i < TABLE_SIZE; i++) {
+
+		tblmap[i] = kmalloc(sizeof(unsigned int) * cnt, GFP_KERNEL);
+		BUG_ON(!tblmap[i]);
+
+		for (j = 0; j < cnt; j++)
+			tblmap[i][j] = tbl[j].frequency;
+	}
+
+	for (j = 0; j < cnt; j++) {
+
+		if (tbl[j].frequency < top1) {
+			tblmap[0][j] += MAX((top1 - tbl[j].frequency)/3, min_diff);
+		}
+
+		if (tbl[j].frequency < top2) {
+			tblmap[1][j] += MAX((top2 - tbl[j].frequency)/3, min_diff);
+			tblmap[2][j] += MAX(((top2 - tbl[j].frequency)*2)/5, min_diff);
+			tblmap[3][j] += MAX((top2 - tbl[j].frequency)/2, min_diff);
+		}
+		else {
+			tblmap[3][j] += MAX((policy->cpuinfo.max_freq - tbl[j].frequency)/3, min_diff);
+		}
+
+		tblmap[4][j] += MAX((policy->cpuinfo.max_freq - tbl[j].frequency)/2, min_diff);
+	}
+
+	switch_normal_mode();
+
+	init_timer(&freq_mode_timer);
+	freq_mode_timer.function = switch_mode_timer;
+	freq_mode_timer.data = 0;
+
+#if 0
+	for (i = 0; i < TABLE_SIZE; i++) {
+		pr_info("Table %d shows:\n", i+1);
+		for (j = 0; j < cnt; j++) {
+			pr_info("%02d: %8u\n", j, tblmap[i][j]);
+		}
+	}
+#endif
+}
+
+static void dbs_deinit_freq_map_table(void)
+{
+	int i;
+
+	if (!tbl)
+		return;
+
+	tbl = NULL;
+
+	for (i = 0; i < TABLE_SIZE; i++)
+		kfree(tblmap[i]);
+
+	del_timer(&freq_mode_timer);
+}
+
+static inline int get_cpu_freq_index(unsigned int freq)
+{
+	static int saved_index = 0;
+	int index;
+
+	if (!tbl) {
+		pr_warn("tbl is NULL, use previous value %d\n", saved_index);
+		return saved_index;
+	}
+
+	for (index = 0; (tbl[index].frequency != CPUFREQ_TABLE_END); index++) {
+		if (tbl[index].frequency >= freq) {
+			saved_index = index;
+			break;
+		}
+	}
+
+	return index;
+}
+
+static void dbs_freq_increase(struct cpufreq_policy *p, unsigned load, unsigned int freq)
+{
+	if (dbs_tuners_ins.powersave_bias)
+		freq = powersave_bias_target(p, freq, CPUFREQ_RELATION_H);
+	else if (p->cur == p->max) {
+		trace_cpufreq_electroactive_already (p->cpu, load, p->cur, p->cur, p->cur);
+		return;
+	}
+
+	trace_cpufreq_electroactive_target (p->cpu, load, p->cur, p->cur, freq);
+
+	__cpufreq_driver_target(p, freq, (dbs_tuners_ins.powersave_bias || freq < p->max) ?
+			CPUFREQ_RELATION_L : CPUFREQ_RELATION_H);
+
+	trace_cpufreq_electroactive_up (p->cpu, freq, p->cur);
+}
+
+int set_two_phase_freq(int cpufreq)
+{
+	int i  = 0;
+	for ( i = 0 ; i < NR_CPUS; i++)
+		two_phase_freq_array[i] = cpufreq;
+	return 0;
+}
+
+void set_two_phase_freq_by_cpu ( int cpu_nr, int cpufreq){
+	two_phase_freq_array[cpu_nr-1] = cpufreq;
+}
+
+int input_event_boosted(void)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&input_boost_lock, flags);
+	if (input_event_boost) {
+		if (time_before(jiffies, input_event_boost_expired)) {
+			spin_unlock_irqrestore(&input_boost_lock, flags);
+			return 1;
+		}
+		input_event_boost = false;
+		dbs_tuners_ins.sampling_rate = dbs_tuners_ins.origin_sampling_rate;
+	}
+	spin_unlock_irqrestore(&input_boost_lock, flags);
+
+	return 0;
+}
+
+static void boost_min_freq(int min_freq)
+{
+	int i;
+	struct cpu_dbs_info_s *dbs_info;
+
+	for_each_online_cpu(i) {
+		dbs_info = &per_cpu(od_cpu_dbs_info, i);
+
+		if (dbs_info->cur_policy
+			&& dbs_info->cur_policy->cur < min_freq) {
+			dbs_info->input_event_freq = min_freq;
+			wake_up_process(per_cpu(up_task, i));
+		}
+	}
+}
+
+static unsigned int get_cpu_current_load(unsigned int j, unsigned int *record)
+{
+	unsigned int cur_load = 0;
+	struct cpu_dbs_info_s *j_dbs_info;
+	cputime64_t cur_wall_time, cur_idle_time, cur_iowait_time;
+	unsigned int idle_time, wall_time, iowait_time;
+
+	j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+
+	if (record)
+		*record = j_dbs_info->prev_load;
+
+	cur_idle_time = get_cpu_idle_time(j, &cur_wall_time, 0);
+	cur_iowait_time = get_cpu_iowait_time(j, &cur_wall_time);
+
+	wall_time = (unsigned int)
+		(cur_wall_time - j_dbs_info->prev_cpu_wall);
+	j_dbs_info->prev_cpu_wall = cur_wall_time;
+
+	idle_time = (unsigned int)
+		(cur_idle_time - j_dbs_info->prev_cpu_idle);
+	j_dbs_info->prev_cpu_idle = cur_idle_time;
+
+	iowait_time = (unsigned int)
+		(cur_iowait_time - j_dbs_info->prev_cpu_iowait);
+	j_dbs_info->prev_cpu_iowait = cur_iowait_time;
+
+	if (dbs_tuners_ins.ignore_nice) {
+		u64 cur_nice;
+		unsigned long cur_nice_jiffies;
+
+		cur_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE] -
+				 j_dbs_info->prev_cpu_nice;
+		cur_nice_jiffies = (unsigned long)
+				cputime64_to_jiffies64(cur_nice);
+
+		j_dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+		idle_time += jiffies_to_usecs(cur_nice_jiffies);
+	}
+
+	if (dbs_tuners_ins.io_is_busy && idle_time >= iowait_time)
+		idle_time -= iowait_time;
+
+	if (unlikely(!wall_time || wall_time < idle_time))
+		return j_dbs_info->prev_load;
+
+	cur_load = 100 * (wall_time - idle_time) / wall_time;
+	j_dbs_info->max_load  = max(cur_load, j_dbs_info->prev_load);
+	j_dbs_info->prev_load = cur_load;
+
+	return cur_load;
+}
+
+static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
+{
+	unsigned int load_at_max_freq = 0;
+	unsigned int max_load_freq;
+
+	unsigned int cur_load = 0;
+
+	unsigned int max_load_other_cpu = 0;
+	struct cpufreq_policy *policy;
+	unsigned int j, prev_load = 0, freq_next;
+
+	static unsigned int phase = 0;
+	static unsigned int counter = 0;
+	unsigned int nr_cpus;
+
+	this_dbs_info->freq_lo = 0;
+	policy = this_dbs_info->cur_policy;
+
+	max_load_freq = 0;
+
+	for_each_cpu(j, policy->cpus) {
+		unsigned int load_freq;
+		int freq_avg;
+
+		cur_load = get_cpu_current_load(j, &prev_load);
+		freq_avg = __cpufreq_driver_getavg(policy, j);
+		if (freq_avg <= 0)
+			freq_avg = policy->cur;
+
+		load_freq = cur_load * freq_avg;
+		if (load_freq > max_load_freq)
+			max_load_freq = load_freq;
+
+		load_at_max_freq += (cur_load * policy->cur) /
+					policy->cpuinfo.max_freq;
+	}
+
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *j_dbs_info;
+		j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+
+		if (j == policy->cpu)
+			continue;
+
+		if (max_load_other_cpu < j_dbs_info->max_load)
+			max_load_other_cpu = j_dbs_info->max_load;
+
+		if ((j_dbs_info->cur_policy != NULL)
+			&& (j_dbs_info->cur_policy->cur ==
+					j_dbs_info->cur_policy->max)) {
+
+			if (policy->cur >= dbs_tuners_ins.optimal_freq)
+				max_load_other_cpu =
+				dbs_tuners_ins.up_threshold_any_cpu_load;
+		}
+	}
+
+	cpufreq_notify_utilization(policy, load_at_max_freq);
+
+	//gboost
+	if (g_count > 30) {
+
+		if (max_load_freq > dbs_tuners_ins.up_threshold * policy->cur) {
+
+			if (counter < 5) {
+				counter++;
+				if (counter > 2) {
+					phase = 1;
+				}
+			}
+
+			nr_cpus = num_online_cpus();
+			dbs_tuners_ins.two_phase_freq = two_phase_freq_array[nr_cpus-1];
+			if (dbs_tuners_ins.two_phase_freq < policy->cur)
+				phase=1;
+			if (dbs_tuners_ins.two_phase_freq != 0 && phase == 0) {
+				dbs_freq_increase(policy, cur_load, dbs_tuners_ins.two_phase_freq);
+			} else {
+				if (policy->cur < policy->max)
+					this_dbs_info->rate_mult =
+						dbs_tuners_ins.sampling_down_factor;
+				dbs_freq_increase(policy, cur_load, policy->max);
+			}
+			return;
+		}
+
+	} else {
+
+		if (max_load_freq > up_threshold_level[1] * policy->cur) {
+			unsigned int avg_load = (prev_load + cur_load) >> 1;
+			int index = get_cpu_freq_index(policy->cur);
+
+			if (FREQ_NEED_BURST(policy->cur) && cur_load > up_threshold_level[0]) {
+				freq_next = tblmap[tbl_select[3]][index];
+			} else if (avg_load > up_threshold_level[0]) {
+				freq_next = tblmap[tbl_select[3]][index];
+			} else if (avg_load <= up_threshold_level[1]) {
+				freq_next = tblmap[tbl_select[0]][index];
+			} else {
+
+				if (cur_load > up_threshold_level[0]) {
+					freq_next = tblmap[tbl_select[2]][index];
+				} else {
+					freq_next = tblmap[tbl_select[1]][index];
+				}
+			}
+			dbs_freq_increase(policy, cur_load, freq_next);
+			if (policy->cur == policy->max)
+				this_dbs_info->rate_mult = dbs_tuners_ins.sampling_down_factor;
+
+			return;
+		}
+	}
+
+	if (dbs_tuners_ins.gboost) {
+		if (counter > 0) {
+			counter--;
+			if (counter == 0) {
+				phase = 0;
+			}
+		}
+	}
+
+	if (dbs_tuners_ins.gboost) {
+
+		if (g_count < 100 && graphics_boost_electroactive < 5) {
+			++g_count;
+		} else if (g_count > 1) {
+			--g_count;
+			--g_count;
+		}
+
+		if (graphics_boost_electroactive < 4 && g_count > 80) {
+			dbs_tuners_ins.up_threshold = 60 + (graphics_boost_electroactive * 10);
+		} else {
+			dbs_tuners_ins.up_threshold = orig_up_threshold;
+		}
+
+		if (g_count > 80)
+			boost_min_freq(1267200);
+	}
+	//end
+
+	if (num_online_cpus() > 1) {
+		if (max_load_other_cpu >
+				dbs_tuners_ins.up_threshold_any_cpu_load) {
+			if (policy->cur < dbs_tuners_ins.sync_freq)
+				dbs_freq_increase(policy, cur_load,
+						dbs_tuners_ins.sync_freq);
+			return;
+		}
+
+		if (max_load_freq > dbs_tuners_ins.up_threshold_multi_core *
+								policy->cur) {
+			if (policy->cur < dbs_tuners_ins.optimal_freq)
+				dbs_freq_increase(policy, cur_load,
+						dbs_tuners_ins.optimal_freq);
+			return;
+		}
+	}
+
+	if (input_event_boosted())
+	{
+		trace_cpufreq_electroactive_already (policy->cpu, cur_load, policy->cur, policy->cur, policy->cur);
+		return;
+	}
+
+	if (policy->cur == policy->min){
+		trace_cpufreq_electroactive_already (policy->cpu, cur_load, policy->cur, policy->cur, policy->cur);
+		return;
+	}
+
+	if (max_load_freq <
+	    (dbs_tuners_ins.up_threshold - dbs_tuners_ins.down_differential) *
+	     policy->cur) {
+		freq_next = max_load_freq /
+				(dbs_tuners_ins.up_threshold -
+				 dbs_tuners_ins.down_differential);
+
+		this_dbs_info->rate_mult = 1;
+
+		if (freq_next < policy->min)
+			freq_next = policy->min;
+
+		if (num_online_cpus() > 1) {
+			if (max_load_other_cpu >
+			(dbs_tuners_ins.up_threshold_multi_core -
+			dbs_tuners_ins.down_differential) &&
+			freq_next < dbs_tuners_ins.sync_freq)
+				freq_next = dbs_tuners_ins.sync_freq;
+
+			if (dbs_tuners_ins.optimal_freq > policy->min && max_load_freq >
+				 (dbs_tuners_ins.up_threshold_multi_core -
+				  dbs_tuners_ins.down_differential_multi_core) *
+				  policy->cur)
+				freq_next = dbs_tuners_ins.optimal_freq;
+		}
+
+		if (dbs_tuners_ins.powersave_bias)
+			freq_next = powersave_bias_target(policy, freq_next, CPUFREQ_RELATION_L);
+
+		trace_cpufreq_electroactive_target (policy->cpu, cur_load, policy->cur, policy->cur, freq_next);
+		__cpufreq_driver_target(policy, freq_next,
+			CPUFREQ_RELATION_L);
+		trace_cpufreq_electroactive_down (policy->cpu, freq_next, policy->cur);
+	}
+}
+
+static void do_dbs_timer(struct work_struct *work)
+{
+	struct cpu_dbs_info_s *dbs_info =
+		container_of(work, struct cpu_dbs_info_s, work.work);
+	unsigned int cpu = dbs_info->cpu;
+	int sample_type = dbs_info->sample_type;
+	int delay = msecs_to_jiffies(50);
+
+	mutex_lock(&dbs_info->timer_mutex);
+
+	if (skip_electroactive)
+		goto sched_wait;
+
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	if (!dbs_tuners_ins.powersave_bias ||
+	    sample_type == DBS_NORMAL_SAMPLE) {
+		dbs_check_cpu(dbs_info);
+		if (dbs_info->freq_lo) {
+
+			dbs_info->sample_type = DBS_SUB_SAMPLE;
+			delay = dbs_info->freq_hi_jiffies;
+		} else {
+			delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate
+				* dbs_info->rate_mult);
+
+			if (num_online_cpus() > 1)
+				delay -= jiffies % delay;
+		}
+	} else {
+		if (input_event_boosted())
+			goto sched_wait;
+
+		__cpufreq_driver_target(dbs_info->cur_policy,
+			dbs_info->freq_lo, CPUFREQ_RELATION_H);
+		delay = dbs_info->freq_lo_jiffies;
+	}
+
+sched_wait:
+	schedule_delayed_work_on(cpu, &dbs_info->work, delay);
+	mutex_unlock(&dbs_info->timer_mutex);
+}
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
+{
+
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+	if (num_online_cpus() > 1)
+		delay -= jiffies % delay;
+
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	INIT_DEFERRABLE_WORK(&dbs_info->work, do_dbs_timer);
+	schedule_delayed_work_on(dbs_info->cpu, &dbs_info->work, delay);
+}
+
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
+{
+	cancel_delayed_work_sync(&dbs_info->work);
+}
+
+
+static void dbs_input_event(struct input_handle *handle, unsigned int type,
+		unsigned int code, int value)
+{
+	unsigned long flags;
+	int input_event_min_freq;
+
+	if (dbs_tuners_ins.input_event_timeout == 0)
+		return;
+
+	if ((dbs_tuners_ins.powersave_bias == POWERSAVE_BIAS_MAXLEVEL) ||
+		(dbs_tuners_ins.powersave_bias == POWERSAVE_BIAS_MINLEVEL)) {
+		return;
+	}
+
+	if (type == EV_ABS && code == ABS_MT_TRACKING_ID) {
+		if (value != -1) {
+
+			input_event_min_freq = input_event_min_freq_array[num_online_cpus() - 1];
+
+			input_event_counter++;
+			switch_turbo_mode(0);
+
+			spin_lock_irqsave(&input_boost_lock, flags);
+			input_event_boost = true;
+			input_event_boost_expired = jiffies + usecs_to_jiffies(dbs_tuners_ins.input_event_timeout * 1000);
+			dbs_tuners_ins.sampling_rate = dbs_tuners_ins.ui_sampling_rate;
+			spin_unlock_irqrestore(&input_boost_lock, flags);
+
+			boost_min_freq(input_event_min_freq);
+		} else {
+			if (likely(input_event_counter > 0))
+				input_event_counter--;
+			else
+				pr_debug("dbs_input_event: Touch isn't paired!\n");
+
+			switch_turbo_mode(DBS_SWITCH_MODE_TIMEOUT);
+		}
+	}
+}
+
+static int dbs_input_connect(struct input_handler *handler,
+		struct input_dev *dev, const struct input_device_id *id)
+{
+	struct input_handle *handle;
+	int error;
+
+	handle = kzalloc(sizeof(struct input_handle), GFP_KERNEL);
+	if (!handle)
+		return -ENOMEM;
+
+	handle->dev = dev;
+	handle->handler = handler;
+	handle->name = "cpufreq";
+
+	error = input_register_handle(handle);
+	if (error)
+		goto err2;
+
+	error = input_open_device(handle);
+	if (error)
+		goto err1;
+
+	return 0;
+err1:
+	input_unregister_handle(handle);
+err2:
+	kfree(handle);
+	return error;
+}
+
+static void dbs_input_disconnect(struct input_handle *handle)
+{
+	input_close_device(handle);
+	input_unregister_handle(handle);
+	kfree(handle);
+}
+
+static const struct input_device_id dbs_ids[] = {
+	/* multi-touch touchscreen */
+	{
+		.flags = INPUT_DEVICE_ID_MATCH_EVBIT |
+			INPUT_DEVICE_ID_MATCH_ABSBIT,
+		.evbit = { BIT_MASK(EV_ABS) },
+		.absbit = { [BIT_WORD(ABS_MT_POSITION_X)] =
+			BIT_MASK(ABS_MT_POSITION_X) |
+			BIT_MASK(ABS_MT_POSITION_Y) },
+	},
+	/* touchpad */
+	{
+		.flags = INPUT_DEVICE_ID_MATCH_KEYBIT |
+			INPUT_DEVICE_ID_MATCH_ABSBIT,
+		.keybit = { [BIT_WORD(BTN_TOUCH)] = BIT_MASK(BTN_TOUCH) },
+		.absbit = { [BIT_WORD(ABS_X)] =
+			BIT_MASK(ABS_X) | BIT_MASK(ABS_Y) },
+	},
+	/* Keypad */
+	{
+		.flags = INPUT_DEVICE_ID_MATCH_EVBIT,
+		.evbit = { BIT_MASK(EV_KEY) },
+	},
+	{ },
+};
+
+static struct input_handler dbs_input_handler = {
+	.event		= dbs_input_event,
+	.connect	= dbs_input_connect,
+	.disconnect	= dbs_input_disconnect,
+	.name		= "cpufreq_electroactive",
+	.id_table	= dbs_ids,
+};
+
+
+void set_input_event_min_freq_by_cpu ( int cpu_nr, int cpufreq){
+	input_event_min_freq_array[cpu_nr-1] = cpufreq;
+}
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int rc;
+
+	this_dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy->cur))
+			return -EINVAL;
+
+		mutex_lock(&dbs_mutex);
+
+		dbs_enable++;
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&j_dbs_info->prev_cpu_wall, 0);
+			if (dbs_tuners_ins.ignore_nice)
+				j_dbs_info->prev_cpu_nice =
+						kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+		}
+		this_dbs_info->cpu = cpu;
+		this_dbs_info->rate_mult = 1;
+		electroactive_powersave_bias_init_cpu(cpu);
+		if (dbs_enable == 1) {
+			unsigned int latency;
+
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&dbs_attr_group);
+			if (rc) {
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+
+			latency = policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+
+			min_sampling_rate = max(min_sampling_rate,
+					MIN_LATENCY_MULTIPLIER * latency);
+			dbs_tuners_ins.sampling_rate =
+				max(min_sampling_rate,
+				    latency * LATENCY_MULTIPLIER);
+			if (dbs_tuners_ins.sampling_rate < DEF_SAMPLING_RATE)
+				dbs_tuners_ins.sampling_rate = DEF_SAMPLING_RATE;
+			dbs_tuners_ins.origin_sampling_rate = dbs_tuners_ins.sampling_rate;
+
+			if (dbs_tuners_ins.optimal_freq == 0)
+				dbs_tuners_ins.optimal_freq = policy->min;
+
+			if (dbs_tuners_ins.sync_freq == 0)
+				dbs_tuners_ins.sync_freq = policy->min;
+
+			dbs_init_freq_map_table(policy);
+		}
+		if (!cpu)
+			rc = input_register_handler(&dbs_input_handler);
+		mutex_unlock(&dbs_mutex);
+
+		mutex_init(&this_dbs_info->timer_mutex);
+
+		if (!electroactive_powersave_bias_setspeed(
+					this_dbs_info->cur_policy,
+					NULL,
+					dbs_tuners_ins.powersave_bias))
+			dbs_timer_init(this_dbs_info);
+		trace_cpufreq_electroactive_target (cpu, 0, 0, 0, 0);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		dbs_timer_exit(this_dbs_info);
+
+		mutex_lock(&dbs_mutex);
+		dbs_enable--;
+		this_dbs_info->cur_policy = NULL;
+		if (!cpu)
+			input_unregister_handler(&dbs_input_handler);
+		mutex_unlock(&dbs_mutex);
+		if (!dbs_enable) {
+			dbs_deinit_freq_map_table();
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &dbs_attr_group);
+		}
+		trace_cpufreq_electroactive_target (cpu, 0, 0, 0, 0);
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&this_dbs_info->timer_mutex);
+		if(this_dbs_info->cur_policy){
+			if (policy->max < this_dbs_info->cur_policy->cur)
+				__cpufreq_driver_target(this_dbs_info->cur_policy,
+					policy->max, CPUFREQ_RELATION_H);
+			else if (policy->min > this_dbs_info->cur_policy->cur)
+				__cpufreq_driver_target(this_dbs_info->cur_policy,
+					policy->min, CPUFREQ_RELATION_L);
+			else if (dbs_tuners_ins.powersave_bias != 0)
+				electroactive_powersave_bias_setspeed(
+					this_dbs_info->cur_policy,
+					policy,
+					dbs_tuners_ins.powersave_bias);
+		}
+		mutex_unlock(&this_dbs_info->timer_mutex);
+		break;
+	}
+	return 0;
+}
+
+static int cpufreq_gov_dbs_up_task(void *data)
+{
+	struct cpufreq_policy *policy;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int cpu = smp_processor_id();
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule();
+
+		if (kthread_should_stop())
+			break;
+
+		set_current_state(TASK_RUNNING);
+
+		get_online_cpus();
+
+//		if (lock_policy_rwsem_write(cpu) < 0)
+			goto bail_acq_sema_failed;
+
+		this_dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+		policy = this_dbs_info->cur_policy;
+		if (!policy) {
+			goto bail_incorrect_governor;
+		}
+
+		mutex_lock(&this_dbs_info->timer_mutex);
+
+		dbs_tuners_ins.powersave_bias = 0;
+		dbs_freq_increase(policy, this_dbs_info->prev_load, this_dbs_info->input_event_freq);
+		this_dbs_info->prev_cpu_idle = get_cpu_idle_time(cpu, &this_dbs_info->prev_cpu_wall, 0);
+
+		mutex_unlock(&this_dbs_info->timer_mutex);
+
+bail_incorrect_governor:
+		up_write(&policy->rwsem);
+
+bail_acq_sema_failed:
+		put_online_cpus();
+
+		dbs_tuners_ins.sampling_rate = dbs_tuners_ins.ui_sampling_rate;
+	}
+
+	return 0;
+}
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	u64 idle_time;
+	unsigned int i;
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+	struct task_struct *pthread;
+	int cpu = get_cpu();
+
+	idle_time = get_cpu_idle_time_us(cpu, NULL);
+	put_cpu();
+	if (idle_time != -1ULL) {
+
+		dbs_tuners_ins.up_threshold = MICRO_FREQUENCY_UP_THRESHOLD;
+		dbs_tuners_ins.down_differential =
+					MICRO_FREQUENCY_DOWN_DIFFERENTIAL;
+		min_sampling_rate = MICRO_FREQUENCY_MIN_SAMPLE_RATE;
+	} else {
+
+		min_sampling_rate =
+			MIN_SAMPLING_RATE_RATIO * jiffies_to_usecs(10);
+	}
+
+	spin_lock_init(&input_boost_lock);
+
+	for_each_possible_cpu(i) {
+		pthread = kthread_create_on_node(cpufreq_gov_dbs_up_task,
+								NULL, cpu_to_node(i),
+								"kdbs_up/%d", i);
+		if (likely(!IS_ERR(pthread))) {
+			kthread_bind(pthread, i);
+			sched_setscheduler_nocheck(pthread, SCHED_FIFO, &param);
+			get_task_struct(pthread);
+			per_cpu(up_task, i) = pthread;
+		}
+	}
+	return cpufreq_register_governor(&cpufreq_gov_electroactive);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	unsigned int i;
+
+	cpufreq_unregister_governor(&cpufreq_gov_electroactive);
+	for_each_possible_cpu(i) {
+		struct cpu_dbs_info_s *this_dbs_info =
+			&per_cpu(od_cpu_dbs_info, i);
+		mutex_destroy(&this_dbs_info->timer_mutex);
+		if (per_cpu(up_task, i)) {
+			kthread_stop(per_cpu(up_task, i));
+			put_task_struct(per_cpu(up_task, i));
+		}
+	}
+}
+
+MODULE_AUTHOR("Yusuf Mostafa <ymostafa30@gmail.com>");
+MODULE_DESCRIPTION("'cpufreq_electroactive' - multiphase dynamic cpufreq governor");
+MODULE_LICENSE("GPLv2");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_ELECTROACTIVE
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_electrodemand.c b/drivers/cpufreq/cpufreq_electrodemand.c
new file mode 100644
index 0000000..d10f7c8
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_electrodemand.c
@@ -0,0 +1,1130 @@
+/*
+ *  drivers/cpufreq/cpufreq_electrodemand.c
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *
+ *                 2012 Minor Edits by Sar Castillo <sar.castillo@gmail.com>
+ *                 2012 MAR heavy addons by DORIMANX <yuri@bynet.co.il>
+ *                 2015 Minor Edits by Electrex <ymostafa30@gmail.com>
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/sched/rt.h>
+#include <linux/input.h>
+#include <linux/slab.h>
+
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+#define DEF_FREQUENCY_DOWN_DIFFERENTIAL		(10)
+#define MIN_FREQUENCY_DOWN_DIFFERENTIAL		(1)
+#define DEF_FREQUENCY_UP_THRESHOLD		(70)
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define BOOSTED_SAMPLING_DOWN_FACTOR		(40)
+#define MAX_SAMPLING_DOWN_FACTOR		(100000)
+#define MICRO_FREQUENCY_DOWN_DIFFERENTIAL	(5)
+#define MICRO_FREQUENCY_UP_THRESHOLD		(70)
+#define MICRO_FREQUENCY_MIN_SAMPLE_RATE		(5000)
+#define MIN_FREQUENCY_UP_THRESHOLD		(11)
+#define MAX_FREQUENCY_UP_THRESHOLD		(100)
+#define MIN_FREQUENCY_DOWN_DIFFERENTIAL		(1)
+#define FREQ_STEP				(55)
+#define DEFAULT_FREQ_BOOST_TIME			(500000)
+#define MAX_FREQ_BOOST_TIME			(5000000)
+#define UP_THRESHOLD_AT_MIN_FREQ		(40)
+#define FREQ_FOR_RESPONSIVENESS			(2265600)
+
+static u64 electrodemand_freq_boosted_time;
+
+/*
+ * The polling frequency of this governor depends on the capability of
+ * the processor. Default polling frequency is 1000 times the transition
+ * latency of the processor. The governor will work on any processor with
+ * transition latency <= 10mS, using appropriate sampling
+ * rate.
+ * For CPUs with transition latency > 10mS (mostly drivers with CPUFREQ_ETERNAL)
+ * this governor will not work.
+ * All times here are in uS.
+ */
+#define MIN_SAMPLING_RATE_RATIO			(2)
+
+static unsigned int min_sampling_rate;
+#define DEFAULT_SAMPLING_RATE			(40000)
+#define BOOSTED_SAMPLING_RATE			(20000)
+#define LATENCY_MULTIPLIER			(1000)
+#define MIN_LATENCY_MULTIPLIER			(20)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+/* have the timer rate booted for this much time 4s*/
+#define TIMER_RATE_BOOST_TIME 4000000
+static int electrodemand_sampling_rate_boosted;
+static u64 electrodemand_sampling_rate_boosted_time;
+unsigned int electrodemand_current_sampling_rate;
+
+static void do_dbs_timer(struct work_struct *work);
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_electrodemand
+static
+#endif
+struct cpufreq_governor cpufreq_gov_electrodemand = {
+       .name                   = "electrodemand",
+       .governor               = cpufreq_governor_dbs,
+       .max_transition_latency = TRANSITION_LATENCY_LIMIT,
+       .owner                  = THIS_MODULE,
+};
+
+/* Sampling types */
+enum {DBS_NORMAL_SAMPLE, DBS_SUB_SAMPLE};
+
+struct cpu_dbs_info_s {
+	cputime64_t prev_cpu_idle;
+	cputime64_t prev_cpu_iowait;
+	cputime64_t prev_cpu_wall;
+	unsigned int prev_cpu_wall_delta;
+	cputime64_t prev_cpu_nice;
+	struct cpufreq_policy *cur_policy;
+	struct delayed_work work;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int freq_lo;
+	unsigned int freq_lo_jiffies;
+	unsigned int freq_hi_jiffies;
+	unsigned int rate_mult;
+	unsigned int load_at_prev_sample;
+	int cpu;
+	unsigned int sample_type:1;
+	/*
+	 * percpu mutex that serializes governor limit change with
+	 * do_dbs_timer invocation. We do not want do_dbs_timer to run
+	 * when user is changing the governor or limits.
+	 */
+	struct mutex timer_mutex;
+	bool activated; /* dbs_timer_init is in effect */
+};
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, od_cpu_dbs_info);
+
+static unsigned int dbs_enable;	/* number of CPUs using this policy */
+
+/*
+ * dbs_mutex protects dbs_enable in governor start/stop.
+ */
+static DEFINE_MUTEX(dbs_mutex);
+
+static struct dbs_tuners {
+	unsigned int sampling_rate;
+	unsigned int up_threshold;
+	unsigned int up_threshold_min_freq;
+	unsigned int down_differential;
+	unsigned int micro_freq_up_threshold;
+	unsigned int ignore_nice;
+	unsigned int sampling_down_factor;
+	unsigned int powersave_bias;
+	unsigned int io_is_busy;
+	unsigned int boosted;
+	unsigned int freq_boost_time;
+	unsigned int boostfreq;
+	unsigned int freq_step;
+	unsigned int freq_responsiveness;
+
+} dbs_tuners_ins = {
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.up_threshold_min_freq = UP_THRESHOLD_AT_MIN_FREQ,
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.down_differential = DEF_FREQUENCY_DOWN_DIFFERENTIAL,
+	.micro_freq_up_threshold = MICRO_FREQUENCY_UP_THRESHOLD,
+	.ignore_nice = 0,
+	.powersave_bias = 0,
+	.freq_boost_time = DEFAULT_FREQ_BOOST_TIME,
+	.boostfreq = 2265600,
+	.freq_step = FREQ_STEP,
+	.freq_responsiveness = FREQ_FOR_RESPONSIVENESS
+};
+
+static unsigned int dbs_enable = 0;	/* number of CPUs using this policy */
+
+static inline cputime64_t get_cpu_iowait_time(unsigned int cpu, cputime64_t *wall)
+{
+	u64 iowait_time = get_cpu_iowait_time_us(cpu, wall);
+
+	if (iowait_time == -1ULL)
+		return 0;
+
+	return iowait_time;
+}
+
+/*
+ * Find right sampling rate based on sampling_rate and
+ * QoS requests on dvfs latency.
+ */
+static unsigned int effective_sampling_rate(void)
+{
+	unsigned int effective;
+
+		effective = dbs_tuners_ins.sampling_rate;
+
+	return max(effective, min_sampling_rate);
+}
+
+/*
+ * Find right freq to be set now with powersave_bias on.
+ * Returns the freq_hi to be used right now and will set freq_hi_jiffies,
+ * freq_lo, and freq_lo_jiffies in percpu area for averaging freqs.
+ */
+static unsigned int powersave_bias_target(struct cpufreq_policy *policy,
+					  unsigned int freq_next,
+					  unsigned int relation)
+{
+	unsigned int freq_req, freq_reduc, freq_avg;
+	unsigned int freq_hi, freq_lo;
+	unsigned int index = 0;
+	unsigned int jiffies_total, jiffies_hi, jiffies_lo;
+	struct cpu_dbs_info_s *dbs_info = &per_cpu(od_cpu_dbs_info,
+						   policy->cpu);
+
+	if (!dbs_info->freq_table) {
+		dbs_info->freq_lo = 0;
+		dbs_info->freq_lo_jiffies = 0;
+		return freq_next;
+	}
+
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_next,
+			relation, &index);
+	freq_req = dbs_info->freq_table[index].frequency;
+	freq_reduc = freq_req * dbs_tuners_ins.powersave_bias / 1000;
+	freq_avg = freq_req - freq_reduc;
+
+	/* Find freq bounds for freq_avg in freq_table */
+	index = 0;
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_avg,
+			CPUFREQ_RELATION_H, &index);
+	freq_lo = dbs_info->freq_table[index].frequency;
+	index = 0;
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_avg,
+			CPUFREQ_RELATION_L, &index);
+	freq_hi = dbs_info->freq_table[index].frequency;
+
+	/* Find out how long we have to be in hi and lo freqs */
+	if (freq_hi == freq_lo) {
+		dbs_info->freq_lo = 0;
+		dbs_info->freq_lo_jiffies = 0;
+		return freq_lo;
+	}
+	jiffies_total = usecs_to_jiffies(effective_sampling_rate());
+	jiffies_hi = (freq_avg - freq_lo) * jiffies_total;
+	jiffies_hi += ((freq_hi - freq_lo) / 2);
+	jiffies_hi /= (freq_hi - freq_lo);
+	jiffies_lo = jiffies_total - jiffies_hi;
+	dbs_info->freq_lo = freq_lo;
+	dbs_info->freq_lo_jiffies = jiffies_lo;
+	dbs_info->freq_hi_jiffies = jiffies_hi;
+	return freq_hi;
+}
+
+static void electrodemand_powersave_bias_init_cpu(int cpu)
+{
+	struct cpu_dbs_info_s *dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+	dbs_info->freq_table = cpufreq_frequency_get_table(cpu);
+	dbs_info->freq_lo = 0;
+}
+
+static void electrodemand_powersave_bias_init(void)
+{
+	int i;
+	for_each_online_cpu(i) {
+		electrodemand_powersave_bias_init_cpu(i);
+	}
+}
+
+/************************** sysfs interface ************************/
+
+static ssize_t show_sampling_rate_min(struct kobject *kobj,
+				      struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", min_sampling_rate);
+}
+
+define_one_global_ro(sampling_rate_min);
+
+/* cpufreq_electrodemand_power Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)              \
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(io_is_busy, io_is_busy);
+show_one(up_threshold, up_threshold);
+show_one(up_threshold_min_freq, up_threshold_min_freq);
+show_one(sampling_down_factor, sampling_down_factor);
+show_one(ignore_nice_load, ignore_nice);
+show_one(powersave_bias, powersave_bias);
+show_one(micro_freq_up_threshold, micro_freq_up_threshold);
+show_one(down_differential, down_differential);
+show_one(boostpulse, boosted);
+show_one(boostfreq, boostfreq);
+show_one(freq_step, freq_step);
+show_one(freq_responsiveness, freq_responsiveness);
+
+/**
+ * update_sampling_rate - update sampling rate effective immediately if needed.
+ * @new_rate: new sampling rate. If it is 0, regard sampling rate is not
+ *		changed and assume that qos request value is changed.
+ *
+ * If new rate is smaller than the old, simply updaing
+ * dbs_tuners_int.sampling_rate might not be appropriate. For example,
+ * if the original sampling_rate was 1 second and the requested new sampling
+ * rate is 10 ms because the user needs immediate reaction from ondemand
+ * governor, but not sure if higher frequency will be required or not,
+ * then, the governor may change the sampling rate too late; up to 1 second
+ * later. Thus, if we are reducing the sampling rate, we need to make the
+ * new value effective immediately.
+ */
+static void update_sampling_rate(unsigned int new_rate)
+{
+	int cpu;
+	unsigned int effective;
+
+	if (new_rate)
+		dbs_tuners_ins.sampling_rate = max(new_rate, min_sampling_rate);
+
+	effective = effective_sampling_rate();
+
+	get_online_cpus();
+	for_each_online_cpu(cpu) {
+		struct cpufreq_policy *policy;
+		struct cpu_dbs_info_s *dbs_info;
+		unsigned long next_sampling, appointed_at;
+
+		/*
+		 * mutex_destory(&dbs_info->timer_mutex) should not happen
+		 * in this context. dbs_mutex is locked/unlocked at GOV_START
+		 * and GOV_STOP context only other than here.
+		 */
+		mutex_lock(&dbs_mutex);
+
+		policy = cpufreq_cpu_get(cpu);
+		if (!policy) {
+			mutex_unlock(&dbs_mutex);
+			continue;
+		}
+		dbs_info = &per_cpu(od_cpu_dbs_info, policy->cpu);
+		cpufreq_cpu_put(policy);
+
+		/* timer_mutex is destroyed or will be destroyed soon */
+		if (!dbs_info->activated) {
+			mutex_unlock(&dbs_mutex);
+			continue;
+		}
+
+		mutex_lock(&dbs_info->timer_mutex);
+
+		if (!delayed_work_pending(&dbs_info->work)) {
+			mutex_unlock(&dbs_info->timer_mutex);
+			mutex_unlock(&dbs_mutex);
+			continue;
+		}
+
+		next_sampling = jiffies + usecs_to_jiffies(new_rate);
+		appointed_at = dbs_info->work.timer.expires;
+
+		if (time_before(next_sampling, appointed_at)) {
+			mutex_unlock(&dbs_info->timer_mutex);
+			cancel_delayed_work_sync(&dbs_info->work);
+			mutex_lock(&dbs_info->timer_mutex);
+
+			schedule_delayed_work_on(dbs_info->cpu, &dbs_info->work,
+						 usecs_to_jiffies(effective));
+		}
+		mutex_unlock(&dbs_info->timer_mutex);
+
+		/*
+		 * For the little possiblity that dbs_timer_exit() has been
+		 * called after checking dbs_info->activated above.
+		 * If cancel_delayed_work_syn() has been calld by
+		 * dbs_timer_exit() before schedule_delayed_work_on() of this
+		 * function, it should be revoked by calling cancel again
+		 * before releasing dbs_mutex, which will trigger mutex_destroy
+		 * to be called.
+		 */
+		if (!dbs_info->activated)
+			cancel_delayed_work_sync(&dbs_info->work);
+
+		mutex_unlock(&dbs_mutex);
+	}
+	put_online_cpus();
+}
+
+static ssize_t store_boostpulse(struct kobject *kobj, struct attribute *attr,
+				const char *buf, size_t count)
+{
+	int ret;
+	unsigned int input;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret < 0)
+		return ret;
+
+	if (input > 1 && input <= MAX_FREQ_BOOST_TIME)
+		dbs_tuners_ins.freq_boost_time = input;
+	else
+		dbs_tuners_ins.freq_boost_time = DEFAULT_FREQ_BOOST_TIME;
+
+	dbs_tuners_ins.boosted = 1;
+	electrodemand_freq_boosted_time = ktime_to_us(ktime_get());
+
+	if (electrodemand_sampling_rate_boosted) {
+		electrodemand_sampling_rate_boosted = 0;
+		dbs_tuners_ins.sampling_rate = electrodemand_current_sampling_rate;
+	}
+	return count;
+}
+
+static ssize_t store_boostfreq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.boostfreq = input;
+	return count;
+}
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	update_sampling_rate(input);
+	electrodemand_current_sampling_rate = dbs_tuners_ins.sampling_rate;
+
+	return count;
+}
+
+/* io_is_busy */
+static ssize_t store_io_is_busy(struct kobject *a, struct attribute *b,
+				const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.io_is_busy = !!input;
+
+	return count;
+}
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold = input;
+
+	return count;
+}
+
+static ssize_t store_micro_freq_up_threshold(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.micro_freq_up_threshold = input;
+	return count;
+}
+
+static ssize_t store_down_differential(struct kobject *a, struct attribute *b,
+				const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.down_differential = min(input, 100u);
+
+	return count;
+}
+
+static ssize_t store_up_threshold_min_freq(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold_min_freq = input;
+
+	return count;
+}
+
+static ssize_t store_sampling_down_factor(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input, j;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_down_factor = input;
+
+	/* Reset down sampling multiplier in case it was active */
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(od_cpu_dbs_info, j);
+		dbs_info->rate_mult = 1;
+	}
+	return count;
+}
+
+/* ignore_nice_load */
+static ssize_t store_ignore_nice_load(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	unsigned int j;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == dbs_tuners_ins.ignore_nice) {/* nothing to do */
+		return count;
+	}
+	dbs_tuners_ins.ignore_nice = input;
+
+	/* we need to re-evaluate prev_cpu_idle */
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(od_cpu_dbs_info, j);
+		dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&dbs_info->prev_cpu_wall, 0);
+		if (dbs_tuners_ins.ignore_nice)
+			dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+	}
+	return count;
+}
+
+static ssize_t store_powersave_bias(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1000)
+		input = 1000;
+
+	dbs_tuners_ins.powersave_bias = input;
+	electrodemand_powersave_bias_init();
+
+	return count;
+}
+
+static ssize_t store_freq_step(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.freq_step = min(input, 100u);
+	return count;
+}
+
+static ssize_t store_freq_responsiveness(struct kobject *a, struct attribute *b,
+				const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 2803200)
+		input = 2803200;
+
+	if (input < 96000)
+		input = 96000;
+
+	dbs_tuners_ins.freq_responsiveness = input;
+
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(io_is_busy);
+define_one_global_rw(up_threshold);
+define_one_global_rw(up_threshold_min_freq);
+define_one_global_rw(sampling_down_factor);
+define_one_global_rw(ignore_nice_load);
+define_one_global_rw(powersave_bias);
+define_one_global_rw(down_differential);
+define_one_global_rw(micro_freq_up_threshold);
+define_one_global_rw(boostpulse);
+define_one_global_rw(boostfreq);
+define_one_global_rw(freq_step);
+define_one_global_rw(freq_responsiveness);
+
+static struct attribute *dbs_attributes[] = {
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&up_threshold.attr,
+	&up_threshold_min_freq.attr,
+	&sampling_down_factor.attr,
+	&ignore_nice_load.attr,
+	&powersave_bias.attr,
+	&io_is_busy.attr,
+	&down_differential.attr,
+	&micro_freq_up_threshold.attr,
+	&boostpulse.attr,
+	&boostfreq.attr,
+	&freq_step.attr,
+	&freq_responsiveness.attr,
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "electrodemand",
+};
+
+/************************** sysfs end ************************/
+
+static void dbs_freq_increase(struct cpufreq_policy *p, unsigned int freq)
+{
+	if (dbs_tuners_ins.powersave_bias)
+		freq = powersave_bias_target(p, freq, CPUFREQ_RELATION_H);
+	else if (p->cur == p->max)
+		return;
+
+	__cpufreq_driver_target(p, freq, dbs_tuners_ins.powersave_bias ?
+			CPUFREQ_RELATION_L : CPUFREQ_RELATION_H);
+}
+
+static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
+{
+	/* Extrapolated load of this CPU */
+	unsigned int load_at_max_freq = 0;
+	unsigned int avg_load_at_max_freq = 0;
+	unsigned int max_load_freq;
+	/* Current load across this CPU */
+	unsigned int cur_load = 0;
+
+	struct cpufreq_policy *policy;
+	struct cpu_dbs_info_s *j_dbs_info;
+	unsigned int j = 0;
+	int up_threshold = dbs_tuners_ins.up_threshold;
+
+	this_dbs_info->freq_lo = 0;
+	policy = this_dbs_info->cur_policy;
+	j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+
+	/* Only core0 controls the boost */
+	if (dbs_tuners_ins.boosted && policy->cpu == 0) {
+		if (ktime_to_us(ktime_get()) - electrodemand_freq_boosted_time >=
+					dbs_tuners_ins.freq_boost_time) {
+			dbs_tuners_ins.boosted = 0;
+		}
+	}
+
+	/* Only core0 controls the timer_rate */
+	if (electrodemand_sampling_rate_boosted && policy->cpu == 0) {
+		if (ktime_to_us(ktime_get()) - electrodemand_sampling_rate_boosted_time >=
+					TIMER_RATE_BOOST_TIME) {
+
+			dbs_tuners_ins.sampling_rate = electrodemand_current_sampling_rate;
+			electrodemand_sampling_rate_boosted = 0;
+		}
+	}
+
+	/*
+	 * Every sampling_rate, we check, if current idle time is less
+	 * than 20% (default), then we try to increase frequency
+	 * Every sampling_rate, we look for a the lowest
+	 * frequency which can sustain the load while keeping idle time over
+	 * 30%. If such a frequency exist, we try to decrease to this frequency.
+	 *
+	 * Any frequency increase takes it to the maximum frequency.
+	 * Frequency reduction happens at minimum steps of
+	 * 5% (default) of current frequency
+	 */
+
+	/* Get Absolute Load - in terms of freq */
+	max_load_freq = 0;
+
+	for_each_cpu(j, policy->cpus) {
+		cputime64_t cur_wall_time, cur_idle_time, cur_iowait_time;
+		unsigned int idle_time, wall_time, iowait_time;
+		unsigned int load_freq;
+		int freq_avg;
+		bool deep_sleep_detected = false;
+		/* the evil magic numbers, only 2 at least */
+		const unsigned int deep_sleep_backoff = 10;
+		const unsigned int deep_sleep_factor = 5;
+
+		j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time, 0);
+		cur_iowait_time = get_cpu_iowait_time(j, &cur_wall_time);
+
+		wall_time = (unsigned int)
+			(cur_wall_time - j_dbs_info->prev_cpu_wall);
+		j_dbs_info->prev_cpu_wall = cur_wall_time;
+
+		/*
+		 * Ignore wall delta jitters in both directions.  An
+		 * exceptionally long wall_time will likely result
+		 * idle but it was waken up to do work so the next
+		 * slice is less likely to want to run at low
+		 * frequency. Let's evaluate the next slice instead of
+		 * the idle long one that passed already and it's too
+		 * late to reduce in frequency.  As opposed an
+		 * exceptionally short slice that just run at low
+		 * frequency is unlikely to be idle, but we may go
+		 * back to idle pretty soon and that not idle slice
+		 * already passed. If short slices will keep coming
+		 * after a series of long slices the exponential
+		 * backoff will converge faster and we'll react faster
+		 * to high load. As opposed we'll decay slower
+		 * towards low load and long idle times.
+		 */
+		if (j_dbs_info->prev_cpu_wall_delta >
+		    wall_time * deep_sleep_factor ||
+		    j_dbs_info->prev_cpu_wall_delta * deep_sleep_factor <
+		    wall_time)
+			deep_sleep_detected = true;
+		j_dbs_info->prev_cpu_wall_delta =
+			(j_dbs_info->prev_cpu_wall_delta * deep_sleep_backoff
+			 + wall_time) / (deep_sleep_backoff+1);
+
+		idle_time = (unsigned int)
+			(cur_idle_time - j_dbs_info->prev_cpu_idle);
+		j_dbs_info->prev_cpu_idle = cur_idle_time;
+
+		iowait_time = (unsigned int)
+			(cur_iowait_time - j_dbs_info->prev_cpu_iowait);
+		j_dbs_info->prev_cpu_iowait = cur_iowait_time;
+
+		if (dbs_tuners_ins.ignore_nice) {
+			u64 cur_nice;
+			unsigned long cur_nice_jiffies;
+
+			cur_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE] -
+					 j_dbs_info->prev_cpu_nice;
+			/*
+			 * Assumption: nice time between sampling periods will
+			 * be less than 2^32 jiffies for 32 bit sys
+			 */
+			cur_nice_jiffies = (unsigned long)
+					cputime64_to_jiffies64(cur_nice);
+
+			j_dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			idle_time += jiffies_to_usecs(cur_nice_jiffies);
+		}
+
+		if (deep_sleep_detected)
+			continue;
+
+		/*
+		 * For the purpose of this, waiting for disk IO is an
+		 * indication that you're performance critical, and not that
+		 * the system is actually idle. So subtract the iowait time
+		 * from the cpu idle time.
+		 */
+
+		if (dbs_tuners_ins.io_is_busy && idle_time >= iowait_time)
+			idle_time -= iowait_time;
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		cur_load = 100 * (wall_time - idle_time) / wall_time;
+
+		freq_avg = __cpufreq_driver_getavg(policy, j);
+		if (freq_avg <= 0)
+			freq_avg = policy->cur;
+
+		load_freq = cur_load * freq_avg;
+		if (load_freq > max_load_freq)
+			max_load_freq = load_freq;
+
+		/* calculate the scaled load across CPU */
+		load_at_max_freq += (cur_load * policy->cur) /
+					policy->cpuinfo.max_freq;
+
+		avg_load_at_max_freq += ((load_at_max_freq +
+				j_dbs_info->load_at_prev_sample) / 2);
+
+		j_dbs_info->load_at_prev_sample = load_at_max_freq;
+	}
+
+	if (dbs_tuners_ins.sampling_rate < DEFAULT_SAMPLING_RATE)
+		cpufreq_notify_utilization(policy, avg_load_at_max_freq);
+	else
+		cpufreq_notify_utilization(policy, load_at_max_freq);
+
+	/* Check for frequency increase */
+	if (policy->cur < dbs_tuners_ins.freq_responsiveness)
+			up_threshold = dbs_tuners_ins.up_threshold_min_freq;
+
+	if (max_load_freq > up_threshold * policy->cur) {
+		int inc = (policy->max * dbs_tuners_ins.freq_step) / 100;
+		int target = min(policy->max, policy->cur + inc);
+
+		/* If switching to max speed, apply sampling_down_factor */
+		if (policy->cur < policy->max && target == policy->max) {
+			if (electrodemand_sampling_rate_boosted &&
+				(dbs_tuners_ins.sampling_down_factor <
+					BOOSTED_SAMPLING_DOWN_FACTOR)) {
+				this_dbs_info->rate_mult =
+					BOOSTED_SAMPLING_DOWN_FACTOR;
+			} else {
+				this_dbs_info->rate_mult =
+					dbs_tuners_ins.sampling_down_factor;
+			}
+		}
+		dbs_freq_increase(policy, target);
+		return;
+	}
+
+	/* check for frequency boost */
+	if (dbs_tuners_ins.boosted && policy->cur < dbs_tuners_ins.boostfreq) {
+		dbs_freq_increase(policy, dbs_tuners_ins.boostfreq);
+		dbs_tuners_ins.boostfreq = policy->cur;
+		return;
+	}
+
+	/* Check for frequency decrease */
+	/* if we cannot reduce the frequency anymore, break out early */
+	if (policy->cur == policy->min)
+		return;
+
+	/*
+	 * The optimal frequency is the frequency that is the lowest that
+	 * can support the current CPU usage without triggering the up
+	 * policy. To be safe, we focus 10 points under the threshold.
+	 */
+	if (max_load_freq <
+	    (dbs_tuners_ins.up_threshold - dbs_tuners_ins.down_differential) *
+			policy->cur) {
+		unsigned int freq_next;
+		unsigned int down_thres;
+
+		freq_next = max_load_freq /
+				(dbs_tuners_ins.up_threshold -
+					dbs_tuners_ins.down_differential);
+
+		if (dbs_tuners_ins.boosted &&
+				freq_next < dbs_tuners_ins.boostfreq) {
+			freq_next = dbs_tuners_ins.boostfreq;
+		}
+		/* No longer fully busy, reset rate_mult */
+		this_dbs_info->rate_mult = 1;
+
+		if (freq_next < policy->min)
+			freq_next = policy->min;
+
+		down_thres = dbs_tuners_ins.up_threshold_min_freq
+			- dbs_tuners_ins.down_differential;
+
+		if (freq_next < dbs_tuners_ins.freq_responsiveness
+			&& (max_load_freq / freq_next) > down_thres)
+			freq_next = dbs_tuners_ins.freq_responsiveness;
+
+		if (!dbs_tuners_ins.powersave_bias) {
+			__cpufreq_driver_target(policy, freq_next,
+					CPUFREQ_RELATION_L);
+		} else {
+			int freq = powersave_bias_target(policy, freq_next,
+					CPUFREQ_RELATION_L);
+			__cpufreq_driver_target(policy, freq,
+				CPUFREQ_RELATION_L);
+		}
+	}
+}
+
+static void do_dbs_timer(struct work_struct *work)
+{
+	struct cpu_dbs_info_s *dbs_info =
+		container_of(work, struct cpu_dbs_info_s, work.work);
+	unsigned int cpu = dbs_info->cpu;
+	int sample_type = dbs_info->sample_type;
+
+	int delay;
+
+	mutex_lock(&dbs_info->timer_mutex);
+
+	/* Common NORMAL_SAMPLE setup */
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	if (!dbs_tuners_ins.powersave_bias ||
+	    sample_type == DBS_NORMAL_SAMPLE) {
+		dbs_check_cpu(dbs_info);
+		if (dbs_info->freq_lo) {
+			/* Setup timer for SUB_SAMPLE */
+			dbs_info->sample_type = DBS_SUB_SAMPLE;
+			delay = dbs_info->freq_hi_jiffies;
+		} else {
+			/* We want all CPUs to do sampling nearly on
+			 * same jiffy
+			 */
+			delay = usecs_to_jiffies(effective_sampling_rate()
+				* dbs_info->rate_mult);
+
+			if (num_online_cpus() > 1)
+				delay -= jiffies % delay;
+		}
+	} else {
+		__cpufreq_driver_target(dbs_info->cur_policy,
+			dbs_info->freq_lo, CPUFREQ_RELATION_H);
+		delay = dbs_info->freq_lo_jiffies;
+	}
+	schedule_delayed_work_on(cpu, &dbs_info->work, delay);
+	mutex_unlock(&dbs_info->timer_mutex);
+}
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
+{
+	/* We want all CPUs to do sampling nearly on same jiffy */
+	int delay = usecs_to_jiffies(effective_sampling_rate());
+
+	if (num_online_cpus() > 1)
+		delay -= jiffies % delay;
+
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	INIT_DEFERRABLE_WORK(&dbs_info->work, do_dbs_timer);
+	schedule_delayed_work_on(dbs_info->cpu, &dbs_info->work, 10 * delay);
+	dbs_info->activated = true;
+}
+
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
+{
+	dbs_info->activated = false;
+	cancel_delayed_work_sync(&dbs_info->work);
+}
+
+/*
+ * Not all CPUs want IO time to be accounted as busy; this dependson how
+ * efficient idling at a higher frequency/voltage is.
+ * Pavel Machek says this is not so for various generations of AMD and old
+ * Intel systems.
+ * Mike Chan (androidlcom) calis this is also not true for ARM.
+ * Because of this, whitelist specific known (series) of CPUs by default, and
+ * leave all others up to the user.
+ */
+static int should_io_be_busy(void)
+{
+	return 0;
+}
+
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int rc;
+
+	this_dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy->cur))
+			return -EINVAL;
+
+		mutex_lock(&dbs_mutex);
+
+		dbs_enable++;
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&j_dbs_info->prev_cpu_wall, 0);
+			if (dbs_tuners_ins.ignore_nice)
+				j_dbs_info->prev_cpu_nice =
+						kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+		}
+		this_dbs_info->cpu = cpu;
+		this_dbs_info->rate_mult = 1;
+		electrodemand_powersave_bias_init_cpu(cpu);
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 1) {
+			unsigned int latency;
+
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&dbs_attr_group);
+			if (rc) {
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+
+			/* policy latency is in nS. Convert it to uS first */
+			latency = policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+			/* Bring kernel and HW constraints together */
+			min_sampling_rate = max(min_sampling_rate,
+					MIN_LATENCY_MULTIPLIER * latency);
+			dbs_tuners_ins.sampling_rate =
+				max(min_sampling_rate,
+				    latency * LATENCY_MULTIPLIER);
+			dbs_tuners_ins.io_is_busy = should_io_be_busy();
+		}
+		mutex_unlock(&dbs_mutex);
+
+		mutex_init(&this_dbs_info->timer_mutex);
+
+		dbs_timer_init(this_dbs_info);
+
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		dbs_timer_exit(this_dbs_info);
+
+		mutex_lock(&dbs_mutex);
+		mutex_destroy(&this_dbs_info->timer_mutex);
+		dbs_enable--;
+
+		if (!dbs_enable)
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &dbs_attr_group);
+		mutex_unlock(&dbs_mutex);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&this_dbs_info->timer_mutex);
+		if (policy->max < this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(this_dbs_info->cur_policy,
+				policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(this_dbs_info->cur_policy,
+				policy->min, CPUFREQ_RELATION_L);
+		dbs_check_cpu(this_dbs_info);
+		mutex_unlock(&this_dbs_info->timer_mutex);
+
+		break;
+	}
+	return 0;
+}
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	cputime64_t wall;
+	u64 idle_time;
+	int cpu = get_cpu();
+	int err = 0;
+
+	idle_time = get_cpu_idle_time_us(cpu, &wall);
+	put_cpu();
+	if (idle_time != -1ULL) {
+		/* Idle micro accounting is supported. Use finer thresholds */
+		dbs_tuners_ins.up_threshold = dbs_tuners_ins.micro_freq_up_threshold;
+		dbs_tuners_ins.down_differential =
+					MICRO_FREQUENCY_DOWN_DIFFERENTIAL;
+		/*
+		 * In no_hz/micro accounting case we set the minimum frequency
+		 * not depending on HZ, but fixed (very low). The deferred
+		 * timer might skip some samples if idle/sleeping as needed.
+		*/
+		min_sampling_rate = MICRO_FREQUENCY_MIN_SAMPLE_RATE;
+	} else {
+		/* For correct statistics, we need 10 ticks for each measure */
+		min_sampling_rate =
+			MIN_SAMPLING_RATE_RATIO * jiffies_to_usecs(10);
+	}
+
+	err = cpufreq_register_governor(&cpufreq_gov_electrodemand);
+	if (err) {
+		goto error_reg;
+	}
+
+	return err;
+error_reg:
+	kfree(&dbs_tuners_ins);
+	return err;
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_electrodemand);
+	kfree(&dbs_tuners_ins);
+}
+
+MODULE_AUTHOR("Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>");
+MODULE_AUTHOR("Alexey Starikovskiy <alexey.y.starikovskiy@intel.com>");
+MODULE_DESCRIPTION("'cpufreq_electrodemand' - A dynamic cpufreq governor for "
+	"Low Latency Frequency Transition capable processors, based off of ondemand");
+MODULE_LICENSE("GPLv2");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_ELECTRODEMAND
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_elementalx.c b/drivers/cpufreq/cpufreq_elementalx.c
new file mode 100644
index 0000000..b224e11
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_elementalx.c
@@ -0,0 +1,589 @@
+/*
+ *  drivers/cpufreq/cpufreq_elementalx.c
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *            (C)  2015 Aaron Segaert <asegaert@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/slab.h>
+#include <linux/fb.h>
+#include <linux/msm_kgsl.h>
+#include "cpufreq_governor.h"
+
+/* elementalx governor macros */
+#define DEF_FREQUENCY_UP_THRESHOLD		(90)
+#define DEF_FREQUENCY_DOWN_DIFFERENTIAL		(20)
+#define DEF_ACTIVE_FLOOR_FREQ			(960000)
+#define DEF_GBOOST_MIN_FREQ			(1574400)
+#define DEF_MAX_SCREEN_OFF_FREQ			(2265000)
+#define MIN_SAMPLING_RATE			(10000)
+#define DEF_SAMPLING_DOWN_FACTOR		(8)
+#define MAX_SAMPLING_DOWN_FACTOR		(20)
+#define FREQ_NEED_BURST(x)			(x < 600000 ? 1 : 0)
+#define MAX(x,y)				(x > y ? x : y)
+#define MIN(x,y)				(x < y ? x : y)
+#define TABLE_SIZE				5
+
+static DEFINE_PER_CPU(struct ex_cpu_dbs_info_s, ex_cpu_dbs_info);
+
+static unsigned int up_threshold_level[2] __read_mostly = {95, 85};
+static struct cpufreq_frequency_table *tbl = NULL;
+static unsigned int *tblmap[TABLE_SIZE] __read_mostly;
+static unsigned int tbl_select[4];
+
+static struct ex_governor_data {
+	unsigned int active_floor_freq;
+	unsigned int max_screen_off_freq;
+	unsigned int prev_load;
+	unsigned int g_count;
+	bool suspended;
+	struct notifier_block notif;
+} ex_data = {
+	.active_floor_freq = DEF_ACTIVE_FLOOR_FREQ,
+	.max_screen_off_freq = DEF_MAX_SCREEN_OFF_FREQ,
+	.prev_load = 0,
+	.g_count = 0,
+	.suspended = false
+};
+
+static void dbs_init_freq_map_table(void)
+{
+	unsigned int min_diff, top1, top2;
+	int cnt, i, j;
+	struct cpufreq_policy *policy;
+
+	policy = cpufreq_cpu_get(0);
+	tbl = cpufreq_frequency_get_table(0);
+	min_diff = policy->cpuinfo.max_freq;
+
+	for (cnt = 0; (tbl[cnt].frequency != CPUFREQ_TABLE_END); cnt++) {
+		if (cnt > 0)
+			min_diff = MIN(tbl[cnt].frequency - tbl[cnt-1].frequency, min_diff);
+	}
+
+	top1 = (policy->cpuinfo.max_freq + policy->cpuinfo.min_freq) / 2;
+	top2 = (policy->cpuinfo.max_freq + top1) / 2;
+
+	for (i = 0; i < TABLE_SIZE; i++) {
+		tblmap[i] = kmalloc(sizeof(unsigned int) * cnt, GFP_KERNEL);
+		BUG_ON(!tblmap[i]);
+		for (j = 0; j < cnt; j++)
+			tblmap[i][j] = tbl[j].frequency;
+	}
+
+	for (j = 0; j < cnt; j++) {
+		if (tbl[j].frequency < top1) {
+			tblmap[0][j] += MAX((top1 - tbl[j].frequency)/3, min_diff);
+		}
+
+		if (tbl[j].frequency < top2) {
+			tblmap[1][j] += MAX((top2 - tbl[j].frequency)/3, min_diff);
+			tblmap[2][j] += MAX(((top2 - tbl[j].frequency)*2)/5, min_diff);
+			tblmap[3][j] += MAX((top2 - tbl[j].frequency)/2, min_diff);
+		} else {
+			tblmap[3][j] += MAX((policy->cpuinfo.max_freq - tbl[j].frequency)/3, min_diff);
+		}
+
+		tblmap[4][j] += MAX((policy->cpuinfo.max_freq - tbl[j].frequency)/2, min_diff);
+	}
+
+	tbl_select[0] = 0;
+	tbl_select[1] = 1;
+	tbl_select[2] = 2;
+	tbl_select[3] = 4;
+}
+
+static void dbs_deinit_freq_map_table(void)
+{
+	int i;
+
+	if (!tbl)
+		return;
+
+	tbl = NULL;
+
+	for (i = 0; i < TABLE_SIZE; i++)
+		kfree(tblmap[i]);
+}
+
+static inline int get_cpu_freq_index(unsigned int freq)
+{
+	static int saved_index = 0;
+	int index;
+
+	if (!tbl) {
+		pr_warn("tbl is NULL, use previous value %d\n", saved_index);
+		return saved_index;
+	}
+
+	for (index = 0; (tbl[index].frequency != CPUFREQ_TABLE_END); index++) {
+		if (tbl[index].frequency >= freq) {
+			saved_index = index;
+			break;
+		}
+	}
+
+	return index;
+}
+
+static inline unsigned int ex_freq_increase(struct cpufreq_policy *p, unsigned int freq)
+{
+	if (freq > p->max) {
+		return p->max;
+	} 
+	
+	else if (ex_data.suspended) {
+		freq = MIN(freq, ex_data.max_screen_off_freq);
+	}
+
+	return freq;
+}
+
+static void ex_check_cpu(int cpu, unsigned int load)
+{
+	struct ex_cpu_dbs_info_s *dbs_info = &per_cpu(ex_cpu_dbs_info, cpu);
+	struct cpufreq_policy *policy = dbs_info->cdbs.cur_policy;
+	struct dbs_data *dbs_data = policy->governor_data;
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	unsigned int max_load_freq = 0, freq_next = 0;
+	unsigned int j, avg_load, cur_freq, max_freq, target_freq = 0;
+
+	cpufreq_notify_utilization(policy, load);
+
+	cur_freq = policy->cur;
+	max_freq = policy->max;
+
+	for_each_cpu(j, policy->cpus) {
+		if (load > max_load_freq)
+			max_load_freq = load * policy->cur;
+	}
+	avg_load = (ex_data.prev_load + load) >> 1;
+
+	if (ex_tuners->gboost) {
+		if (ex_data.g_count < 500 && graphics_boost_elementalx < 3)
+			++ex_data.g_count;
+		else if (ex_data.g_count > 1)
+			--ex_data.g_count;
+	}
+
+	//gboost mode
+	if (ex_tuners->gboost && ex_data.g_count > 300) {
+				
+		if (avg_load > 40 + (graphics_boost_elementalx * 10)) {
+			freq_next = max_freq;
+		} else {
+			freq_next = max_freq * avg_load / 100;
+			freq_next = MAX(freq_next, ex_tuners->gboost_min_freq);
+		}
+
+		target_freq = ex_freq_increase(policy, freq_next);
+
+		__cpufreq_driver_target(policy, target_freq, CPUFREQ_RELATION_H);
+
+		goto finished;
+	} 
+
+	//normal mode
+	if (max_load_freq > up_threshold_level[1] * cur_freq) {
+		int index = get_cpu_freq_index(cur_freq);
+
+		if (FREQ_NEED_BURST(cur_freq) &&
+				load > up_threshold_level[0]) {
+			freq_next = max_freq;
+		}
+		
+		else if (avg_load > up_threshold_level[0]) {
+			freq_next = tblmap[tbl_select[3]][index];
+		}
+		
+		else if (avg_load <= up_threshold_level[1]) {
+			freq_next = tblmap[tbl_select[1]][index];
+		}
+	
+		else {
+			if (load > up_threshold_level[0]) {
+				freq_next = tblmap[tbl_select[3]][index];
+			}
+		
+			else {
+				freq_next = tblmap[tbl_select[2]][index];
+			}
+		}
+
+		target_freq = ex_freq_increase(policy, freq_next);
+
+		__cpufreq_driver_target(policy, target_freq, CPUFREQ_RELATION_H);
+
+		if (target_freq > ex_data.active_floor_freq)
+			dbs_info->down_floor = 0;
+
+		goto finished;
+	}
+
+	if (cur_freq == policy->min)
+		goto finished;
+
+	if (cur_freq >= ex_data.active_floor_freq) {
+		if (++dbs_info->down_floor > ex_tuners->sampling_down_factor)
+			dbs_info->down_floor = 0;
+	} else {
+		dbs_info->down_floor = 0;
+	}
+
+	if (max_load_freq <
+	    (ex_tuners->up_threshold - ex_tuners->down_differential) *
+	     cur_freq) {
+
+		freq_next = max_load_freq /
+				(ex_tuners->up_threshold -
+				 ex_tuners->down_differential);
+
+		if (dbs_info->down_floor && !ex_data.suspended) {
+			freq_next = MAX(freq_next, ex_data.active_floor_freq);
+		} else {
+			freq_next = MAX(freq_next, policy->min);
+			if (freq_next < ex_data.active_floor_freq)
+				dbs_info->down_floor = ex_tuners->sampling_down_factor;
+		}
+
+		__cpufreq_driver_target(policy, freq_next,
+			CPUFREQ_RELATION_L);
+	}
+
+finished:
+	ex_data.prev_load = load;
+	return;
+}
+
+static void ex_dbs_timer(struct work_struct *work)
+{
+	struct ex_cpu_dbs_info_s *dbs_info = container_of(work,
+			struct ex_cpu_dbs_info_s, cdbs.work.work);
+	unsigned int cpu = dbs_info->cdbs.cur_policy->cpu;
+	struct ex_cpu_dbs_info_s *core_dbs_info = &per_cpu(ex_cpu_dbs_info,
+			cpu);
+	struct dbs_data *dbs_data = dbs_info->cdbs.cur_policy->governor_data;
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	int delay = delay_for_sampling_rate(ex_tuners->sampling_rate);
+	bool modify_all = true;
+
+	mutex_lock(&core_dbs_info->cdbs.timer_mutex);
+	if (!need_load_eval(&core_dbs_info->cdbs, ex_tuners->sampling_rate))
+		modify_all = false;
+	else
+		dbs_check_cpu(dbs_data, cpu);
+
+	gov_queue_work(dbs_data, dbs_info->cdbs.cur_policy, delay, modify_all);
+	mutex_unlock(&core_dbs_info->cdbs.timer_mutex);
+}
+
+static int fb_notifier_callback(struct notifier_block *this,
+				unsigned long event, void *data)
+{
+	struct fb_event *evdata = data;
+	int *blank;
+
+	if (evdata && evdata->data && event == FB_EVENT_BLANK) {
+		blank = evdata->data;
+		switch (*blank) {
+			case FB_BLANK_UNBLANK:
+				//display on
+				ex_data.suspended = false;
+				break;
+			case FB_BLANK_POWERDOWN:
+			case FB_BLANK_HSYNC_SUSPEND:
+			case FB_BLANK_VSYNC_SUSPEND:
+			case FB_BLANK_NORMAL:
+				//display off
+				ex_data.suspended = true;
+				break;
+		}
+	}
+
+	return NOTIFY_OK;
+}
+
+/************************** sysfs interface ************************/
+static struct common_dbs_data ex_dbs_cdata;
+
+static ssize_t store_sampling_rate(struct dbs_data *dbs_data, const char *buf,
+		size_t count)
+{
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	ex_tuners->sampling_rate = max(input, dbs_data->min_sampling_rate);
+	return count;
+}
+
+static ssize_t store_up_threshold(struct dbs_data *dbs_data, const char *buf,
+		size_t count)
+{
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 || input <= ex_tuners->down_differential)
+		return -EINVAL;
+
+	ex_tuners->up_threshold = input;
+	return count;
+}
+
+static ssize_t store_down_differential(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 || input >= ex_tuners->up_threshold)
+		return -EINVAL;
+
+	ex_tuners->down_differential = input;
+	return count;
+}
+
+static ssize_t store_gboost(struct dbs_data *dbs_data, const char *buf,
+		size_t count)
+{
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 1)
+		return -EINVAL;
+
+	if (input == 0)
+		ex_data.g_count = 0;
+
+	ex_tuners->gboost = input;
+	return count;
+}
+
+static ssize_t store_gboost_min_freq(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	ex_tuners->gboost_min_freq = input;
+	return count;
+}
+
+static ssize_t store_active_floor_freq(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	ex_tuners->active_floor_freq = input;
+	ex_data.active_floor_freq = ex_tuners->active_floor_freq;
+	return count;
+}
+
+static ssize_t store_max_screen_off_freq(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input == 0)
+		input = UINT_MAX;
+
+	ex_tuners->max_screen_off_freq = input;
+	ex_data.max_screen_off_freq = ex_tuners->max_screen_off_freq;
+	return count;
+}
+
+static ssize_t store_sampling_down_factor(struct dbs_data *dbs_data,
+		const char *buf, size_t count)
+{
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 0)
+		return -EINVAL;
+
+	ex_tuners->sampling_down_factor = input;
+	return count;
+}
+
+show_store_one(ex, sampling_rate);
+show_store_one(ex, up_threshold);
+show_store_one(ex, down_differential);
+show_store_one(ex, gboost);
+show_store_one(ex, gboost_min_freq);
+show_store_one(ex, active_floor_freq);
+show_store_one(ex, max_screen_off_freq);
+show_store_one(ex, sampling_down_factor);
+declare_show_sampling_rate_min(ex);
+
+gov_sys_pol_attr_rw(sampling_rate);
+gov_sys_pol_attr_rw(up_threshold);
+gov_sys_pol_attr_rw(down_differential);
+gov_sys_pol_attr_rw(gboost);
+gov_sys_pol_attr_rw(gboost_min_freq);
+gov_sys_pol_attr_rw(active_floor_freq);
+gov_sys_pol_attr_rw(max_screen_off_freq);
+gov_sys_pol_attr_rw(sampling_down_factor);
+gov_sys_pol_attr_ro(sampling_rate_min);
+
+static struct attribute *dbs_attributes_gov_sys[] = {
+	&sampling_rate_min_gov_sys.attr,
+	&sampling_rate_gov_sys.attr,
+	&up_threshold_gov_sys.attr,
+	&down_differential_gov_sys.attr,
+	&gboost_gov_sys.attr,
+	&gboost_min_freq_gov_sys.attr,
+	&active_floor_freq_gov_sys.attr,
+	&max_screen_off_freq_gov_sys.attr,
+	&sampling_down_factor_gov_sys.attr,
+	NULL
+};
+
+static struct attribute_group ex_attr_group_gov_sys = {
+	.attrs = dbs_attributes_gov_sys,
+	.name = "elementalx",
+};
+
+static struct attribute *dbs_attributes_gov_pol[] = {
+	&sampling_rate_min_gov_pol.attr,
+	&sampling_rate_gov_pol.attr,
+	&up_threshold_gov_pol.attr,
+	&down_differential_gov_pol.attr,
+	&gboost_gov_pol.attr,
+	&gboost_min_freq_gov_pol.attr,
+	&active_floor_freq_gov_pol.attr,
+	&max_screen_off_freq_gov_pol.attr,
+	&sampling_down_factor_gov_pol.attr,
+	NULL
+};
+
+static struct attribute_group ex_attr_group_gov_pol = {
+	.attrs = dbs_attributes_gov_pol,
+	.name = "elementalx",
+};
+
+/************************** sysfs end ************************/
+
+static int ex_init(struct dbs_data *dbs_data)
+{
+	struct ex_dbs_tuners *tuners;
+
+	tuners = kzalloc(sizeof(*tuners), GFP_KERNEL);
+	if (!tuners) {
+		pr_err("%s: kzalloc failed\n", __func__);
+		return -ENOMEM;
+	}
+
+	tuners->up_threshold = DEF_FREQUENCY_UP_THRESHOLD;
+	tuners->down_differential = DEF_FREQUENCY_DOWN_DIFFERENTIAL;
+	tuners->ignore_nice_load = 0;
+	tuners->gboost = 1;
+	tuners->gboost_min_freq = DEF_GBOOST_MIN_FREQ;
+	tuners->active_floor_freq = DEF_ACTIVE_FLOOR_FREQ;
+	tuners->max_screen_off_freq = DEF_MAX_SCREEN_OFF_FREQ;
+	tuners->sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR;
+
+	dbs_data->tuners = tuners;
+	dbs_data->min_sampling_rate = MIN_SAMPLING_RATE;
+
+	dbs_init_freq_map_table();
+
+	ex_data.notif.notifier_call = fb_notifier_callback;
+	if (fb_register_client(&ex_data.notif))
+		pr_err("%s: Failed to register fb_notifier\n", __func__);
+
+	mutex_init(&dbs_data->mutex);
+	return 0;
+}
+
+static void ex_exit(struct dbs_data *dbs_data)
+{
+	dbs_deinit_freq_map_table();
+	fb_unregister_client(&ex_data.notif);
+	kfree(dbs_data->tuners);
+}
+
+define_get_cpu_dbs_routines(ex_cpu_dbs_info);
+
+static struct common_dbs_data ex_dbs_cdata = {
+	.governor = GOV_ELEMENTALX,
+	.attr_group_gov_sys = &ex_attr_group_gov_sys,
+	.attr_group_gov_pol = &ex_attr_group_gov_pol,
+	.get_cpu_cdbs = get_cpu_cdbs,
+	.get_cpu_dbs_info_s = get_cpu_dbs_info_s,
+	.gov_dbs_timer = ex_dbs_timer,
+	.gov_check_cpu = ex_check_cpu,
+	.init = ex_init,
+	.exit = ex_exit,
+};
+
+static int ex_cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	return cpufreq_governor_dbs(policy, &ex_dbs_cdata, event);
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_ELEMENTALX
+static
+#endif
+struct cpufreq_governor cpufreq_gov_elementalx = {
+	.name			= "elementalx",
+	.governor		= ex_cpufreq_governor_dbs,
+	.max_transition_latency	= TRANSITION_LATENCY_LIMIT,
+	.owner			= THIS_MODULE,
+};
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	return cpufreq_register_governor(&cpufreq_gov_elementalx);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_elementalx);
+}
+
+MODULE_AUTHOR("Aaron Segaert <asegaert@gmail.com>");
+MODULE_DESCRIPTION("'cpufreq_elementalx' - multiphase cpufreq governor");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_ELEMENTALX
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_governor.c b/drivers/cpufreq/cpufreq_governor.c
index e6be635..af2a789 100755
--- a/drivers/cpufreq/cpufreq_governor.c
+++ b/drivers/cpufreq/cpufreq_governor.c
@@ -35,15 +35,34 @@ void dbs_check_cpu(struct dbs_data *dbs_data, int cpu)
 	struct cpu_dbs_common_info *cdbs = dbs_data->cdata->get_cpu_cdbs(cpu);
 	struct od_dbs_tuners *od_tuners = dbs_data->tuners;
 	struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
+	struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
 	struct cpufreq_policy *policy;
+	unsigned int sampling_rate;
 	unsigned int max_load = 0;
 	unsigned int ignore_nice;
 	unsigned int j;
 
-	if (dbs_data->cdata->governor == GOV_ONDEMAND)
+	if (dbs_data->cdata->governor == GOV_ONDEMAND) {
+		struct od_cpu_dbs_info_s *od_dbs_info =
+				dbs_data->cdata->get_cpu_dbs_info_s(cpu);
+
+		/*
+		 * Sometimes, the ondemand governor uses an additional
+		 * multiplier to give long delays. So apply this multiplier to
+		 * the 'sampling_rate', so as to keep the wake-up-from-idle
+		 * detection logic a bit conservative.
+		 */
+		sampling_rate = od_tuners->sampling_rate;
+		sampling_rate *= od_dbs_info->rate_mult;
+
 		ignore_nice = od_tuners->ignore_nice_load;
-	else
+	} else if (dbs_data->cdata->governor == GOV_ELEMENTALX) {
+		sampling_rate = ex_tuners->sampling_rate;
+		ignore_nice = ex_tuners->ignore_nice_load;
+	} else {
+		sampling_rate = cs_tuners->sampling_rate;
 		ignore_nice = cs_tuners->ignore_nice_load;
+	}
 
 	policy = cdbs->cur_policy;
 
@@ -96,7 +115,46 @@ void dbs_check_cpu(struct dbs_data *dbs_data, int cpu)
 		if (unlikely(!wall_time || wall_time < idle_time))
 			continue;
 
-		load = 100 * (wall_time - idle_time) / wall_time;
+		/*
+		 * If the CPU had gone completely idle, and a task just woke up
+		 * on this CPU now, it would be unfair to calculate 'load' the
+		 * usual way for this elapsed time-window, because it will show
+		 * near-zero load, irrespective of how CPU intensive that task
+		 * actually is. This is undesirable for latency-sensitive bursty
+		 * workloads.
+		 *
+		 * To avoid this, we reuse the 'load' from the previous
+		 * time-window and give this task a chance to start with a
+		 * reasonably high CPU frequency. (However, we shouldn't over-do
+		 * this copy, lest we get stuck at a high load (high frequency)
+		 * for too long, even when the current system load has actually
+		 * dropped down. So we perform the copy only once, upon the
+		 * first wake-up from idle.)
+		 *
+		 * Detecting this situation is easy: the governor's deferrable
+		 * timer would not have fired during CPU-idle periods. Hence
+		 * an unusually large 'wall_time' (as compared to the sampling
+		 * rate) indicates this scenario.
+		 *
+		 * prev_load can be zero in two cases and we must recalculate it
+		 * for both cases:
+		 * - during long idle intervals
+		 * - explicitly set to zero
+		 */
+		if (unlikely(wall_time > (2 * sampling_rate) &&
+			     j_cdbs->prev_load)) {
+			load = j_cdbs->prev_load;
+
+			/*
+			 * Perform a destructive copy, to ensure that we copy
+			 * the previous load only once, upon the first wake-up
+			 * from idle.
+			 */
+			j_cdbs->prev_load = 0;
+		} else {
+			load = 100 * (wall_time - idle_time) / wall_time;
+			j_cdbs->prev_load = load;
+		}
 
 		if (load > max_load)
 			max_load = load;
@@ -119,8 +177,9 @@ void gov_queue_work(struct dbs_data *dbs_data, struct cpufreq_policy *policy,
 {
 	int i;
 
+	mutex_lock(&cpufreq_governor_lock);
 	if (!policy->governor_enabled)
-		return;
+		goto out_unlock;
 
 	if (!all_cpus) {
 		/*
@@ -135,6 +194,9 @@ void gov_queue_work(struct dbs_data *dbs_data, struct cpufreq_policy *policy,
 		for_each_cpu(i, policy->cpus)
 			__gov_queue_work(i, dbs_data, delay);
 	}
+
+out_unlock:
+	mutex_unlock(&cpufreq_governor_lock);
 }
 EXPORT_SYMBOL_GPL(gov_queue_work);
 
@@ -175,6 +237,9 @@ static void set_sampling_rate(struct dbs_data *dbs_data,
 	if (dbs_data->cdata->governor == GOV_CONSERVATIVE) {
 		struct cs_dbs_tuners *cs_tuners = dbs_data->tuners;
 		cs_tuners->sampling_rate = sampling_rate;
+	} else if (dbs_data->cdata->governor == GOV_ELEMENTALX) {
+		struct ex_dbs_tuners *ex_tuners = dbs_data->tuners;
+		ex_tuners->sampling_rate = sampling_rate;
 	} else {
 		struct od_dbs_tuners *od_tuners = dbs_data->tuners;
 		od_tuners->sampling_rate = sampling_rate;
@@ -187,9 +252,11 @@ int cpufreq_governor_dbs(struct cpufreq_policy *policy,
 	struct dbs_data *dbs_data;
 	struct od_cpu_dbs_info_s *od_dbs_info = NULL;
 	struct cs_cpu_dbs_info_s *cs_dbs_info = NULL;
+	struct ex_cpu_dbs_info_s *ex_dbs_info = NULL;
 	struct od_ops *od_ops = NULL;
 	struct od_dbs_tuners *od_tuners = NULL;
 	struct cs_dbs_tuners *cs_tuners = NULL;
+	struct ex_dbs_tuners *ex_tuners = NULL;
 	struct cpu_dbs_common_info *cpu_cdbs;
 	unsigned int sampling_rate, latency, ignore_nice, j, cpu = policy->cpu;
 	int io_busy = 0;
@@ -295,6 +362,11 @@ int cpufreq_governor_dbs(struct cpufreq_policy *policy,
 		cs_dbs_info = dbs_data->cdata->get_cpu_dbs_info_s(cpu);
 		sampling_rate = cs_tuners->sampling_rate;
 		ignore_nice = cs_tuners->ignore_nice_load;
+	} else if (dbs_data->cdata->governor == GOV_ELEMENTALX) {
+		ex_tuners = dbs_data->tuners;
+		ex_dbs_info = dbs_data->cdata->get_cpu_dbs_info_s(cpu);
+		sampling_rate = ex_tuners->sampling_rate;
+		ignore_nice = ex_tuners->ignore_nice_load;
 	} else {
 		od_tuners = dbs_data->tuners;
 		od_dbs_info = dbs_data->cdata->get_cpu_dbs_info_s(cpu);
@@ -314,11 +386,18 @@ int cpufreq_governor_dbs(struct cpufreq_policy *policy,
 		for_each_cpu(j, policy->cpus) {
 			struct cpu_dbs_common_info *j_cdbs =
 				dbs_data->cdata->get_cpu_cdbs(j);
+			unsigned int prev_load;
 
 			j_cdbs->cpu = j;
 			j_cdbs->cur_policy = policy;
 			j_cdbs->prev_cpu_idle = get_cpu_idle_time(j,
 					       &j_cdbs->prev_cpu_wall, io_busy);
+
+			prev_load = (unsigned int)
+				(j_cdbs->prev_cpu_wall - j_cdbs->prev_cpu_idle);
+			j_cdbs->prev_load = 100 * prev_load /
+					(unsigned int) j_cdbs->prev_cpu_wall;
+
 			if (ignore_nice)
 				j_cdbs->prev_cpu_nice =
 					kcpustat_cpu(j).cpustat[CPUTIME_NICE];
@@ -332,6 +411,9 @@ int cpufreq_governor_dbs(struct cpufreq_policy *policy,
 			cs_dbs_info->down_skip = 0;
 			cs_dbs_info->enable = 1;
 			cs_dbs_info->requested_freq = policy->cur;
+		} else if (dbs_data->cdata->governor == GOV_ELEMENTALX) {
+			ex_dbs_info->down_floor = 0;
+			ex_dbs_info->enable = 1;
 		} else {
 			od_dbs_info->rate_mult = 1;
 			od_dbs_info->sample_type = OD_NORMAL_SAMPLE;
@@ -351,6 +433,9 @@ int cpufreq_governor_dbs(struct cpufreq_policy *policy,
 		if (dbs_data->cdata->governor == GOV_CONSERVATIVE)
 			cs_dbs_info->enable = 0;
 
+		if (dbs_data->cdata->governor == GOV_ELEMENTALX)
+			ex_dbs_info->enable = 0;
+
 		gov_cancel_work(dbs_data, policy);
 
 		mutex_lock(&dbs_data->mutex);
@@ -362,6 +447,11 @@ int cpufreq_governor_dbs(struct cpufreq_policy *policy,
 		break;
 
 	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&dbs_data->mutex);
+		if (!cpu_cdbs->cur_policy) {
+			mutex_unlock(&dbs_data->mutex);
+			break;
+		}
 		mutex_lock(&cpu_cdbs->timer_mutex);
 		if (policy->max < cpu_cdbs->cur_policy->cur)
 			__cpufreq_driver_target(cpu_cdbs->cur_policy,
@@ -371,6 +461,7 @@ int cpufreq_governor_dbs(struct cpufreq_policy *policy,
 					policy->min, CPUFREQ_RELATION_L);
 		dbs_check_cpu(dbs_data, cpu);
 		mutex_unlock(&cpu_cdbs->timer_mutex);
+		mutex_unlock(&dbs_data->mutex);
 		break;
 	}
 	return 0;
diff --git a/drivers/cpufreq/cpufreq_governor.h b/drivers/cpufreq/cpufreq_governor.h
index b5f2b86..af5d1ee 100755
--- a/drivers/cpufreq/cpufreq_governor.h
+++ b/drivers/cpufreq/cpufreq_governor.h
@@ -126,6 +126,7 @@ static void *get_cpu_dbs_info_s(int cpu)				\
  * cdbs: common dbs
  * od_*: On-demand governor
  * cs_*: Conservative governor
+ * ex_*: ElementalX governor
  */
 
 /* Per cpu structures */
@@ -134,6 +135,13 @@ struct cpu_dbs_common_info {
 	u64 prev_cpu_idle;
 	u64 prev_cpu_wall;
 	u64 prev_cpu_nice;
+	/*
+	 * Used to keep track of load in the previous interval. However, when
+	 * explicitly set to zero, it is used as a flag to ensure that we copy
+	 * the previous load to the current interval only once, upon the first
+	 * wake-up from idle.
+	 */
+	unsigned int prev_load;
 	struct cpufreq_policy *cur_policy;
 	struct delayed_work work;
 	/*
@@ -160,6 +168,14 @@ struct cs_cpu_dbs_info_s {
 	unsigned int down_skip;
 	unsigned int requested_freq;
 	unsigned int enable:1;
+	unsigned int twostep_counter;
+	u64 twostep_time;
+};
+
+struct ex_cpu_dbs_info_s {
+	struct cpu_dbs_common_info cdbs;
+	unsigned int down_floor;
+	unsigned int enable:1;
 };
 
 /* Per policy Governors sysfs tunables */
@@ -179,6 +195,22 @@ struct cs_dbs_tuners {
 	unsigned int up_threshold;
 	unsigned int down_threshold;
 	unsigned int freq_step;
+	unsigned int input_boost_freq;
+	unsigned int input_boost_duration;
+	unsigned int twostep_threshold;
+	unsigned int min_load;
+};
+
+struct ex_dbs_tuners {
+	unsigned int ignore_nice_load;
+	unsigned int sampling_rate;
+	unsigned int up_threshold;
+	unsigned int down_differential;
+	unsigned int gboost;
+	unsigned int gboost_min_freq;
+	unsigned int active_floor_freq;
+	unsigned int max_screen_off_freq;
+	unsigned int sampling_down_factor;
 };
 
 /* Common Governor data across policies */
@@ -187,6 +219,8 @@ struct common_dbs_data {
 	/* Common across governors */
 	#define GOV_ONDEMAND		0
 	#define GOV_CONSERVATIVE	1
+	#define GOV_ELEMENTALX		2
+	#define GOV_CONSERVATIVE_X	3
 	int governor;
 	struct attribute_group *attr_group_gov_sys; /* one governor - system */
 	struct attribute_group *attr_group_gov_pol; /* one governor - policy */
@@ -257,6 +291,8 @@ static ssize_t show_sampling_rate_min_gov_pol				\
 	return sprintf(buf, "%u\n", dbs_data->min_sampling_rate);	\
 }
 
+extern struct mutex cpufreq_governor_lock;
+
 void dbs_check_cpu(struct dbs_data *dbs_data, int cpu);
 bool need_load_eval(struct cpu_dbs_common_info *cdbs,
 		unsigned int sampling_rate);
diff --git a/drivers/cpufreq/cpufreq_hellsactive.c b/drivers/cpufreq/cpufreq_hellsactive.c
new file mode 100644
index 0000000..5da3adb
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_hellsactive.c
@@ -0,0 +1,1548 @@
+/*
+ * drivers/cpufreq/cpufreq_hellsactive.c
+ *
+ * Copyright (C) 2010 Google, Inc.
+ * Copyright (C) 2014 Paul Reioux
+ * Copyright (C) 2014 Laurent Hess
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * Author: Mike Chan (mike@android.com)
+ * Author: Paul Reioux (reioux@gmail.com) Modified for intelliactive
+ * Author: Laurent Hess (hellsgod@gmx.ch) Modified for hellsactive
+ */
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/rwsem.h>
+#include <linux/sched.h>
+#include <linux/tick.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/kernel_stat.h>
+#include <asm/cputime.h>
+#include <linux/touchboost.h>
+#include <linux/sched/rt.h>
+
+static int active_count;
+
+struct cpufreq_interactive_cpuinfo {
+	struct timer_list cpu_timer;
+	struct timer_list cpu_slack_timer;
+	spinlock_t load_lock; /* protects the next 4 fields */
+	u64 time_in_idle;
+	u64 time_in_idle_timestamp;
+	u64 cputime_speedadj;
+	u64 cputime_speedadj_timestamp;
+	u64 last_evaluated_jiffy;
+	struct cpufreq_policy *policy;
+	struct cpufreq_frequency_table *freq_table;
+	spinlock_t target_freq_lock; /*protects target freq */
+	unsigned int target_freq;
+	unsigned int floor_freq;
+	unsigned int max_freq;
+	u64 floor_validate_time;
+	u64 hispeed_validate_time; /* cluster hispeed_validate_time */
+	u64 local_hvtime; /* per-cpu hispeed_validate_time */
+	u64 max_freq_idle_start_time;
+	struct rw_semaphore enable_sem;
+	int governor_enabled;
+	unsigned int two_phase_freq;
+};
+
+static DEFINE_PER_CPU(struct cpufreq_interactive_cpuinfo, cpuinfo);
+
+/* realtime thread handles frequency scaling */
+static struct task_struct *speedchange_task;
+static cpumask_t speedchange_cpumask;
+static spinlock_t speedchange_cpumask_lock;
+static struct mutex gov_lock;
+
+/* Hi speed to bump to from lo speed when load burst (default max) */
+static unsigned int hispeed_freq = 1190400;
+
+/* Go to hi speed when CPU load at or above this value. */
+#define DEFAULT_GO_HISPEED_LOAD 99
+static unsigned long go_hispeed_load = DEFAULT_GO_HISPEED_LOAD;
+
+/* Target load.  Lower values result in higher CPU speeds. */
+#define DEFAULT_TARGET_LOAD 90
+static unsigned int default_target_loads[] = {DEFAULT_TARGET_LOAD};
+static spinlock_t target_loads_lock;
+static unsigned int *target_loads = default_target_loads;
+static int ntarget_loads = ARRAY_SIZE(default_target_loads);
+
+/*
+ * Frequency calculation threshold. Avoid freq oscillations up to this
+ * threshold and allow for dynamic changes above (default policy->min).
+ */
+static unsigned long freq_calc_thresh;
+
+/*
+ * The minimum amount of time to spend at a frequency before we can ramp down.
+ */
+#define DEFAULT_MIN_SAMPLE_TIME (80 * USEC_PER_MSEC)
+static unsigned long min_sample_time = DEFAULT_MIN_SAMPLE_TIME;
+
+/*
+ * The sample rate of the timer used to increase frequency
+ */
+#define DEFAULT_TIMER_RATE (20 * USEC_PER_MSEC)
+static unsigned long timer_rate = DEFAULT_TIMER_RATE;
+
+/*
+ * Wait this long before raising speed above hispeed, by default a single
+ * timer interval.
+ */
+#define DEFAULT_ABOVE_HISPEED_DELAY DEFAULT_TIMER_RATE
+static unsigned int default_above_hispeed_delay[] = {
+	DEFAULT_ABOVE_HISPEED_DELAY };
+static spinlock_t above_hispeed_delay_lock;
+static unsigned int *above_hispeed_delay = default_above_hispeed_delay;
+static int nabove_hispeed_delay = ARRAY_SIZE(default_above_hispeed_delay);
+
+/* 1000000us - 1s */
+#define DEFAULT_BOOSTPULSE_DURATION 1000000
+static int boostpulse_duration_val = DEFAULT_BOOSTPULSE_DURATION;
+#define DEFAULT_INPUT_BOOST_FREQ 1036800
+unsigned int hells_input_boost_freq = DEFAULT_INPUT_BOOST_FREQ;
+
+/*
+ * Making sure cpufreq stays low when it needs to stay low
+ */
+#define DOWN_LOW_LOAD_THRESHOLD 5
+
+/*
+ * Default thread migration boost cpufreq
+ */
+#define CPU_SYNC_FREQ 1036800
+
+/*
+ * Max additional time to wait in idle, beyond timer_rate, at speeds above
+ * minimum before wakeup to reduce speed, or -1 if unnecessary.
+ */
+#define DEFAULT_TIMER_SLACK (4 * DEFAULT_TIMER_RATE)
+static int timer_slack_val = DEFAULT_TIMER_SLACK;
+
+/*
+ * Whether to align timer windows across all CPUs. When
+ * use_sched_load is true, this flag is ignored and windows
+ * will always be aligned.
+ */
+static bool align_windows = true;
+
+/* Improves frequency selection for more energy */
+static bool closest_freq_selection;
+
+/*
+ * Stay at max freq for at least max_freq_hysteresis before dropping
+ * frequency.
+ */
+static unsigned int max_freq_hysteresis;
+
+static bool io_is_busy = 1;
+
+static bool use_freq_calc_thresh = true;
+
+static int two_phase_freq_array[NR_CPUS] = {[0 ... NR_CPUS-1] = 1728000} ;
+
+/* Round to starting jiffy of next evaluation window */
+static u64 round_to_nw_start(u64 jif)
+{
+	unsigned long step = usecs_to_jiffies(timer_rate);
+	u64 ret;
+
+	if (align_windows) {
+		do_div(jif, step);
+		ret = (jif + 1) * step;
+	} else {
+		ret = jiffies + usecs_to_jiffies(timer_rate);
+	}
+
+	return ret;
+}
+
+static void cpufreq_interactive_timer_resched(unsigned long cpu)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 expires;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->time_in_idle =
+		get_cpu_idle_time(smp_processor_id(),
+				     &pcpu->time_in_idle_timestamp, io_is_busy);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	expires = round_to_nw_start(pcpu->last_evaluated_jiffy);
+	del_timer(&pcpu->cpu_timer);
+	pcpu->cpu_timer.expires = expires;
+	add_timer_on(&pcpu->cpu_timer, cpu);
+
+	if (timer_slack_val >= 0 && pcpu->target_freq > pcpu->policy->min) {
+		expires += usecs_to_jiffies(timer_slack_val);
+		del_timer(&pcpu->cpu_slack_timer);
+		pcpu->cpu_slack_timer.expires = expires;
+		add_timer_on(&pcpu->cpu_slack_timer, cpu);
+	}
+
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+/* The caller shall take enable_sem write semaphore to avoid any timer race.
+ * The cpu_timer and cpu_slack_timer must be deactivated when calling this
+ * function.
+ */
+static void cpufreq_interactive_timer_start(int cpu)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 expires = round_to_nw_start(pcpu->last_evaluated_jiffy);
+	unsigned long flags;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->cpu_timer.expires = expires;
+	if (cpu_online(cpu)) {
+		add_timer_on(&pcpu->cpu_timer, cpu);
+		if (timer_slack_val >= 0 && pcpu->target_freq >
+		     pcpu->policy->min) {
+			expires += usecs_to_jiffies(timer_slack_val);
+			pcpu->cpu_slack_timer.expires = expires;
+			add_timer_on(&pcpu->cpu_slack_timer, cpu);
+		}
+	}
+
+	pcpu->time_in_idle =
+		get_cpu_idle_time(cpu, &pcpu->time_in_idle_timestamp,
+				  io_is_busy);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+static unsigned int freq_to_above_hispeed_delay(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay - 1 &&
+			freq >= above_hispeed_delay[i+1]; i += 2)
+		;
+
+	ret = above_hispeed_delay[i];
+
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static unsigned int freq_to_targetload(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+
+	for (i = 0; i < ntarget_loads - 1 && freq >= target_loads[i+1]; i += 2)
+		;
+
+	ret = target_loads[i];
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+/*
+ * If increasing frequencies never map to a lower target load then
+ * choose_freq() will find the minimum frequency that does not exceed its
+ * target load given the current load.
+ */
+
+static unsigned int choose_freq(
+	struct cpufreq_interactive_cpuinfo *pcpu, unsigned int loadadjfreq)
+{
+	unsigned int freq = pcpu->policy->cur;
+	unsigned int prevfreq, freqmin, freqmax;
+	unsigned int tl;
+	int index;
+
+	freqmin = 0;
+	freqmax = UINT_MAX;
+
+	do {
+		prevfreq = freq;
+		tl = freq_to_targetload(freq);
+
+		/*
+		 * Find the lowest frequency where the computed load is less
+		 * than or equal to the target load.
+		 */
+
+		if (cpufreq_frequency_table_target(
+			    pcpu->policy, pcpu->freq_table, loadadjfreq / tl,
+			    CPUFREQ_RELATION_L, &index))
+			break;
+		freq = pcpu->freq_table[index].frequency;
+
+		if (freq > prevfreq) {
+			/* The previous frequency is too low. */
+			freqmin = prevfreq;
+
+			if (freq >= freqmax) {
+				/*
+				 * Find the highest frequency that is less
+				 * than freqmax.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmax - 1, CPUFREQ_RELATION_H,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				if (freq == freqmin) {
+					/*
+					 * The first frequency below freqmax
+					 * has already been found to be too
+					 * low.  freqmax is the lowest speed
+					 * we found that is fast enough.
+					 */
+					freq = freqmax;
+					break;
+				}
+			}
+		} else if (freq < prevfreq) {
+			/* The previous frequency is high enough. */
+			freqmax = prevfreq;
+
+			if (freq <= freqmin) {
+				/*
+				 * Find the lowest frequency that is higher
+				 * than freqmin.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmin + 1, CPUFREQ_RELATION_L,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				/*
+				 * If freqmax is the first frequency above
+				 * freqmin then we have already found that
+				 * this speed is fast enough.
+				 */
+				if (freq == freqmax)
+					break;
+			}
+		}
+
+		/* If same frequency chosen as previous then done. */
+	} while (freq != prevfreq);
+
+	return freq;
+}
+
+static u64 update_load(int cpu)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 now;
+	u64 now_idle;
+	unsigned int delta_idle;
+	unsigned int delta_time;
+	u64 active_time;
+
+	now_idle = get_cpu_idle_time(cpu, &now, io_is_busy);
+	delta_idle = (unsigned int)(now_idle - pcpu->time_in_idle);
+	delta_time = (unsigned int)(now - pcpu->time_in_idle_timestamp);
+
+	if (delta_time <= delta_idle)
+		active_time = 0;
+	else
+		active_time = delta_time - delta_idle;
+
+	pcpu->cputime_speedadj += active_time * pcpu->policy->cur;
+
+	pcpu->time_in_idle = now_idle;
+	pcpu->time_in_idle_timestamp = now;
+	return now;
+}
+
+static void cpufreq_interactive_timer(unsigned long data)
+{
+	u64 now;
+	unsigned int delta_time;
+	u64 cputime_speedadj;
+	int cpu_load;
+	struct cpufreq_interactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, data);
+	unsigned int new_freq;
+	unsigned int loadadjfreq;
+	unsigned int index;
+	unsigned long flags;
+	static unsigned int phase = 0;
+	static unsigned int counter = 0;
+	unsigned int nr_cpus;
+	bool boosted;
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled)
+		goto exit;
+
+	if (cpu_is_offline(data))
+		goto exit;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	now = update_load(data);
+	delta_time = (unsigned int)(now - pcpu->cputime_speedadj_timestamp);
+	cputime_speedadj = pcpu->cputime_speedadj;
+	pcpu->last_evaluated_jiffy = get_jiffies_64();
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+
+	if (WARN_ON_ONCE(!delta_time))
+		goto rearm;
+
+	spin_lock_irqsave(&pcpu->target_freq_lock, flags);
+	do_div(cputime_speedadj, delta_time);
+	loadadjfreq = (unsigned int)cputime_speedadj * 100;
+	cpu_load = loadadjfreq / pcpu->policy->cur;
+	boosted = now < (get_input_time() + boostpulse_duration_val);
+
+	if (counter < 5) {
+		counter++;
+		if (counter > 2) {
+			phase = 1;
+		}
+	}
+
+	cpufreq_notify_utilization(pcpu->policy, cpu_load);
+
+	if (cpu_load >= go_hispeed_load) {
+		if (pcpu->policy->cur < hispeed_freq) {
+			nr_cpus = num_online_cpus();
+
+			pcpu->two_phase_freq = two_phase_freq_array[nr_cpus-1];
+			if (pcpu->two_phase_freq < pcpu->policy->cur)
+				phase = 1;
+			if (pcpu->two_phase_freq != 0 && phase == 0) {
+				new_freq = pcpu->two_phase_freq;
+			} else
+				new_freq = hispeed_freq;
+		} else {
+			if (use_freq_calc_thresh && new_freq > freq_calc_thresh)
+				new_freq = pcpu->policy->max * cpu_load / 100;
+			else
+				new_freq = choose_freq(pcpu, loadadjfreq);
+
+			if (new_freq < hispeed_freq)
+				new_freq = hispeed_freq;
+		}
+
+	} else if (cpu_load <= DOWN_LOW_LOAD_THRESHOLD) {
+		new_freq = pcpu->policy->min;
+	} else {
+
+	if (use_freq_calc_thresh && new_freq > freq_calc_thresh)
+				new_freq = pcpu->policy->max * cpu_load / 100;
+			else
+				new_freq = choose_freq(pcpu, loadadjfreq);
+	}
+
+	if (boosted) {
+		if (new_freq < hells_input_boost_freq)
+			new_freq = hells_input_boost_freq;
+ 	}
+
+	if (counter > 0) {
+		counter--;
+		if (counter == 0) {
+			phase = 0;
+		}
+	}
+
+	if (pcpu->policy->cur >= hispeed_freq &&
+	    new_freq > pcpu->policy->cur &&
+	    now - pcpu->hispeed_validate_time <
+	    freq_to_above_hispeed_delay(pcpu->policy->cur)) {
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	pcpu->local_hvtime = now;
+
+	if (!closest_freq_selection && cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table,
+					   new_freq, CPUFREQ_RELATION_H,
+					   &index)) {
+
+	} else if (cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table,
+					   new_freq, CPUFREQ_RELATION_C,
+					   &index)) {
+	
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	new_freq = pcpu->freq_table[index].frequency;
+
+	if (pcpu->target_freq >= pcpu->policy->max
+	    && new_freq < pcpu->target_freq
+	    && now - pcpu->max_freq_idle_start_time < max_freq_hysteresis) {
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	/*
+	 * Do not scale below floor_freq unless we have been at or above the
+	 * floor frequency for the minimum sample time since last validated.
+	 */
+	if (new_freq < pcpu->floor_freq) {
+		if (now - pcpu->floor_validate_time < min_sample_time) {
+			spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+			goto rearm;
+		}
+	}
+
+	/*
+	 * Update the timestamp for checking whether speed has been held at
+	 * or above the selected frequency for a minimum of min_sample_time,
+	 * if not boosted to hispeed_freq.  If boosted to hispeed_freq then we
+	 * allow the speed to drop as soon as the boostpulse duration expires
+	 * (or the indefinite boost is turned off).
+	 */
+
+	if (!boosted || new_freq > hispeed_freq) {
+		pcpu->floor_freq = new_freq;
+		pcpu->floor_validate_time = now;
+	}
+
+	/* In case actual freq set in target(policy->cur) is not updated
+	 * till next timer interrupt arrives, new_freq remains same as
+	 * actual freq. Don't go for setting same frequency again.
+	 */
+	if (pcpu->target_freq == new_freq
+		&& pcpu->policy->cur == new_freq) {
+	spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm_if_notmax;
+	}
+
+	pcpu->target_freq = new_freq;
+	spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+	cpumask_set_cpu(data, &speedchange_cpumask);
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+	wake_up_process(speedchange_task);
+
+rearm_if_notmax:
+	/*
+	 * Already set max speed and don't see a need to change that,
+	 * wait until next idle to re-evaluate, don't need timer.
+	 */
+	if (pcpu->target_freq == pcpu->policy->max)
+		goto exit;
+
+rearm:
+	if (!timer_pending(&pcpu->cpu_timer))
+		cpufreq_interactive_timer_resched(data);
+
+exit:
+	up_read(&pcpu->enable_sem);
+	return;
+}
+
+static void cpufreq_interactive_idle_start(void)
+{
+	int cpu = smp_processor_id();
+	struct cpufreq_interactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+	int pending;
+	unsigned long flags;
+	u64 now;
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled)
+		goto exit;
+
+	/* Cancel the timer if cpu is offline */
+	if (cpu_is_offline(cpu)) {
+		del_timer(&pcpu->cpu_timer);
+		del_timer(&pcpu->cpu_slack_timer);
+		goto exit;
+	}
+
+	pending = timer_pending(&pcpu->cpu_timer);
+
+	if (pcpu->target_freq != pcpu->policy->min) {
+		/*
+		 * Entering idle while not at lowest speed.  On some
+		 * platforms this can hold the other CPU(s) at that speed
+		 * even though the CPU is idle. Set a timer to re-evaluate
+		 * speed so this idle CPU doesn't hold the other CPUs above
+		 * min indefinitely.  This should probably be a quirk of
+		 * the CPUFreq driver.
+		 */
+		if (!pending) {
+			pcpu->last_evaluated_jiffy = get_jiffies_64();
+			cpufreq_interactive_timer_resched(smp_processor_id());
+
+			/*
+ 			 * If timer is cancelled because CPU is running at
+ 			 * policy->max, record the time CPU first goes to
+ 			 * idle.
+ 			 */
+			now = ktime_to_us(ktime_get());
+			if (max_freq_hysteresis) {
+				spin_lock_irqsave(&pcpu->target_freq_lock,
+						  flags);
+				pcpu->max_freq_idle_start_time = now;
+				spin_unlock_irqrestore(&pcpu->target_freq_lock,
+						       flags);
+			}
+		}
+	}
+exit:
+	up_read(&pcpu->enable_sem);
+}
+
+static void cpufreq_interactive_idle_end(void)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled) {
+		up_read(&pcpu->enable_sem);
+		return;
+	}
+
+	/* Arm the timer for 1-2 ticks later if not already. */
+	if (!timer_pending(&pcpu->cpu_timer)) {
+		cpufreq_interactive_timer_resched(smp_processor_id());
+	} else if (time_after_eq(jiffies, pcpu->cpu_timer.expires)) {
+		del_timer(&pcpu->cpu_timer);
+		del_timer(&pcpu->cpu_slack_timer);
+		cpufreq_interactive_timer(smp_processor_id());
+	}
+
+	up_read(&pcpu->enable_sem);
+}
+
+static int cpufreq_interactive_speedchange_task(void *data)
+{
+	unsigned int cpu;
+	cpumask_t tmp_mask;
+	unsigned long flags;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+		if (cpumask_empty(&speedchange_cpumask)) {
+			spin_unlock_irqrestore(&speedchange_cpumask_lock,
+					       flags);
+			schedule();
+
+			if (kthread_should_stop())
+				break;
+
+			spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+		}
+
+		set_current_state(TASK_RUNNING);
+		tmp_mask = speedchange_cpumask;
+		cpumask_clear(&speedchange_cpumask);
+		spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+		for_each_cpu(cpu, &tmp_mask) {
+			unsigned int j;
+			unsigned int max_freq = 0;
+			struct cpufreq_interactive_cpuinfo *pjcpu;
+			u64 hvt;
+
+			pcpu = &per_cpu(cpuinfo, cpu);
+			if (!down_read_trylock(&pcpu->enable_sem))
+				continue;
+			if (!pcpu->governor_enabled) {
+				up_read(&pcpu->enable_sem);
+				continue;
+			}
+
+			for_each_cpu(j, pcpu->policy->cpus) {
+				pjcpu = &per_cpu(cpuinfo, j);
+
+				if (pjcpu->target_freq > max_freq) {
+					max_freq = pjcpu->target_freq;
+					hvt = pjcpu->local_hvtime;
+				} else if (pjcpu->target_freq == max_freq) {
+					hvt = min(hvt, pjcpu->local_hvtime);
+				}
+			}
+
+			if (max_freq != pcpu->policy->cur) {
+				if (!closest_freq_selection)
+					__cpufreq_driver_target(pcpu->policy,
+								max_freq,
+								CPUFREQ_RELATION_H);
+				else
+					__cpufreq_driver_target(pcpu->policy,
+								max_freq,
+								CPUFREQ_RELATION_C);
+
+				for_each_cpu(j, pcpu->policy->cpus) {
+					pjcpu = &per_cpu(cpuinfo, j);
+					pjcpu->hispeed_validate_time = hvt;
+				}
+			}
+
+			up_read(&pcpu->enable_sem);
+		}
+	}
+
+	return 0;
+}
+
+static int cpufreq_interactive_notifier(
+	struct notifier_block *nb, unsigned long val, void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+	int cpu;
+	unsigned long flags;
+
+	if (val == CPUFREQ_POSTCHANGE) {
+		pcpu = &per_cpu(cpuinfo, freq->cpu);
+		if (!down_read_trylock(&pcpu->enable_sem))
+			return 0;
+		if (!pcpu->governor_enabled) {
+			up_read(&pcpu->enable_sem);
+			return 0;
+		}
+
+		for_each_cpu(cpu, pcpu->policy->cpus) {
+			struct cpufreq_interactive_cpuinfo *pjcpu =
+				&per_cpu(cpuinfo, cpu);
+			if (cpu != freq->cpu) {
+				if (!down_read_trylock(&pjcpu->enable_sem))
+					continue;
+				if (!pjcpu->governor_enabled) {
+					up_read(&pjcpu->enable_sem);
+					continue;
+				}
+			}
+			spin_lock_irqsave(&pjcpu->load_lock, flags);
+			update_load(cpu);
+			spin_unlock_irqrestore(&pjcpu->load_lock, flags);
+			if (cpu != freq->cpu)
+				up_read(&pjcpu->enable_sem);
+		}
+
+		up_read(&pcpu->enable_sem);
+	}
+	return 0;
+}
+
+static struct notifier_block cpufreq_notifier_block = {
+	.notifier_call = cpufreq_interactive_notifier,
+};
+
+static unsigned int *get_tokenized_data(const char *buf, int *num_tokens)
+{
+	const char *cp;
+	int i;
+	int ntokens = 1;
+	unsigned int *tokenized_data;
+	int err = -EINVAL;
+
+	cp = buf;
+	while ((cp = strpbrk(cp + 1, " :")))
+		ntokens++;
+
+	if (!(ntokens & 0x1))
+		goto err;
+
+	tokenized_data = kmalloc(ntokens * sizeof(unsigned int), GFP_KERNEL);
+	if (!tokenized_data) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	cp = buf;
+	i = 0;
+	while (i < ntokens) {
+		if (sscanf(cp, "%u", &tokenized_data[i++]) != 1)
+			goto err_kfree;
+
+		cp = strpbrk(cp, " :");
+		if (!cp)
+			break;
+		cp++;
+	}
+
+	if (i != ntokens)
+		goto err_kfree;
+
+	*num_tokens = ntokens;
+	return tokenized_data;
+
+err_kfree:
+	kfree(tokenized_data);
+err:
+	return ERR_PTR(err);
+}
+
+static int thread_migration_notify(struct notifier_block *nb,
+						unsigned long target_cpu, void *arg)
+{
+	unsigned long flags;
+	unsigned int boost_freq;
+	unsigned int sync_freq = CPU_SYNC_FREQ;
+	struct cpufreq_interactive_cpuinfo *target, *source;
+	target = &per_cpu(cpuinfo, target_cpu);
+	source = &per_cpu(cpuinfo, (int)arg);
+
+	/*
+	* If there's a thread migration in the same core we don't want to
+	* boost it
+	*/
+	if ((int)arg == target_cpu)
+		return NOTIFY_OK;
+
+	if (source->policy->util <= 10)
+		return NOTIFY_OK;
+
+	if (source->policy->cur > target->policy->cur)
+	{
+		boost_freq = max(sync_freq, source->policy->cur);
+
+		target->target_freq = boost_freq;
+
+		spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+		cpumask_set_cpu(target_cpu, &speedchange_cpumask);
+		spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+		wake_up_process(speedchange_task);
+	}
+
+	return NOTIFY_OK;
+}
+
+static struct notifier_block thread_migration_nb = {
+	.notifier_call = thread_migration_notify,
+};
+
+static ssize_t show_two_phase_freq
+(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i = 0 ;
+	int shift = 0 ;
+	char *buf_pos = buf;
+	for ( i = 0 ; i < NR_CPUS; i++) {
+		shift = sprintf(buf_pos,"%d,",two_phase_freq_array[i]);
+		buf_pos += shift;
+	}
+	*(buf_pos-1) = '\0';
+	return strlen(buf);
+}
+
+static ssize_t store_two_phase_freq(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count)
+{
+
+	int ret = 0;
+	if (NR_CPUS == 1)
+		ret = sscanf(buf,"%u",&two_phase_freq_array[0]);
+	else if (NR_CPUS == 2)
+		ret = sscanf(buf,"%u,%u",&two_phase_freq_array[0],
+				&two_phase_freq_array[1]);
+	else if (NR_CPUS == 4)
+		ret = sscanf(buf, "%u,%u,%u,%u", &two_phase_freq_array[0],
+				&two_phase_freq_array[1],
+				&two_phase_freq_array[2],
+				&two_phase_freq_array[3]);
+	if (ret < NR_CPUS)
+		return -EINVAL;
+
+	return count;
+}
+
+static struct global_attr two_phase_freq_attr =
+	__ATTR(two_phase_freq, S_IRUGO | S_IWUSR,
+		show_two_phase_freq, store_two_phase_freq);
+
+static ssize_t show_target_loads(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+
+	for (i = 0; i < ntarget_loads; i++)
+		ret += sprintf(buf + ret, "%u%s", target_loads[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+static ssize_t store_target_loads(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_target_loads = NULL;
+	unsigned long flags;
+
+	new_target_loads = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_target_loads))
+		return PTR_RET(new_target_loads);
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+	if (target_loads != default_target_loads)
+		kfree(target_loads);
+	target_loads = new_target_loads;
+	ntarget_loads = ntokens;
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return count;
+}
+
+static struct global_attr target_loads_attr =
+	__ATTR(target_loads, S_IRUGO | S_IWUSR,
+		show_target_loads, store_target_loads);
+
+static ssize_t show_freq_calc_thresh(struct kobject *kobj,
+				struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", freq_calc_thresh);
+}
+
+static ssize_t store_freq_calc_thresh(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+	return ret;
+	freq_calc_thresh = val;
+	return count;
+}
+
+static struct global_attr freq_calc_thresh_attr = __ATTR(freq_calc_thresh, 0644,
+		show_freq_calc_thresh, store_freq_calc_thresh);
+
+static ssize_t show_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay; i++)
+		ret += sprintf(buf + ret, "%u%s", above_hispeed_delay[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static ssize_t store_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_above_hispeed_delay = NULL;
+	unsigned long flags;
+
+	new_above_hispeed_delay = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_above_hispeed_delay))
+		return PTR_RET(new_above_hispeed_delay);
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+	if (above_hispeed_delay != default_above_hispeed_delay)
+		kfree(above_hispeed_delay);
+	above_hispeed_delay = new_above_hispeed_delay;
+	nabove_hispeed_delay = ntokens;
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return count;
+
+}
+
+static struct global_attr above_hispeed_delay_attr =
+	__ATTR(above_hispeed_delay, S_IRUGO | S_IWUSR,
+		show_above_hispeed_delay, store_above_hispeed_delay);
+
+static ssize_t show_hispeed_freq(struct kobject *kobj,
+				 struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", hispeed_freq);
+}
+
+static ssize_t store_hispeed_freq(struct kobject *kobj,
+				  struct attribute *attr, const char *buf,
+				  size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	hispeed_freq = val;
+	return count;
+}
+
+static struct global_attr hispeed_freq_attr = __ATTR(hispeed_freq, 0644,
+		show_hispeed_freq, store_hispeed_freq);
+
+static ssize_t show_max_freq_hysteresis(struct kobject *kobj,
+					 struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", max_freq_hysteresis);
+}
+
+static ssize_t store_max_freq_hysteresis(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	max_freq_hysteresis = val;
+	return count;
+}
+
+static struct global_attr max_freq_hysteresis_attr =
+	__ATTR(max_freq_hysteresis, 0644, show_max_freq_hysteresis,
+		store_max_freq_hysteresis);
+
+static ssize_t show_align_windows(struct kobject *kobj,
+					struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", align_windows);
+}
+
+static ssize_t store_align_windows(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	align_windows = val;
+	return count;
+}
+
+static struct global_attr align_windows_attr = __ATTR(align_windows, 0644,
+		show_align_windows, store_align_windows);
+
+static ssize_t show_go_hispeed_load(struct kobject *kobj,
+				     struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", go_hispeed_load);
+}
+
+static ssize_t store_go_hispeed_load(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	go_hispeed_load = val;
+	return count;
+}
+
+static struct global_attr go_hispeed_load_attr = __ATTR(go_hispeed_load, 0644,
+		show_go_hispeed_load, store_go_hispeed_load);
+
+static ssize_t show_min_sample_time(struct kobject *kobj,
+				struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", min_sample_time);
+}
+
+static ssize_t store_min_sample_time(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	min_sample_time = val;
+	return count;
+}
+
+static struct global_attr min_sample_time_attr = __ATTR(min_sample_time, 0644,
+		show_min_sample_time, store_min_sample_time);
+
+static ssize_t show_timer_rate(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", timer_rate);
+}
+
+static ssize_t store_timer_rate(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val, val_round;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	val_round = jiffies_to_usecs(usecs_to_jiffies(val));
+	if (val != val_round)
+		pr_warn("timer_rate not aligned to jiffy. Rounded up to %lu\n",
+				val_round);
+
+timer_rate = val_round;
+	return count;
+}
+
+static struct global_attr timer_rate_attr = __ATTR(timer_rate, 0644,
+		show_timer_rate, store_timer_rate);
+
+static ssize_t show_hells_input_boost_freq(struct kobject *kobj, struct attribute *attr,
+                                     char *buf)
+{
+	return sprintf(buf, "%d\n", hells_input_boost_freq);
+}
+
+static ssize_t store_hells_input_boost_freq(struct kobject *kobj, struct attribute *attr,
+                                      const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	hells_input_boost_freq = val;
+	return count;
+
+}
+
+static struct global_attr hells_input_boost_freq_attr = __ATTR(hells_input_boost_freq, 0644,
+		show_hells_input_boost_freq, store_hells_input_boost_freq);
+
+static ssize_t show_timer_slack(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", timer_slack_val);
+}
+
+static ssize_t store_timer_slack(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret < 0)
+		return ret;
+
+	timer_slack_val = val;
+	return count;
+}
+
+define_one_global_rw(timer_slack);
+
+static ssize_t show_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", boostpulse_duration_val);
+}
+
+static ssize_t store_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_duration_val = val;
+	return count;
+}
+
+define_one_global_rw(boostpulse_duration);
+
+static ssize_t show_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	io_is_busy = val;
+	return count;
+}
+
+static struct global_attr io_is_busy_attr = __ATTR(io_is_busy, 0644,
+		show_io_is_busy, store_io_is_busy);
+
+static ssize_t show_use_freq_calc_thresh(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", use_freq_calc_thresh);
+}
+
+static ssize_t store_use_freq_calc_thresh(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	use_freq_calc_thresh = val;
+	return count;
+}
+
+static struct global_attr use_freq_calc_thresh_attr = __ATTR(use_freq_calc_thresh, 0644,
+		show_use_freq_calc_thresh, store_use_freq_calc_thresh);
+
+static ssize_t show_closest_freq_selection(struct kobject *kobj,
+				     struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", closest_freq_selection);
+}
+
+static ssize_t store_closest_freq_selection(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	closest_freq_selection = val;
+	return count;
+}
+
+static struct global_attr closest_freq_selection_attr = __ATTR(closest_freq_selection, 0644,
+		show_closest_freq_selection, store_closest_freq_selection);
+
+static struct attribute *interactive_attributes[] = {
+	&target_loads_attr.attr,
+	&freq_calc_thresh_attr.attr,
+	&above_hispeed_delay_attr.attr,
+	&hispeed_freq_attr.attr,
+	&go_hispeed_load_attr.attr,
+	&min_sample_time_attr.attr,
+	&timer_rate_attr.attr,
+	&hells_input_boost_freq_attr.attr,
+	&timer_slack.attr,
+	&io_is_busy_attr.attr,
+	&boostpulse_duration.attr,
+	&max_freq_hysteresis_attr.attr,
+	&two_phase_freq_attr.attr,
+	&align_windows_attr.attr,
+	&use_freq_calc_thresh_attr.attr,
+	&closest_freq_selection_attr.attr,
+	NULL,
+};
+
+static struct attribute_group interactive_attr_group = {
+	.attrs = interactive_attributes,
+	.name = "hellsactive",
+};
+
+static int cpufreq_interactive_idle_notifier(struct notifier_block *nb,
+					     unsigned long val,
+					     void *data)
+{
+	switch (val) {
+	case IDLE_START:
+		cpufreq_interactive_idle_start();
+		break;
+	case IDLE_END:
+		cpufreq_interactive_idle_end();
+		break;
+	}
+
+	return 0;
+}
+
+static struct notifier_block cpufreq_interactive_idle_nb = {
+	.notifier_call = cpufreq_interactive_idle_notifier,
+};
+
+static int cpufreq_governor_hellsactive(struct cpufreq_policy *policy,
+		unsigned int event)
+{
+	int rc;
+	unsigned int j;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned long flags;
+	unsigned int anyboost;
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if (!cpu_online(policy->cpu))
+			return -EINVAL;
+
+		mutex_lock(&gov_lock);
+
+		freq_table =
+			cpufreq_frequency_get_table(policy->cpu);
+		if (!hispeed_freq)
+			hispeed_freq = policy->max;
+		freq_calc_thresh = policy->min;
+
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			pcpu->policy = policy;
+			pcpu->target_freq = policy->cur;
+			pcpu->freq_table = freq_table;
+			pcpu->floor_freq = pcpu->target_freq;
+			pcpu->floor_validate_time =
+				ktime_to_us(ktime_get());
+			pcpu->hispeed_validate_time =
+				pcpu->floor_validate_time;
+			pcpu->local_hvtime = pcpu->floor_validate_time;
+			pcpu->max_freq = policy->max;
+			down_write(&pcpu->enable_sem);
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			pcpu->last_evaluated_jiffy = get_jiffies_64();
+			cpufreq_interactive_timer_start(j);
+			pcpu->governor_enabled = 1;
+			up_write(&pcpu->enable_sem);
+		}
+
+		/*
+		 * Do not register the idle hook and create sysfs
+		 * entries if we have already done so.
+		 */
+		if (++active_count > 1) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		rc = sysfs_create_group(cpufreq_global_kobject,
+				&interactive_attr_group);
+		if (rc) {
+			mutex_unlock(&gov_lock);
+			return rc;
+		}
+		
+		atomic_notifier_chain_register(&migration_notifier_head,
+					&thread_migration_nb);
+		
+		idle_notifier_register(&cpufreq_interactive_idle_nb);
+		cpufreq_register_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		mutex_unlock(&gov_lock);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		mutex_lock(&gov_lock);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			down_write(&pcpu->enable_sem);
+			pcpu->governor_enabled = 0;
+			pcpu->target_freq = 0;
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			up_write(&pcpu->enable_sem);
+		}
+
+		if (--active_count > 0) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		atomic_notifier_chain_unregister(
+				&migration_notifier_head,
+				&thread_migration_nb);
+
+		cpufreq_unregister_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		idle_notifier_unregister(&cpufreq_interactive_idle_nb);
+		sysfs_remove_group(cpufreq_global_kobject,
+				&interactive_attr_group);
+		mutex_unlock(&gov_lock);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		if (policy->max < policy->cur)
+			__cpufreq_driver_target(policy,
+					policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > policy->cur)
+			__cpufreq_driver_target(policy,
+					policy->min, CPUFREQ_RELATION_L);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+
+			down_read(&pcpu->enable_sem);
+			if (pcpu->governor_enabled == 0) {
+				up_read(&pcpu->enable_sem);
+				continue;
+			}
+
+			spin_lock_irqsave(&pcpu->target_freq_lock, flags);
+			if (policy->max < pcpu->target_freq) {
+				pcpu->target_freq = policy->max;
+			} else if (policy->min >= pcpu->target_freq) {
+				pcpu->target_freq = policy->min;
+				anyboost = 1;
+			}
+
+			spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+			up_read(&pcpu->enable_sem);
+
+			/* Reschedule timer only if policy->max is raised.
+			 * Delete the timers, else the timer callback may
+			 * return without re-arm the timer when failed
+			 * acquire the semaphore. This race may cause timer
+			 * stopped unexpectedly.
+			 */
+			if (policy->max > pcpu->max_freq) {
+				down_write(&pcpu->enable_sem);
+				del_timer_sync(&pcpu->cpu_timer);
+				del_timer_sync(&pcpu->cpu_slack_timer);
+				cpufreq_interactive_timer_start(j);
+				up_write(&pcpu->enable_sem);
+			} else if (anyboost) {
+				u64 now = ktime_to_us(ktime_get());
+
+				cpumask_set_cpu(j, &speedchange_cpumask);
+				pcpu->hispeed_validate_time = now;
+				pcpu->floor_freq = policy->min;
+				pcpu->floor_validate_time = now;
+			}
+
+			pcpu->max_freq = policy->max;
+		}
+		if (anyboost)
+			wake_up_process(speedchange_task);
+
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_HELLSACTIVE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_hellsactive = {
+	.name = "hellsactive",
+	.governor = cpufreq_governor_hellsactive,
+	.max_transition_latency = 10000000,
+	.owner = THIS_MODULE,
+};
+
+static void cpufreq_interactive_nop_timer(unsigned long data)
+{
+}
+
+static int __init cpufreq_hellsactive_init(void)
+{
+	unsigned int i;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+
+	/* Initalize per-cpu timers */
+	for_each_possible_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+		init_timer_deferrable(&pcpu->cpu_timer);
+		pcpu->cpu_timer.function = cpufreq_interactive_timer;
+		pcpu->cpu_timer.data = i;
+		init_timer(&pcpu->cpu_slack_timer);
+		pcpu->cpu_slack_timer.function = cpufreq_interactive_nop_timer;
+		spin_lock_init(&pcpu->load_lock);
+		spin_lock_init(&pcpu->target_freq_lock);
+		init_rwsem(&pcpu->enable_sem);
+	}
+
+	spin_lock_init(&target_loads_lock);
+	spin_lock_init(&speedchange_cpumask_lock);
+	spin_lock_init(&above_hispeed_delay_lock);
+	mutex_init(&gov_lock);
+	speedchange_task =
+		kthread_create(cpufreq_interactive_speedchange_task, NULL,
+			       "cfhellsactive");
+	if (IS_ERR(speedchange_task))
+		return PTR_ERR(speedchange_task);
+
+	sched_setscheduler_nocheck(speedchange_task, SCHED_FIFO, &param);
+	get_task_struct(speedchange_task);
+
+	/* NB: wake up so the thread does not look hung to the freezer */
+	wake_up_process(speedchange_task);
+
+	return cpufreq_register_governor(&cpufreq_gov_hellsactive);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_HELLSACTIVE
+fs_initcall(cpufreq_hellsactive_init);
+#else
+module_init(cpufreq_hellsactive_init);
+#endif
+
+static void __exit cpufreq_interactive_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_hellsactive);
+	kthread_stop(speedchange_task);
+	put_task_struct(speedchange_task);
+}
+
+module_exit(cpufreq_interactive_exit);
+
+MODULE_AUTHOR("Mike Chan <mike@android.com>");
+MODULE_AUTHOR("Paul Reioux <reioux@gmail.com>");
+MODULE_AUTHOR("Laurent Hess <hellsgod@gmx.ch>");
+MODULE_DESCRIPTION("'cpufreq_hellsactive' - A cpufreq governor for "
+	"Latency sensitive workloads based on Google, faux, CAF and some patches from franciscofranco");
+MODULE_LICENSE("GPL");
diff --git a/drivers/cpufreq/cpufreq_impulse.c b/drivers/cpufreq/cpufreq_impulse.c
new file mode 100644
index 0000000..cff8540
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_impulse.c
@@ -0,0 +1,1653 @@
+/*
+ * Impulse - Load Sensitive CPU Frequency Governor
+ *
+ * Copyright (c) 2014-2016, Pranav Vashi <neobuddy89@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/rwsem.h>
+#include <linux/sched.h>
+#include <linux/sched/rt.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <asm/cputime.h>
+#ifdef CONFIG_STATE_NOTIFIER
+#include <linux/state_notifier.h>
+#endif
+
+struct cpufreq_impulse_policyinfo {
+	struct timer_list policy_timer;
+	struct timer_list policy_slack_timer;
+	spinlock_t load_lock; /* protects load tracking stat */
+	u64 last_evaluated_jiffy;
+	struct cpufreq_policy *policy;
+	struct cpufreq_frequency_table *freq_table;
+	spinlock_t target_freq_lock; /*protects target freq */
+	unsigned int target_freq;
+	unsigned int floor_freq;
+	unsigned int min_freq;
+	u64 floor_validate_time;
+	u64 hispeed_validate_time;
+	u64 max_freq_hyst_start_time;
+	struct rw_semaphore enable_sem;
+	bool reject_notification;
+	int governor_enabled;
+	struct cpufreq_impulse_tunables *cached_tunables;
+	unsigned long *cpu_busy_times;
+};
+
+/* Protected by per-policy load_lock */
+struct cpufreq_impulse_cpuinfo {
+	u64 time_in_idle;
+	u64 time_in_idle_timestamp;
+	u64 cputime_speedadj;
+	u64 cputime_speedadj_timestamp;
+	unsigned int loadadjfreq;
+};
+
+static DEFINE_PER_CPU(struct cpufreq_impulse_policyinfo *, polinfo);
+static DEFINE_PER_CPU(struct cpufreq_impulse_cpuinfo, cpuinfo);
+
+/* realtime thread handles frequency scaling */
+static struct task_struct *speedchange_task;
+static cpumask_t speedchange_cpumask;
+static spinlock_t speedchange_cpumask_lock;
+static struct mutex gov_lock;
+
+static int set_window_count;
+static int migration_register_count;
+static struct mutex sched_lock;
+
+/* Target load.  Lower values result in higher CPU speeds. */
+#define DEFAULT_TARGET_LOAD 90
+static unsigned int default_target_loads[] = {DEFAULT_TARGET_LOAD};
+
+#define DEFAULT_TIMER_RATE (20 * USEC_PER_MSEC)
+#define DEFAULT_TIMER_RATE_SUSP ((unsigned long)(50 * USEC_PER_MSEC))
+#define DEFAULT_ABOVE_HISPEED_DELAY DEFAULT_TIMER_RATE
+static unsigned int default_above_hispeed_delay[] = {
+	DEFAULT_ABOVE_HISPEED_DELAY };
+
+struct cpufreq_impulse_tunables {
+	int usage_count;
+	/* Hi speed to bump to from lo speed when load burst (default max) */
+	unsigned int hispeed_freq;
+	/* Go to hi speed when CPU load at or above this value. */
+#define DEFAULT_GO_HISPEED_LOAD 99
+	unsigned long go_hispeed_load;
+
+	/* Target load. Lower values result in higher CPU speeds. */
+	spinlock_t target_loads_lock;
+	unsigned int *target_loads;
+	int ntarget_loads;
+	/*
+	 * The minimum amount of time to spend at a frequency before we can ramp
+	 * down.
+	 */
+#define DEFAULT_MIN_SAMPLE_TIME (80 * USEC_PER_MSEC)
+	unsigned long min_sample_time;
+	/*
+	 * The sample rate of the timer used to increase frequency
+	 */
+	unsigned long timer_rate;
+#ifdef CONFIG_STATE_NOTIFIER
+	unsigned long timer_rate_prev;
+#endif
+	/*
+	 * Wait this long before raising speed above hispeed, by default a
+	 * single timer interval.
+	 */
+	spinlock_t above_hispeed_delay_lock;
+	unsigned int *above_hispeed_delay;
+	int nabove_hispeed_delay;
+	bool boosted;
+	/*
+	 * Max additional time to wait in idle, beyond timer_rate, at speeds
+	 * above minimum before wakeup to reduce speed, or -1 if unnecessary.
+	 */
+#define DEFAULT_TIMER_SLACK (4 * DEFAULT_TIMER_RATE)
+	int timer_slack_val;
+	bool io_is_busy;
+
+	/* scheduler input related flags */
+	bool use_sched_load;
+	bool use_migration_notif;
+
+	/*
+	 * Whether to align timer windows across all CPUs. When
+	 * use_sched_load is true, this flag is ignored and windows
+	 * will always be aligned.
+	 */
+	bool align_windows;
+
+	/*
+	 * Stay at max freq for at least max_freq_hysteresis before dropping
+	 * frequency.
+	 */
+	unsigned int max_freq_hysteresis;
+
+	/* Improves frequency selection for more energy */
+	bool powersave_bias;
+};
+
+/* For cases where we have single governor instance for system */
+static struct cpufreq_impulse_tunables *common_tunables;
+static struct cpufreq_impulse_tunables *cached_common_tunables;
+
+static struct attribute_group *get_sysfs_attr(void);
+
+/* Round to starting jiffy of next evaluation window */
+static u64 round_to_nw_start(u64 jif,
+			     struct cpufreq_impulse_tunables *tunables)
+{
+	unsigned long step = usecs_to_jiffies(tunables->timer_rate);
+	u64 ret;
+
+	if (tunables->use_sched_load || tunables->align_windows) {
+		do_div(jif, step);
+		ret = (jif + 1) * step;
+	} else {
+		ret = jiffies + usecs_to_jiffies(tunables->timer_rate);
+	}
+
+	return ret;
+}
+
+static inline int set_window_helper(
+			struct cpufreq_impulse_tunables *tunables)
+{
+	return sched_set_window(round_to_nw_start(get_jiffies_64(), tunables),
+			 usecs_to_jiffies(tunables->timer_rate));
+}
+
+static void cpufreq_impulse_timer_resched(unsigned long cpu,
+					      bool slack_only)
+{
+	struct cpufreq_impulse_policyinfo *ppol = per_cpu(polinfo, cpu);
+	struct cpufreq_impulse_cpuinfo *pcpu;
+	struct cpufreq_impulse_tunables *tunables =
+		ppol->policy->governor_data;
+	u64 expires;
+	unsigned long flags;
+	int i;
+
+	spin_lock_irqsave(&ppol->load_lock, flags);
+	expires = round_to_nw_start(ppol->last_evaluated_jiffy, tunables);
+	if (!slack_only) {
+		for_each_cpu(i, ppol->policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, i);
+			pcpu->time_in_idle = get_cpu_idle_time(i,
+						&pcpu->time_in_idle_timestamp,
+						tunables->io_is_busy);
+			pcpu->cputime_speedadj = 0;
+			pcpu->cputime_speedadj_timestamp =
+						pcpu->time_in_idle_timestamp;
+		}
+		del_timer(&ppol->policy_timer);
+		ppol->policy_timer.expires = expires;
+		add_timer(&ppol->policy_timer);
+	}
+
+	if (tunables->timer_slack_val >= 0 &&
+	    ppol->target_freq > ppol->policy->min) {
+		expires += usecs_to_jiffies(tunables->timer_slack_val);
+		del_timer(&ppol->policy_slack_timer);
+		ppol->policy_slack_timer.expires = expires;
+		add_timer(&ppol->policy_slack_timer);
+	}
+
+	spin_unlock_irqrestore(&ppol->load_lock, flags);
+}
+
+/* The caller shall take enable_sem write semaphore to avoid any timer race.
+ * The policy_timer and policy_slack_timer must be deactivated when calling
+ * this function.
+ */
+static void cpufreq_impulse_timer_start(
+	struct cpufreq_impulse_tunables *tunables, int cpu)
+{
+	struct cpufreq_impulse_policyinfo *ppol = per_cpu(polinfo, cpu);
+	struct cpufreq_impulse_cpuinfo *pcpu;
+	u64 expires = round_to_nw_start(ppol->last_evaluated_jiffy, tunables);
+	unsigned long flags;
+	int i;
+
+	spin_lock_irqsave(&ppol->load_lock, flags);
+	ppol->policy_timer.expires = expires;
+	add_timer(&ppol->policy_timer);
+	if (tunables->timer_slack_val >= 0 &&
+	    ppol->target_freq > ppol->policy->min) {
+		expires += usecs_to_jiffies(tunables->timer_slack_val);
+		ppol->policy_slack_timer.expires = expires;
+		add_timer(&ppol->policy_slack_timer);
+	}
+
+	for_each_cpu(i, ppol->policy->cpus) {
+		pcpu = &per_cpu(cpuinfo, i);
+		pcpu->time_in_idle =
+			get_cpu_idle_time(i, &pcpu->time_in_idle_timestamp,
+					  tunables->io_is_busy);
+		pcpu->cputime_speedadj = 0;
+		pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	}
+	spin_unlock_irqrestore(&ppol->load_lock, flags);
+}
+
+static unsigned int freq_to_above_hispeed_delay(
+	struct cpufreq_impulse_tunables *tunables,
+	unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&tunables->above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < tunables->nabove_hispeed_delay - 1 &&
+			freq >= tunables->above_hispeed_delay[i+1]; i += 2)
+		;
+
+	ret = tunables->above_hispeed_delay[i];
+	spin_unlock_irqrestore(&tunables->above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static unsigned int freq_to_targetload(
+	struct cpufreq_impulse_tunables *tunables, unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&tunables->target_loads_lock, flags);
+
+	for (i = 0; i < tunables->ntarget_loads - 1 &&
+		    freq >= tunables->target_loads[i+1]; i += 2)
+		;
+
+	ret = tunables->target_loads[i];
+	spin_unlock_irqrestore(&tunables->target_loads_lock, flags);
+	return ret;
+}
+
+/*
+ * If increasing frequencies never map to a lower target load then
+ * choose_freq() will find the minimum frequency that does not exceed its
+ * target load given the current load.
+ */
+static unsigned int choose_freq(struct cpufreq_impulse_policyinfo *pcpu,
+		unsigned int loadadjfreq)
+{
+	unsigned int freq = pcpu->policy->cur;
+	unsigned int prevfreq, freqmin, freqmax;
+	unsigned int tl;
+	int index;
+
+	freqmin = 0;
+	freqmax = UINT_MAX;
+
+	do {
+		prevfreq = freq;
+		tl = freq_to_targetload(pcpu->policy->governor_data, freq);
+
+		/*
+		 * Find the lowest frequency where the computed load is less
+		 * than or equal to the target load.
+		 */
+
+		if (cpufreq_frequency_table_target(
+			    pcpu->policy, pcpu->freq_table, loadadjfreq / tl,
+			    CPUFREQ_RELATION_L, &index))
+			break;
+		freq = pcpu->freq_table[index].frequency;
+
+		if (freq > prevfreq) {
+			/* The previous frequency is too low. */
+			freqmin = prevfreq;
+
+			if (freq >= freqmax) {
+				/*
+				 * Find the highest frequency that is less
+				 * than freqmax.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmax - 1, CPUFREQ_RELATION_H,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				if (freq == freqmin) {
+					/*
+					 * The first frequency below freqmax
+					 * has already been found to be too
+					 * low.  freqmax is the lowest speed
+					 * we found that is fast enough.
+					 */
+					freq = freqmax;
+					break;
+				}
+			}
+		} else if (freq < prevfreq) {
+			/* The previous frequency is high enough. */
+			freqmax = prevfreq;
+
+			if (freq <= freqmin) {
+				/*
+				 * Find the lowest frequency that is higher
+				 * than freqmin.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmin + 1, CPUFREQ_RELATION_L,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				/*
+				 * If freqmax is the first frequency above
+				 * freqmin then we have already found that
+				 * this speed is fast enough.
+				 */
+				if (freq == freqmax)
+					break;
+			}
+		}
+
+		/* If same frequency chosen as previous then done. */
+	} while (freq != prevfreq);
+
+	return freq;
+}
+
+static u64 update_load(int cpu)
+{
+	struct cpufreq_impulse_policyinfo *ppol = per_cpu(polinfo, cpu);
+	struct cpufreq_impulse_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	struct cpufreq_impulse_tunables *tunables =
+		ppol->policy->governor_data;
+	u64 now;
+	u64 now_idle;
+	unsigned int delta_idle;
+	unsigned int delta_time;
+	u64 active_time;
+
+	now_idle = get_cpu_idle_time(cpu, &now, tunables->io_is_busy);
+	delta_idle = (unsigned int)(now_idle - pcpu->time_in_idle);
+	delta_time = (unsigned int)(now - pcpu->time_in_idle_timestamp);
+
+	if (delta_time <= delta_idle)
+		active_time = 0;
+	else
+		active_time = delta_time - delta_idle;
+
+	pcpu->cputime_speedadj += active_time * ppol->policy->cur;
+
+	pcpu->time_in_idle = now_idle;
+	pcpu->time_in_idle_timestamp = now;
+	return now;
+}
+
+#define MAX_LOCAL_LOAD 100
+static void cpufreq_impulse_timer(unsigned long data)
+{
+	u64 now;
+	unsigned int delta_time;
+	u64 cputime_speedadj;
+	int cpu_load;
+	struct cpufreq_impulse_policyinfo *ppol = per_cpu(polinfo, data);
+	struct cpufreq_impulse_tunables *tunables =
+		ppol->policy->governor_data;
+	struct cpufreq_impulse_cpuinfo *pcpu;
+	unsigned int new_freq;
+	unsigned int loadadjfreq = 0, tmploadadjfreq;
+	unsigned int index;
+	unsigned int this_hispeed_freq;
+	unsigned long flags;
+	unsigned long max_cpu;
+	int i, fcpu;
+	struct cpufreq_govinfo govinfo;
+
+	if (!down_read_trylock(&ppol->enable_sem))
+		return;
+	if (!ppol->governor_enabled)
+		goto exit;
+
+	if (cpu_is_offline(data))
+		goto exit;
+
+	fcpu = cpumask_first(ppol->policy->related_cpus);
+	now = ktime_to_us(ktime_get());
+	spin_lock_irqsave(&ppol->load_lock, flags);
+	ppol->last_evaluated_jiffy = get_jiffies_64();
+
+#ifdef CONFIG_STATE_NOTIFIER
+	if (!state_suspended &&
+		tunables->timer_rate != tunables->timer_rate_prev)
+		tunables->timer_rate = tunables->timer_rate_prev;
+	else if (state_suspended &&
+		tunables->timer_rate != DEFAULT_TIMER_RATE_SUSP) {
+		tunables->timer_rate_prev = tunables->timer_rate;
+		tunables->timer_rate
+			= max(tunables->timer_rate,
+				DEFAULT_TIMER_RATE_SUSP);
+	}
+#endif
+
+	if (tunables->use_sched_load)
+		sched_get_cpus_busy(ppol->cpu_busy_times,
+				    ppol->policy->related_cpus);
+	max_cpu = cpumask_first(ppol->policy->cpus);
+	for_each_cpu(i, ppol->policy->cpus) {
+		pcpu = &per_cpu(cpuinfo, i);
+		if (tunables->use_sched_load) {
+			cputime_speedadj = (u64)ppol->cpu_busy_times[i - fcpu]
+					* ppol->policy->cpuinfo.max_freq;
+			do_div(cputime_speedadj, tunables->timer_rate);
+		} else {
+			now = update_load(i);
+			delta_time = (unsigned int)
+				(now - pcpu->cputime_speedadj_timestamp);
+			if (WARN_ON_ONCE(!delta_time))
+				continue;
+			cputime_speedadj = pcpu->cputime_speedadj;
+			do_div(cputime_speedadj, delta_time);
+		}
+		tmploadadjfreq = (unsigned int)cputime_speedadj * 100;
+		pcpu->loadadjfreq = tmploadadjfreq;
+
+		if (tmploadadjfreq > loadadjfreq) {
+			loadadjfreq = tmploadadjfreq;
+			max_cpu = i;
+		}
+	}
+	spin_unlock_irqrestore(&ppol->load_lock, flags);
+
+	/*
+	 * Send govinfo notification.
+	 * Govinfo notification could potentially wake up another thread
+	 * managed by its clients. Thread wakeups might trigger a load
+	 * change callback that executes this function again. Therefore
+	 * no spinlock could be held when sending the notification.
+	 */
+	for_each_cpu(i, ppol->policy->cpus) {
+		pcpu = &per_cpu(cpuinfo, i);
+		govinfo.cpu = i;
+		govinfo.load = pcpu->loadadjfreq / ppol->policy->max;
+		govinfo.sampling_rate_us = tunables->timer_rate;
+		atomic_notifier_call_chain(&cpufreq_govinfo_notifier_list,
+					   CPUFREQ_LOAD_CHANGE, &govinfo);
+	}
+
+	spin_lock_irqsave(&ppol->target_freq_lock, flags);
+	cpu_load = loadadjfreq / ppol->policy->cur;
+	tunables->boosted = cpu_load >= tunables->go_hispeed_load;
+#ifdef CONFIG_CPU_BOOST
+	tunables->boosted = check_cpuboost(data) || tunables->boosted;
+#endif			
+#ifdef CONFIG_MSM_HOTPLUG
+	tunables->boosted = fast_lane_mode || tunables->boosted;
+#endif
+#ifdef CONFIG_STATE_NOTIFIER
+	tunables->boosted = tunables->boosted && !state_suspended;
+#endif
+	this_hispeed_freq = max(tunables->hispeed_freq, ppol->policy->min);
+
+	if (tunables->boosted) {
+		if (ppol->target_freq < this_hispeed_freq &&
+		    cpu_load <= MAX_LOCAL_LOAD) {
+			new_freq = this_hispeed_freq;
+		} else {
+			new_freq = choose_freq(ppol, loadadjfreq);
+
+			if (new_freq < this_hispeed_freq)
+				new_freq = this_hispeed_freq;
+		}
+	} else {
+		new_freq = choose_freq(ppol, loadadjfreq);
+		if (new_freq > tunables->hispeed_freq &&
+				ppol->target_freq < tunables->hispeed_freq)
+			new_freq = tunables->hispeed_freq;
+	}
+
+	if (cpu_load <= MAX_LOCAL_LOAD &&
+	    ppol->policy->cur >= this_hispeed_freq &&
+	    new_freq > ppol->policy->cur &&
+	    now - ppol->hispeed_validate_time <
+	    freq_to_above_hispeed_delay(tunables, ppol->policy->cur)) {
+		spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	ppol->hispeed_validate_time = now;
+
+	if (cpufreq_frequency_table_target(ppol->policy, ppol->freq_table,
+					   new_freq, CPUFREQ_RELATION_L,
+					   &index)) {
+		spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	new_freq = ppol->freq_table[index].frequency;
+
+	if (new_freq < ppol->target_freq &&
+	    now - ppol->max_freq_hyst_start_time <
+	    tunables->max_freq_hysteresis) {
+		spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	/*
+	 * Do not scale below floor_freq unless we have been at or above the
+	 * floor frequency for the minimum sample time since last validated.
+	 */
+	if (new_freq < ppol->floor_freq) {
+		if (now - ppol->floor_validate_time <
+				tunables->min_sample_time) {
+			spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+			goto rearm;
+		}
+	}
+
+	/*
+	 * Update the timestamp for checking whether speed has been held at
+	 * or above the selected frequency for a minimum of min_sample_time,
+	 * if not boosted to this_hispeed_freq.  If boosted to this_hispeed_freq then we
+	 * allow the speed to drop as soon as the boostpulse duration expires
+	 * (or the indefinite boost is turned off).
+	 */
+
+	if (!tunables->boosted || new_freq > this_hispeed_freq) {
+		ppol->floor_freq = new_freq;
+		ppol->floor_validate_time = now;
+	}
+
+	if (new_freq == ppol->policy->max)
+		ppol->max_freq_hyst_start_time = now;
+
+	if (ppol->target_freq == new_freq &&
+			ppol->target_freq <= ppol->policy->cur) {
+		spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	ppol->target_freq = new_freq;
+	spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+	cpumask_set_cpu(max_cpu, &speedchange_cpumask);
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+	wake_up_process(speedchange_task);
+
+rearm:
+	if (!timer_pending(&ppol->policy_timer))
+		cpufreq_impulse_timer_resched(data, false);
+
+exit:
+	up_read(&ppol->enable_sem);
+	return;
+}
+
+
+static int cpufreq_impulse_speedchange_task(void *data)
+{
+	unsigned int cpu;
+	cpumask_t tmp_mask;
+	unsigned long flags;
+	struct cpufreq_impulse_policyinfo *ppol;
+	struct cpufreq_impulse_tunables *tunables;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+		if (cpumask_empty(&speedchange_cpumask)) {
+			spin_unlock_irqrestore(&speedchange_cpumask_lock,
+					       flags);
+			schedule();
+
+			if (kthread_should_stop())
+				break;
+
+			spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+		}
+
+		set_current_state(TASK_RUNNING);
+		tmp_mask = speedchange_cpumask;
+		cpumask_clear(&speedchange_cpumask);
+		spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+		for_each_cpu(cpu, &tmp_mask) {
+			ppol = per_cpu(polinfo, cpu);
+			if (!down_read_trylock(&ppol->enable_sem))
+				continue;
+			if (!ppol->governor_enabled) {
+				up_read(&ppol->enable_sem);
+				continue;
+			}
+
+ 			if (ppol->target_freq != ppol->policy->cur) {
+				tunables = ppol->policy->governor_data;
+#ifdef CONFIG_STATE_NOTIFIER
+				if (tunables->powersave_bias || state_suspended)
+#else
+				if (tunables->powersave_bias)
+#endif
+					__cpufreq_driver_target(ppol->policy,
+								ppol->target_freq,
+								CPUFREQ_RELATION_C);
+				else
+					__cpufreq_driver_target(ppol->policy,
+								ppol->target_freq,
+								CPUFREQ_RELATION_H);
+			}
+			up_read(&ppol->enable_sem);
+		}
+	}
+
+	return 0;
+}
+
+static int load_change_callback(struct notifier_block *nb, unsigned long val,
+				void *data)
+{
+	unsigned long cpu = (unsigned long) data;
+	struct cpufreq_impulse_policyinfo *ppol = per_cpu(polinfo, cpu);
+	struct cpufreq_impulse_tunables *tunables;
+
+	if (speedchange_task == current)
+		return 0;
+	if (!ppol || ppol->reject_notification)
+		return 0;
+
+	if (!down_read_trylock(&ppol->enable_sem))
+		return 0;
+	if (!ppol->governor_enabled) {
+		up_read(&ppol->enable_sem);
+		return 0;
+	}
+	tunables = ppol->policy->governor_data;
+	if (!tunables->use_sched_load || !tunables->use_migration_notif) {
+		up_read(&ppol->enable_sem);
+		return 0;
+	}
+
+	del_timer(&ppol->policy_timer);
+	del_timer(&ppol->policy_slack_timer);
+	cpufreq_impulse_timer(cpu);
+
+	up_read(&ppol->enable_sem);
+	return 0;
+}
+
+static struct notifier_block load_notifier_block = {
+	.notifier_call = load_change_callback,
+};
+
+static int cpufreq_impulse_notifier(
+	struct notifier_block *nb, unsigned long val, void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpufreq_impulse_policyinfo *ppol;
+	int cpu;
+	unsigned long flags;
+
+	if (val == CPUFREQ_PRECHANGE) {
+		ppol = per_cpu(polinfo, freq->cpu);
+		if (!ppol)
+			return 0;
+		if (!down_read_trylock(&ppol->enable_sem))
+			return 0;
+		if (!ppol->governor_enabled) {
+			up_read(&ppol->enable_sem);
+			return 0;
+		}
+
+		if (cpumask_first(ppol->policy->cpus) != freq->cpu) {
+			up_read(&ppol->enable_sem);
+			return 0;
+		}
+		spin_lock_irqsave(&ppol->load_lock, flags);
+		for_each_cpu(cpu, ppol->policy->cpus)
+			update_load(cpu);
+		spin_unlock_irqrestore(&ppol->load_lock, flags);
+
+		up_read(&ppol->enable_sem);
+	}
+	return 0;
+}
+
+static struct notifier_block cpufreq_notifier_block = {
+	.notifier_call = cpufreq_impulse_notifier,
+};
+
+static unsigned int *get_tokenized_data(const char *buf, int *num_tokens)
+{
+	const char *cp;
+	int i;
+	int ntokens = 1;
+	unsigned int *tokenized_data;
+	int err = -EINVAL;
+
+	cp = buf;
+	while ((cp = strpbrk(cp + 1, " :")))
+		ntokens++;
+
+	if (!(ntokens & 0x1))
+		goto err;
+
+	tokenized_data = kmalloc(ntokens * sizeof(unsigned int), GFP_KERNEL);
+	if (!tokenized_data) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	cp = buf;
+	i = 0;
+	while (i < ntokens) {
+		if (sscanf(cp, "%u", &tokenized_data[i++]) != 1)
+			goto err_kfree;
+
+		cp = strpbrk(cp, " :");
+		if (!cp)
+			break;
+		cp++;
+	}
+
+	if (i != ntokens)
+		goto err_kfree;
+
+	*num_tokens = ntokens;
+	return tokenized_data;
+
+err_kfree:
+	kfree(tokenized_data);
+err:
+	return ERR_PTR(err);
+}
+
+static ssize_t show_target_loads(
+	struct cpufreq_impulse_tunables *tunables,
+	char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&tunables->target_loads_lock, flags);
+
+	for (i = 0; i < tunables->ntarget_loads; i++)
+		ret += sprintf(buf + ret, "%u%s", tunables->target_loads[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&tunables->target_loads_lock, flags);
+	return ret;
+}
+
+static ssize_t store_target_loads(
+	struct cpufreq_impulse_tunables *tunables,
+	const char *buf, size_t count)
+{
+	int ntokens;
+	unsigned int *new_target_loads = NULL;
+	unsigned long flags;
+
+	new_target_loads = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_target_loads))
+		return PTR_RET(new_target_loads);
+
+	spin_lock_irqsave(&tunables->target_loads_lock, flags);
+	if (tunables->target_loads != default_target_loads)
+		kfree(tunables->target_loads);
+	tunables->target_loads = new_target_loads;
+	tunables->ntarget_loads = ntokens;
+	spin_unlock_irqrestore(&tunables->target_loads_lock, flags);
+	return count;
+}
+
+static ssize_t show_above_hispeed_delay(
+	struct cpufreq_impulse_tunables *tunables, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&tunables->above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < tunables->nabove_hispeed_delay; i++)
+		ret += sprintf(buf + ret, "%u%s",
+			       tunables->above_hispeed_delay[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&tunables->above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static ssize_t store_above_hispeed_delay(
+	struct cpufreq_impulse_tunables *tunables,
+	const char *buf, size_t count)
+{
+	int ntokens, i;
+	unsigned int *new_above_hispeed_delay = NULL;
+	unsigned long flags;
+
+	new_above_hispeed_delay = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_above_hispeed_delay))
+		return PTR_RET(new_above_hispeed_delay);
+
+	/* Make sure frequencies are in ascending order. */
+	for (i = 3; i < ntokens; i += 2) {
+		if (new_above_hispeed_delay[i] <=
+			new_above_hispeed_delay[i - 2]) {
+			kfree(new_above_hispeed_delay);
+			return -EINVAL;
+		}
+	}
+
+	spin_lock_irqsave(&tunables->above_hispeed_delay_lock, flags);
+	if (tunables->above_hispeed_delay != default_above_hispeed_delay)
+		kfree(tunables->above_hispeed_delay);
+	tunables->above_hispeed_delay = new_above_hispeed_delay;
+	tunables->nabove_hispeed_delay = ntokens;
+	spin_unlock_irqrestore(&tunables->above_hispeed_delay_lock, flags);
+	return count;
+
+}
+
+static ssize_t show_hispeed_freq(struct cpufreq_impulse_tunables *tunables,
+		char *buf)
+{
+	return sprintf(buf, "%u\n", tunables->hispeed_freq);
+}
+
+static ssize_t store_hispeed_freq(struct cpufreq_impulse_tunables *tunables,
+		const char *buf, size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	tunables->hispeed_freq = val;
+	return count;
+}
+
+#define show_store_one(file_name)					\
+static ssize_t show_##file_name(					\
+	struct cpufreq_impulse_tunables *tunables, char *buf)	\
+{									\
+	return snprintf(buf, PAGE_SIZE, "%u\n", tunables->file_name);	\
+}									\
+static ssize_t store_##file_name(					\
+		struct cpufreq_impulse_tunables *tunables,		\
+		const char *buf, size_t count)				\
+{									\
+	int ret;							\
+	long unsigned int val;						\
+									\
+	ret = kstrtoul(buf, 0, &val);				\
+	if (ret < 0)							\
+		return ret;						\
+	tunables->file_name = val;					\
+	return count;							\
+}
+show_store_one(max_freq_hysteresis);
+show_store_one(align_windows);
+
+static ssize_t show_go_hispeed_load(struct cpufreq_impulse_tunables
+		*tunables, char *buf)
+{
+	return sprintf(buf, "%lu\n", tunables->go_hispeed_load);
+}
+
+static ssize_t store_go_hispeed_load(struct cpufreq_impulse_tunables
+		*tunables, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	tunables->go_hispeed_load = val;
+	return count;
+}
+
+static ssize_t show_min_sample_time(struct cpufreq_impulse_tunables
+		*tunables, char *buf)
+{
+	return sprintf(buf, "%lu\n", tunables->min_sample_time);
+}
+
+static ssize_t store_min_sample_time(struct cpufreq_impulse_tunables
+		*tunables, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	tunables->min_sample_time = val;
+	return count;
+}
+
+static ssize_t show_timer_rate(struct cpufreq_impulse_tunables *tunables,
+		char *buf)
+{
+	return sprintf(buf, "%lu\n", tunables->timer_rate);
+}
+
+static ssize_t store_timer_rate(struct cpufreq_impulse_tunables *tunables,
+		const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val, val_round;
+	struct cpufreq_impulse_tunables *t;
+	int cpu;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	val_round = jiffies_to_usecs(usecs_to_jiffies(val));
+	if (val != val_round)
+		pr_warn("timer_rate not aligned to jiffy. Rounded up to %lu\n",
+			val_round);
+	tunables->timer_rate = val_round;
+#ifdef CONFIG_STATE_NOTIFIER
+	tunables->timer_rate_prev = val_round;
+#endif
+
+	if (!tunables->use_sched_load)
+		return count;
+
+	for_each_possible_cpu(cpu) {
+		if (!per_cpu(polinfo, cpu))
+			continue;
+		t = per_cpu(polinfo, cpu)->cached_tunables;
+		if (t && t->use_sched_load) {
+			t->timer_rate = val_round;
+#ifdef CONFIG_STATE_NOTIFIER
+			t->timer_rate_prev = val_round;
+#endif
+		}
+	}
+	set_window_helper(tunables);
+
+	return count;
+}
+
+static ssize_t show_timer_slack(struct cpufreq_impulse_tunables *tunables,
+		char *buf)
+{
+	return sprintf(buf, "%d\n", tunables->timer_slack_val);
+}
+
+static ssize_t store_timer_slack(struct cpufreq_impulse_tunables *tunables,
+		const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret < 0)
+		return ret;
+
+	tunables->timer_slack_val = val;
+	return count;
+}
+
+static ssize_t show_io_is_busy(struct cpufreq_impulse_tunables *tunables,
+		char *buf)
+{
+	return sprintf(buf, "%u\n", tunables->io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct cpufreq_impulse_tunables *tunables,
+		const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+	struct cpufreq_impulse_tunables *t;
+	int cpu;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	tunables->io_is_busy = val;
+
+	if (!tunables->use_sched_load)
+		return count;
+
+	for_each_possible_cpu(cpu) {
+		if (!per_cpu(polinfo, cpu))
+			continue;
+		t = per_cpu(polinfo, cpu)->cached_tunables;
+		if (t && t->use_sched_load)
+			t->io_is_busy = val;
+	}
+	sched_set_io_is_busy(val);
+
+	return count;
+}
+
+static int cpufreq_impulse_enable_sched_input(
+			struct cpufreq_impulse_tunables *tunables)
+{
+	int rc = 0, j;
+	struct cpufreq_impulse_tunables *t;
+
+	mutex_lock(&sched_lock);
+
+	set_window_count++;
+	if (set_window_count > 1) {
+		for_each_possible_cpu(j) {
+			if (!per_cpu(polinfo, j))
+				continue;
+			t = per_cpu(polinfo, j)->cached_tunables;
+			if (t && t->use_sched_load) {
+				tunables->timer_rate = t->timer_rate;
+				tunables->io_is_busy = t->io_is_busy;
+				break;
+			}
+		}
+	} else {
+		rc = set_window_helper(tunables);
+		if (rc) {
+			pr_err("%s: Failed to set sched window\n", __func__);
+			set_window_count--;
+			goto out;
+		}
+		sched_set_io_is_busy(tunables->io_is_busy);
+	}
+
+	if (!tunables->use_migration_notif)
+		goto out;
+
+	migration_register_count++;
+	if (migration_register_count > 1)
+		goto out;
+	else
+		atomic_notifier_chain_register(&load_alert_notifier_head,
+						&load_notifier_block);
+out:
+	mutex_unlock(&sched_lock);
+	return rc;
+}
+
+static int cpufreq_impulse_disable_sched_input(
+			struct cpufreq_impulse_tunables *tunables)
+{
+	mutex_lock(&sched_lock);
+
+	if (tunables->use_migration_notif) {
+		migration_register_count--;
+		if (migration_register_count < 1)
+			atomic_notifier_chain_unregister(
+					&load_alert_notifier_head,
+					&load_notifier_block);
+	}
+	set_window_count--;
+
+	mutex_unlock(&sched_lock);
+	return 0;
+}
+
+static ssize_t show_use_sched_load(
+		struct cpufreq_impulse_tunables *tunables, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%d\n", tunables->use_sched_load);
+}
+
+static ssize_t store_use_sched_load(
+			struct cpufreq_impulse_tunables *tunables,
+			const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	if (tunables->use_sched_load == (bool) val)
+		return count;
+
+	tunables->use_sched_load = val;
+
+	if (val)
+		ret = cpufreq_impulse_enable_sched_input(tunables);
+	else
+		ret = cpufreq_impulse_disable_sched_input(tunables);
+
+	if (ret) {
+		tunables->use_sched_load = !val;
+		return ret;
+	}
+
+	return count;
+}
+
+static ssize_t show_use_migration_notif(
+		struct cpufreq_impulse_tunables *tunables, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%d\n",
+			tunables->use_migration_notif);
+}
+
+static ssize_t store_use_migration_notif(
+			struct cpufreq_impulse_tunables *tunables,
+			const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	if (tunables->use_migration_notif == (bool) val)
+		return count;
+	tunables->use_migration_notif = val;
+
+	if (!tunables->use_sched_load)
+		return count;
+
+	mutex_lock(&sched_lock);
+	if (val) {
+		migration_register_count++;
+		if (migration_register_count == 1)
+			atomic_notifier_chain_register(
+					&load_alert_notifier_head,
+					&load_notifier_block);
+	} else {
+		migration_register_count--;
+		if (!migration_register_count)
+			atomic_notifier_chain_unregister(
+					&load_alert_notifier_head,
+					&load_notifier_block);
+	}
+	mutex_unlock(&sched_lock);
+
+	return count;
+}
+
+static ssize_t show_powersave_bias(struct cpufreq_impulse_tunables *tunables,
+		char *buf)
+{
+	return sprintf(buf, "%u\n", tunables->powersave_bias);
+}
+
+static ssize_t store_powersave_bias(struct cpufreq_impulse_tunables *tunables,
+		const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	tunables->powersave_bias = val;
+	return count;
+}
+
+/*
+ * Create show/store routines
+ * - sys: One governor instance for complete SYSTEM
+ * - pol: One governor instance per struct cpufreq_policy
+ */
+#define show_gov_pol_sys(file_name)					\
+static ssize_t show_##file_name##_gov_sys				\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return show_##file_name(common_tunables, buf);			\
+}									\
+									\
+static ssize_t show_##file_name##_gov_pol				\
+(struct cpufreq_policy *policy, char *buf)				\
+{									\
+	return show_##file_name(policy->governor_data, buf);		\
+}
+
+#define store_gov_pol_sys(file_name)					\
+static ssize_t store_##file_name##_gov_sys				\
+(struct kobject *kobj, struct attribute *attr, const char *buf,		\
+	size_t count)							\
+{									\
+	return store_##file_name(common_tunables, buf, count);		\
+}									\
+									\
+static ssize_t store_##file_name##_gov_pol				\
+(struct cpufreq_policy *policy, const char *buf, size_t count)		\
+{									\
+	return store_##file_name(policy->governor_data, buf, count);	\
+}
+
+#define show_store_gov_pol_sys(file_name)				\
+show_gov_pol_sys(file_name);						\
+store_gov_pol_sys(file_name)
+
+show_store_gov_pol_sys(target_loads);
+show_store_gov_pol_sys(above_hispeed_delay);
+show_store_gov_pol_sys(hispeed_freq);
+show_store_gov_pol_sys(go_hispeed_load);
+show_store_gov_pol_sys(min_sample_time);
+show_store_gov_pol_sys(timer_rate);
+show_store_gov_pol_sys(timer_slack);
+show_store_gov_pol_sys(io_is_busy);
+show_store_gov_pol_sys(use_sched_load);
+show_store_gov_pol_sys(use_migration_notif);
+show_store_gov_pol_sys(max_freq_hysteresis);
+show_store_gov_pol_sys(align_windows);
+show_store_gov_pol_sys(powersave_bias);
+
+#define gov_sys_attr_rw(_name)						\
+static struct global_attr _name##_gov_sys =				\
+__ATTR(_name, 0644, show_##_name##_gov_sys, store_##_name##_gov_sys)
+
+#define gov_pol_attr_rw(_name)						\
+static struct freq_attr _name##_gov_pol =				\
+__ATTR(_name, 0644, show_##_name##_gov_pol, store_##_name##_gov_pol)
+
+#define gov_sys_pol_attr_rw(_name)					\
+	gov_sys_attr_rw(_name);						\
+	gov_pol_attr_rw(_name)
+
+gov_sys_pol_attr_rw(target_loads);
+gov_sys_pol_attr_rw(above_hispeed_delay);
+gov_sys_pol_attr_rw(hispeed_freq);
+gov_sys_pol_attr_rw(go_hispeed_load);
+gov_sys_pol_attr_rw(min_sample_time);
+gov_sys_pol_attr_rw(timer_rate);
+gov_sys_pol_attr_rw(timer_slack);
+gov_sys_pol_attr_rw(io_is_busy);
+gov_sys_pol_attr_rw(use_sched_load);
+gov_sys_pol_attr_rw(use_migration_notif);
+gov_sys_pol_attr_rw(max_freq_hysteresis);
+gov_sys_pol_attr_rw(align_windows);
+gov_sys_pol_attr_rw(powersave_bias);
+
+/* One Governor instance for entire system */
+static struct attribute *impulse_attributes_gov_sys[] = {
+	&target_loads_gov_sys.attr,
+	&above_hispeed_delay_gov_sys.attr,
+	&hispeed_freq_gov_sys.attr,
+	&go_hispeed_load_gov_sys.attr,
+	&min_sample_time_gov_sys.attr,
+	&timer_rate_gov_sys.attr,
+	&timer_slack_gov_sys.attr,
+	&io_is_busy_gov_sys.attr,
+	&use_sched_load_gov_sys.attr,
+	&use_migration_notif_gov_sys.attr,
+	&max_freq_hysteresis_gov_sys.attr,
+	&align_windows_gov_sys.attr,
+	&powersave_bias_gov_sys.attr,
+	NULL,
+};
+
+static struct attribute_group impulse_attr_group_gov_sys = {
+	.attrs = impulse_attributes_gov_sys,
+	.name = "impulse",
+};
+
+/* Per policy governor instance */
+static struct attribute *impulse_attributes_gov_pol[] = {
+	&target_loads_gov_pol.attr,
+	&above_hispeed_delay_gov_pol.attr,
+	&hispeed_freq_gov_pol.attr,
+	&go_hispeed_load_gov_pol.attr,
+	&min_sample_time_gov_pol.attr,
+	&timer_rate_gov_pol.attr,
+	&timer_slack_gov_pol.attr,
+	&io_is_busy_gov_pol.attr,
+	&use_sched_load_gov_pol.attr,
+	&use_migration_notif_gov_pol.attr,
+	&max_freq_hysteresis_gov_pol.attr,
+	&align_windows_gov_pol.attr,
+	&powersave_bias_gov_pol.attr,
+	NULL,
+};
+
+static struct attribute_group impulse_attr_group_gov_pol = {
+	.attrs = impulse_attributes_gov_pol,
+	.name = "impulse",
+};
+
+static struct attribute_group *get_sysfs_attr(void)
+{
+	if (have_governor_per_policy())
+		return &impulse_attr_group_gov_pol;
+	else
+		return &impulse_attr_group_gov_sys;
+}
+
+static void cpufreq_impulse_nop_timer(unsigned long data)
+{
+}
+
+static struct cpufreq_impulse_tunables *alloc_tunable(
+					struct cpufreq_policy *policy)
+{
+	struct cpufreq_impulse_tunables *tunables;
+
+	tunables = kzalloc(sizeof(*tunables), GFP_KERNEL);
+	if (!tunables)
+		return ERR_PTR(-ENOMEM);
+
+	tunables->above_hispeed_delay = default_above_hispeed_delay;
+	tunables->nabove_hispeed_delay =
+		ARRAY_SIZE(default_above_hispeed_delay);
+	tunables->go_hispeed_load = DEFAULT_GO_HISPEED_LOAD;
+	tunables->target_loads = default_target_loads;
+	tunables->ntarget_loads = ARRAY_SIZE(default_target_loads);
+	tunables->min_sample_time = DEFAULT_MIN_SAMPLE_TIME;
+	tunables->timer_rate = DEFAULT_TIMER_RATE;
+#ifdef CONFIG_STATE_NOTIFIER
+	tunables->timer_rate_prev = DEFAULT_TIMER_RATE;
+#endif
+	tunables->timer_slack_val = DEFAULT_TIMER_SLACK;
+
+	spin_lock_init(&tunables->target_loads_lock);
+	spin_lock_init(&tunables->above_hispeed_delay_lock);
+
+	return tunables;
+}
+
+static struct cpufreq_impulse_policyinfo *get_policyinfo(
+					struct cpufreq_policy *policy)
+{
+	struct cpufreq_impulse_policyinfo *ppol =
+				per_cpu(polinfo, policy->cpu);
+	int i;
+	unsigned long *busy;
+
+	/* polinfo already allocated for policy, return */
+	if (ppol)
+		return ppol;
+
+	ppol = kzalloc(sizeof(*ppol), GFP_KERNEL);
+	if (!ppol)
+		return ERR_PTR(-ENOMEM);
+
+	busy = kcalloc(cpumask_weight(policy->related_cpus), sizeof(*busy),
+		       GFP_KERNEL);
+	if (!busy) {
+		kfree(ppol);
+		return ERR_PTR(-ENOMEM);
+	}
+	ppol->cpu_busy_times = busy;
+
+	init_timer_deferrable(&ppol->policy_timer);
+	ppol->policy_timer.function = cpufreq_impulse_timer;
+	init_timer(&ppol->policy_slack_timer);
+	ppol->policy_slack_timer.function = cpufreq_impulse_nop_timer;
+	spin_lock_init(&ppol->load_lock);
+	spin_lock_init(&ppol->target_freq_lock);
+	init_rwsem(&ppol->enable_sem);
+
+	for_each_cpu(i, policy->related_cpus)
+		per_cpu(polinfo, i) = ppol;
+	return ppol;
+}
+
+/* This function is not multithread-safe. */
+static void free_policyinfo(int cpu)
+{
+	struct cpufreq_impulse_policyinfo *ppol = per_cpu(polinfo, cpu);
+	int j;
+
+	if (!ppol)
+		return;
+
+	for_each_possible_cpu(j)
+		if (per_cpu(polinfo, j) == ppol)
+			per_cpu(polinfo, cpu) = NULL;
+	kfree(ppol->cached_tunables);
+	kfree(ppol->cpu_busy_times);
+	kfree(ppol);
+}
+
+static struct cpufreq_impulse_tunables *get_tunables(
+				struct cpufreq_impulse_policyinfo *ppol)
+{
+	if (have_governor_per_policy())
+		return ppol->cached_tunables;
+	else
+		return cached_common_tunables;
+}
+
+static int cpufreq_governor_impulse(struct cpufreq_policy *policy,
+		unsigned int event)
+{
+	int rc;
+	struct cpufreq_impulse_policyinfo *ppol;
+	struct cpufreq_frequency_table *freq_table;
+	struct cpufreq_impulse_tunables *tunables;
+	unsigned long flags;
+
+	if (have_governor_per_policy())
+		tunables = policy->governor_data;
+	else
+		tunables = common_tunables;
+
+	BUG_ON(!tunables && (event != CPUFREQ_GOV_POLICY_INIT));
+
+	switch (event) {
+	case CPUFREQ_GOV_POLICY_INIT:
+		ppol = get_policyinfo(policy);
+		if (IS_ERR(ppol))
+			return PTR_ERR(ppol);
+
+		if (have_governor_per_policy()) {
+			WARN_ON(tunables);
+		} else if (tunables) {
+			tunables->usage_count++;
+			policy->governor_data = tunables;
+			return 0;
+		}
+
+		tunables = get_tunables(ppol);
+		if (!tunables) {
+			tunables = alloc_tunable(policy);
+			if (IS_ERR(tunables))
+				return PTR_ERR(tunables);
+		}
+
+		tunables->usage_count = 1;
+		policy->governor_data = tunables;
+		if (!have_governor_per_policy()) {
+			WARN_ON(cpufreq_get_global_kobject());
+			common_tunables = tunables;
+		}
+
+		rc = sysfs_create_group(get_governor_parent_kobj(policy),
+				get_sysfs_attr());
+		if (rc) {
+			kfree(tunables);
+			policy->governor_data = NULL;
+			if (!have_governor_per_policy()) {
+				common_tunables = NULL;
+				cpufreq_put_global_kobject();
+			}
+			return rc;
+		}
+
+		if (!policy->governor->initialized)
+			cpufreq_register_notifier(&cpufreq_notifier_block,
+					CPUFREQ_TRANSITION_NOTIFIER);
+
+		if (tunables->use_sched_load)
+			cpufreq_impulse_enable_sched_input(tunables);
+
+		if (have_governor_per_policy())
+			ppol->cached_tunables = tunables;
+		else
+			cached_common_tunables = tunables;
+
+		break;
+
+	case CPUFREQ_GOV_POLICY_EXIT:
+		if (!--tunables->usage_count) {
+			if (policy->governor->initialized == 1)
+				cpufreq_unregister_notifier(&cpufreq_notifier_block,
+						CPUFREQ_TRANSITION_NOTIFIER);
+
+			sysfs_remove_group(get_governor_parent_kobj(policy),
+					get_sysfs_attr());
+			if (!have_governor_per_policy())
+				cpufreq_put_global_kobject();
+			common_tunables = NULL;
+		}
+
+		policy->governor_data = NULL;
+
+		if (tunables->use_sched_load)
+			cpufreq_impulse_disable_sched_input(tunables);
+
+		break;
+
+	case CPUFREQ_GOV_START:
+		mutex_lock(&gov_lock);
+
+		freq_table = cpufreq_frequency_get_table(policy->cpu);
+		if (!tunables->hispeed_freq)
+			tunables->hispeed_freq = policy->max;
+
+		ppol = per_cpu(polinfo, policy->cpu);
+		ppol->policy = policy;
+		ppol->target_freq = policy->cur;
+		ppol->freq_table = freq_table;
+		ppol->floor_freq = ppol->target_freq;
+		ppol->floor_validate_time = ktime_to_us(ktime_get());
+		ppol->hispeed_validate_time = ppol->floor_validate_time;
+		ppol->min_freq = policy->min;
+		ppol->reject_notification = true;
+		down_write(&ppol->enable_sem);
+		del_timer_sync(&ppol->policy_timer);
+		del_timer_sync(&ppol->policy_slack_timer);
+		ppol->policy_timer.data = policy->cpu;
+		ppol->last_evaluated_jiffy = get_jiffies_64();
+		if (cpu_online(policy->cpu))
+			cpufreq_impulse_timer_start(tunables, policy->cpu);
+		ppol->governor_enabled = 1;
+		up_write(&ppol->enable_sem);
+		ppol->reject_notification = false;
+
+		mutex_unlock(&gov_lock);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		mutex_lock(&gov_lock);
+
+		ppol = per_cpu(polinfo, policy->cpu);
+		ppol->reject_notification = true;
+		down_write(&ppol->enable_sem);
+		ppol->governor_enabled = 0;
+		ppol->target_freq = 0;
+		del_timer_sync(&ppol->policy_timer);
+		del_timer_sync(&ppol->policy_slack_timer);
+		up_write(&ppol->enable_sem);
+		ppol->reject_notification = false;
+
+		mutex_unlock(&gov_lock);
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		__cpufreq_driver_target(policy,
+				policy->cur, CPUFREQ_RELATION_L);
+
+		ppol = per_cpu(polinfo, policy->cpu);
+
+		down_read(&ppol->enable_sem);
+		if (ppol->governor_enabled) {
+			spin_lock_irqsave(&ppol->target_freq_lock, flags);
+			if (policy->max < ppol->target_freq)
+				ppol->target_freq = policy->max;
+			else if (policy->min >= ppol->target_freq)
+				ppol->target_freq = policy->min;
+			spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+
+			if (policy->min < ppol->min_freq)
+				cpufreq_impulse_timer_resched(policy->cpu,
+								  true);
+			ppol->min_freq = policy->min;
+		}
+
+		up_read(&ppol->enable_sem);
+
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_IMPULSE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_impulse = {
+	.name = "impulse",
+	.governor = cpufreq_governor_impulse,
+	.max_transition_latency = 10000000,
+	.owner = THIS_MODULE,
+};
+
+static int __init cpufreq_impulse_init(void)
+{
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+
+	spin_lock_init(&speedchange_cpumask_lock);
+	mutex_init(&gov_lock);
+	mutex_init(&sched_lock);
+	speedchange_task =
+		kthread_create(cpufreq_impulse_speedchange_task, NULL,
+			       "cfimpulse");
+	if (IS_ERR(speedchange_task))
+		return PTR_ERR(speedchange_task);
+
+	sched_setscheduler_nocheck(speedchange_task, SCHED_FIFO, &param);
+	get_task_struct(speedchange_task);
+
+	/* NB: wake up so the thread does not look hung to the freezer */
+	wake_up_process(speedchange_task);
+
+	return cpufreq_register_governor(&cpufreq_gov_impulse);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_IMPULSE
+fs_initcall(cpufreq_impulse_init);
+#else
+module_init(cpufreq_impulse_init);
+#endif
+
+static void __exit cpufreq_impulse_exit(void)
+{
+	int cpu;
+
+	cpufreq_unregister_governor(&cpufreq_gov_impulse);
+	kthread_stop(speedchange_task);
+	put_task_struct(speedchange_task);
+
+	for_each_possible_cpu(cpu)
+		free_policyinfo(cpu);
+}
+
+module_exit(cpufreq_impulse_exit);
+
+MODULE_AUTHOR("Pranav Vashi <neobuddy89@gmail.com>");
+MODULE_DESCRIPTION("'cpufreq_impulse' - A cpufreq governor for "
+	"Latency sensitive workloads");
+MODULE_LICENSE("GPLv2");
\ No newline at end of file
diff --git a/drivers/cpufreq/cpufreq_intelliactive.c b/drivers/cpufreq/cpufreq_intelliactive.c
new file mode 100755
index 0000000..963c08a
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_intelliactive.c
@@ -0,0 +1,1439 @@
+/*
+ * drivers/cpufreq/cpufreq_intelliactive.c
+ *
+ * Copyright (C) 2010 Google, Inc.
+ * Copyright (C) 2014 Paul Reioux
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * Author: Mike Chan (mike@android.com)
+ * Author: Paul Reioux (reioux@gmail.com) Modified for intelliactive
+ */
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/rwsem.h>
+#include <linux/sched.h>
+#include <linux/sched/rt.h>
+#include <linux/tick.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <asm/cputime.h>
+
+static int active_count;
+
+struct cpufreq_interactive_cpuinfo {
+	struct timer_list cpu_timer;
+	struct timer_list cpu_slack_timer;
+	spinlock_t load_lock; /* protects the next 4 fields */
+	u64 time_in_idle;
+	u64 time_in_idle_timestamp;
+	u64 cputime_speedadj;
+	u64 cputime_speedadj_timestamp;
+	struct cpufreq_policy *policy;
+	struct cpufreq_frequency_table *freq_table;
+	spinlock_t target_freq_lock; /*protects target freq */
+	unsigned int target_freq;
+	unsigned int floor_freq;
+	u64 floor_validate_time;
+	u64 hispeed_validate_time; /* cluster hispeed_validate_time */
+	struct rw_semaphore enable_sem;
+	int governor_enabled;
+	int prev_load;
+	unsigned int two_phase_freq;
+};
+
+static DEFINE_PER_CPU(struct cpufreq_interactive_cpuinfo, cpuinfo);
+
+/* realtime thread handles frequency scaling */
+static struct task_struct *speedchange_task;
+static cpumask_t speedchange_cpumask;
+static spinlock_t speedchange_cpumask_lock;
+static struct mutex gov_lock;
+
+/* Hi speed to bump to from lo speed when load burst (default max) */
+static unsigned int hispeed_freq = 1574400;
+
+/* Go to hi speed when CPU load at or above this value. */
+#define DEFAULT_GO_HISPEED_LOAD 99
+static unsigned long go_hispeed_load = DEFAULT_GO_HISPEED_LOAD;
+
+/* Sampling down factor to be applied to min_sample_time at max freq */
+static unsigned int sampling_down_factor = 1;
+
+/* Target load.  Lower values result in higher CPU speeds. */
+#define DEFAULT_TARGET_LOAD 80
+static unsigned int default_target_loads[] = {DEFAULT_TARGET_LOAD};
+static spinlock_t target_loads_lock;
+static unsigned int *target_loads = default_target_loads;
+static int ntarget_loads = ARRAY_SIZE(default_target_loads);
+
+/*
+ * The minimum amount of time to spend at a frequency before we can ramp down.
+ */
+#define DEFAULT_MIN_SAMPLE_TIME (50 * USEC_PER_MSEC)
+static unsigned long min_sample_time = DEFAULT_MIN_SAMPLE_TIME;
+
+/*
+ * The sample rate of the timer used to increase frequency
+ */
+#define DEFAULT_TIMER_RATE (20 * USEC_PER_MSEC)
+static unsigned long timer_rate = DEFAULT_TIMER_RATE;
+
+/* Busy SDF parameters*/
+#define MIN_BUSY_TIME (100 * USEC_PER_MSEC)
+
+/*
+ * Wait this long before raising speed above hispeed, by default a single
+ * timer interval.
+ */
+#define DEFAULT_ABOVE_HISPEED_DELAY DEFAULT_TIMER_RATE
+static unsigned int default_above_hispeed_delay[] = {
+	DEFAULT_ABOVE_HISPEED_DELAY };
+static spinlock_t above_hispeed_delay_lock;
+static unsigned int *above_hispeed_delay = default_above_hispeed_delay;
+static int nabove_hispeed_delay = ARRAY_SIZE(default_above_hispeed_delay);
+
+/* Non-zero means indefinite speed boost active */
+static int boost_val;
+/* Duration of a boot pulse in usecs */
+static int boostpulse_duration_val = DEFAULT_MIN_SAMPLE_TIME;
+/* End time of boost pulse in ktime converted to usecs */
+static u64 boostpulse_endtime;
+
+/*
+ * Max additional time to wait in idle, beyond timer_rate, at speeds above
+ * minimum before wakeup to reduce speed, or -1 if unnecessary.
+ */
+#define DEFAULT_TIMER_SLACK (4 * DEFAULT_TIMER_RATE)
+static int timer_slack_val = DEFAULT_TIMER_SLACK;
+
+/*
+ * If the max load among other CPUs is higher than up_threshold_any_cpu_load
+ * or if the highest frequency among the other CPUs is higher than
+ * up_threshold_any_cpu_freq then do not let the frequency to drop below
+ * sync_freq
+ */
+static unsigned int up_threshold_any_cpu_load = 80;
+static unsigned int sync_freq = 1574400;
+static unsigned int up_threshold_any_cpu_freq = 1574400;
+
+static int two_phase_freq_array[NR_CPUS] = {[0 ... NR_CPUS-1] = 1958400} ;
+
+static void cpufreq_interactive_timer_resched(
+	struct cpufreq_interactive_cpuinfo *pcpu)
+{
+	unsigned long expires;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->time_in_idle =
+		get_cpu_idle_time(smp_processor_id(),
+				     &pcpu->time_in_idle_timestamp, 0);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	expires = jiffies + usecs_to_jiffies(timer_rate);
+	mod_timer_pinned(&pcpu->cpu_timer, expires);
+
+	if (timer_slack_val >= 0 && pcpu->target_freq > pcpu->policy->min) {
+		expires += usecs_to_jiffies(timer_slack_val);
+		mod_timer_pinned(&pcpu->cpu_slack_timer, expires);
+	}
+
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+/* The caller shall take enable_sem write semaphore to avoid any timer race.
+ * The cpu_timer and cpu_slack_timer must be deactivated when calling this
+ * function.
+ */
+static void cpufreq_interactive_timer_start(int cpu)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	unsigned long expires = jiffies + usecs_to_jiffies(timer_rate);
+	unsigned long flags;
+
+	pcpu->cpu_timer.expires = expires;
+	if (cpu_online(cpu)) {
+		add_timer_on(&pcpu->cpu_timer, cpu);
+		if (timer_slack_val >= 0 &&
+				pcpu->target_freq > pcpu->policy->min) {
+			expires += usecs_to_jiffies(timer_slack_val);
+			pcpu->cpu_slack_timer.expires = expires;
+			add_timer_on(&pcpu->cpu_slack_timer, cpu);
+		}
+	}
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->time_in_idle =
+		get_cpu_idle_time(cpu, &pcpu->time_in_idle_timestamp,
+				0);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+static unsigned int freq_to_above_hispeed_delay(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay - 1 &&
+			freq >= above_hispeed_delay[i+1]; i += 2)
+		;
+
+	ret = above_hispeed_delay[i];
+	ret = (ret > (1 * USEC_PER_MSEC)) ? (ret - (1 * USEC_PER_MSEC)) : ret;
+
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static unsigned int freq_to_targetload(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+
+	for (i = 0; i < ntarget_loads - 1 && freq >= target_loads[i+1]; i += 2)
+		;
+
+	ret = target_loads[i];
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+/*
+ * If increasing frequencies never map to a lower target load then
+ * choose_freq() will find the minimum frequency that does not exceed its
+ * target load given the current load.
+ */
+
+static unsigned int choose_freq(
+	struct cpufreq_interactive_cpuinfo *pcpu, unsigned int loadadjfreq)
+{
+	unsigned int freq = pcpu->policy->cur;
+	unsigned int prevfreq, freqmin, freqmax;
+	unsigned int tl;
+	int index;
+
+	freqmin = 0;
+	freqmax = UINT_MAX;
+
+	do {
+		prevfreq = freq;
+		tl = freq_to_targetload(freq);
+
+		/*
+		 * Find the lowest frequency where the computed load is less
+		 * than or equal to the target load.
+		 */
+
+		if (cpufreq_frequency_table_target(
+			    pcpu->policy, pcpu->freq_table, loadadjfreq / tl,
+			    CPUFREQ_RELATION_L, &index))
+			break;
+		freq = pcpu->freq_table[index].frequency;
+
+		if (freq > prevfreq) {
+			/* The previous frequency is too low. */
+			freqmin = prevfreq;
+
+			if (freq >= freqmax) {
+				/*
+				 * Find the highest frequency that is less
+				 * than freqmax.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmax - 1, CPUFREQ_RELATION_H,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				if (freq == freqmin) {
+					/*
+					 * The first frequency below freqmax
+					 * has already been found to be too
+					 * low.  freqmax is the lowest speed
+					 * we found that is fast enough.
+					 */
+					freq = freqmax;
+					break;
+				}
+			}
+		} else if (freq < prevfreq) {
+			/* The previous frequency is high enough. */
+			freqmax = prevfreq;
+
+			if (freq <= freqmin) {
+				/*
+				 * Find the lowest frequency that is higher
+				 * than freqmin.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmin + 1, CPUFREQ_RELATION_L,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				/*
+				 * If freqmax is the first frequency above
+				 * freqmin then we have already found that
+				 * this speed is fast enough.
+				 */
+				if (freq == freqmax)
+					break;
+			}
+		}
+
+		/* If same frequency chosen as previous then done. */
+	} while (freq != prevfreq);
+
+	return freq;
+}
+
+static u64 update_load(int cpu)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 now;
+	u64 now_idle;
+	unsigned int delta_idle;
+	unsigned int delta_time;
+	u64 active_time;
+
+	now_idle = get_cpu_idle_time(cpu, &now, 0);
+	delta_idle = (unsigned int)(now_idle - pcpu->time_in_idle);
+	delta_time = (unsigned int)(now - pcpu->time_in_idle_timestamp);
+
+	if (delta_time <= delta_idle)
+		active_time = 0;
+	else
+		active_time = delta_time - delta_idle;
+
+	pcpu->cputime_speedadj += active_time * pcpu->policy->cur;
+
+	pcpu->time_in_idle = now_idle;
+	pcpu->time_in_idle_timestamp = now;
+	return now;
+}
+
+static void cpufreq_interactive_timer(unsigned long data)
+{
+	u64 now;
+	unsigned int delta_time;
+	u64 cputime_speedadj;
+	int cpu_load;
+	struct cpufreq_interactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, data);
+	unsigned int new_freq;
+	unsigned int loadadjfreq;
+	unsigned int index;
+	unsigned long flags;
+	bool boosted;
+	unsigned long mod_min_sample_time;
+	int i, max_load;
+	unsigned int max_freq;
+	struct cpufreq_interactive_cpuinfo *picpu;
+	static unsigned int phase = 0;
+	static unsigned int counter = 0;
+	unsigned int nr_cpus;
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled)
+		goto exit;
+
+	if (cpu_is_offline(data))
+		goto exit;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	now = update_load(data);
+	delta_time = (unsigned int)(now - pcpu->cputime_speedadj_timestamp);
+	cputime_speedadj = pcpu->cputime_speedadj;
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+
+	if (WARN_ON_ONCE(!delta_time))
+		goto rearm;
+
+	spin_lock_irqsave(&pcpu->target_freq_lock, flags);
+	do_div(cputime_speedadj, delta_time);
+	loadadjfreq = (unsigned int)cputime_speedadj * 100;
+	cpu_load = loadadjfreq / pcpu->target_freq;
+	pcpu->prev_load = cpu_load;
+	boosted = boost_val || now < boostpulse_endtime;
+
+	cpufreq_notify_utilization(pcpu->policy, cpu_load);
+
+	if (counter < 5) {
+		counter++;
+		if (counter > 2) {
+			phase = 1;
+		}
+	}
+
+	if (cpu_load >= go_hispeed_load || boosted) {
+		if (pcpu->target_freq < hispeed_freq) {
+			nr_cpus = num_online_cpus();
+
+			pcpu->two_phase_freq = two_phase_freq_array[nr_cpus-1];
+			if (pcpu->two_phase_freq < pcpu->policy->cur)
+				phase = 1;
+			if (pcpu->two_phase_freq != 0 && phase == 0) {
+				new_freq = pcpu->two_phase_freq;
+			} else
+				new_freq = hispeed_freq;
+		} else {
+			new_freq = choose_freq(pcpu, loadadjfreq);
+
+			if (new_freq < hispeed_freq)
+				new_freq = hispeed_freq;
+		}
+	} else {
+		new_freq = choose_freq(pcpu, loadadjfreq);
+
+		if (sync_freq && new_freq < sync_freq) {
+
+			max_load = 0;
+			max_freq = 0;
+
+			for_each_online_cpu(i) {
+				picpu = &per_cpu(cpuinfo, i);
+
+				if (i == data || picpu->prev_load <
+						up_threshold_any_cpu_load)
+					continue;
+
+				max_load = max(max_load, picpu->prev_load);
+				max_freq = max(max_freq, picpu->target_freq);
+			}
+
+			if (max_freq > up_threshold_any_cpu_freq ||
+				max_load >= up_threshold_any_cpu_load)
+				new_freq = sync_freq;
+		}
+	}
+
+	if (counter > 0) {
+		counter--;
+		if (counter == 0) {
+			phase = 0;
+		}
+	}
+
+	if (pcpu->target_freq >= hispeed_freq &&
+	    new_freq > pcpu->target_freq &&
+	    now - pcpu->hispeed_validate_time <
+	    freq_to_above_hispeed_delay(pcpu->target_freq)) {
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	pcpu->hispeed_validate_time = now;
+
+	if (cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table,
+					   new_freq, CPUFREQ_RELATION_H,
+					   &index)) {
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	new_freq = pcpu->freq_table[index].frequency;
+
+	/*
+	 * Do not scale below floor_freq unless we have been at or above the
+	 * floor frequency for the minimum sample time since last validated.
+	 */
+	if (sampling_down_factor && pcpu->policy->cur == pcpu->policy->max)
+		mod_min_sample_time = sampling_down_factor;
+	else
+		mod_min_sample_time = min_sample_time;
+
+	if (new_freq < pcpu->floor_freq) {
+		if (now - pcpu->floor_validate_time < mod_min_sample_time) {
+			spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+			goto rearm;
+		}
+	}
+
+	/*
+	 * Update the timestamp for checking whether speed has been held at
+	 * or above the selected frequency for a minimum of min_sample_time,
+	 * if not boosted to hispeed_freq.  If boosted to hispeed_freq then we
+	 * allow the speed to drop as soon as the boostpulse duration expires
+	 * (or the indefinite boost is turned off).
+	 */
+
+	if (!boosted || new_freq > hispeed_freq) {
+		pcpu->floor_freq = new_freq;
+		pcpu->floor_validate_time = now;
+	}
+
+	if (pcpu->target_freq == new_freq &&
+			pcpu->target_freq <= pcpu->policy->cur) {
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm_if_notmax;
+	}
+
+	pcpu->target_freq = new_freq;
+	spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+	cpumask_set_cpu(data, &speedchange_cpumask);
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+	wake_up_process(speedchange_task);
+
+rearm_if_notmax:
+	/*
+	 * Already set max speed and don't see a need to change that,
+	 * wait until next idle to re-evaluate, don't need timer.
+	 */
+	if (pcpu->target_freq == pcpu->policy->max)
+		goto exit;
+
+rearm:
+	if (!timer_pending(&pcpu->cpu_timer))
+		cpufreq_interactive_timer_resched(pcpu);
+
+exit:
+	up_read(&pcpu->enable_sem);
+	return;
+}
+
+static void cpufreq_interactive_idle_start(void)
+{
+	int cpu = smp_processor_id();
+	struct cpufreq_interactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	int pending;
+	u64 now;
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled)
+		goto exit;
+
+	/* Cancel the timer if cpu is offline */
+	if (cpu_is_offline(cpu)) {
+		del_timer(&pcpu->cpu_timer);
+		del_timer(&pcpu->cpu_slack_timer);
+		goto exit;
+	}
+
+	pending = timer_pending(&pcpu->cpu_timer);
+
+	if (pcpu->target_freq != pcpu->policy->min) {
+		/*
+		 * Entering idle while not at lowest speed.  On some
+		 * platforms this can hold the other CPU(s) at that speed
+		 * even though the CPU is idle. Set a timer to re-evaluate
+		 * speed so this idle CPU doesn't hold the other CPUs above
+		 * min indefinitely.  This should probably be a quirk of
+		 * the CPUFreq driver.
+		 */
+		if (!pending) {
+			cpufreq_interactive_timer_resched(pcpu);
+
+			now = ktime_to_us(ktime_get());
+			if ((pcpu->policy->cur == pcpu->policy->max) &&
+				(now - pcpu->hispeed_validate_time) >
+							MIN_BUSY_TIME) {
+				pcpu->floor_validate_time = now;
+			}
+
+		}
+	}
+
+exit:
+	up_read(&pcpu->enable_sem);
+}
+
+static void cpufreq_interactive_idle_end(void)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled) {
+		up_read(&pcpu->enable_sem);
+		return;
+	}
+
+	/* Arm the timer for 1-2 ticks later if not already. */
+	if (!timer_pending(&pcpu->cpu_timer)) {
+		cpufreq_interactive_timer_resched(pcpu);
+	} else if (time_after_eq(jiffies, pcpu->cpu_timer.expires)) {
+		del_timer(&pcpu->cpu_timer);
+		del_timer(&pcpu->cpu_slack_timer);
+		cpufreq_interactive_timer(smp_processor_id());
+	}
+
+	up_read(&pcpu->enable_sem);
+}
+
+static int cpufreq_interactive_speedchange_task(void *data)
+{
+	unsigned int cpu;
+	cpumask_t tmp_mask;
+	unsigned long flags;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+		if (cpumask_empty(&speedchange_cpumask)) {
+			spin_unlock_irqrestore(&speedchange_cpumask_lock,
+					       flags);
+			schedule();
+
+			if (kthread_should_stop())
+				break;
+
+			spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+		}
+
+		set_current_state(TASK_RUNNING);
+		tmp_mask = speedchange_cpumask;
+		cpumask_clear(&speedchange_cpumask);
+		spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+		for_each_cpu(cpu, &tmp_mask) {
+			unsigned int j;
+			unsigned int max_freq = 0;
+
+			pcpu = &per_cpu(cpuinfo, cpu);
+			if (!down_read_trylock(&pcpu->enable_sem))
+				continue;
+			if (!pcpu->governor_enabled) {
+				up_read(&pcpu->enable_sem);
+				continue;
+			}
+
+			for_each_cpu(j, pcpu->policy->cpus) {
+				struct cpufreq_interactive_cpuinfo *pjcpu =
+					&per_cpu(cpuinfo, j);
+
+				if (pjcpu->target_freq > max_freq)
+					max_freq = pjcpu->target_freq;
+			}
+
+			if (max_freq != pcpu->policy->cur) {
+					__cpufreq_driver_target(pcpu->policy,
+								max_freq,
+								CPUFREQ_RELATION_H);
+			}
+
+			up_read(&pcpu->enable_sem);
+		}
+	}
+
+	return 0;
+}
+
+static void cpufreq_interactive_boost(void)
+{
+	int i;
+	int anyboost = 0;
+	unsigned long flags[2];
+	struct cpufreq_interactive_cpuinfo *pcpu;
+
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags[0]);
+
+	for_each_online_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+
+		spin_lock_irqsave(&pcpu->target_freq_lock, flags[1]);
+		if (pcpu->target_freq < hispeed_freq) {
+			pcpu->target_freq = hispeed_freq;
+			cpumask_set_cpu(i, &speedchange_cpumask);
+			pcpu->hispeed_validate_time =
+				ktime_to_us(ktime_get());
+			anyboost = 1;
+		}
+
+		/*
+		 * Set floor freq and (re)start timer for when last
+		 * validated.
+		 */
+
+		pcpu->floor_freq = hispeed_freq;
+		pcpu->floor_validate_time = ktime_to_us(ktime_get());
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags[1]);
+	}
+
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags[0]);
+
+	if (anyboost)
+		wake_up_process(speedchange_task);
+}
+
+static int cpufreq_interactive_notifier(
+	struct notifier_block *nb, unsigned long val, void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+	int cpu;
+	unsigned long flags;
+
+	if (val == CPUFREQ_POSTCHANGE) {
+		pcpu = &per_cpu(cpuinfo, freq->cpu);
+		if (!down_read_trylock(&pcpu->enable_sem))
+			return 0;
+		if (!pcpu->governor_enabled) {
+			up_read(&pcpu->enable_sem);
+			return 0;
+		}
+
+		for_each_cpu(cpu, pcpu->policy->cpus) {
+			struct cpufreq_interactive_cpuinfo *pjcpu =
+				&per_cpu(cpuinfo, cpu);
+			if (cpu != freq->cpu) {
+				if (!down_read_trylock(&pjcpu->enable_sem))
+					continue;
+				if (!pjcpu->governor_enabled) {
+					up_read(&pjcpu->enable_sem);
+					continue;
+				}
+			}
+			spin_lock_irqsave(&pjcpu->load_lock, flags);
+			update_load(cpu);
+			spin_unlock_irqrestore(&pjcpu->load_lock, flags);
+			if (cpu != freq->cpu)
+				up_read(&pjcpu->enable_sem);
+		}
+
+		up_read(&pcpu->enable_sem);
+	}
+	return 0;
+}
+
+static struct notifier_block cpufreq_notifier_block = {
+	.notifier_call = cpufreq_interactive_notifier,
+};
+
+static unsigned int *get_tokenized_data(const char *buf, int *num_tokens)
+{
+	const char *cp;
+	int i;
+	int ntokens = 1;
+	int *tokenized_data;
+	int err = -EINVAL;
+
+	cp = buf;
+	while ((cp = strpbrk(cp + 1, " :")))
+		ntokens++;
+
+	if (!(ntokens & 0x1))
+		goto err;
+
+	tokenized_data = kmalloc(ntokens * sizeof(int), GFP_KERNEL);
+	if (!tokenized_data) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	cp = buf;
+	i = 0;
+	while (i < ntokens) {
+		if (sscanf(cp, "%d", &tokenized_data[i++]) != 1)
+			goto err_kfree;
+
+		cp = strpbrk(cp, " :");
+		if (!cp)
+			break;
+		cp++;
+	}
+
+	if (i != ntokens)
+		goto err_kfree;
+
+	*num_tokens = ntokens;
+	return tokenized_data;
+
+err_kfree:
+	kfree(tokenized_data);
+err:
+	return ERR_PTR(err);
+}
+
+static ssize_t show_two_phase_freq
+(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i = 0 ;
+	int shift = 0 ;
+	char *buf_pos = buf;
+	for ( i = 0 ; i < NR_CPUS; i++) {
+		shift = sprintf(buf_pos,"%d,",two_phase_freq_array[i]);
+		buf_pos += shift;
+	}
+	*(buf_pos-1) = '\0';
+	return strlen(buf);
+}
+
+static ssize_t store_two_phase_freq(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count)
+{
+
+	int ret = 0;
+	if (NR_CPUS == 1)
+		ret = sscanf(buf,"%u",&two_phase_freq_array[0]);
+	else if (NR_CPUS == 2)
+		ret = sscanf(buf,"%u,%u",&two_phase_freq_array[0],
+				&two_phase_freq_array[1]);
+	else if (NR_CPUS == 4)
+		ret = sscanf(buf, "%u,%u,%u,%u", &two_phase_freq_array[0],
+				&two_phase_freq_array[1],
+				&two_phase_freq_array[2],
+				&two_phase_freq_array[3]);
+	if (ret < NR_CPUS)
+		return -EINVAL;
+
+	return count;
+}
+
+static struct global_attr two_phase_freq_attr =
+	__ATTR(two_phase_freq, S_IRUGO | S_IWUSR,
+		show_two_phase_freq, store_two_phase_freq);
+
+static ssize_t show_target_loads(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+
+	for (i = 0; i < ntarget_loads; i++)
+		ret += sprintf(buf + ret, "%u%s", target_loads[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+static ssize_t store_target_loads(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_target_loads = NULL;
+	unsigned long flags;
+
+	new_target_loads = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_target_loads))
+		return PTR_RET(new_target_loads);
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+	if (target_loads != default_target_loads)
+		kfree(target_loads);
+	target_loads = new_target_loads;
+	ntarget_loads = ntokens;
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return count;
+}
+
+static struct global_attr target_loads_attr =
+	__ATTR(target_loads, S_IRUGO | S_IWUSR,
+		show_target_loads, store_target_loads);
+
+static ssize_t show_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay; i++)
+		ret += sprintf(buf + ret, "%u%s", above_hispeed_delay[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static ssize_t store_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_above_hispeed_delay = NULL;
+	unsigned long flags;
+
+	new_above_hispeed_delay = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_above_hispeed_delay))
+		return PTR_RET(new_above_hispeed_delay);
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+	if (above_hispeed_delay != default_above_hispeed_delay)
+		kfree(above_hispeed_delay);
+	above_hispeed_delay = new_above_hispeed_delay;
+	nabove_hispeed_delay = ntokens;
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return count;
+}
+
+static struct global_attr above_hispeed_delay_attr =
+	__ATTR(above_hispeed_delay, S_IRUGO | S_IWUSR,
+		show_above_hispeed_delay, store_above_hispeed_delay);
+
+static ssize_t show_hispeed_freq(struct kobject *kobj,
+				 struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", hispeed_freq);
+}
+
+static ssize_t store_hispeed_freq(struct kobject *kobj,
+				  struct attribute *attr, const char *buf,
+				  size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	hispeed_freq = val;
+	return count;
+}
+
+static struct global_attr hispeed_freq_attr = __ATTR(hispeed_freq, 0644,
+		show_hispeed_freq, store_hispeed_freq);
+
+static ssize_t show_sampling_down_factor(struct kobject *kobj,
+				struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sampling_down_factor);
+}
+
+static ssize_t store_sampling_down_factor(struct kobject *kobj,
+				struct attribute *attr, const char *buf,
+				size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	if (val > 3)
+		val = 3;
+	sampling_down_factor = val;
+	return count;
+}
+
+static struct global_attr sampling_down_factor_attr =
+				__ATTR(sampling_down_factor, 0644,
+		show_sampling_down_factor, store_sampling_down_factor);
+
+static ssize_t show_go_hispeed_load(struct kobject *kobj,
+				     struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", go_hispeed_load);
+}
+
+static ssize_t store_go_hispeed_load(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	go_hispeed_load = val;
+	return count;
+}
+
+static struct global_attr go_hispeed_load_attr = __ATTR(go_hispeed_load, 0644,
+		show_go_hispeed_load, store_go_hispeed_load);
+
+static ssize_t show_min_sample_time(struct kobject *kobj,
+				struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", min_sample_time);
+}
+
+static ssize_t store_min_sample_time(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	min_sample_time = val;
+	return count;
+}
+
+static struct global_attr min_sample_time_attr = __ATTR(min_sample_time, 0644,
+		show_min_sample_time, store_min_sample_time);
+
+static ssize_t show_timer_rate(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", timer_rate);
+}
+
+static ssize_t store_timer_rate(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val, val_round;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	val_round = jiffies_to_usecs(usecs_to_jiffies(val));
+	if (val != val_round)
+		pr_warn("timer_rate not aligned to jiffy. Rounded up to %lu\n",
+				val_round);
+
+	timer_rate = val_round;
+	return count;
+}
+
+static struct global_attr timer_rate_attr = __ATTR(timer_rate, 0644,
+		show_timer_rate, store_timer_rate);
+
+static ssize_t show_timer_slack(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", timer_slack_val);
+}
+
+static ssize_t store_timer_slack(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret < 0)
+		return ret;
+
+	timer_slack_val = val;
+	return count;
+}
+
+define_one_global_rw(timer_slack);
+
+static ssize_t show_boost(struct kobject *kobj, struct attribute *attr,
+			  char *buf)
+{
+	return sprintf(buf, "%d\n", boost_val);
+}
+
+static ssize_t store_boost(struct kobject *kobj, struct attribute *attr,
+			   const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boost_val = val;
+
+	if (boost_val)
+		cpufreq_interactive_boost();
+
+	return count;
+}
+
+define_one_global_rw(boost);
+
+static ssize_t show_boostpulse(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", (unsigned)boostpulse_endtime);
+}
+
+static ssize_t store_boostpulse(struct kobject *kobj, struct attribute *attr,
+				const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_endtime = ktime_to_us(ktime_get()) + boostpulse_duration_val;
+	cpufreq_interactive_boost();
+	return count;
+}
+
+static struct global_attr boostpulse =
+	__ATTR(boostpulse, 0644, show_boostpulse, store_boostpulse);
+
+static ssize_t show_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", boostpulse_duration_val);
+}
+
+static ssize_t store_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_duration_val = val;
+	return count;
+}
+
+define_one_global_rw(boostpulse_duration);
+
+static ssize_t show_sync_freq(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sync_freq);
+}
+
+static ssize_t store_sync_freq(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret = 0;
+	unsigned long val;
+	int mpd = strcmp(current->comm, "mpdecision");
+
+	if (mpd == 0)
+		return ret;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	sync_freq = val;
+	return count;
+}
+
+static struct global_attr sync_freq_attr = __ATTR(sync_freq, 0644,
+		show_sync_freq, store_sync_freq);
+
+static ssize_t show_up_threshold_any_cpu_load(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", up_threshold_any_cpu_load);
+}
+
+static ssize_t store_up_threshold_any_cpu_load(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	up_threshold_any_cpu_load = val;
+	return count;
+}
+
+static struct global_attr up_threshold_any_cpu_load_attr =
+		__ATTR(up_threshold_any_cpu_load, 0644,
+		show_up_threshold_any_cpu_load,
+				store_up_threshold_any_cpu_load);
+
+static ssize_t show_up_threshold_any_cpu_freq(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", up_threshold_any_cpu_freq);
+}
+
+static ssize_t store_up_threshold_any_cpu_freq(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	up_threshold_any_cpu_freq = val;
+	return count;
+}
+
+static struct global_attr up_threshold_any_cpu_freq_attr =
+		__ATTR(up_threshold_any_cpu_freq, 0644,
+		show_up_threshold_any_cpu_freq,
+				store_up_threshold_any_cpu_freq);
+
+static struct attribute *interactive_attributes[] = {
+	&target_loads_attr.attr,
+	&above_hispeed_delay_attr.attr,
+	&hispeed_freq_attr.attr,
+	&go_hispeed_load_attr.attr,
+	&min_sample_time_attr.attr,
+	&timer_rate_attr.attr,
+	&timer_slack.attr,
+	&boost.attr,
+	&boostpulse.attr,
+	&boostpulse_duration.attr,
+	&sampling_down_factor_attr.attr,
+	&sync_freq_attr.attr,
+	&up_threshold_any_cpu_load_attr.attr,
+	&up_threshold_any_cpu_freq_attr.attr,
+	&two_phase_freq_attr.attr,
+	NULL,
+};
+
+static struct attribute_group interactive_attr_group = {
+	.attrs = interactive_attributes,
+	.name = "intelliactive",
+};
+
+static int cpufreq_interactive_idle_notifier(struct notifier_block *nb,
+					     unsigned long val,
+					     void *data)
+{
+	switch (val) {
+	case IDLE_START:
+		cpufreq_interactive_idle_start();
+		break;
+	case IDLE_END:
+		cpufreq_interactive_idle_end();
+		break;
+	}
+
+	return 0;
+}
+
+static struct notifier_block cpufreq_interactive_idle_nb = {
+	.notifier_call = cpufreq_interactive_idle_notifier,
+};
+
+static int cpufreq_governor_intelliactive(struct cpufreq_policy *policy,
+		unsigned int event)
+{
+	int rc;
+	unsigned int j;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned long flags;
+	unsigned int cpu = policy->cpu;
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy))
+			return -EINVAL;
+
+		mutex_lock(&gov_lock);
+
+		freq_table =
+			cpufreq_frequency_get_table(cpu);
+		if (!hispeed_freq)
+			hispeed_freq = policy->max;
+
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			pcpu->policy = policy;
+			pcpu->target_freq = policy->cur;
+			pcpu->freq_table = freq_table;
+			pcpu->floor_freq = pcpu->target_freq;
+			pcpu->floor_validate_time =
+				ktime_to_us(ktime_get());
+			pcpu->hispeed_validate_time =
+				pcpu->floor_validate_time;
+			down_write(&pcpu->enable_sem);
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			if (cpu_online(j))
+				cpufreq_interactive_timer_start(j);
+			pcpu->governor_enabled = 1;
+			up_write(&pcpu->enable_sem);
+		}
+
+		/*
+		 * Do not register the idle hook and create sysfs
+		 * entries if we have already done so.
+		 */
+		if (++active_count > 1) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		rc = sysfs_create_group(cpufreq_global_kobject,
+				&interactive_attr_group);
+		if (rc) {
+			mutex_unlock(&gov_lock);
+			return rc;
+		}
+
+		idle_notifier_register(&cpufreq_interactive_idle_nb);
+		cpufreq_register_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		mutex_unlock(&gov_lock);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		mutex_lock(&gov_lock);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			down_write(&pcpu->enable_sem);
+			pcpu->governor_enabled = 0;
+			pcpu->target_freq = 0;
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			up_write(&pcpu->enable_sem);
+		}
+
+		if (--active_count > 0) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		cpufreq_unregister_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		idle_notifier_unregister(&cpufreq_interactive_idle_nb);
+		sysfs_remove_group(cpufreq_global_kobject,
+				&interactive_attr_group);
+		mutex_unlock(&gov_lock);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		/* If device is being removed, skip set limits */
+		if (!policy)
+			break;
+		__cpufreq_driver_target(policy,
+				policy->cur, CPUFREQ_RELATION_L);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+
+			/* hold write semaphore to avoid race */
+			down_write(&pcpu->enable_sem);
+			if (pcpu->governor_enabled == 0) {
+				up_write(&pcpu->enable_sem);
+				continue;
+			}
+
+			spin_lock_irqsave(&pcpu->target_freq_lock, flags);
+			/* update target_freq firstly */
+			if (policy->max < pcpu->target_freq)
+				pcpu->target_freq = policy->max;
+			else if (policy->min > pcpu->target_freq)
+				pcpu->target_freq = policy->min;
+
+			spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+			/* Reschedule timer.
+			 * Delete the timers, else the timer callback may
+			 * return without re-arm the timer when failed
+			 * acquire the semaphore. This race may cause timer
+			 * stopped unexpectedly.
+			 */
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			cpufreq_interactive_timer_start(j);
+			up_write(&pcpu->enable_sem);
+		}
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_intelliactive = {
+	.name = "intelliactive",
+	.governor = cpufreq_governor_intelliactive,
+	.max_transition_latency = 10000000,
+	.owner = THIS_MODULE,
+};
+
+static void cpufreq_interactive_nop_timer(unsigned long data)
+{
+}
+
+static int __init cpufreq_intelliactive_init(void)
+{
+	unsigned int i;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+
+	/* Initalize per-cpu timers */
+	for_each_possible_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+		init_timer_deferrable(&pcpu->cpu_timer);
+		pcpu->cpu_timer.function = cpufreq_interactive_timer;
+		pcpu->cpu_timer.data = i;
+		init_timer(&pcpu->cpu_slack_timer);
+		pcpu->cpu_slack_timer.function = cpufreq_interactive_nop_timer;
+		spin_lock_init(&pcpu->load_lock);
+		spin_lock_init(&pcpu->target_freq_lock);
+		init_rwsem(&pcpu->enable_sem);
+	}
+
+	spin_lock_init(&target_loads_lock);
+	spin_lock_init(&speedchange_cpumask_lock);
+	spin_lock_init(&above_hispeed_delay_lock);
+	mutex_init(&gov_lock);
+	speedchange_task =
+		kthread_create(cpufreq_interactive_speedchange_task, NULL,
+			       "cfintelliactive");
+	if (IS_ERR(speedchange_task))
+		return PTR_ERR(speedchange_task);
+
+	sched_setscheduler_nocheck(speedchange_task, SCHED_FIFO, &param);
+	get_task_struct(speedchange_task);
+
+	/* NB: wake up so the thread does not look hung to the freezer */
+	wake_up_process(speedchange_task);
+
+	return cpufreq_register_governor(&cpufreq_gov_intelliactive);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE
+fs_initcall(cpufreq_intelliactive_init);
+#else
+module_init(cpufreq_intelliactive_init);
+#endif
+
+static void __exit cpufreq_interactive_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_intelliactive);
+	kthread_stop(speedchange_task);
+	put_task_struct(speedchange_task);
+}
+
+module_exit(cpufreq_interactive_exit);
+
+MODULE_AUTHOR("Mike Chan <mike@android.com>");
+MODULE_AUTHOR("Paul Reioux <reioux@gmail.com>");
+MODULE_DESCRIPTION("'cpufreq_intelliactive' - A cpufreq governor for "
+	"Latency sensitive workloads based on Google's Interactive");
+MODULE_LICENSE("GPL");
diff --git a/drivers/cpufreq/cpufreq_intellidemand.c b/drivers/cpufreq/cpufreq_intellidemand.c
new file mode 100755
index 0000000..f676635
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_intellidemand.c
@@ -0,0 +1,1405 @@
+/*
+ *  drivers/cpufreq/cpufreq_intellidemand.c
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *            (C)  2013 The Linux Foundation. All rights reserved.
+ *            (C)  2013 Paul Reioux
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/workqueue.h>
+#include <linux/slab.h>
+
+#define INTELLIDEMAND_MAJOR_VERSION    5
+#define INTELLIDEMAND_MINOR_VERSION    5
+
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+#define DEF_SAMPLING_RATE			(20000)
+#define DEF_FREQUENCY_UP_THRESHOLD		(80)
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define MAX_SAMPLING_DOWN_FACTOR		(3)
+#define MICRO_FREQUENCY_UP_THRESHOLD		(95)
+#define MICRO_FREQUENCY_MIN_SAMPLE_RATE		(10000)
+#define MIN_FREQUENCY_UP_THRESHOLD		(11)
+#define MAX_FREQUENCY_UP_THRESHOLD		(100)
+#define MIN_FREQUENCY_DOWN_DIFFERENTIAL		(1)
+
+#define DEF_FREQ_STEP				(25)
+#define DEF_STEP_UP_EARLY_HISPEED		(1958400)
+#define DEF_STEP_UP_INTERIM_HISPEED		(2265600)
+#define DEF_SAMPLING_EARLY_HISPEED_FACTOR	(2)
+#define DEF_SAMPLING_INTERIM_HISPEED_FACTOR	(3)
+
+/* PATCH : SMART_UP */
+#define MIN(X, Y) ((X) < (Y) ? (X) : (Y))
+
+#define SMART_UP_PLUS (0)
+#define SMART_UP_SLOW_UP_AT_HIGH_FREQ (1)
+#define SUP_MAX_STEP (3)
+#define SUP_CORE_NUM (4)
+#define SUP_SLOW_UP_DUR (5)
+#define SUP_SLOW_UP_DUR_DEFAULT (2)
+
+#define SUP_HIGH_SLOW_UP_DUR (5)
+#define SUP_FREQ_LEVEL (14)
+
+#if defined(SMART_UP_PLUS)
+static unsigned int SUP_THRESHOLD_STEPS[SUP_MAX_STEP] = {85, 90, 95};
+static unsigned int SUP_FREQ_STEPS[SUP_MAX_STEP] = {4, 3, 2};
+typedef struct{
+	unsigned int freq_idx;
+	unsigned int freq_value;
+} freq_table_idx;
+static freq_table_idx pre_freq_idx[SUP_CORE_NUM] = {};
+
+#endif
+
+
+#if defined(SMART_UP_SLOW_UP_AT_HIGH_FREQ)
+
+#define SUP_SLOW_UP_FREQUENCY			(1728000)
+#define SUP_HIGH_SLOW_UP_FREQUENCY		(2265600)
+#define SUP_SLOW_UP_LOAD			(75)
+
+typedef struct {
+	unsigned int hist_max_load[SUP_SLOW_UP_DUR];
+	unsigned int hist_load_cnt;
+} history_load;
+static void reset_hist(history_load *hist_load);
+static history_load hist_load[SUP_CORE_NUM] = {};
+
+typedef struct {
+	unsigned int hist_max_load[SUP_HIGH_SLOW_UP_DUR];
+	unsigned int hist_load_cnt;
+} history_load_high;
+static void reset_hist_high(history_load_high *hist_load);
+static history_load_high hist_load_high[SUP_CORE_NUM] = {};
+
+#endif
+
+
+/*
+ * The polling frequency of this governor depends on the capability of
+ * the processor. Default polling frequency is 1000 times the transition
+ * latency of the processor. The governor will work on any processor with
+ * transition latency <= 10mS, using appropriate sampling
+ * rate.
+ * For CPUs with transition latency > 10mS (mostly drivers with CPUFREQ_ETERNAL)
+ * this governor will not work.
+ * All times here are in uS.
+ */
+#define MIN_SAMPLING_RATE_RATIO			(2)
+
+static unsigned int min_sampling_rate;
+
+#define LATENCY_MULTIPLIER			(1000)
+#define MIN_LATENCY_MULTIPLIER			(100)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+#define POWERSAVE_BIAS_MAXLEVEL			(1000)
+#define POWERSAVE_BIAS_MINLEVEL			(-1000)
+
+static void do_dbs_timer(struct work_struct *work);
+
+/* Sampling types */
+enum {DBS_NORMAL_SAMPLE, DBS_SUB_SAMPLE};
+
+struct cpu_dbs_info_s {
+	u64 prev_cpu_idle;
+	u64 prev_cpu_iowait;
+	u64 prev_cpu_wall;
+	u64 prev_cpu_nice;
+	struct cpufreq_policy *cur_policy;
+	struct delayed_work work;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int freq_lo;
+	unsigned int freq_lo_jiffies;
+	unsigned int freq_hi_jiffies;
+	unsigned int rate_mult;
+	unsigned int prev_load;
+	unsigned int max_load;
+	unsigned int cpu;
+	unsigned int sample_type:1;
+	unsigned int freq_stay_count;
+	/*
+	 * percpu mutex that serializes governor limit change with
+	 * do_dbs_timer invocation. We do not want do_dbs_timer to run
+	 * when user is changing the governor or limits.
+	 */
+	struct mutex timer_mutex;
+};
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, id_cpu_dbs_info);
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info);
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info);
+
+static unsigned int dbs_enable;	/* number of CPUs using this policy */
+
+/*
+ * dbs_mutex protects dbs_enable and dbs_info during start/stop.
+ */
+static DEFINE_MUTEX(dbs_mutex);
+
+static struct workqueue_struct *dbs_wq;
+
+static struct dbs_tuners {
+	unsigned int sampling_rate;
+	unsigned int up_threshold;
+	unsigned int up_threshold_multi_core;
+	unsigned int optimal_freq;
+	unsigned int up_threshold_any_cpu_load;
+	unsigned int sync_freq;
+	unsigned int sampling_down_factor;
+	/* 20130711 smart_up */
+	unsigned int smart_up;
+	unsigned int smart_slow_up_load;
+	unsigned int smart_slow_up_freq;
+	unsigned int smart_slow_up_dur;
+	unsigned int smart_high_slow_up_freq;
+	unsigned int smart_high_slow_up_dur;
+	unsigned int smart_each_off;
+	/* end smart_up */
+	unsigned int freq_step;
+	unsigned int step_up_early_hispeed;
+	unsigned int step_up_interim_hispeed;
+	unsigned int sampling_early_factor;
+	unsigned int sampling_interim_factor;
+	unsigned int two_phase_freq;
+} dbs_tuners_ins = {
+	.up_threshold_multi_core = DEF_FREQUENCY_UP_THRESHOLD,
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.up_threshold_any_cpu_load = DEF_FREQUENCY_UP_THRESHOLD,
+	.sync_freq = 1574400,
+	.optimal_freq = 1574400,
+	/* 20130711 smart_up */
+	.smart_up = SMART_UP_PLUS,
+	.smart_slow_up_load = SUP_SLOW_UP_LOAD,
+	.smart_slow_up_freq = SUP_SLOW_UP_FREQUENCY,
+	.smart_slow_up_dur = SUP_SLOW_UP_DUR_DEFAULT,
+	.smart_high_slow_up_freq = SUP_HIGH_SLOW_UP_FREQUENCY,
+	.smart_high_slow_up_dur = SUP_HIGH_SLOW_UP_DUR,
+	.smart_each_off = 0,
+	/* end smart_up */
+	.freq_step = DEF_FREQ_STEP,
+	.step_up_early_hispeed = DEF_STEP_UP_EARLY_HISPEED,
+	.step_up_interim_hispeed = DEF_STEP_UP_INTERIM_HISPEED,
+	.sampling_early_factor = DEF_SAMPLING_EARLY_HISPEED_FACTOR,
+	.sampling_interim_factor = DEF_SAMPLING_INTERIM_HISPEED_FACTOR,
+	.two_phase_freq = 0,
+	.sampling_rate = DEF_SAMPLING_RATE,
+};
+
+/************************** sysfs interface ************************/
+
+static ssize_t show_sampling_rate_min(struct kobject *kobj,
+				      struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", min_sampling_rate);
+}
+
+define_one_global_ro(sampling_rate_min);
+
+/* cpufreq_intellidemand Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)	      \
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(up_threshold, up_threshold);
+show_one(up_threshold_multi_core, up_threshold_multi_core);
+show_one(sampling_down_factor, sampling_down_factor);
+show_one(optimal_freq, optimal_freq);
+show_one(up_threshold_any_cpu_load, up_threshold_any_cpu_load);
+show_one(sync_freq, sync_freq);
+/* 20130711 smart_up */
+show_one(smart_up, smart_up);
+show_one(smart_slow_up_load, smart_slow_up_load);
+show_one(smart_slow_up_freq, smart_slow_up_freq);
+show_one(smart_slow_up_dur, smart_slow_up_dur);
+show_one(smart_high_slow_up_freq, smart_high_slow_up_freq);
+show_one(smart_high_slow_up_dur, smart_high_slow_up_dur);
+show_one(smart_each_off, smart_each_off);
+/* end smart_up */
+show_one(freq_step, freq_step);
+show_one(step_up_early_hispeed, step_up_early_hispeed);
+show_one(step_up_interim_hispeed, step_up_interim_hispeed);
+show_one(sampling_early_factor, sampling_early_factor);
+show_one(sampling_interim_factor, sampling_interim_factor);
+
+static int two_phase_freq_array[NR_CPUS] = {[0 ... NR_CPUS-1] = 1958400} ;
+
+static ssize_t show_two_phase_freq
+(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i = 0 ;
+	int shift = 0 ;
+	char *buf_pos = buf;
+	for ( i = 0 ; i < NR_CPUS; i++) {
+		shift = sprintf(buf_pos,"%d,",two_phase_freq_array[i]);
+		buf_pos += shift;
+	}
+	*(buf_pos-1) = '\0';
+	return strlen(buf);
+}
+
+static ssize_t store_two_phase_freq(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count)
+{
+
+	int ret = 0;
+	if (NR_CPUS == 1)
+		ret = sscanf(buf,"%u",&two_phase_freq_array[0]);
+	else if (NR_CPUS == 2)
+		ret = sscanf(buf,"%u,%u",&two_phase_freq_array[0],
+				&two_phase_freq_array[1]);
+	else if (NR_CPUS == 4)
+		ret = sscanf(buf, "%u,%u,%u,%u", &two_phase_freq_array[0],
+				&two_phase_freq_array[1],
+				&two_phase_freq_array[2],
+				&two_phase_freq_array[3]);
+	if (ret < NR_CPUS)
+		return -EINVAL;
+
+	return count;
+}
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret = 0;
+	int mpd = strcmp(current->comm, "mpdecision");
+
+	if (mpd == 0)
+		return ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_rate = max(input, min_sampling_rate);
+
+	return count;
+}
+
+static ssize_t store_sync_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret = 0;
+	int mpd = strcmp(current->comm, "mpdecision");
+
+	if (mpd == 0)
+		return ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sync_freq = input;
+
+	return count;
+}
+
+static ssize_t store_optimal_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret = 0;
+	int mpd = strcmp(current->comm, "mpdecision");
+
+	if (mpd == 0)
+		return ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.optimal_freq = input;
+
+	return count;
+}
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold = input;
+
+	return count;
+}
+
+static ssize_t store_up_threshold_multi_core(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold_multi_core = input;
+
+	return count;
+}
+
+static ssize_t store_up_threshold_any_cpu_load(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold_any_cpu_load = input;
+
+	return count;
+}
+
+static ssize_t store_sampling_down_factor(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input, j;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+	dbs_tuners_ins.sampling_down_factor = input;
+
+	/* Reset down sampling multiplier in case it was active */
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(id_cpu_dbs_info, j);
+		dbs_info->rate_mult = 1;
+	}
+	return count;
+}
+
+/* PATCH : SMART_UP */
+#if defined(SMART_UP_SLOW_UP_AT_HIGH_FREQ)
+static void reset_hist(history_load *hist_load)
+{
+	int i;
+
+	for (i = 0; i < SUP_SLOW_UP_DUR ; i++)
+		hist_load->hist_max_load[i] = 0;
+
+	hist_load->hist_load_cnt = 0;
+}
+
+
+static void reset_hist_high(history_load_high *hist_load)
+{	int i;
+
+	for (i = 0; i < SUP_HIGH_SLOW_UP_DUR ; i++)
+		hist_load->hist_max_load[i] = 0;
+
+	hist_load->hist_load_cnt = 0;
+}
+
+#endif
+
+/* 20130711 smart_up */
+static ssize_t store_smart_up(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int i, input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	if (input > 1) {
+		input = 1;
+	} else if (input < 0) {
+		input = 0;
+	}
+
+	/* buffer reset */
+	for_each_online_cpu(i) {
+		reset_hist(&hist_load[i]);
+		reset_hist_high(&hist_load_high[i]);
+	}
+	dbs_tuners_ins.smart_up = input;
+
+	return count;
+}
+
+static ssize_t store_smart_slow_up_load(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int i, input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	if (input > 100) {
+		input = 100;
+	} else if (input < 0) {
+		input = 0;
+	}
+
+        /* buffer reset */
+	for_each_online_cpu(i) {
+		reset_hist(&hist_load[i]);
+		reset_hist_high(&hist_load_high[i]);
+	}
+	dbs_tuners_ins.smart_slow_up_load = input;
+
+	return count;
+}
+
+static ssize_t store_smart_slow_up_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int i, input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	if (input < 0)
+		input = 0;
+
+	/* buffer reset */
+	for_each_online_cpu(i) {
+		reset_hist(&hist_load[i]);
+		reset_hist_high(&hist_load_high[i]);
+	}
+	dbs_tuners_ins.smart_slow_up_freq = input;
+
+	return count;
+}
+
+static ssize_t store_smart_slow_up_dur(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int i, input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	if (input > SUP_SLOW_UP_DUR) {
+		input = SUP_SLOW_UP_DUR;
+	} else if (input < 1) {
+		input = 1;
+	}
+
+	/* buffer reset */
+	for_each_online_cpu(i) {
+		reset_hist(&hist_load[i]);
+		reset_hist_high(&hist_load_high[i]);
+	}
+	dbs_tuners_ins.smart_slow_up_dur = input;
+
+	return count;
+}
+static ssize_t store_smart_high_slow_up_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	unsigned int i;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	if (input < 0)
+		input = 0;
+	/* buffer reset */
+	for_each_online_cpu(i) {
+		reset_hist(&hist_load[i]);
+		reset_hist_high(&hist_load_high[i]);
+	}
+	dbs_tuners_ins.smart_high_slow_up_freq = input;
+
+	return count;
+}
+static ssize_t store_smart_high_slow_up_dur(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	unsigned int i;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	if (input > SUP_HIGH_SLOW_UP_DUR ) {
+		input = SUP_HIGH_SLOW_UP_DUR;
+	}else if (input < 1 ) {
+		input = 1;
+	}
+	/* buffer reset */
+	for_each_online_cpu(i) {
+		reset_hist(&hist_load[i]);
+		reset_hist_high(&hist_load_high[i]);
+	}
+	dbs_tuners_ins.smart_high_slow_up_dur = input;
+
+	return count;
+}
+static ssize_t store_smart_each_off(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int i, input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	if (input > SUP_CORE_NUM) {
+		input = SUP_CORE_NUM;
+	} else if (input < 0) {
+		input = 0;
+	}
+
+	/* buffer reset */
+	for_each_online_cpu(i) {
+		reset_hist(&hist_load[i]);
+		reset_hist_high(&hist_load_high[i]);
+	}
+	dbs_tuners_ins.smart_each_off = input;
+
+	return count;
+}
+/* end smart_up */
+
+static ssize_t store_freq_step(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 ||
+			input < 0) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.freq_step = input;
+
+	return count;
+}
+
+static ssize_t store_step_up_early_hispeed(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 2265600 ||
+			input < 0) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.step_up_early_hispeed = input;
+
+	return count;
+}
+
+static ssize_t store_step_up_interim_hispeed(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > DEF_STEP_UP_INTERIM_HISPEED ||
+			input < 0) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.step_up_interim_hispeed = input;
+
+	return count;
+}
+
+static ssize_t store_sampling_early_factor(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 1)
+		return -EINVAL;
+	dbs_tuners_ins.sampling_early_factor = input;
+
+	return count;
+}
+
+static ssize_t store_sampling_interim_factor(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 1)
+		return -EINVAL;
+	dbs_tuners_ins.sampling_interim_factor = input;
+
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(up_threshold);
+define_one_global_rw(sampling_down_factor);
+define_one_global_rw(up_threshold_multi_core);
+define_one_global_rw(optimal_freq);
+define_one_global_rw(up_threshold_any_cpu_load);
+define_one_global_rw(sync_freq);
+/* 20130711 smart_up */
+define_one_global_rw(smart_up);
+define_one_global_rw(smart_slow_up_load);
+define_one_global_rw(smart_slow_up_freq);
+define_one_global_rw(smart_slow_up_dur);
+define_one_global_rw(smart_high_slow_up_freq);
+define_one_global_rw(smart_high_slow_up_dur);
+define_one_global_rw(smart_each_off);
+/* end smart_up */
+define_one_global_rw(freq_step);
+define_one_global_rw(step_up_early_hispeed);
+define_one_global_rw(step_up_interim_hispeed);
+define_one_global_rw(sampling_early_factor);
+define_one_global_rw(sampling_interim_factor);
+define_one_global_rw(two_phase_freq);
+
+static struct attribute *dbs_attributes[] = {
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&up_threshold.attr,
+	&sampling_down_factor.attr,
+	&up_threshold_multi_core.attr,
+	&optimal_freq.attr,
+	&up_threshold_any_cpu_load.attr,
+	&sync_freq.attr,
+	/* 20130711 smart_up */
+	&smart_up.attr,
+	&smart_slow_up_load.attr,
+	&smart_slow_up_freq.attr,
+	&smart_slow_up_dur.attr,
+	&smart_high_slow_up_freq.attr,
+	&smart_high_slow_up_dur.attr,
+	&smart_each_off.attr,
+	/* end smart_up */
+	&freq_step.attr,
+	&step_up_early_hispeed.attr,
+	&step_up_interim_hispeed.attr,
+	&sampling_early_factor.attr,
+	&sampling_interim_factor.attr,
+	&two_phase_freq.attr,
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "intellidemand",
+};
+
+/************************** sysfs end ************************/
+
+static void dbs_freq_increase(struct cpufreq_policy *p, unsigned int freq)
+{
+	if (p->cur == p->max)
+		return;
+
+	__cpufreq_driver_target(p, freq, CPUFREQ_RELATION_L);
+}
+
+static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
+{
+
+#if defined(SMART_UP_PLUS)
+	unsigned int core_j = 0;
+#endif
+
+	/* Extrapolated load of this CPU */
+	unsigned int load_at_max_freq = 0;
+	unsigned int max_load_freq;
+	/* Current load across this CPU */
+	unsigned int cur_load = 0;
+	unsigned int max_load = 0;
+	unsigned int max_load_other_cpu = 0;
+	struct cpufreq_policy *policy;
+	unsigned int j;
+	static unsigned int phase = 0;
+	static unsigned int counter = 0;
+	unsigned int nr_cpus;
+	unsigned int sampling_rate;
+
+	sampling_rate = dbs_tuners_ins.sampling_rate * this_dbs_info->rate_mult;
+	this_dbs_info->freq_lo = 0;
+	policy = this_dbs_info->cur_policy;
+	if (policy == NULL)
+		return;
+
+	/*
+	 * Every sampling_rate, we check, if current idle time is less
+	 * than 20% (default), then we try to increase frequency
+	 * Every sampling_rate, we look for a the lowest
+	 * frequency which can sustain the load while keeping idle time over
+	 * 30%. If such a frequency exist, we try to decrease to this frequency.
+	 *
+	 * Any frequency increase takes it to the maximum frequency.
+	 * Frequency reduction happens at minimum steps of
+	 * 5% (default) of current frequency
+	 */
+
+	/* Get Absolute Load - in terms of freq */
+	max_load_freq = 0;
+
+	for_each_cpu(j, policy->cpus) {
+		struct cpu_dbs_info_s *j_dbs_info;
+		u64 cur_wall_time, cur_idle_time;
+		unsigned int idle_time, wall_time;
+		unsigned int load_freq;
+		int freq_avg;
+
+		j_dbs_info = &per_cpu(id_cpu_dbs_info, j);
+
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time, 0);
+		wall_time = (unsigned int)
+			(cur_wall_time - j_dbs_info->prev_cpu_wall);
+		j_dbs_info->prev_cpu_wall = cur_wall_time;
+
+		idle_time = (unsigned int)
+			(cur_idle_time - j_dbs_info->prev_cpu_idle);
+		j_dbs_info->prev_cpu_idle = cur_idle_time;
+
+		/*
+		 * For the purpose of intellidemand, waiting for disk IO is an
+		 * indication that you're performance critical, and not that
+		 * the system is actually idle. So subtract the iowait time
+		 * from the cpu idle time.
+		 */
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		/*
+		 * If the CPU had gone completely idle, and a task just woke up
+		 * on this CPU now, it would be unfair to calculate 'load' the
+		 * usual way for this elapsed time-window, because it will show
+		 * near-zero load, irrespective of how CPU intensive that task
+		 * actually is. This is undesirable for latency-sensitive bursty
+		 * workloads.
+		 *
+		 * To avoid this, we reuse the 'load' from the previous
+		 * time-window and give this task a chance to start with a
+		 * reasonably high CPU frequency. (However, we shouldn't over-do
+		 * this copy, lest we get stuck at a high load (high frequency)
+		 * for too long, even when the current system load has actually
+		 * dropped down. So we perform the copy only once, upon the
+		 * first wake-up from idle.)
+		 *
+		 * Detecting this situation is easy: the governor's deferrable
+		 * timer would not have fired during CPU-idle periods. Hence
+		 * an unusually large 'wall_time' (as compared to the sampling
+		 * rate) indicates this scenario.
+		 *
+		 * prev_load can be zero in two cases and we must recalculate it
+		 * for both cases:
+		 * - during long idle intervals
+		 * - explicitly set to zero
+		 */
+		if (unlikely(wall_time > (2 * sampling_rate) &&
+			     j_dbs_info->prev_load)) {
+			cur_load = j_dbs_info->prev_load;
+			j_dbs_info->max_load = cur_load;
+
+			/*
+			 * Perform a destructive copy, to ensure that we copy
+			 * the previous load only once, upon the first wake-up
+			 * from idle.
+			 */
+			j_dbs_info->prev_load = 0;
+		} else {
+			cur_load = 100 * (wall_time - idle_time) / wall_time;
+			j_dbs_info->max_load = max(cur_load, j_dbs_info->prev_load);
+			j_dbs_info->prev_load = cur_load;
+		}
+
+		if (cur_load > max_load)
+			max_load = cur_load;
+
+		freq_avg = __cpufreq_driver_getavg(policy, j);
+		if (policy == NULL)
+			return;
+		if (freq_avg <= 0)
+			freq_avg = policy->cur;
+
+		load_freq = cur_load * freq_avg;
+		if (load_freq > max_load_freq)
+			max_load_freq = load_freq;
+
+#if defined(SMART_UP_PLUS)
+		max_load = cur_load;
+		core_j = j;
+#endif
+
+	}
+
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *j_dbs_info;
+		j_dbs_info = &per_cpu(id_cpu_dbs_info, j);
+
+		if (j == policy->cpu)
+			continue;
+
+		if (max_load_other_cpu < j_dbs_info->max_load)
+			max_load_other_cpu = j_dbs_info->max_load;
+	}
+
+	/* calculate the scaled load across CPU */
+	load_at_max_freq = (cur_load * policy->cur)/policy->max;
+
+	cpufreq_notify_utilization(policy, load_at_max_freq);
+
+/* PATCH : SMART_UP */
+	if (dbs_tuners_ins.smart_up && (core_j + 1) >
+				dbs_tuners_ins.smart_each_off) {
+		if (max_load_freq > SUP_THRESHOLD_STEPS[0] * policy->cur) {
+			int smart_up_inc =
+				(policy->max - policy->cur) / SUP_FREQ_STEPS[0];
+			int freq_next = 0;
+			int i = 0;
+
+			/* 20130429 UPDATE */
+			int check_idx =  0;
+			int check_freq = 0;
+			int temp_up_inc =0;
+
+			if (counter < 5) {
+				counter++;
+				if (counter > 2) {
+					phase = 1;
+				}
+			}
+
+			nr_cpus = num_online_cpus();
+			dbs_tuners_ins.two_phase_freq = two_phase_freq_array[nr_cpus-1];
+			if (dbs_tuners_ins.two_phase_freq < policy->cur)
+				phase = 1;
+			if (dbs_tuners_ins.two_phase_freq != 0 && phase == 0) {
+				dbs_freq_increase(policy, dbs_tuners_ins.two_phase_freq);
+			} else {
+				if (policy->cur < policy->max)
+					this_dbs_info->rate_mult =
+						dbs_tuners_ins.sampling_down_factor;
+				dbs_freq_increase(policy, policy->max);
+			}
+
+			for (i = (SUP_MAX_STEP - 1); i > 0; i--) {
+				if (max_load_freq > SUP_THRESHOLD_STEPS[i]
+							* policy->cur) {
+					smart_up_inc = (policy->max - policy->cur)
+							/ SUP_FREQ_STEPS[i];
+					break;
+				}
+			}
+
+			/* 20130429 UPDATE */
+			check_idx =  pre_freq_idx[core_j].freq_idx;
+			check_freq = pre_freq_idx[core_j].freq_value;
+			if (( check_idx == 0)
+					|| (this_dbs_info->freq_table[check_idx].frequency
+					!=  policy->cur)) {
+				int i = 0;
+				for (i =0; i < SUP_FREQ_LEVEL; i ++) {
+					if (this_dbs_info->freq_table[i].frequency == policy->cur) {
+						pre_freq_idx[core_j].freq_idx = i;
+						pre_freq_idx[core_j].freq_value = policy->cur;
+						check_idx =  i;
+						check_freq = policy->cur;
+						break;
+					}
+				}
+			}
+
+			if (check_idx < SUP_FREQ_LEVEL-1) {
+				temp_up_inc =
+					this_dbs_info->freq_table[check_idx + 1].frequency
+					- check_freq;
+			}
+
+			if (smart_up_inc < temp_up_inc )
+				smart_up_inc = temp_up_inc;
+
+			freq_next = MIN((policy->cur + smart_up_inc), policy->max);
+
+			if (policy->cur >= dbs_tuners_ins.smart_high_slow_up_freq) {
+				int idx = hist_load_high[core_j].hist_load_cnt;
+				int avg_hist_load = 0;
+
+				if (idx >= dbs_tuners_ins.smart_high_slow_up_dur)
+					idx = 0;
+
+				hist_load_high[core_j].hist_max_load[idx] = max_load;
+				hist_load_high[core_j].hist_load_cnt = idx + 1;
+
+				/* note : check history_load and get_sum_hist_load */
+				if (hist_load_high[core_j].
+						hist_max_load[dbs_tuners_ins.smart_high_slow_up_dur - 1] > 0) {
+					int sum_hist_load_freq = 0;
+					int i = 0;
+					for (i = 0; i < dbs_tuners_ins.smart_high_slow_up_dur; i++)
+						sum_hist_load_freq +=
+							hist_load_high[core_j].hist_max_load[i];
+
+					avg_hist_load = sum_hist_load_freq
+								/ dbs_tuners_ins.smart_high_slow_up_dur;
+
+					if (avg_hist_load > dbs_tuners_ins.smart_slow_up_load) {
+						reset_hist_high(&hist_load_high[core_j]);
+						freq_next = MIN((policy->cur + temp_up_inc), policy->max);
+					} else
+						freq_next = policy->cur;
+				} else {
+					freq_next = policy->cur;
+				}
+
+			} else if (policy->cur >= dbs_tuners_ins.smart_slow_up_freq ) {
+				int idx = hist_load[core_j].hist_load_cnt;
+				int avg_hist_load = 0;
+
+				if (idx >= dbs_tuners_ins.smart_slow_up_dur)
+					idx = 0;
+
+				hist_load[core_j].hist_max_load[idx] = max_load;
+				hist_load[core_j].hist_load_cnt = idx + 1;
+
+				/* note : check history_load and get_sum_hist_load */
+				if (hist_load[core_j].
+					hist_max_load[dbs_tuners_ins.smart_slow_up_dur - 1] > 0) {
+					int sum_hist_load_freq = 0;
+					int i = 0;
+					for (i = 0; i < dbs_tuners_ins.smart_slow_up_dur; i++)
+						sum_hist_load_freq +=
+							hist_load[core_j].hist_max_load[i];
+
+					avg_hist_load = sum_hist_load_freq
+							/ dbs_tuners_ins.smart_slow_up_dur;
+
+					if (avg_hist_load > dbs_tuners_ins.smart_slow_up_load) {
+						reset_hist(&hist_load[core_j]);
+						freq_next = MIN((policy->cur + temp_up_inc), policy->max);
+					} else
+						freq_next = policy->cur;
+				} else {
+					freq_next = policy->cur;
+				}
+			} else {
+				reset_hist(&hist_load[core_j]);
+			}
+			if (freq_next == policy->max)
+				this_dbs_info->rate_mult =
+					dbs_tuners_ins.sampling_down_factor;
+
+			dbs_freq_increase(policy, freq_next);
+			return;
+		}
+	} else {
+		/* Check for frequency increase */
+		if (max_load_freq > dbs_tuners_ins.up_threshold * policy->cur) {
+			int target;
+			int inc;
+
+			if (policy->cur < dbs_tuners_ins.step_up_early_hispeed) {
+				target = dbs_tuners_ins.step_up_early_hispeed;
+			} else if (policy->cur < dbs_tuners_ins.step_up_interim_hispeed) {
+				if (policy->cur == dbs_tuners_ins.step_up_early_hispeed) {
+					if (this_dbs_info->freq_stay_count <
+						dbs_tuners_ins.sampling_early_factor) {
+						this_dbs_info->freq_stay_count++;
+						return;
+					}
+				}
+				this_dbs_info->freq_stay_count = 1;
+				inc = (policy->max * dbs_tuners_ins.freq_step) / 100;
+				target = min(dbs_tuners_ins.step_up_interim_hispeed,
+					policy->cur + inc);
+			} else {
+				if (policy->cur == dbs_tuners_ins.step_up_interim_hispeed) {
+					if (this_dbs_info->freq_stay_count <
+						dbs_tuners_ins.sampling_interim_factor) {
+						this_dbs_info->freq_stay_count++;
+						return;
+					}
+				}
+				this_dbs_info->freq_stay_count = 1;
+				target = policy->max;
+			}
+
+			pr_debug("%s: cpu=%d, cur=%d, target=%d\n",
+				__func__, policy->cpu, policy->cur, target);
+
+			/* If switching to max speed, apply sampling_down_factor */
+			if (target == policy->max)
+				this_dbs_info->rate_mult =
+					dbs_tuners_ins.sampling_down_factor;
+
+			dbs_freq_increase(policy, target);
+			return;
+		}
+	}
+	if (counter > 0) {
+		counter--;
+		if (counter == 0) {
+			phase = 0;
+		}
+	}
+
+	if (num_online_cpus() > 1) {
+		if (max_load_other_cpu >
+				dbs_tuners_ins.up_threshold_any_cpu_load) {
+			if (policy->cur < dbs_tuners_ins.sync_freq)
+				dbs_freq_increase(policy,
+						dbs_tuners_ins.sync_freq);
+			return;
+		}
+
+		if (max_load_freq > (dbs_tuners_ins.up_threshold_multi_core *
+								policy->cur)) {
+			if (policy->cur < dbs_tuners_ins.optimal_freq)
+				dbs_freq_increase(policy,
+						dbs_tuners_ins.optimal_freq);
+			return;
+		}
+	}
+
+	/* Check for frequency decrease */
+	/* if we cannot reduce the frequency anymore, break out early */
+	if (policy->cur == policy->min)
+		return;
+
+	/*
+	 * The optimal frequency is the frequency that is the lowest that
+	 * can support the current CPU usage without triggering the up
+	 * policy. To be safe, we focus 10 points under the threshold.
+	 */
+	if (max_load_freq <
+	    (dbs_tuners_ins.up_threshold * policy->cur)) {
+		unsigned int freq_next;
+		freq_next = max_load_freq /
+				(dbs_tuners_ins.up_threshold);
+
+		/* PATCH : SMART_UP */
+		if (dbs_tuners_ins.smart_up && (core_j + 1) >
+					dbs_tuners_ins.smart_each_off) {
+			if (freq_next >= dbs_tuners_ins.smart_high_slow_up_freq) {
+				int idx = hist_load_high[core_j].hist_load_cnt;
+
+				if (idx >= dbs_tuners_ins.smart_high_slow_up_dur)
+					idx = 0;
+
+				hist_load_high[core_j].hist_max_load[idx] = max_load;
+				hist_load_high[core_j].hist_load_cnt = idx + 1;
+			} else if (freq_next >= dbs_tuners_ins.smart_slow_up_freq) {
+				int idx = hist_load[core_j].hist_load_cnt;
+
+				if (idx >= dbs_tuners_ins.smart_slow_up_dur)
+					idx = 0;
+
+				hist_load[core_j].hist_max_load[idx] = max_load;
+				hist_load[core_j].hist_load_cnt = idx + 1;
+
+				reset_hist_high(&hist_load_high[core_j]);
+			} else if (policy->cur >= dbs_tuners_ins.smart_slow_up_freq) {
+				reset_hist(&hist_load[core_j]);
+				reset_hist_high(&hist_load_high[core_j]);
+			}
+		}
+
+		/* No longer fully busy, reset rate_mult */
+		this_dbs_info->rate_mult = 1;
+		this_dbs_info->freq_stay_count = 1;
+
+		if (num_online_cpus() > 1) {
+			if (max_load_other_cpu >
+				dbs_tuners_ins.up_threshold_multi_core &&
+					freq_next < dbs_tuners_ins.sync_freq)
+				freq_next = dbs_tuners_ins.sync_freq;
+
+			if (max_load_freq >
+					(dbs_tuners_ins.up_threshold_multi_core *
+					policy->cur) &&
+					freq_next < dbs_tuners_ins.optimal_freq)
+				freq_next = dbs_tuners_ins.optimal_freq;
+
+		}
+		__cpufreq_driver_target(policy, freq_next,
+				CPUFREQ_RELATION_L);
+	}
+}
+
+static void do_dbs_timer(struct work_struct *work)
+{
+	struct cpu_dbs_info_s *dbs_info =
+		container_of(work, struct cpu_dbs_info_s, work.work);
+	int sample_type = dbs_info->sample_type;
+	int delay;
+
+	if (unlikely(!cpu_online(dbs_info->cpu) || !dbs_info->cur_policy))
+		return;
+
+	mutex_lock(&dbs_info->timer_mutex);
+
+	/* Common NORMAL_SAMPLE setup */
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	if (sample_type == DBS_NORMAL_SAMPLE) {
+		dbs_check_cpu(dbs_info);
+		if (dbs_info->freq_lo) {
+			/* Setup timer for SUB_SAMPLE */
+			dbs_info->sample_type = DBS_SUB_SAMPLE;
+			delay = dbs_info->freq_hi_jiffies;
+		} else {
+			delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate
+				* dbs_info->rate_mult);
+		}
+	} else {
+		__cpufreq_driver_target(dbs_info->cur_policy,
+			dbs_info->freq_lo, CPUFREQ_RELATION_H);
+		delay = dbs_info->freq_lo_jiffies;
+	}
+	queue_delayed_work_on(dbs_info->cpu, dbs_wq, &dbs_info->work, delay);
+	mutex_unlock(&dbs_info->timer_mutex);
+}
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
+{
+	/* We want all CPUs to do sampling nearly on same jiffy */
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+	if (num_online_cpus() > 1)
+		delay -= jiffies % delay;
+
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	INIT_DEFERRABLE_WORK(&dbs_info->work, do_dbs_timer);
+	queue_delayed_work_on(dbs_info->cpu, dbs_wq, &dbs_info->work, delay);
+}
+
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
+{
+	cancel_delayed_work_sync(&dbs_info->work);
+}
+
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int rc;
+
+	this_dbs_info = &per_cpu(id_cpu_dbs_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy))
+			return -EINVAL;
+
+		mutex_lock(&dbs_mutex);
+
+		dbs_enable++;
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			unsigned int prev_load;
+			j_dbs_info = &per_cpu(id_cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&j_dbs_info->prev_cpu_wall,
+						0);
+
+			prev_load = (unsigned int)
+				(j_dbs_info->prev_cpu_wall - j_dbs_info->prev_cpu_idle);
+			j_dbs_info->prev_load = 100 * prev_load /
+				(unsigned int) j_dbs_info->prev_cpu_wall;
+		}
+		cpu = policy->cpu;
+		this_dbs_info->cpu = cpu;
+		this_dbs_info->rate_mult = 1;
+		this_dbs_info->freq_stay_count = 1;
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 1) {
+			unsigned int latency;
+
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&dbs_attr_group);
+			if (rc) {
+				dbs_enable--;
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+
+			/* policy latency is in nS. Convert it to uS first */
+			latency = policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+			/* Bring kernel and HW constraints together */
+			min_sampling_rate = max(min_sampling_rate,
+					MIN_LATENCY_MULTIPLIER * latency);
+			if (latency != 1)
+				dbs_tuners_ins.sampling_rate =
+					max(dbs_tuners_ins.sampling_rate,
+						latency * LATENCY_MULTIPLIER);
+
+			if (dbs_tuners_ins.optimal_freq == 0)
+				dbs_tuners_ins.optimal_freq = policy->min;
+
+			if (dbs_tuners_ins.sync_freq == 0)
+				dbs_tuners_ins.sync_freq = policy->min;
+		}
+		mutex_unlock(&dbs_mutex);
+
+		dbs_timer_init(this_dbs_info);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		dbs_timer_exit(this_dbs_info);
+
+		mutex_lock(&dbs_mutex);
+
+		dbs_enable--;
+
+		/* If device is being removed, policy is no longer
+		 * valid. */
+		this_dbs_info->cur_policy = NULL;
+		if (!dbs_enable)
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &dbs_attr_group);
+
+		mutex_unlock(&dbs_mutex);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		/* If device is being removed, skip set limits */
+		if (!this_dbs_info->cur_policy)
+			break;
+		mutex_lock(&this_dbs_info->timer_mutex);
+		__cpufreq_driver_target(this_dbs_info->cur_policy,
+				policy->cur, CPUFREQ_RELATION_L);
+		dbs_check_cpu(this_dbs_info);
+		mutex_unlock(&this_dbs_info->timer_mutex);
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND
+static
+#endif
+struct cpufreq_governor cpufreq_gov_intellidemand = {
+	.name			= "intellidemand",
+	.governor		= cpufreq_governor_dbs,
+	.max_transition_latency	= TRANSITION_LATENCY_LIMIT,
+	.owner			= THIS_MODULE,
+};
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	u64 idle_time;
+	unsigned int i;
+	int cpu = get_cpu();
+
+	idle_time = get_cpu_idle_time_us(cpu, NULL);
+	put_cpu();
+	if (idle_time != -1ULL) {
+		/* Idle micro accounting is supported. Use finer thresholds */
+		dbs_tuners_ins.up_threshold = MICRO_FREQUENCY_UP_THRESHOLD;
+		/*
+		 * In nohz/micro accounting case we set the minimum frequency
+		 * not depending on HZ, but fixed (very low). The deferred
+		 * timer might skip some samples if idle/sleeping as needed.
+		*/
+		min_sampling_rate = MICRO_FREQUENCY_MIN_SAMPLE_RATE;
+	} else {
+		/* For correct statistics, we need 10 ticks for each measure */
+		min_sampling_rate =
+			MIN_SAMPLING_RATE_RATIO * jiffies_to_usecs(10);
+	}
+
+	dbs_wq = alloc_workqueue("intellidemand_dbs_wq", WQ_HIGHPRI, 0);
+	if (!dbs_wq) {
+		printk(KERN_ERR "Failed to create intellidemand_dbs_wq workqueue\n");
+		return -EFAULT;
+	}
+	for_each_possible_cpu(i) {
+		struct cpu_dbs_info_s *this_dbs_info =
+			&per_cpu(id_cpu_dbs_info, i);
+
+		mutex_init(&this_dbs_info->timer_mutex);
+	}
+
+	return cpufreq_register_governor(&cpufreq_gov_intellidemand);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	unsigned int i;
+
+	cpufreq_unregister_governor(&cpufreq_gov_intellidemand);
+	for_each_possible_cpu(i) {
+		struct cpu_dbs_info_s *this_dbs_info =
+			&per_cpu(id_cpu_dbs_info, i);
+		mutex_destroy(&this_dbs_info->timer_mutex);
+	}
+	destroy_workqueue(dbs_wq);
+}
+
+MODULE_AUTHOR("Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>");
+MODULE_AUTHOR("Alexey Starikovskiy <alexey.y.starikovskiy@intel.com>");
+MODULE_AUTHOR("Paul Reioux <reioux@gmail.com>");
+MODULE_DESCRIPTION("'cpufreq_intellidemand' - A dynamic cpufreq governor for "
+	"Low Latency Frequency Transition capable processors");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_intellimm.c b/drivers/cpufreq/cpufreq_intellimm.c
new file mode 100644
index 0000000..599bd5f
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_intellimm.c
@@ -0,0 +1,1462 @@
+/*
+ *  drivers/cpufreq/cpufreq_intellimm.c
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *            (c)  2013 The Linux Foundation. All rights reserved.
+ *            (c)  2014 Paul Reioux (aka Faux123)
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/kthread.h>
+#include <linux/sched.h>
+#include <linux/sched/rt.h>
+#include <linux/workqueue.h>
+#include <linux/slab.h>
+
+#define DEF_SAMPLING_RATE			(50000)
+#define DEF_FREQUENCY_DOWN_DIFFERENTIAL		(10)
+#define DEF_FREQUENCY_UP_THRESHOLD		(80)
+#define DEF_FREQUENCY_UP_THRESHOLD_MULTY	(70)
+#define DEF_FREQUENCY_UP_THRESHOLD_ANY_CPU	(70)
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define MAX_SAMPLING_DOWN_FACTOR		(100000)
+#define MICRO_FREQUENCY_DOWN_DIFFERENTIAL	(3)
+#define MICRO_FREQUENCY_MIN_SAMPLE_RATE		(10000)
+#define MIN_FREQUENCY_UP_THRESHOLD		(11)
+#define MAX_FREQUENCY_UP_THRESHOLD		(100)
+#define MIN_FREQUENCY_DOWN_DIFFERENTIAL		(1)
+
+#define DEF_POWER_SAVE_FREQUENCY		(652800)
+#define DEF_TWO_PHASE_FREQUENCY			(1036800)
+#define DBS_INPUT_EVENT_MIN_FREQ		(883200)
+#define DEF_FREQUENCY_OPTIMAL			(729600)
+#define DEF_FREQ_DOWN_STEP			(300000)
+#define DEF_FREQ_DOWN_STEP_BARRIER		(729600)
+
+#define CPU0					(0)
+
+#define MIN_SAMPLING_RATE_RATIO			(2)
+
+static unsigned int min_sampling_rate;
+
+#define LATENCY_MULTIPLIER			(1000)
+#define MIN_LATENCY_MULTIPLIER			(100)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+#define POWERSAVE_BIAS_MAXLEVEL			(1000)
+#define POWERSAVE_BIAS_MINLEVEL			(-1000)
+
+static void do_dbs_timer(struct work_struct *work);
+
+enum {DBS_NORMAL_SAMPLE, DBS_SUB_SAMPLE};
+
+struct cpu_dbs_info_s {
+	cputime64_t prev_cpu_idle;
+	cputime64_t prev_cpu_iowait;
+	cputime64_t prev_cpu_wall;
+	cputime64_t prev_cpu_nice;
+	struct cpufreq_policy *cur_policy;
+	struct delayed_work work;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int freq_lo;
+	unsigned int freq_lo_jiffies;
+	unsigned int freq_hi_jiffies;
+	unsigned int rate_mult;
+	unsigned int prev_load;
+	unsigned int cur_load;
+	unsigned int max_load;
+	int input_event_freq;
+	unsigned int cpu;
+	unsigned int sample_type:1;
+	struct mutex timer_mutex;
+
+};
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, imm_cpu_dbs_info);
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info);
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info);
+
+static unsigned int dbs_enable;
+
+static DEFINE_PER_CPU(struct task_struct *, up_task);
+static spinlock_t input_boost_lock;
+static bool input_event_boost = false;
+static unsigned long input_event_boost_expired = 0;
+
+#define TABLE_SIZE			1
+
+#define MAX(x,y)			(x > y ? x : y)
+#define MIN(x,y)			(x < y ? x : y)
+#define FREQ_NEED_BURST(x)		(x < 600000 ? 1 : 0)
+
+static struct cpufreq_frequency_table *tbl = NULL;
+static unsigned int *tblmap[TABLE_SIZE] __read_mostly;
+static unsigned int tbl_select[4] = {0};
+
+static unsigned int up_threshold_level[2] __read_mostly = {90, 80};
+static void reset_freq_map_table(struct cpufreq_policy *);
+
+static DEFINE_MUTEX(dbs_mutex);
+
+static struct workqueue_struct *dbs_wq;
+
+static struct dbs_tuners {
+	unsigned int sampling_rate;
+	unsigned int up_threshold;
+	unsigned int up_threshold_multi_core;
+	unsigned int down_differential;
+	unsigned int down_differential_multi_core;
+	unsigned int optimal_freq_speed;
+	unsigned int up_threshold_any_cpu_load;
+	unsigned int ignore_nice;
+	unsigned int sampling_down_factor;
+	int          powersave_bias;
+	unsigned int io_is_busy;
+	unsigned int shortcut;
+	unsigned int power_save_freq;
+	unsigned int two_phase_freq;
+	unsigned int freq_down_step;
+	unsigned int freq_down_step_barrier;
+} dbs_tuners_ins = {
+	.up_threshold_multi_core = DEF_FREQUENCY_UP_THRESHOLD_MULTY,
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.down_differential = DEF_FREQUENCY_DOWN_DIFFERENTIAL,
+	.down_differential_multi_core = MICRO_FREQUENCY_DOWN_DIFFERENTIAL,
+	.up_threshold_any_cpu_load = DEF_FREQUENCY_UP_THRESHOLD_ANY_CPU,
+	.ignore_nice = 0,
+	.powersave_bias = 0,
+	.optimal_freq_speed = 1728000,
+	.shortcut = 0,
+	.io_is_busy = 0,
+	.power_save_freq = DEF_POWER_SAVE_FREQUENCY,
+	.two_phase_freq = DEF_TWO_PHASE_FREQUENCY,
+	.freq_down_step = DEF_FREQ_DOWN_STEP,
+	.freq_down_step_barrier = DEF_FREQ_DOWN_STEP_BARRIER,
+};
+
+static inline cputime64_t get_cpu_iowait_time(unsigned int cpu,
+						cputime64_t *wall)
+{
+	u64 iowait_time = get_cpu_iowait_time_us(cpu, wall);
+
+	if (iowait_time == -1ULL)
+		return 0;
+
+	return iowait_time;
+}
+
+static unsigned int powersave_bias_target(struct cpufreq_policy *policy,
+					  unsigned int freq_next,
+					  unsigned int relation)
+{
+	unsigned int freq_req, freq_avg;
+	unsigned int freq_hi, freq_lo;
+	unsigned int index = 0;
+	unsigned int jiffies_total, jiffies_hi, jiffies_lo;
+	int freq_reduc;
+	struct cpu_dbs_info_s *dbs_info = &per_cpu(imm_cpu_dbs_info,
+						   policy->cpu);
+
+	if (!dbs_info->freq_table) {
+		dbs_info->freq_lo = 0;
+		dbs_info->freq_lo_jiffies = 0;
+		return freq_next;
+	}
+
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_next,
+			relation, &index);
+	freq_req = dbs_info->freq_table[index].frequency;
+	freq_reduc = freq_req * dbs_tuners_ins.powersave_bias / 1000;
+	freq_avg = freq_req - freq_reduc;
+
+
+	index = 0;
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_avg,
+			CPUFREQ_RELATION_H, &index);
+	freq_lo = dbs_info->freq_table[index].frequency;
+	index = 0;
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_avg,
+			CPUFREQ_RELATION_L, &index);
+	freq_hi = dbs_info->freq_table[index].frequency;
+
+
+	if (freq_hi == freq_lo) {
+		dbs_info->freq_lo = 0;
+		dbs_info->freq_lo_jiffies = 0;
+		return freq_lo;
+	}
+	jiffies_total = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+	jiffies_hi = (freq_avg - freq_lo) * jiffies_total;
+	jiffies_hi += ((freq_hi - freq_lo) / 2);
+	jiffies_hi /= (freq_hi - freq_lo);
+	jiffies_lo = jiffies_total - jiffies_hi;
+	dbs_info->freq_lo = freq_lo;
+	dbs_info->freq_lo_jiffies = jiffies_lo;
+	dbs_info->freq_hi_jiffies = jiffies_hi;
+	return freq_hi;
+}
+
+static int intellimm_powersave_bias_setspeed(struct cpufreq_policy *policy,
+					    struct cpufreq_policy *altpolicy,
+					    int level)
+{
+	if (level == POWERSAVE_BIAS_MAXLEVEL) {
+
+		__cpufreq_driver_target(policy,
+			(altpolicy) ? altpolicy->min : policy->min,
+			CPUFREQ_RELATION_L);
+		return 1;
+	} else if (level == POWERSAVE_BIAS_MINLEVEL) {
+
+		__cpufreq_driver_target(policy,
+			(altpolicy) ? altpolicy->max : policy->max,
+			CPUFREQ_RELATION_H);
+		return 1;
+	}
+	return 0;
+}
+
+static void intellimm_powersave_bias_init_cpu(int cpu)
+{
+	struct cpu_dbs_info_s *dbs_info = &per_cpu(imm_cpu_dbs_info, cpu);
+	dbs_info->freq_table = cpufreq_frequency_get_table(cpu);
+	dbs_info->freq_lo = 0;
+}
+
+static void intellimm_powersave_bias_init(void)
+{
+	int i;
+	for_each_online_cpu(i) {
+		intellimm_powersave_bias_init_cpu(i);
+	}
+}
+
+
+static ssize_t show_sampling_rate_min(struct kobject *kobj,
+				      struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", min_sampling_rate);
+}
+
+define_one_global_ro(sampling_rate_min);
+
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name	(struct kobject *kobj,			\
+		struct attribute *attr, char *buf)			\
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+
+show_one(sampling_rate, sampling_rate);
+show_one(io_is_busy, io_is_busy);
+show_one(shortcut, shortcut);
+show_one(up_threshold, up_threshold);
+show_one(up_threshold_multi_core, up_threshold_multi_core);
+show_one(down_differential, down_differential);
+show_one(sampling_down_factor, sampling_down_factor);
+show_one(ignore_nice_load, ignore_nice);
+show_one(down_differential_multi_core, down_differential_multi_core);
+show_one(optimal_freq_speed, optimal_freq_speed);
+show_one(up_threshold_any_cpu_load, up_threshold_any_cpu_load);
+show_one(freq_down_step, freq_down_step);
+show_one(freq_down_step_barrier, freq_down_step_barrier);
+
+static ssize_t show_powersave_bias(struct kobject *kobj,
+					struct attribute *attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%d\n", dbs_tuners_ins.powersave_bias);
+}
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_rate = max(input, min_sampling_rate);
+	return count;
+}
+
+static ssize_t store_io_is_busy(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.io_is_busy = !!input;
+	return count;
+}
+
+static ssize_t store_shortcut(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (dbs_tuners_ins.shortcut != input)
+		dbs_tuners_ins.shortcut = input;
+
+	return count;
+}
+
+static ssize_t store_down_differential_multi_core(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.down_differential_multi_core = input;
+	return count;
+}
+
+
+static ssize_t store_optimal_freq_speed(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.optimal_freq_speed = input;
+	return count;
+}
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold = input;
+	return count;
+}
+
+static ssize_t store_up_threshold_multi_core(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold_multi_core = input;
+	return count;
+}
+
+static ssize_t store_up_threshold_any_cpu_load(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold_any_cpu_load = input;
+	return count;
+}
+
+static ssize_t store_down_differential(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input >= dbs_tuners_ins.up_threshold ||
+			input < MIN_FREQUENCY_DOWN_DIFFERENTIAL) {
+		return -EINVAL;
+	}
+
+	dbs_tuners_ins.down_differential = input;
+
+	return count;
+}
+
+static ssize_t store_sampling_down_factor(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input, j;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+	dbs_tuners_ins.sampling_down_factor = input;
+
+
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(imm_cpu_dbs_info, j);
+		dbs_info->rate_mult = 1;
+	}
+	return count;
+}
+
+static ssize_t store_ignore_nice_load(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	unsigned int j;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == dbs_tuners_ins.ignore_nice) {
+		return count;
+	}
+	dbs_tuners_ins.ignore_nice = input;
+
+
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(imm_cpu_dbs_info, j);
+		dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+					&dbs_info->prev_cpu_wall,
+					dbs_tuners_ins.io_is_busy);
+		if (dbs_tuners_ins.ignore_nice)
+			dbs_info->prev_cpu_nice =
+				kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+
+	}
+	return count;
+}
+
+static ssize_t store_powersave_bias(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	int input  = 0;
+	int bypass = 0;
+	int ret, cpu, reenable_timer, j;
+	struct cpufreq_policy *policy;
+	struct cpu_dbs_info_s *dbs_info;
+
+	struct cpumask cpus_timer_done;
+	cpumask_clear(&cpus_timer_done);
+
+	ret = sscanf(buf, "%d", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input >= POWERSAVE_BIAS_MAXLEVEL) {
+		input  = POWERSAVE_BIAS_MAXLEVEL;
+		bypass = 1;
+	} else if (input <= POWERSAVE_BIAS_MINLEVEL) {
+		input  = POWERSAVE_BIAS_MINLEVEL;
+		bypass = 1;
+	}
+
+	if (input == dbs_tuners_ins.powersave_bias) {
+
+		return count;
+	}
+
+	reenable_timer = ((dbs_tuners_ins.powersave_bias ==
+				POWERSAVE_BIAS_MAXLEVEL) ||
+				(dbs_tuners_ins.powersave_bias ==
+				POWERSAVE_BIAS_MINLEVEL));
+
+	dbs_tuners_ins.powersave_bias = input;
+
+	get_online_cpus();
+	mutex_lock(&dbs_mutex);
+
+	if (!bypass) {
+		if (reenable_timer) {
+
+			for_each_online_cpu(cpu) {
+//				if (down_write(&policy->rwsem) < 0)
+					continue;
+
+				dbs_info = &per_cpu(imm_cpu_dbs_info, cpu);
+
+				if (!dbs_info->cur_policy) {
+					pr_err("Dbs policy is NULL\n");
+					goto skip_this_cpu;
+				}
+
+				for_each_cpu(j, &cpus_timer_done) {
+					if (cpumask_test_cpu(j, dbs_info->
+							cur_policy->cpus))
+						goto skip_this_cpu;
+				}
+
+				cpumask_set_cpu(cpu, &cpus_timer_done);
+				if (dbs_info->cur_policy) {
+					dbs_timer_exit(dbs_info);
+
+					mutex_lock(&dbs_info->timer_mutex);
+					dbs_timer_init(dbs_info);
+				}
+skip_this_cpu:
+				up_write(&policy->rwsem);
+			}
+		}
+		intellimm_powersave_bias_init();
+	} else {
+		for_each_online_cpu(cpu) {
+//			if (down_write(&policy->rwsem) < 0)
+				continue;
+
+			dbs_info = &per_cpu(imm_cpu_dbs_info, cpu);
+
+			if (!dbs_info->cur_policy) {
+				pr_err("Dbs policy is NULL\n");
+				goto skip_this_cpu_bypass;
+			}
+
+			for_each_cpu(j, &cpus_timer_done) {
+				if (cpumask_test_cpu(j, dbs_info->
+							cur_policy->cpus))
+					goto skip_this_cpu_bypass;
+			}
+
+			cpumask_set_cpu(cpu, &cpus_timer_done);
+
+			if (dbs_info->cur_policy) {
+
+				dbs_timer_exit(dbs_info);
+
+				mutex_lock(&dbs_info->timer_mutex);
+				intellimm_powersave_bias_setspeed(
+					dbs_info->cur_policy,
+					NULL,
+					input);
+				mutex_unlock(&dbs_info->timer_mutex);
+
+			}
+skip_this_cpu_bypass:
+			up_write(&policy->rwsem);
+		}
+	}
+
+	mutex_unlock(&dbs_mutex);
+	put_online_cpus();
+
+	return count;
+}
+
+static int input_event_min_freq_array[NR_CPUS] =
+		{[0 ... NR_CPUS-1] =
+			DBS_INPUT_EVENT_MIN_FREQ} ;
+
+static ssize_t show_input_event_min_freq(struct kobject *kobj,
+					struct attribute *attr, char *buf)
+{
+	int i = 0;
+	int shift = 0;
+	char *buf_pos = buf;
+	for ( i = 0 ; i < NR_CPUS; i++) {
+		shift = sprintf(buf_pos,"%d,",input_event_min_freq_array[i]);
+		buf_pos += shift;
+	}
+	*(buf_pos-1) = '\n';
+	*buf_pos = '\0';
+	buf_pos++;
+	return strlen(buf);
+}
+
+static ssize_t store_input_event_min_freq(struct kobject *a,
+					struct attribute *b,
+					const char *buf, size_t count)
+{
+	int ret = 0;
+	if (NR_CPUS == 1)
+		ret = sscanf(buf,"%u",&input_event_min_freq_array[0]);
+	else if (NR_CPUS == 2)
+		ret = sscanf(buf,"%u,%u",&input_event_min_freq_array[0],
+				&input_event_min_freq_array[1]);
+	else if (NR_CPUS == 4)
+		ret = sscanf(buf, "%u,%u,%u,%u",
+				&input_event_min_freq_array[0],
+				&input_event_min_freq_array[1],
+				&input_event_min_freq_array[2],
+				&input_event_min_freq_array[3]);
+	if (ret < NR_CPUS)
+		return -EINVAL;
+
+	return count;
+}
+
+static int freq_cnt = 0;
+static int low_index = 0;
+static int nom_index = 0;
+
+static ssize_t show_multi_phase_freq_tbl(struct kobject *kobj,
+		struct attribute *attr,
+		char *buf)
+{
+	int i = 0,j = 0;
+	int shift = 0;
+	char *buf_pos = buf;
+
+	for (i = 0; i < TABLE_SIZE; i++) {
+		shift = snprintf(buf_pos,PAGE_SIZE-(buf_pos - buf),
+			"%s %d %s","Table",i+1,"shows:\n");
+		buf_pos += shift;
+		for (j = 0; j < freq_cnt; j++) {
+			shift = snprintf(buf_pos,PAGE_SIZE-(buf_pos - buf),
+				"%02d: %8u\n",j,tblmap[i][j]);
+			buf_pos += shift;
+		}
+	}
+
+	*(buf_pos) = '\0';
+
+	return strlen(buf);
+}
+
+static ssize_t store_multi_phase_freq_tbl(struct kobject *a,
+		struct attribute *b, const char *buf, size_t count)
+{
+	return count;
+}
+
+show_one(two_phase_freq, two_phase_freq);
+
+static ssize_t store_two_phase_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	struct cpu_dbs_info_s *dbs_info;
+	struct cpufreq_policy *policy;
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_info = &per_cpu(imm_cpu_dbs_info, 0);
+	policy = dbs_info->cur_policy;
+	if (policy) {
+		if (input < policy->cpuinfo.min_freq ||
+		    input > policy->cpuinfo.max_freq)
+			return -EINVAL;
+		if (dbs_tuners_ins.two_phase_freq != input) {
+			dbs_tuners_ins.two_phase_freq = input;
+			reset_freq_map_table(policy);
+		}
+	}
+
+	return count;
+}
+
+static ssize_t store_freq_down_step(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.freq_down_step = input;
+	return count;
+}
+
+static ssize_t store_freq_down_step_barrier(struct kobject *a,
+			struct attribute *b,
+			const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.freq_down_step_barrier = input;
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(io_is_busy);
+define_one_global_rw(shortcut);
+define_one_global_rw(up_threshold);
+define_one_global_rw(down_differential);
+define_one_global_rw(sampling_down_factor);
+define_one_global_rw(ignore_nice_load);
+define_one_global_rw(powersave_bias);
+define_one_global_rw(up_threshold_multi_core);
+define_one_global_rw(down_differential_multi_core);
+define_one_global_rw(optimal_freq_speed);
+define_one_global_rw(up_threshold_any_cpu_load);
+define_one_global_rw(input_event_min_freq);
+define_one_global_rw(multi_phase_freq_tbl);
+define_one_global_rw(two_phase_freq);
+define_one_global_rw(freq_down_step);
+define_one_global_rw(freq_down_step_barrier);
+
+static struct attribute *dbs_attributes[] = {
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&up_threshold.attr,
+	&down_differential.attr,
+	&sampling_down_factor.attr,
+	&ignore_nice_load.attr,
+	&powersave_bias.attr,
+	&io_is_busy.attr,
+	&shortcut.attr,
+	&up_threshold_multi_core.attr,
+	&down_differential_multi_core.attr,
+	&optimal_freq_speed.attr,
+	&up_threshold_any_cpu_load.attr,
+	&input_event_min_freq.attr,
+	&multi_phase_freq_tbl.attr,
+	&two_phase_freq.attr,
+	&freq_down_step.attr,
+	&freq_down_step_barrier.attr,
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "intellimm",
+};
+
+static int adjust_freq_map_table(int freq, int cnt,
+		struct cpufreq_policy *policy)
+{
+	int i, upper, lower;
+
+	if (policy && tbl) {
+		upper = policy->cpuinfo.max_freq;
+		lower = policy->cpuinfo.min_freq;
+	}
+	else
+		return freq;
+
+	for(i = 0; i < cnt; i++)
+	{
+		if(freq >= tbl[i].frequency)
+		{
+			lower = tbl[i].frequency;
+		}
+		else
+		{
+			upper = tbl[i].frequency;
+			break;
+		}
+	}
+
+	return (freq - lower < upper - freq)?lower:upper;
+}
+
+static void reset_freq_map_table(struct cpufreq_policy *policy)
+{
+	unsigned int real_freq, low_freq;
+	int index;
+
+	if (!tbl)
+		return;
+
+	for (index = 0; index < freq_cnt; index++)
+		tblmap[0][index] = tbl[index].frequency;
+
+	low_freq = adjust_freq_map_table(dbs_tuners_ins.power_save_freq,
+			freq_cnt, policy);
+
+	for (index = 0; index < 5; index++) {
+		tblmap[0][index] = low_freq;
+		low_index = index;
+	}
+
+	real_freq = adjust_freq_map_table(dbs_tuners_ins.two_phase_freq,
+		freq_cnt, policy);
+
+	for (index = 5; index < freq_cnt; index++) {
+		if (tbl[index].frequency <= real_freq) {
+			tblmap[0][index] = real_freq;
+			nom_index = index;
+		}
+	}
+}
+
+static void dbs_init_freq_map_table(struct cpufreq_policy *policy)
+{
+	tbl = cpufreq_frequency_get_table(0);
+
+	for (freq_cnt = 0;
+		(tbl[freq_cnt].frequency != CPUFREQ_TABLE_END);
+		freq_cnt++)
+		;
+
+	tblmap[0] = kmalloc(sizeof(unsigned int) * freq_cnt, GFP_KERNEL);
+	BUG_ON(!tblmap[0]);
+
+	reset_freq_map_table(policy);
+}
+
+static void dbs_deinit_freq_map_table(void)
+{
+	int i;
+
+	if (!tbl)
+		return;
+
+	tbl = NULL;
+
+	for (i = 0; i < TABLE_SIZE; i++)
+		kfree(tblmap[i]);
+}
+
+static inline int get_cpu_freq_next(unsigned int freq)
+{
+	static int saved_index = 0;
+	int index;
+
+	if (!tbl) {
+		pr_warn("tbl is NULL, use previous value %d\n", saved_index);
+		return saved_index;
+	}
+
+	for (index = 0; (tbl[index].frequency != CPUFREQ_TABLE_END); index++) {
+		if (tbl[index].frequency > freq) {
+			if (index >= freq_cnt)
+				index = freq_cnt - 1;
+			saved_index = index;
+			break;
+		}
+	}
+	return index;
+}
+
+static void dbs_freq_increase(struct cpufreq_policy *p, unsigned int freq)
+{
+	if (dbs_tuners_ins.powersave_bias)
+		freq = powersave_bias_target(p, freq, CPUFREQ_RELATION_H);
+	else if (p->cur == p->max){
+		return;
+	}
+
+	__cpufreq_driver_target(p, freq, (dbs_tuners_ins.powersave_bias ||
+						freq < p->max) ?
+			CPUFREQ_RELATION_L : CPUFREQ_RELATION_H);
+}
+
+int input_event_boosted_intelli(void)
+{
+	unsigned long flags;
+
+
+	spin_lock_irqsave(&input_boost_lock, flags);
+	if (input_event_boost) {
+		if (time_before(jiffies, input_event_boost_expired)) {
+			spin_unlock_irqrestore(&input_boost_lock, flags);
+			return 1;
+		}
+		input_event_boost = false;
+	}
+	spin_unlock_irqrestore(&input_boost_lock, flags);
+
+	return 0;
+}
+
+static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
+{
+	struct cpufreq_policy *policy;
+	unsigned int j;
+	struct cpu_dbs_info_s *j_dbs_info;
+
+	unsigned int up_threshold;
+	unsigned max_load = 0, avg_load = 0;
+	unsigned avg_load_freq = 0;
+	unsigned num_of_cpus = 0;
+	unsigned int sampling_rate;
+
+	sampling_rate = dbs_tuners_ins.sampling_rate * this_dbs_info->rate_mult;
+	this_dbs_info->freq_lo = 0;
+	policy = this_dbs_info->cur_policy;
+
+	/* per cpu load calculation */
+	for_each_cpu(j, policy->cpus) {
+		cputime64_t cur_wall_time, cur_idle_time, cur_iowait_time;
+		unsigned int idle_time, wall_time, iowait_time;
+		int io_busy = dbs_tuners_ins.io_is_busy;
+		unsigned int cur_load;
+
+		j_dbs_info = &per_cpu(imm_cpu_dbs_info, j);
+
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time, io_busy);
+		cur_iowait_time = get_cpu_iowait_time(j, &cur_wall_time);
+
+		wall_time = (unsigned int)
+			(cur_wall_time - j_dbs_info->prev_cpu_wall);
+		j_dbs_info->prev_cpu_wall = cur_wall_time;
+
+		idle_time = (unsigned int)
+			(cur_idle_time - j_dbs_info->prev_cpu_idle);
+		j_dbs_info->prev_cpu_idle = cur_idle_time;
+
+		iowait_time = (unsigned int)
+			(cur_iowait_time - j_dbs_info->prev_cpu_iowait);
+		j_dbs_info->prev_cpu_iowait = cur_iowait_time;
+
+		if (dbs_tuners_ins.ignore_nice) {
+			u64 cur_nice;
+			unsigned long cur_nice_jiffies;
+
+			cur_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE] -
+					 j_dbs_info->prev_cpu_nice;
+			cur_nice_jiffies = (unsigned long)
+					cputime64_to_jiffies64(cur_nice);
+
+			j_dbs_info->prev_cpu_nice =
+				kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			idle_time += jiffies_to_usecs(cur_nice_jiffies);
+		}
+
+
+		if (dbs_tuners_ins.io_is_busy && idle_time >= iowait_time)
+			idle_time -= iowait_time;
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		/*
+		 * If the CPU had gone completely idle, and a task just woke up
+		 * on this CPU now, it would be unfair to calculate 'load' the
+		 * usual way for this elapsed time-window, because it will show
+		 * near-zero load, irrespective of how CPU intensive that task
+		 * actually is. This is undesirable for latency-sensitive bursty
+		 * workloads.
+		 *
+		 * To avoid this, we reuse the 'load' from the previous
+		 * time-window and give this task a chance to start with a
+		 * reasonably high CPU frequency. (However, we shouldn't over-do
+		 * this copy, lest we get stuck at a high load (high frequency)
+		 * for too long, even when the current system load has actually
+		 * dropped down. So we perform the copy only once, upon the
+		 * first wake-up from idle.)
+		 *
+		 * Detecting this situation is easy: the governor's deferrable
+		 * timer would not have fired during CPU-idle periods. Hence
+		 * an unusually large 'wall_time' (as compared to the sampling
+		 * rate) indicates this scenario.
+		 *
+		 * prev_load can be zero in two cases and we must recalculate it
+		 * for both cases:
+		 * - during long idle intervals
+		 * - explicitly set to zero
+		 */
+		if (unlikely(wall_time > (2 * sampling_rate) &&
+			     j_dbs_info->prev_load)) {
+			cur_load = j_dbs_info->prev_load;
+
+			/*
+			 * Perform a destructive copy, to ensure that we copy
+			 * the previous load only once, upon the first wake-up
+			 * from idle.
+			 */
+			j_dbs_info->prev_load = 0;
+		} else {
+			cur_load = 100 * (wall_time - idle_time) / wall_time;
+			j_dbs_info->prev_load = cur_load;
+		}
+
+		j_dbs_info->cur_load = cur_load;
+		j_dbs_info->max_load  = max(cur_load, j_dbs_info->prev_load);
+	}
+
+	num_of_cpus = num_online_cpus();
+	for_each_online_cpu(j) {
+		j_dbs_info = &per_cpu(imm_cpu_dbs_info, j);
+		avg_load += j_dbs_info->cur_load;
+		max_load += j_dbs_info->max_load;
+	}
+
+	avg_load = avg_load / num_of_cpus;
+	max_load = max_load / num_of_cpus;
+
+	if (max_load > avg_load)
+		avg_load_freq = (avg_load + max_load) / 2 * policy->cur;
+	else
+		avg_load_freq = avg_load * policy->cur;
+
+	// normal path up
+	if (dbs_tuners_ins.shortcut)
+		up_threshold = dbs_tuners_ins.up_threshold;
+	else
+		up_threshold = up_threshold_level[1];
+
+	if (avg_load_freq > up_threshold * policy->cur) {
+		unsigned int freq_next;
+		int index;
+
+		index = get_cpu_freq_next(policy->cur);
+
+		if (dbs_tuners_ins.shortcut) {
+			freq_next = policy->max;
+			goto set_freq;
+		}
+
+		if (FREQ_NEED_BURST(policy->cur) &&
+		    avg_load > up_threshold_level[0])
+			freq_next = tblmap[tbl_select[0]][nom_index];
+		else if (avg_load > up_threshold_level[1] &&
+		    avg_load < up_threshold_level[0])
+			freq_next =
+				tblmap[tbl_select[0]][max(index, nom_index-1)];
+		else if (avg_load >= up_threshold_level[0])
+			freq_next =
+				tblmap[tbl_select[0]][index];
+		else
+			freq_next = tblmap[tbl_select[0]]
+					[max(index, low_index-1)];
+
+set_freq:
+		dbs_freq_increase(policy, freq_next);
+
+		if (policy->cur == policy->max)
+			this_dbs_info->rate_mult =
+				dbs_tuners_ins.sampling_down_factor;
+		return;
+	}
+
+	// shortcuts
+	if (input_event_boosted_intelli()) {
+		return;
+	}
+
+	if (num_of_cpus > 1) {
+		if (avg_load_freq > dbs_tuners_ins.up_threshold_multi_core *
+								policy->cur) {
+			if (policy->cur < dbs_tuners_ins.optimal_freq_speed)
+				dbs_freq_increase(policy,
+					dbs_tuners_ins.optimal_freq_speed);
+			return;
+		}
+	}
+
+	if (policy->cur == policy->min){
+		return;
+	}
+
+	// normal path down
+	if (avg_load_freq <
+	    (dbs_tuners_ins.up_threshold - dbs_tuners_ins.down_differential) *
+	     policy->cur) {
+		unsigned int freq_next;
+		freq_next = avg_load_freq /
+				(dbs_tuners_ins.up_threshold -
+				 dbs_tuners_ins.down_differential);
+
+		this_dbs_info->rate_mult = 1;
+
+		if (freq_next < policy->min)
+			freq_next = policy->min;
+
+		if (num_of_cpus > 1) {
+			if (dbs_tuners_ins.optimal_freq_speed >
+				policy->min && avg_load_freq >
+			    ((dbs_tuners_ins.up_threshold_multi_core -
+			    dbs_tuners_ins.down_differential_multi_core) *
+			    policy->cur) &&
+			    freq_next < dbs_tuners_ins.optimal_freq_speed)
+				freq_next = dbs_tuners_ins.optimal_freq_speed;
+		}
+
+		if (dbs_tuners_ins.powersave_bias) {
+			freq_next = powersave_bias_target(policy, freq_next,
+					CPUFREQ_RELATION_L);
+		}
+
+		if (dbs_tuners_ins.freq_down_step) {
+			unsigned int new_freq_next = freq_next;
+			if ((policy->cur - freq_next) >
+				dbs_tuners_ins.freq_down_step) {
+				new_freq_next =
+					policy->cur -
+					dbs_tuners_ins.freq_down_step;
+			}
+
+			if (dbs_tuners_ins.freq_down_step_barrier) {
+				if (dbs_tuners_ins.freq_down_step_barrier <
+					new_freq_next) {
+					new_freq_next =
+					dbs_tuners_ins.freq_down_step_barrier;
+				}
+
+				if (policy->cur <=
+					dbs_tuners_ins.freq_down_step_barrier)
+					new_freq_next = freq_next;
+			}
+
+			freq_next = new_freq_next;
+		}
+
+		__cpufreq_driver_target(policy, freq_next, CPUFREQ_RELATION_L);
+	}
+}
+
+static void do_dbs_timer(struct work_struct *work)
+{
+	struct cpu_dbs_info_s *dbs_info =
+		container_of(work, struct cpu_dbs_info_s, work.work);
+	unsigned int cpu = dbs_info->cpu;
+	int sample_type = dbs_info->sample_type;
+
+	int delay;
+
+	if (unlikely(!cpu_online(cpu) || !dbs_info->cur_policy))
+		return;
+
+	mutex_lock(&dbs_info->timer_mutex);
+
+
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	if (!dbs_tuners_ins.powersave_bias ||
+	    sample_type == DBS_NORMAL_SAMPLE) {
+		dbs_check_cpu(dbs_info);
+		if (dbs_info->freq_lo) {
+
+			dbs_info->sample_type = DBS_SUB_SAMPLE;
+			delay = dbs_info->freq_hi_jiffies;
+		} else {
+			delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate
+				* dbs_info->rate_mult);
+
+			if (num_online_cpus() > 1)
+				delay -= jiffies % delay;
+		}
+	} else {
+		delay = dbs_info->freq_lo_jiffies;
+		if (input_event_boosted_intelli())
+			goto sched_wait;
+
+		__cpufreq_driver_target(dbs_info->cur_policy,
+			dbs_info->freq_lo, CPUFREQ_RELATION_H);
+	}
+
+sched_wait:
+	queue_delayed_work_on(cpu, dbs_wq, &dbs_info->work, delay);
+	mutex_unlock(&dbs_info->timer_mutex);
+}
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
+{
+
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+	if (num_online_cpus() > 1)
+		delay -= jiffies % delay;
+
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	INIT_DEFERRABLE_WORK(&dbs_info->work, do_dbs_timer);
+	queue_delayed_work_on(dbs_info->cpu, dbs_wq, &dbs_info->work, delay);
+}
+
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
+{
+	cancel_delayed_work_sync(&dbs_info->work);
+}
+
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	unsigned int cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int io_busy;
+	int rc;
+
+	io_busy = dbs_tuners_ins.io_is_busy;
+	this_dbs_info = &per_cpu(imm_cpu_dbs_info, policy->cpu);
+	cpu = this_dbs_info->cpu;
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy->cur))
+			return -EINVAL;
+
+		mutex_lock(&dbs_mutex);
+
+		dbs_enable++;
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			unsigned int prev_load;
+			j_dbs_info = &per_cpu(imm_cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&j_dbs_info->prev_cpu_wall,
+						io_busy);
+			if (dbs_tuners_ins.ignore_nice)
+				j_dbs_info->prev_cpu_nice =
+					kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+
+			prev_load = (unsigned int)
+				(j_dbs_info->prev_cpu_wall - j_dbs_info->prev_cpu_idle);
+			j_dbs_info->prev_load = 100 * prev_load /
+				(unsigned int) j_dbs_info->prev_cpu_wall;
+		}
+		this_dbs_info->rate_mult = 1;
+		intellimm_powersave_bias_init_cpu(cpu);
+		if (dbs_enable == 1) {
+			unsigned int latency;
+
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&dbs_attr_group);
+			if (rc) {
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+
+
+			latency = policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+
+			min_sampling_rate = max(min_sampling_rate,
+					MIN_LATENCY_MULTIPLIER * latency);
+			dbs_tuners_ins.sampling_rate =
+				max(min_sampling_rate,
+				    latency * LATENCY_MULTIPLIER);
+
+			if (dbs_tuners_ins.sampling_rate < DEF_SAMPLING_RATE)
+				dbs_tuners_ins.sampling_rate =
+					DEF_SAMPLING_RATE;
+
+			if (dbs_tuners_ins.optimal_freq_speed == 0)
+				dbs_tuners_ins.optimal_freq_speed =
+						policy->min;
+
+			dbs_init_freq_map_table(policy);
+
+		}
+		mutex_unlock(&dbs_mutex);
+
+		if (!intellimm_powersave_bias_setspeed(
+					this_dbs_info->cur_policy,
+					NULL,
+					dbs_tuners_ins.powersave_bias))
+			dbs_timer_init(this_dbs_info);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		dbs_timer_exit(this_dbs_info);
+		this_dbs_info->prev_load = 0;
+
+		mutex_lock(&dbs_mutex);
+		dbs_enable--;
+
+		this_dbs_info->cur_policy = NULL;
+		if (!dbs_enable) {
+			dbs_deinit_freq_map_table();
+
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &dbs_attr_group);
+
+		}
+
+		mutex_unlock(&dbs_mutex);
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&this_dbs_info->timer_mutex);
+		if (this_dbs_info->cur_policy) {
+			if (policy->max < this_dbs_info->cur_policy->cur)
+				__cpufreq_driver_target(this_dbs_info->
+							cur_policy,
+					policy->max, CPUFREQ_RELATION_H);
+			else if (policy->min > this_dbs_info->cur_policy->cur)
+				__cpufreq_driver_target(this_dbs_info->
+							cur_policy,
+					policy->min, CPUFREQ_RELATION_L);
+			else if (dbs_tuners_ins.powersave_bias != 0)
+				intellimm_powersave_bias_setspeed(
+					this_dbs_info->cur_policy,
+					policy,
+					dbs_tuners_ins.powersave_bias);
+		}
+		mutex_unlock(&this_dbs_info->timer_mutex);
+		break;
+	}
+	return 0;
+}
+
+static int cpufreq_gov_dbs_up_task(void *data)
+{
+	struct cpufreq_policy *policy;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int cpu = smp_processor_id();
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule();
+
+		if (kthread_should_stop())
+			break;
+
+		set_current_state(TASK_RUNNING);
+
+		get_online_cpus();
+
+//		if (down_write(&policy->rwsem) < 0)
+			goto bail_acq_sema_failed;
+
+		this_dbs_info = &per_cpu(imm_cpu_dbs_info, cpu);
+		policy = this_dbs_info->cur_policy;
+		if (!policy) {
+
+			goto bail_incorrect_governor;
+		}
+
+		mutex_lock(&this_dbs_info->timer_mutex);
+
+
+		dbs_tuners_ins.powersave_bias = 0;
+		dbs_freq_increase(policy, this_dbs_info->input_event_freq);
+		this_dbs_info->prev_cpu_idle = get_cpu_idle_time(cpu,
+						&this_dbs_info->prev_cpu_wall,
+						dbs_tuners_ins.io_is_busy);
+
+		mutex_unlock(&this_dbs_info->timer_mutex);
+
+bail_incorrect_governor:
+		up_write(&policy->rwsem);
+
+bail_acq_sema_failed:
+		put_online_cpus();
+	}
+
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIMM
+static
+#endif
+struct cpufreq_governor cpufreq_gov_intellimm = {
+	.name			= "intellimm",
+	.governor		= cpufreq_governor_dbs,
+	.max_transition_latency	= TRANSITION_LATENCY_LIMIT,
+	.owner			= THIS_MODULE,
+};
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	u64 idle_time;
+	unsigned int i;
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+	struct task_struct *pthread;
+	int cpu = get_cpu();
+
+	idle_time = get_cpu_idle_time_us(cpu, NULL);
+	put_cpu();
+	if (idle_time != -1ULL) {
+		dbs_tuners_ins.down_differential =
+					MICRO_FREQUENCY_DOWN_DIFFERENTIAL;
+		min_sampling_rate = MICRO_FREQUENCY_MIN_SAMPLE_RATE;
+	} else {
+		min_sampling_rate =
+			MIN_SAMPLING_RATE_RATIO * jiffies_to_usecs(10);
+	}
+
+	dbs_wq = alloc_workqueue("intellimm_dbs_wq", WQ_HIGHPRI, 0);
+	if (!dbs_wq) {
+	  printk(KERN_ERR "Failed to create intellimm_dbs_wq workqueue\n");
+		return -EFAULT;
+	}
+
+	spin_lock_init(&input_boost_lock);
+
+	for_each_possible_cpu(i) {
+		struct cpu_dbs_info_s *this_dbs_info =
+			&per_cpu(imm_cpu_dbs_info, i);
+
+		pthread = kthread_create_on_node(
+				cpufreq_gov_dbs_up_task, NULL,
+				cpu_to_node(i), "kimm_up/%d", i);
+		if (likely(!IS_ERR(pthread))) {
+			kthread_bind(pthread, i);
+			sched_setscheduler_nocheck(pthread,
+						SCHED_FIFO, &param);
+			get_task_struct(pthread);
+			per_cpu(up_task, i) = pthread;
+		}
+		mutex_init(&this_dbs_info->timer_mutex);
+	}
+
+	return cpufreq_register_governor(&cpufreq_gov_intellimm);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	unsigned int i;
+
+	cpufreq_unregister_governor(&cpufreq_gov_intellimm);
+	for_each_possible_cpu(i) {
+		struct cpu_dbs_info_s *this_dbs_info =
+			&per_cpu(imm_cpu_dbs_info, i);
+		if (per_cpu(up_task, i)) {
+			kthread_stop(per_cpu(up_task, i));
+			put_task_struct(per_cpu(up_task, i));
+		}
+
+		mutex_destroy(&this_dbs_info->timer_mutex);
+	}
+	destroy_workqueue(dbs_wq);
+}
+
+MODULE_AUTHOR("Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>");
+MODULE_AUTHOR("Alexey Starikovskiy <alexey.y.starikovskiy@intel.com>");
+MODULE_AUTHOR("Paul Reioux <reioux@gmail.com>");
+MODULE_DESCRIPTION("'cpufreq_intellimm' - A simple min/max cpufreq governor"
+	"for Low Latency Frequency Transition capable processors");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIMM
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_interactive_pro.c b/drivers/cpufreq/cpufreq_interactive_pro.c
new file mode 100644
index 0000000..2e448fb
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_interactive_pro.c
@@ -0,0 +1,1562 @@
+/*
+ * drivers/cpufreq/cpufreq_interactive.c
+ *
+ * Copyright (C) 2010 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * Author: Mike Chan (mike@android.com)
+ *
+ */
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/rwsem.h>
+#include <linux/sched.h>
+#include <linux/sched/rt.h>
+#include <linux/tick.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/kernel_stat.h>
+#include <asm/cputime.h>
+
+#include <trace/events/cpufreq_interactive.h>
+
+static int active_count;
+
+struct cpufreq_interactive_cpuinfo {
+	struct timer_list cpu_timer;
+	struct timer_list cpu_slack_timer;
+	spinlock_t load_lock; /* protects the next 4 fields */
+	u64 time_in_idle;
+	u64 time_in_idle_timestamp;
+	u64 cputime_speedadj;
+	u64 cputime_speedadj_timestamp;
+	u64 last_evaluated_jiffy;
+	struct cpufreq_policy *policy;
+	struct cpufreq_frequency_table *freq_table;
+	spinlock_t target_freq_lock; /*protects target freq */
+	unsigned int target_freq;
+	unsigned int floor_freq;
+	unsigned int max_freq;
+	u64 floor_validate_time;
+	u64 hispeed_validate_time;
+	struct rw_semaphore enable_sem;
+	int governor_enabled;
+	int prev_load;
+	bool limits_changed;
+};
+
+static DEFINE_PER_CPU(struct cpufreq_interactive_cpuinfo, cpuinfo);
+
+/* realtime thread handles frequency scaling */
+static struct task_struct *speedchange_task;
+static cpumask_t speedchange_cpumask;
+static spinlock_t speedchange_cpumask_lock;
+static struct mutex gov_lock;
+
+/* Hi speed to bump to from lo speed when load burst (default max) */
+static unsigned int hispeed_freq;
+
+/* Go to hi speed when CPU load at or above this value. */
+#define DEFAULT_GO_HISPEED_LOAD 99
+static unsigned long go_hispeed_load = DEFAULT_GO_HISPEED_LOAD;
+
+/* Sampling down factor to be applied to min_sample_time at max freq */
+static unsigned int sampling_down_factor;
+
+/* Target load.  Lower values result in higher CPU speeds. */
+#define DEFAULT_TARGET_LOAD 90
+static unsigned int default_target_loads[] = {DEFAULT_TARGET_LOAD};
+static spinlock_t target_loads_lock;
+static unsigned int *target_loads = default_target_loads;
+static int ntarget_loads = ARRAY_SIZE(default_target_loads);
+
+/*
+ * Frequency calculation threshold.  Avoid freq oscillations up to this
+ * threshold and allow for dynamic changes above (default cpuinfo min).
+*/
+static unsigned long freq_calc_thresh;
+
+/*
+ * The minimum amount of time to spend at a frequency before we can ramp down.
+ */
+#define DEFAULT_MIN_SAMPLE_TIME (80 * USEC_PER_MSEC)
+static unsigned long min_sample_time = DEFAULT_MIN_SAMPLE_TIME;
+
+/*
+ * The sample rate of the timer used to increase frequency
+ */
+#define DEFAULT_TIMER_RATE (20 * USEC_PER_MSEC)
+static unsigned long timer_rate = DEFAULT_TIMER_RATE;
+
+/* Busy SDF parameters*/
+#define MIN_BUSY_TIME (100 * USEC_PER_MSEC)
+
+/*
+ * Wait this long before raising speed above hispeed, by default a single
+ * timer interval.
+ */
+#define DEFAULT_ABOVE_HISPEED_DELAY DEFAULT_TIMER_RATE
+static unsigned int default_above_hispeed_delay[] = {
+	DEFAULT_ABOVE_HISPEED_DELAY };
+static spinlock_t above_hispeed_delay_lock;
+static unsigned int *above_hispeed_delay = default_above_hispeed_delay;
+static int nabove_hispeed_delay = ARRAY_SIZE(default_above_hispeed_delay);
+
+/* Non-zero means indefinite speed boost active */
+static int boost_val;
+/* Duration of a boot pulse in usecs */
+static int boostpulse_duration_val = DEFAULT_MIN_SAMPLE_TIME;
+/* End time of boost pulse in ktime converted to usecs */
+static u64 boostpulse_endtime;
+
+/*
+ * Max additional time to wait in idle, beyond timer_rate, at speeds above
+ * minimum before wakeup to reduce speed, or -1 if unnecessary.
+ */
+#define DEFAULT_TIMER_SLACK (4 * DEFAULT_TIMER_RATE)
+static int timer_slack_val = DEFAULT_TIMER_SLACK;
+
+static bool io_is_busy;
+
+/*
+ * If the max load among other CPUs is higher than up_threshold_any_cpu_load
+ * or if the highest frequency among the other CPUs is higher than
+ * up_threshold_any_cpu_freq then do not let the frequency to drop below
+ * sync_freq
+ */
+static unsigned int up_threshold_any_cpu_load;
+static unsigned int sync_freq;
+static unsigned int up_threshold_any_cpu_freq;
+
+static bool use_sched_hint;
+
+/* Round to starting jiffy of next evaluation window */
+static u64 round_to_nw_start(u64 jif)
+{
+	unsigned long step = usecs_to_jiffies(timer_rate);
+
+	do_div(jif, step);
+	return (jif + 1) * step;
+}
+
+static inline int set_window_helper(void)
+{
+	return sched_set_window(round_to_nw_start(get_jiffies_64()),
+			 usecs_to_jiffies(timer_rate));
+}
+
+/*
+ * Schedule the timer to fire immediately. This is a helper function for
+ * atomic notification from scheduler so that CPU load can be re-evaluated
+ * immediately. Since we are just re-evaluating previous window's load, we
+ * should not push back slack timer.
+ */
+static void cpufreq_interactive_timer_resched_now(unsigned long cpu)
+{
+	unsigned long flags;
+	struct cpufreq_interactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	del_timer(&pcpu->cpu_timer);
+	pcpu->cpu_timer.expires = jiffies;
+	add_timer_on(&pcpu->cpu_timer, cpu);
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+static void cpufreq_interactive_timer_resched(unsigned long cpu)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 expires;
+	unsigned long flags;
+	u64 now = ktime_to_us(ktime_get());
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->time_in_idle =
+		get_cpu_idle_time(smp_processor_id(),
+				     &pcpu->time_in_idle_timestamp, io_is_busy);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	expires = round_to_nw_start(pcpu->last_evaluated_jiffy);
+	del_timer(&pcpu->cpu_timer);
+	pcpu->cpu_timer.expires = expires;
+	add_timer_on(&pcpu->cpu_timer, cpu);
+
+	if (timer_slack_val >= 0 &&
+	    (pcpu->target_freq > pcpu->policy->min ||
+		(pcpu->target_freq == pcpu->policy->min &&
+		 now < boostpulse_endtime))) {
+		expires += usecs_to_jiffies(timer_slack_val);
+		del_timer(&pcpu->cpu_slack_timer);
+		pcpu->cpu_slack_timer.expires = expires;
+		add_timer_on(&pcpu->cpu_slack_timer, cpu);
+	}
+
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+/* The caller shall take enable_sem write semaphore to avoid any timer race.
+ * The cpu_timer and cpu_slack_timer must be deactivated when calling this
+ * function.
+ */
+static void cpufreq_interactive_timer_start(int cpu)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 expires = round_to_nw_start(pcpu->last_evaluated_jiffy);
+	unsigned long flags;
+	u64 now = ktime_to_us(ktime_get());
+
+	pcpu->cpu_timer.expires = expires;
+	add_timer_on(&pcpu->cpu_timer, cpu);
+	if (timer_slack_val >= 0 &&
+	    (pcpu->target_freq > pcpu->policy->min ||
+		(pcpu->target_freq == pcpu->policy->min &&
+		 now < boostpulse_endtime))) {
+		expires += usecs_to_jiffies(timer_slack_val);
+		pcpu->cpu_slack_timer.expires = expires;
+		add_timer_on(&pcpu->cpu_slack_timer, cpu);
+	}
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->time_in_idle =
+		get_cpu_idle_time(cpu, &pcpu->time_in_idle_timestamp, io_is_busy);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+static unsigned int freq_to_above_hispeed_delay(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay - 1 &&
+			freq >= above_hispeed_delay[i+1]; i += 2)
+		;
+
+	ret = above_hispeed_delay[i];
+	ret = (ret > (1 * USEC_PER_MSEC)) ? (ret - (1 * USEC_PER_MSEC)) : ret;
+
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static unsigned int freq_to_targetload(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+
+	for (i = 0; i < ntarget_loads - 1 && freq >= target_loads[i+1]; i += 2)
+		;
+
+	ret = target_loads[i];
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+/*
+ * If increasing frequencies never map to a lower target load then
+ * choose_freq() will find the minimum frequency that does not exceed its
+ * target load given the current load.
+ */
+
+static unsigned int choose_freq(
+	struct cpufreq_interactive_cpuinfo *pcpu, unsigned int loadadjfreq)
+{
+	unsigned int freq = pcpu->policy->cur;
+	unsigned int prevfreq, freqmin, freqmax;
+	unsigned int tl;
+	int index;
+
+	freqmin = 0;
+	freqmax = UINT_MAX;
+
+	do {
+		prevfreq = freq;
+		tl = freq_to_targetload(freq);
+
+		/*
+		 * Find the lowest frequency where the computed load is less
+		 * than or equal to the target load.
+		 */
+
+		if (cpufreq_frequency_table_target(
+			    pcpu->policy, pcpu->freq_table, loadadjfreq / tl,
+			    CPUFREQ_RELATION_L, &index))
+			break;
+		freq = pcpu->freq_table[index].frequency;
+
+		if (freq > prevfreq) {
+			/* The previous frequency is too low. */
+			freqmin = prevfreq;
+
+			if (freq >= freqmax) {
+				/*
+				 * Find the highest frequency that is less
+				 * than freqmax.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmax - 1, CPUFREQ_RELATION_H,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				if (freq == freqmin) {
+					/*
+					 * The first frequency below freqmax
+					 * has already been found to be too
+					 * low.  freqmax is the lowest speed
+					 * we found that is fast enough.
+					 */
+					freq = freqmax;
+					break;
+				}
+			}
+		} else if (freq < prevfreq) {
+			/* The previous frequency is high enough. */
+			freqmax = prevfreq;
+
+			if (freq <= freqmin) {
+				/*
+				 * Find the lowest frequency that is higher
+				 * than freqmin.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmin + 1, CPUFREQ_RELATION_L,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				/*
+				 * If freqmax is the first frequency above
+				 * freqmin then we have already found that
+				 * this speed is fast enough.
+				 */
+				if (freq == freqmax)
+					break;
+			}
+		}
+
+		/* If same frequency chosen as previous then done. */
+	} while (freq != prevfreq);
+
+	return freq;
+}
+
+static u64 update_load(int cpu)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 now;
+	u64 now_idle;
+	unsigned int delta_idle;
+	unsigned int delta_time;
+	u64 active_time;
+
+	now_idle = get_cpu_idle_time(cpu, &now, io_is_busy);
+	delta_idle = (unsigned int)(now_idle - pcpu->time_in_idle);
+	delta_time = (unsigned int)(now - pcpu->time_in_idle_timestamp);
+
+	if (delta_time <= delta_idle)
+		active_time = 0;
+	else
+		active_time = delta_time - delta_idle;
+
+	pcpu->cputime_speedadj += active_time * pcpu->policy->cur;
+
+	pcpu->time_in_idle = now_idle;
+	pcpu->time_in_idle_timestamp = now;
+	return now;
+}
+
+static void cpufreq_interactive_timer(unsigned long data)
+{
+	u64 now;
+	unsigned int delta_time;
+	u64 cputime_speedadj;
+	int cpu_load;
+	struct cpufreq_interactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, data);
+	unsigned int new_freq;
+	unsigned int loadadjfreq;
+	unsigned int index;
+	unsigned long flags;
+	bool boosted;
+	unsigned long mod_min_sample_time;
+	int i, max_load;
+	unsigned int max_freq;
+	unsigned int boosted_freq;
+	struct cpufreq_interactive_cpuinfo *picpu;
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled)
+		goto exit;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->last_evaluated_jiffy = get_jiffies_64();
+	now = update_load(data);
+	if (use_sched_hint) {
+		/*
+		 * Unlock early to avoid deadlock.
+		 *
+		 * cpufreq_interactive_timer_resched_now() is called
+		 * in thread migration notification which already holds
+		 * rq lock. Then it locks load_lock to avoid racing with
+		 * cpufreq_interactive_timer_resched/start().
+		 * sched_get_busy() will also acquire rq lock. Thus we
+		 * can't hold load_lock when calling sched_get_busy().
+		 *
+		 * load_lock used in this function protects time
+		 * and load information. These stats are not used when
+		 * scheduler hint is available. Thus unlocking load_lock
+		 * early is perfectly OK.
+		 */
+		spin_unlock_irqrestore(&pcpu->load_lock, flags);
+		cputime_speedadj = (u64)sched_get_busy(data) *
+				pcpu->policy->cpuinfo.max_freq;
+		do_div(cputime_speedadj, timer_rate);
+	} else {
+		delta_time = (unsigned int)
+				(now - pcpu->cputime_speedadj_timestamp);
+		cputime_speedadj = pcpu->cputime_speedadj;
+		spin_unlock_irqrestore(&pcpu->load_lock, flags);
+		if (WARN_ON_ONCE(!delta_time))
+			goto rearm;
+		do_div(cputime_speedadj, delta_time);
+	}
+
+	spin_lock_irqsave(&pcpu->target_freq_lock, flags);
+	loadadjfreq = (unsigned int)cputime_speedadj * 100;
+	cpu_load = loadadjfreq / pcpu->target_freq;
+	pcpu->prev_load = cpu_load;
+	boosted = boost_val || now < boostpulse_endtime;
+	boosted_freq = max(hispeed_freq, pcpu->policy->min);
+
+	if (cpu_load >= go_hispeed_load || boosted) {
+		if (pcpu->target_freq < boosted_freq) {
+			new_freq = boosted_freq;
+		} else {
+			new_freq = choose_freq(pcpu, loadadjfreq);
+
+			if (new_freq > freq_calc_thresh)
+				new_freq = pcpu->policy->max * cpu_load / 100;
+
+			if (new_freq < boosted_freq)
+				new_freq = boosted_freq;
+		}
+	} else {
+		new_freq = choose_freq(pcpu, loadadjfreq);
+
+		if (new_freq > freq_calc_thresh)
+			new_freq = pcpu->policy->max * cpu_load / 100;
+
+		if (sync_freq && new_freq < sync_freq) {
+
+			max_load = 0;
+			max_freq = 0;
+
+			for_each_online_cpu(i) {
+				picpu = &per_cpu(cpuinfo, i);
+
+				if (i == data || picpu->prev_load <
+						up_threshold_any_cpu_load)
+					continue;
+
+				max_load = max(max_load, picpu->prev_load);
+				max_freq = max(max_freq, picpu->target_freq);
+			}
+
+			if (max_freq > up_threshold_any_cpu_freq ||
+				max_load >= up_threshold_any_cpu_load)
+				new_freq = sync_freq;
+		}
+	}
+
+	if (pcpu->target_freq >= boosted_freq &&
+	    new_freq > pcpu->target_freq &&
+	    now - pcpu->hispeed_validate_time <
+	    freq_to_above_hispeed_delay(pcpu->target_freq)) {
+		trace_cpufreq_interactive_notyet(
+			data, cpu_load, pcpu->target_freq,
+			pcpu->policy->cur, new_freq);
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	pcpu->hispeed_validate_time = now;
+
+	if (cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table,
+					   new_freq, CPUFREQ_RELATION_L,
+					   &index)) {
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	new_freq = pcpu->freq_table[index].frequency;
+
+	/*
+	 * Do not scale below floor_freq unless we have been at or above the
+	 * floor frequency for the minimum sample time since last validated.
+	 */
+	if (sampling_down_factor && pcpu->policy->cur == pcpu->policy->max)
+		mod_min_sample_time = sampling_down_factor;
+	else
+		mod_min_sample_time = min_sample_time;
+
+	if (pcpu->limits_changed) {
+		if (sampling_down_factor &&
+			(pcpu->policy->cur != pcpu->policy->max))
+			mod_min_sample_time = 0;
+
+		pcpu->limits_changed = false;
+	}
+
+	if (new_freq < pcpu->floor_freq) {
+		if (now - pcpu->floor_validate_time < mod_min_sample_time) {
+			trace_cpufreq_interactive_notyet(
+				data, cpu_load, pcpu->target_freq,
+				pcpu->policy->cur, new_freq);
+			spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+			goto rearm;
+		}
+	}
+
+	/*
+	 * Update the timestamp for checking whether speed has been held at
+	 * or above the selected frequency for a minimum of min_sample_time,
+	 * if not boosted to boosted_freq.  If boosted to boosted_freq then we
+	 * allow the speed to drop as soon as the boostpulse duration expires
+	 * (or the indefinite boost is turned off).
+	 */
+
+	if (!boosted || new_freq > boosted_freq) {
+		pcpu->floor_freq = new_freq;
+		pcpu->floor_validate_time = now;
+	}
+
+	if (pcpu->target_freq == new_freq) {
+		trace_cpufreq_interactive_already(
+			data, cpu_load, pcpu->target_freq,
+			pcpu->policy->cur, new_freq);
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm_if_notmax;
+	}
+
+	trace_cpufreq_interactive_target(data, cpu_load, pcpu->target_freq,
+					 pcpu->policy->cur, new_freq);
+
+	pcpu->target_freq = new_freq;
+	spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+	cpumask_set_cpu(data, &speedchange_cpumask);
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+	wake_up_process(speedchange_task);
+
+rearm_if_notmax:
+	/*
+	 * Already set max speed and don't see a need to change that,
+	 * wait until next idle to re-evaluate, don't need timer.
+	 */
+	if (pcpu->target_freq == pcpu->policy->max)
+		goto exit;
+
+rearm:
+	if (!timer_pending(&pcpu->cpu_timer))
+		cpufreq_interactive_timer_resched(data);
+
+exit:
+	up_read(&pcpu->enable_sem);
+	return;
+}
+
+static void cpufreq_interactive_idle_start(void)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+	int pending;
+	u64 now;
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled) {
+		up_read(&pcpu->enable_sem);
+		return;
+	}
+
+	now = ktime_to_us(ktime_get());
+	pending = timer_pending(&pcpu->cpu_timer);
+
+	if (pcpu->target_freq > pcpu->policy->min ||
+	    (pcpu->target_freq == pcpu->policy->min &&
+	     now < boostpulse_endtime)) {
+		/*
+		 * Entering idle while not at lowest speed.  On some
+		 * platforms this can hold the other CPU(s) at that speed
+		 * even though the CPU is idle. Set a timer to re-evaluate
+		 * speed so this idle CPU doesn't hold the other CPUs above
+		 * min indefinitely.  This should probably be a quirk of
+		 * the CPUFreq driver.
+		 */
+		if (!pending) {
+			cpufreq_interactive_timer_resched(smp_processor_id());
+
+			now = ktime_to_us(ktime_get());
+			if ((pcpu->policy->cur == pcpu->policy->max) &&
+				(now - pcpu->hispeed_validate_time) >
+							MIN_BUSY_TIME) {
+				pcpu->floor_validate_time = now;
+			}
+
+		}
+	}
+
+	up_read(&pcpu->enable_sem);
+}
+
+static void cpufreq_interactive_idle_end(void)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled) {
+		up_read(&pcpu->enable_sem);
+		return;
+	}
+
+	/* Arm the timer for 1-2 ticks later if not already. */
+	if (!timer_pending(&pcpu->cpu_timer)) {
+		cpufreq_interactive_timer_resched(smp_processor_id());
+	} else if (time_after_eq(jiffies, pcpu->cpu_timer.expires)) {
+		del_timer(&pcpu->cpu_timer);
+		del_timer(&pcpu->cpu_slack_timer);
+		cpufreq_interactive_timer(smp_processor_id());
+	}
+
+	up_read(&pcpu->enable_sem);
+}
+
+static int cpufreq_interactive_speedchange_task(void *data)
+{
+	unsigned int cpu;
+	cpumask_t tmp_mask;
+	unsigned long flags;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+		if (cpumask_empty(&speedchange_cpumask)) {
+			spin_unlock_irqrestore(&speedchange_cpumask_lock,
+					       flags);
+			schedule();
+
+			if (kthread_should_stop())
+				break;
+
+			spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+		}
+
+		set_current_state(TASK_RUNNING);
+		tmp_mask = speedchange_cpumask;
+		cpumask_clear(&speedchange_cpumask);
+		spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+		for_each_cpu(cpu, &tmp_mask) {
+			unsigned int j;
+			unsigned int max_freq = 0;
+
+			pcpu = &per_cpu(cpuinfo, cpu);
+			if (!down_read_trylock(&pcpu->enable_sem))
+				continue;
+			if (!pcpu->governor_enabled) {
+				up_read(&pcpu->enable_sem);
+				continue;
+			}
+
+			for_each_cpu(j, pcpu->policy->cpus) {
+				struct cpufreq_interactive_cpuinfo *pjcpu =
+					&per_cpu(cpuinfo, j);
+
+				if (pjcpu->target_freq > max_freq)
+					max_freq = pjcpu->target_freq;
+			}
+
+			if (max_freq != pcpu->policy->cur)
+				__cpufreq_driver_target(pcpu->policy,
+							max_freq,
+							CPUFREQ_RELATION_H);
+			trace_cpufreq_interactive_setspeed(cpu,
+						     pcpu->target_freq,
+						     pcpu->policy->cur);
+
+			up_read(&pcpu->enable_sem);
+		}
+	}
+
+	return 0;
+}
+
+static void cpufreq_interactive_boost(void)
+{
+	int i;
+	int anyboost = 0;
+	unsigned long flags[2];
+	struct cpufreq_interactive_cpuinfo *pcpu;
+
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags[0]);
+
+	for_each_online_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+
+		spin_lock_irqsave(&pcpu->target_freq_lock, flags[1]);
+		if (pcpu->target_freq < hispeed_freq) {
+			pcpu->target_freq = hispeed_freq;
+			cpumask_set_cpu(i, &speedchange_cpumask);
+			pcpu->hispeed_validate_time =
+				ktime_to_us(ktime_get());
+			anyboost = 1;
+		}
+
+		/*
+		 * Set floor freq and (re)start timer for when last
+		 * validated.
+		 */
+
+		pcpu->floor_freq = hispeed_freq;
+		pcpu->floor_validate_time = ktime_to_us(ktime_get());
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags[1]);
+	}
+
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags[0]);
+
+	if (anyboost)
+		wake_up_process(speedchange_task);
+}
+
+static int load_change_callback(struct notifier_block *nb, unsigned long val,
+				void *data)
+{
+	unsigned long cpu = (unsigned long) data;
+	struct cpufreq_interactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+
+	/*
+	 * We can't acquire enable_sem here. However, this doesn't matter
+	 * because timer function will grab it and check if governor is
+	 * enabled before doing anything. Therefore the execution is
+	 * still correct.
+	 */
+	if (pcpu->governor_enabled)
+		cpufreq_interactive_timer_resched_now(cpu);
+
+	return 0;
+}
+
+static struct notifier_block load_notifier_block = {
+	.notifier_call = load_change_callback,
+};
+
+static int cpufreq_interactive_notifier(
+	struct notifier_block *nb, unsigned long val, void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+	int cpu;
+	unsigned long flags;
+
+	if (val == CPUFREQ_POSTCHANGE) {
+		pcpu = &per_cpu(cpuinfo, freq->cpu);
+		if (!down_read_trylock(&pcpu->enable_sem))
+			return 0;
+		if (!pcpu->governor_enabled) {
+			up_read(&pcpu->enable_sem);
+			return 0;
+		}
+
+		for_each_cpu(cpu, pcpu->policy->cpus) {
+			struct cpufreq_interactive_cpuinfo *pjcpu =
+				&per_cpu(cpuinfo, cpu);
+			if (cpu != freq->cpu) {
+				if (!down_read_trylock(&pjcpu->enable_sem))
+					continue;
+				if (!pjcpu->governor_enabled) {
+					up_read(&pjcpu->enable_sem);
+					continue;
+				}
+			}
+			spin_lock_irqsave(&pjcpu->load_lock, flags);
+			update_load(cpu);
+			spin_unlock_irqrestore(&pjcpu->load_lock, flags);
+			if (cpu != freq->cpu)
+				up_read(&pjcpu->enable_sem);
+		}
+
+		up_read(&pcpu->enable_sem);
+	}
+	return 0;
+}
+
+static struct notifier_block cpufreq_notifier_block = {
+	.notifier_call = cpufreq_interactive_notifier,
+};
+
+static unsigned int *get_tokenized_data(const char *buf, int *num_tokens)
+{
+	const char *cp;
+	int i;
+	int ntokens = 1;
+	unsigned int *tokenized_data;
+	int err = -EINVAL;
+
+	cp = buf;
+	while ((cp = strpbrk(cp + 1, " :")))
+		ntokens++;
+
+	if (!(ntokens & 0x1))
+		goto err;
+
+	tokenized_data = kmalloc(ntokens * sizeof(unsigned int), GFP_KERNEL);
+	if (!tokenized_data) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	cp = buf;
+	i = 0;
+	while (i < ntokens) {
+		if (sscanf(cp, "%u", &tokenized_data[i++]) != 1)
+			goto err_kfree;
+
+		cp = strpbrk(cp, " :");
+		if (!cp)
+			break;
+		cp++;
+	}
+
+	if (i != ntokens)
+		goto err_kfree;
+
+	*num_tokens = ntokens;
+	return tokenized_data;
+
+err_kfree:
+	kfree(tokenized_data);
+err:
+	return ERR_PTR(err);
+}
+
+static ssize_t show_target_loads(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+
+	for (i = 0; i < ntarget_loads; i++)
+		ret += sprintf(buf + ret, "%u%s", target_loads[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+static ssize_t store_target_loads(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_target_loads = NULL;
+	unsigned long flags;
+
+	new_target_loads = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_target_loads))
+		return PTR_RET(new_target_loads);
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+	if (target_loads != default_target_loads)
+		kfree(target_loads);
+	target_loads = new_target_loads;
+	ntarget_loads = ntokens;
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return count;
+}
+
+static struct global_attr target_loads_attr =
+	__ATTR(target_loads, S_IRUGO | S_IWUSR,
+		show_target_loads, store_target_loads);
+
+static ssize_t show_freq_calc_thresh(struct kobject *kobj,
+				     struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", freq_calc_thresh);
+}
+
+static ssize_t store_freq_calc_thresh(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	freq_calc_thresh = val;
+	return count;
+}
+
+static struct global_attr freq_calc_thresh_attr = __ATTR(freq_calc_thresh, 0644,
+		show_freq_calc_thresh, store_freq_calc_thresh);
+
+static ssize_t show_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay; i++)
+		ret += sprintf(buf + ret, "%u%s", above_hispeed_delay[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static ssize_t store_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_above_hispeed_delay = NULL;
+	unsigned long flags;
+
+	new_above_hispeed_delay = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_above_hispeed_delay))
+		return PTR_RET(new_above_hispeed_delay);
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+	if (above_hispeed_delay != default_above_hispeed_delay)
+		kfree(above_hispeed_delay);
+	above_hispeed_delay = new_above_hispeed_delay;
+	nabove_hispeed_delay = ntokens;
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return count;
+
+}
+
+static struct global_attr above_hispeed_delay_attr =
+	__ATTR(above_hispeed_delay, S_IRUGO | S_IWUSR,
+		show_above_hispeed_delay, store_above_hispeed_delay);
+
+static ssize_t show_hispeed_freq(struct kobject *kobj,
+				 struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", hispeed_freq);
+}
+
+static ssize_t store_hispeed_freq(struct kobject *kobj,
+				  struct attribute *attr, const char *buf,
+				  size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	hispeed_freq = val;
+	return count;
+}
+
+static struct global_attr hispeed_freq_attr = __ATTR(hispeed_freq, 0644,
+		show_hispeed_freq, store_hispeed_freq);
+
+static ssize_t show_sampling_down_factor(struct kobject *kobj,
+				struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sampling_down_factor);
+}
+
+static ssize_t store_sampling_down_factor(struct kobject *kobj,
+				struct attribute *attr, const char *buf,
+				size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	sampling_down_factor = val;
+	return count;
+}
+
+static struct global_attr sampling_down_factor_attr =
+				__ATTR(sampling_down_factor, 0644,
+		show_sampling_down_factor, store_sampling_down_factor);
+
+static ssize_t show_go_hispeed_load(struct kobject *kobj,
+				     struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", go_hispeed_load);
+}
+
+static ssize_t store_go_hispeed_load(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	go_hispeed_load = val;
+	return count;
+}
+
+static struct global_attr go_hispeed_load_attr = __ATTR(go_hispeed_load, 0644,
+		show_go_hispeed_load, store_go_hispeed_load);
+
+static ssize_t show_min_sample_time(struct kobject *kobj,
+				struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", min_sample_time);
+}
+
+static ssize_t store_min_sample_time(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	min_sample_time = val;
+	return count;
+}
+
+static struct global_attr min_sample_time_attr = __ATTR(min_sample_time, 0644,
+		show_min_sample_time, store_min_sample_time);
+
+static ssize_t show_timer_rate(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", timer_rate);
+}
+
+static ssize_t store_timer_rate(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	timer_rate = val;
+	set_window_helper();
+	return count;
+}
+
+static struct global_attr timer_rate_attr = __ATTR(timer_rate, 0644,
+		show_timer_rate, store_timer_rate);
+
+static ssize_t show_timer_slack(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", timer_slack_val);
+}
+
+static ssize_t store_timer_slack(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret < 0)
+		return ret;
+
+	timer_slack_val = val;
+	return count;
+}
+
+define_one_global_rw(timer_slack);
+
+static ssize_t show_boost(struct kobject *kobj, struct attribute *attr,
+			  char *buf)
+{
+	return sprintf(buf, "%d\n", boost_val);
+}
+
+static ssize_t store_boost(struct kobject *kobj, struct attribute *attr,
+			   const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boost_val = val;
+
+	if (boost_val) {
+		trace_cpufreq_interactive_boost("on");
+		cpufreq_interactive_boost();
+	} else {
+		trace_cpufreq_interactive_unboost("off");
+	}
+
+	return count;
+}
+
+define_one_global_rw(boost);
+
+static ssize_t store_boostpulse(struct kobject *kobj, struct attribute *attr,
+				const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_endtime = ktime_to_us(ktime_get()) + boostpulse_duration_val;
+	trace_cpufreq_interactive_boost("pulse");
+	cpufreq_interactive_boost();
+	return count;
+}
+
+static struct global_attr boostpulse =
+	__ATTR(boostpulse, 0200, NULL, store_boostpulse);
+
+static ssize_t show_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", boostpulse_duration_val);
+}
+
+static ssize_t store_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_duration_val = val;
+	return count;
+}
+
+define_one_global_rw(boostpulse_duration);
+
+static ssize_t show_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	io_is_busy = val;
+	sched_set_io_is_busy(val);
+	return count;
+}
+
+static struct global_attr io_is_busy_attr = __ATTR(io_is_busy, 0644,
+		show_io_is_busy, store_io_is_busy);
+
+static ssize_t show_sync_freq(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sync_freq);
+}
+
+static ssize_t store_sync_freq(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	sync_freq = val;
+	return count;
+}
+
+static struct global_attr sync_freq_attr = __ATTR(sync_freq, 0644,
+		show_sync_freq, store_sync_freq);
+
+static ssize_t show_up_threshold_any_cpu_load(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", up_threshold_any_cpu_load);
+}
+
+static ssize_t store_up_threshold_any_cpu_load(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	up_threshold_any_cpu_load = val;
+	return count;
+}
+
+static struct global_attr up_threshold_any_cpu_load_attr =
+		__ATTR(up_threshold_any_cpu_load, 0644,
+		show_up_threshold_any_cpu_load,
+				store_up_threshold_any_cpu_load);
+
+static ssize_t show_up_threshold_any_cpu_freq(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", up_threshold_any_cpu_freq);
+}
+
+static ssize_t store_up_threshold_any_cpu_freq(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	up_threshold_any_cpu_freq = val;
+	return count;
+}
+
+static struct global_attr up_threshold_any_cpu_freq_attr =
+		__ATTR(up_threshold_any_cpu_freq, 0644,
+		show_up_threshold_any_cpu_freq,
+				store_up_threshold_any_cpu_freq);
+
+static struct attribute *interactive_attributes[] = {
+	&target_loads_attr.attr,
+	&freq_calc_thresh_attr.attr,
+	&above_hispeed_delay_attr.attr,
+	&hispeed_freq_attr.attr,
+	&go_hispeed_load_attr.attr,
+	&min_sample_time_attr.attr,
+	&timer_rate_attr.attr,
+	&timer_slack.attr,
+	&boost.attr,
+	&boostpulse.attr,
+	&boostpulse_duration.attr,
+	&io_is_busy_attr.attr,
+	&sampling_down_factor_attr.attr,
+	&sync_freq_attr.attr,
+	&up_threshold_any_cpu_load_attr.attr,
+	&up_threshold_any_cpu_freq_attr.attr,
+	NULL,
+};
+
+static struct attribute_group interactive_attr_group = {
+	.attrs = interactive_attributes,
+	.name = "interactive_pro",
+};
+
+static int cpufreq_interactive_idle_notifier(struct notifier_block *nb,
+					     unsigned long val,
+					     void *data)
+{
+	switch (val) {
+	case IDLE_START:
+		cpufreq_interactive_idle_start();
+		break;
+	case IDLE_END:
+		cpufreq_interactive_idle_end();
+		break;
+	}
+
+	return 0;
+}
+
+static struct notifier_block cpufreq_interactive_idle_nb = {
+	.notifier_call = cpufreq_interactive_idle_notifier,
+};
+
+static int cpufreq_governor_interactive(struct cpufreq_policy *policy,
+		unsigned int event)
+{
+	int rc;
+	unsigned int j;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned long flags;
+	unsigned int anyboost;
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		mutex_lock(&gov_lock);
+
+		freq_table =
+			cpufreq_frequency_get_table(policy->cpu);
+		if (!hispeed_freq)
+			hispeed_freq = policy->max;
+		freq_calc_thresh = policy->cpuinfo.min_freq;
+
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			pcpu->policy = policy;
+			pcpu->target_freq = policy->cur;
+			pcpu->freq_table = freq_table;
+			pcpu->floor_freq = pcpu->target_freq;
+			pcpu->floor_validate_time =
+				ktime_to_us(ktime_get());
+			pcpu->hispeed_validate_time =
+				pcpu->floor_validate_time;
+			pcpu->max_freq = policy->max;
+			down_write(&pcpu->enable_sem);
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			pcpu->last_evaluated_jiffy = get_jiffies_64();
+			cpufreq_interactive_timer_start(j);
+			pcpu->governor_enabled = 1;
+			up_write(&pcpu->enable_sem);
+		}
+
+		/*
+		 * Do not register the idle hook and create sysfs
+		 * entries if we have already done so.
+		 */
+		if (++active_count > 1) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		rc = sysfs_create_group(cpufreq_global_kobject,
+				&interactive_attr_group);
+		if (rc) {
+			mutex_unlock(&gov_lock);
+			return rc;
+		}
+
+		idle_notifier_register(&cpufreq_interactive_idle_nb);
+		cpufreq_register_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+
+		rc = set_window_helper();
+		if (!rc) {
+			use_sched_hint = true;
+			sched_set_io_is_busy(io_is_busy);
+			atomic_notifier_chain_register(
+				&load_alert_notifier_head,
+				&load_notifier_block);
+		}
+		mutex_unlock(&gov_lock);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		mutex_lock(&gov_lock);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			down_write(&pcpu->enable_sem);
+			pcpu->governor_enabled = 0;
+			pcpu->target_freq = 0;
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			up_write(&pcpu->enable_sem);
+		}
+
+		if (--active_count > 0) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		cpufreq_unregister_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		idle_notifier_unregister(&cpufreq_interactive_idle_nb);
+		sysfs_remove_group(cpufreq_global_kobject,
+				&interactive_attr_group);
+
+		if (use_sched_hint) {
+			atomic_notifier_chain_unregister(
+					&load_alert_notifier_head,
+					&load_notifier_block);
+			use_sched_hint = false;
+		}
+		mutex_unlock(&gov_lock);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		if (policy->max < policy->cur)
+			__cpufreq_driver_target(policy,
+					policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > policy->cur)
+			__cpufreq_driver_target(policy,
+					policy->min, CPUFREQ_RELATION_L);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+
+			down_read(&pcpu->enable_sem);
+			if (pcpu->governor_enabled == 0) {
+				up_read(&pcpu->enable_sem);
+				continue;
+			}
+
+			spin_lock_irqsave(&pcpu->target_freq_lock, flags);
+			if (policy->max < pcpu->target_freq) {
+				pcpu->target_freq = policy->max;
+			} else if (policy->min >= pcpu->target_freq) {
+				pcpu->target_freq = policy->min;
+				anyboost = 1;
+			}
+
+			spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+			up_read(&pcpu->enable_sem);
+
+			/* Reschedule timer only if policy->max is raised.
+			 * Delete the timers, else the timer callback may
+			 * return without re-arm the timer when failed
+			 * acquire the semaphore. This race may cause timer
+			 * stopped unexpectedly.
+			 */
+
+			if (policy->max > pcpu->max_freq) {
+				down_write(&pcpu->enable_sem);
+				del_timer_sync(&pcpu->cpu_timer);
+				del_timer_sync(&pcpu->cpu_slack_timer);
+				cpufreq_interactive_timer_start(j);
+				up_write(&pcpu->enable_sem);
+			} else if (anyboost) {
+				u64 now = ktime_to_us(ktime_get());
+
+				cpumask_set_cpu(j, &speedchange_cpumask);
+				pcpu->hispeed_validate_time = now;
+				pcpu->floor_freq = policy->min;
+				pcpu->floor_validate_time = now;
+			}
+
+			pcpu->max_freq = policy->max;
+			pcpu->limits_changed = true;
+		}
+		if (anyboost)
+			wake_up_process(speedchange_task);
+
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_interactive = {
+	.name = "interactive_pro",
+	.governor = cpufreq_governor_interactive,
+	.max_transition_latency = 10000000,
+	.owner = THIS_MODULE,
+};
+
+static void cpufreq_interactive_nop_timer(unsigned long data)
+{
+}
+
+static int __init cpufreq_interactive_init(void)
+{
+	unsigned int i;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+
+	/* Initalize per-cpu timers */
+	for_each_possible_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+		init_timer_deferrable(&pcpu->cpu_timer);
+		pcpu->cpu_timer.function = cpufreq_interactive_timer;
+		pcpu->cpu_timer.data = i;
+		init_timer(&pcpu->cpu_slack_timer);
+		pcpu->cpu_slack_timer.function = cpufreq_interactive_nop_timer;
+		spin_lock_init(&pcpu->load_lock);
+		spin_lock_init(&pcpu->target_freq_lock);
+		init_rwsem(&pcpu->enable_sem);
+	}
+
+	spin_lock_init(&target_loads_lock);
+	spin_lock_init(&speedchange_cpumask_lock);
+	spin_lock_init(&above_hispeed_delay_lock);
+	mutex_init(&gov_lock);
+	speedchange_task =
+		kthread_create(cpufreq_interactive_speedchange_task, NULL,
+			       "cfinteractive");
+	if (IS_ERR(speedchange_task))
+		return PTR_ERR(speedchange_task);
+
+	sched_setscheduler_nocheck(speedchange_task, SCHED_FIFO, &param);
+	get_task_struct(speedchange_task);
+
+	/* NB: wake up so the thread does not look hung to the freezer */
+	wake_up_process(speedchange_task);
+
+	return cpufreq_register_governor(&cpufreq_gov_interactive);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE
+fs_initcall(cpufreq_interactive_init);
+#else
+module_init(cpufreq_interactive_init);
+#endif
+
+static void __exit cpufreq_interactive_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_interactive);
+	kthread_stop(speedchange_task);
+	put_task_struct(speedchange_task);
+}
+
+module_exit(cpufreq_interactive_exit);
+
+MODULE_AUTHOR("Mike Chan <mike@android.com>");
+MODULE_DESCRIPTION("'cpufreq_interactive' - A cpufreq governor for "
+	"Latency sensitive workloads");
+MODULE_LICENSE("GPL");
diff --git a/drivers/cpufreq/cpufreq_interactive_x.c b/drivers/cpufreq/cpufreq_interactive_x.c
new file mode 100644
index 0000000..02ef593
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_interactive_x.c
@@ -0,0 +1,1388 @@
+/*
+ * drivers/cpufreq/cpufreq_interactive_x.c
+ *
+ * Copyright (C) 2010 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * Author: Mike Chan (mike@android.com)
+ *
+ */
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/rwsem.h>
+#include <linux/sched.h>
+#include <linux/sched/rt.h>
+#include <linux/tick.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/kernel_stat.h>
+#include <linux/touchboost.h>
+#include <asm/cputime.h>
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/cpufreq_interactive_x.h>
+
+static int active_count;
+
+struct cpufreq_interactive_x_cpuinfo {
+	struct timer_list cpu_timer;
+	struct timer_list cpu_slack_timer;
+	spinlock_t load_lock; /* protects the next 4 fields */
+	u64 time_in_idle;
+	u64 time_in_idle_timestamp;
+	u64 cputime_speedadj;
+	u64 cputime_speedadj_timestamp;
+	u64 last_evaluated_jiffy;
+	struct cpufreq_policy *policy;
+	struct cpufreq_frequency_table *freq_table;
+	spinlock_t target_freq_lock; /*protects target freq */
+	unsigned int target_freq;
+	unsigned int floor_freq;
+	unsigned int min_freq;
+	u64 floor_validate_time;
+	u64 hispeed_validate_time; /* cluster hispeed_validate_time */
+	u64 local_hvtime; /* per-cpu hispeed_validate_time */
+	u64 max_freq_hyst_start_time;
+	struct rw_semaphore enable_sem;
+	int governor_enabled;
+};
+
+static DEFINE_PER_CPU(struct cpufreq_interactive_x_cpuinfo, cpuinfo);
+
+/* realtime thread handles frequency scaling */
+static struct task_struct *speedchange_task;
+static cpumask_t speedchange_cpumask;
+static spinlock_t speedchange_cpumask_lock;
+static struct mutex gov_lock;
+
+/* Hi speed to bump to from lo speed when load burst (default max) */
+static unsigned int hispeed_freq;
+
+/* Go to hi speed when CPU load at or above this value. */
+#define DEFAULT_GO_HISPEED_LOAD 99
+static unsigned long go_hispeed_load = DEFAULT_GO_HISPEED_LOAD;
+
+/* Target load.  Lower values result in higher CPU speeds. */
+#define DEFAULT_TARGET_LOAD 90
+static unsigned int default_target_loads[] = {DEFAULT_TARGET_LOAD};
+static spinlock_t target_loads_lock;
+static unsigned int *target_loads = default_target_loads;
+static int ntarget_loads = ARRAY_SIZE(default_target_loads);
+
+/*
+ * The minimum amount of time to spend at a frequency before we can ramp down.
+ */
+#define DEFAULT_MIN_SAMPLE_TIME (80 * USEC_PER_MSEC)
+static unsigned long min_sample_time = DEFAULT_MIN_SAMPLE_TIME;
+
+/*
+ * The sample rate of the timer used to increase frequency
+ */
+#define DEFAULT_TIMER_RATE (20 * USEC_PER_MSEC)
+static unsigned long timer_rate = DEFAULT_TIMER_RATE;
+
+/*
+ * Wait this long before raising speed above hispeed, by default a single
+ * timer interval.
+ */
+#define DEFAULT_ABOVE_HISPEED_DELAY DEFAULT_TIMER_RATE
+static unsigned int default_above_hispeed_delay[] = {
+	DEFAULT_ABOVE_HISPEED_DELAY };
+static spinlock_t above_hispeed_delay_lock;
+static unsigned int *above_hispeed_delay = default_above_hispeed_delay;
+static int nabove_hispeed_delay = ARRAY_SIZE(default_above_hispeed_delay);
+
+/* Non-zero means indefinite speed boost active */
+static int boost_val;
+/* Duration of a boot pulse in usecs */
+static int boostpulse_duration_val = DEFAULT_MIN_SAMPLE_TIME;
+/* End time of boost pulse in ktime converted to usecs */
+static u64 boostpulse_endtime;
+#define DEFAULT_INPUT_BOOST_FREQ 1728000
+unsigned int input_boost_freq = DEFAULT_INPUT_BOOST_FREQ;
+
+/*
+ * Max additional time to wait in idle, beyond timer_rate, at speeds above
+ * minimum before wakeup to reduce speed, or -1 if unnecessary.
+ */
+#define DEFAULT_TIMER_SLACK (4 * DEFAULT_TIMER_RATE)
+static int timer_slack_val = DEFAULT_TIMER_SLACK;
+
+/*
+ * Whether to align timer windows across all CPUs. When
+ * use_sched_load is true, this flag is ignored and windows
+ * will always be aligned.
+ */
+static bool align_windows = true;
+
+/*
+ * Stay at max freq for at least max_freq_hysteresis before dropping
+ * frequency.
+ */
+static unsigned int max_freq_hysteresis;
+
+static bool io_is_busy;
+
+#define DOWN_LOW_LOAD_THRESHOLD 5
+unsigned int down_low_load_threshold = DOWN_LOW_LOAD_THRESHOLD;
+
+/* Round to starting jiffy of next evaluation window */
+static u64 round_to_nw_start(u64 jif)
+{
+	unsigned long step = usecs_to_jiffies(timer_rate);
+	u64 ret;
+
+	if (align_windows) {
+		do_div(jif, step);
+		ret = (jif + 1) * step;
+	} else {
+		ret = jiffies + usecs_to_jiffies(timer_rate);
+	}
+
+	return ret;
+}
+
+static void cpufreq_interactive_x_timer_resched(unsigned long cpu,
+					      bool slack_only)
+{
+	struct cpufreq_interactive_x_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 expires;
+	unsigned long flags;
+	u64 now = ktime_to_us(ktime_get());
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	expires = round_to_nw_start(pcpu->last_evaluated_jiffy);
+	if (!slack_only) {
+		pcpu->time_in_idle =
+			get_cpu_idle_time(smp_processor_id(),
+				  &pcpu->time_in_idle_timestamp, io_is_busy);
+		pcpu->cputime_speedadj = 0;
+		pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+		del_timer(&pcpu->cpu_timer);
+		pcpu->cpu_timer.expires = expires;
+		add_timer_on(&pcpu->cpu_timer, cpu);
+	}
+
+	if (timer_slack_val >= 0 &&
+	    (pcpu->target_freq > pcpu->policy->min ||
+		(pcpu->target_freq == pcpu->policy->min &&
+		 now < boostpulse_endtime))) {
+		expires += usecs_to_jiffies(timer_slack_val);
+		del_timer(&pcpu->cpu_slack_timer);
+		pcpu->cpu_slack_timer.expires = expires;
+		add_timer_on(&pcpu->cpu_slack_timer, cpu);
+	}
+
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+/* The caller shall take enable_sem write semaphore to avoid any timer race.
+ * The cpu_timer and cpu_slack_timer must be deactivated when calling this
+ * function.
+ */
+static void cpufreq_interactive_x_timer_start(int cpu)
+{
+	struct cpufreq_interactive_x_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 expires = round_to_nw_start(pcpu->last_evaluated_jiffy);
+	unsigned long flags;
+	u64 now = ktime_to_us(ktime_get());
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->cpu_timer.expires = expires;
+	add_timer_on(&pcpu->cpu_timer, cpu);
+	if (timer_slack_val >= 0 &&
+	    (pcpu->target_freq > pcpu->policy->min ||
+		(pcpu->target_freq == pcpu->policy->min &&
+		 now < boostpulse_endtime))) {
+		expires += usecs_to_jiffies(timer_slack_val);
+		pcpu->cpu_slack_timer.expires = expires;
+		add_timer_on(&pcpu->cpu_slack_timer, cpu);
+	}
+
+	pcpu->time_in_idle =
+		get_cpu_idle_time(cpu, &pcpu->time_in_idle_timestamp,
+				  io_is_busy);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+static unsigned int freq_to_above_hispeed_delay(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay - 1 &&
+			freq >= above_hispeed_delay[i+1]; i += 2)
+		;
+
+	ret = above_hispeed_delay[i];
+
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static unsigned int freq_to_targetload(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+
+	for (i = 0; i < ntarget_loads - 1 && freq >= target_loads[i+1]; i += 2)
+		;
+
+	ret = target_loads[i];
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+/*
+ * If increasing frequencies never map to a lower target load then
+ * choose_freq() will find the minimum frequency that does not exceed its
+ * target load given the current load.
+ */
+
+static unsigned int choose_freq(
+	struct cpufreq_interactive_x_cpuinfo *pcpu, unsigned int loadadjfreq)
+{
+	unsigned int freq = pcpu->policy->cur;
+	unsigned int prevfreq, freqmin, freqmax;
+	unsigned int tl;
+	int index;
+
+	freqmin = 0;
+	freqmax = UINT_MAX;
+
+	do {
+		prevfreq = freq;
+		tl = freq_to_targetload(freq);
+
+		/*
+		 * Find the lowest frequency where the computed load is less
+		 * than or equal to the target load.
+		 */
+
+		if (cpufreq_frequency_table_target(
+			    pcpu->policy, pcpu->freq_table, loadadjfreq / tl,
+			    CPUFREQ_RELATION_L, &index))
+			break;
+		freq = pcpu->freq_table[index].frequency;
+
+		if (freq > prevfreq) {
+			/* The previous frequency is too low. */
+			freqmin = prevfreq;
+
+			if (freq >= freqmax) {
+				/*
+				 * Find the highest frequency that is less
+				 * than freqmax.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmax - 1, CPUFREQ_RELATION_H,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				if (freq == freqmin) {
+					/*
+					 * The first frequency below freqmax
+					 * has already been found to be too
+					 * low.  freqmax is the lowest speed
+					 * we found that is fast enough.
+					 */
+					freq = freqmax;
+					break;
+				}
+			}
+		} else if (freq < prevfreq) {
+			/* The previous frequency is high enough. */
+			freqmax = prevfreq;
+
+			if (freq <= freqmin) {
+				/*
+				 * Find the lowest frequency that is higher
+				 * than freqmin.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmin + 1, CPUFREQ_RELATION_L,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				/*
+				 * If freqmax is the first frequency above
+				 * freqmin then we have already found that
+				 * this speed is fast enough.
+				 */
+				if (freq == freqmax)
+					break;
+			}
+		}
+
+		/* If same frequency chosen as previous then done. */
+	} while (freq != prevfreq);
+
+	return freq;
+}
+
+static u64 update_load(int cpu)
+{
+	struct cpufreq_interactive_x_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 now;
+	u64 now_idle;
+	unsigned int delta_idle;
+	unsigned int delta_time;
+	u64 active_time;
+
+	now_idle = get_cpu_idle_time(cpu, &now, io_is_busy);
+	delta_idle = (unsigned int)(now_idle - pcpu->time_in_idle);
+	delta_time = (unsigned int)(now - pcpu->time_in_idle_timestamp);
+
+	if (delta_time <= delta_idle)
+		active_time = 0;
+	else
+		active_time = delta_time - delta_idle;
+
+	pcpu->cputime_speedadj += active_time * pcpu->policy->cur;
+
+	pcpu->time_in_idle = now_idle;
+	pcpu->time_in_idle_timestamp = now;
+	return now;
+}
+
+static void cpufreq_interactive_x_timer(unsigned long data)
+{
+	u64 now;
+	unsigned int delta_time;
+	u64 cputime_speedadj;
+	int cpu_load;
+	struct cpufreq_interactive_x_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, data);
+	unsigned int new_freq;
+	unsigned int loadadjfreq;
+	unsigned int index;
+	unsigned long flags;
+	unsigned int this_hispeed_freq;
+	bool boosted;
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled)
+		goto exit;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	now = update_load(data);
+	delta_time = (unsigned int)(now - pcpu->cputime_speedadj_timestamp);
+	cputime_speedadj = pcpu->cputime_speedadj;
+	pcpu->last_evaluated_jiffy = get_jiffies_64();
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+
+	if (WARN_ON_ONCE(!delta_time))
+		goto rearm;
+
+	spin_lock_irqsave(&pcpu->target_freq_lock, flags);
+	do_div(cputime_speedadj, delta_time);
+	loadadjfreq = (unsigned int)cputime_speedadj * 100;
+	cpu_load = loadadjfreq / pcpu->policy->cur;
+	boosted = boost_val || now < (get_input_time() + boostpulse_duration_val);
+	this_hispeed_freq = max(hispeed_freq, pcpu->policy->min);
+
+	cpufreq_notify_utilization(pcpu->policy, cpu_load);
+
+	if (cpu_load >= go_hispeed_load || boosted) {
+		if (pcpu->policy->cur < this_hispeed_freq) {
+			new_freq = this_hispeed_freq;
+		} else {
+			new_freq = choose_freq(pcpu, loadadjfreq);
+
+			if (new_freq < this_hispeed_freq)
+				new_freq = this_hispeed_freq;
+		}
+	} else if (cpu_load <= DOWN_LOW_LOAD_THRESHOLD) {
+		new_freq = pcpu->policy->cpuinfo.min_freq;
+	} else {
+		new_freq = choose_freq(pcpu, loadadjfreq);
+		if (new_freq > this_hispeed_freq &&
+				pcpu->target_freq < this_hispeed_freq)
+			new_freq = this_hispeed_freq;
+	}
+
+	if (boosted)
+		new_freq = max(new_freq, input_boost_freq);
+
+	if (pcpu->policy->cur >= this_hispeed_freq &&
+	    new_freq > pcpu->policy->cur &&
+	    now - pcpu->hispeed_validate_time <
+	    freq_to_above_hispeed_delay(pcpu->policy->cur)) {
+		trace_cpufreq_interactive_x_notyet(
+			data, cpu_load, pcpu->target_freq,
+			pcpu->policy->cur, new_freq);
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	pcpu->local_hvtime = now;
+
+	if (cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table,
+					   new_freq, CPUFREQ_RELATION_L,
+					   &index)) {
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	new_freq = pcpu->freq_table[index].frequency;
+
+	if (new_freq < pcpu->target_freq &&
+	    now - pcpu->max_freq_hyst_start_time < max_freq_hysteresis) {
+		trace_cpufreq_interactive_x_notyet(data, cpu_load,
+			pcpu->target_freq, pcpu->policy->cur, new_freq);
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	/*
+	 * Do not scale below floor_freq unless we have been at or above the
+	 * floor frequency for the minimum sample time since last validated.
+	 */
+	if (new_freq < pcpu->floor_freq) {
+		if (now - pcpu->floor_validate_time < min_sample_time) {
+			trace_cpufreq_interactive_x_notyet(
+				data, cpu_load, pcpu->target_freq,
+				pcpu->policy->cur, new_freq);
+			spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+			goto rearm;
+		}
+	}
+
+	/*
+	 * Update the timestamp for checking whether speed has been held at
+	 * or above the selected frequency for a minimum of min_sample_time,
+	 * if not boosted to this_hispeed_freq.  If boosted to this_hispeed_freq
+	 * then we allow the speed to drop as soon as the boostpulse duration
+	 * expires (or the indefinite boost is turned off).
+	 */
+
+	if (!boosted || new_freq > this_hispeed_freq) {
+		pcpu->floor_freq = new_freq;
+		pcpu->floor_validate_time = now;
+	}
+
+	if (new_freq == pcpu->policy->max)
+		pcpu->max_freq_hyst_start_time = now;
+
+	if (pcpu->target_freq == new_freq &&
+			pcpu->target_freq <= pcpu->policy->cur) {
+		trace_cpufreq_interactive_x_already(
+			data, cpu_load, pcpu->target_freq,
+			pcpu->policy->cur, new_freq);
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	trace_cpufreq_interactive_x_target(data, cpu_load, pcpu->target_freq,
+					 pcpu->policy->cur, new_freq);
+
+	pcpu->target_freq = new_freq;
+	spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+	cpumask_set_cpu(data, &speedchange_cpumask);
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+	wake_up_process(speedchange_task);
+
+rearm:
+	if (!timer_pending(&pcpu->cpu_timer))
+		cpufreq_interactive_x_timer_resched(data, false);
+
+exit:
+	up_read(&pcpu->enable_sem);
+	return;
+}
+
+static void cpufreq_interactive_x_idle_end(void)
+{
+	struct cpufreq_interactive_x_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled) {
+		up_read(&pcpu->enable_sem);
+		return;
+	}
+
+	/* Arm the timer for 1-2 ticks later if not already. */
+	if (!timer_pending(&pcpu->cpu_timer)) {
+		cpufreq_interactive_x_timer_resched(smp_processor_id(), false);
+	} else if (time_after_eq(jiffies, pcpu->cpu_timer.expires)) {
+		del_timer(&pcpu->cpu_timer);
+		del_timer(&pcpu->cpu_slack_timer);
+		cpufreq_interactive_x_timer(smp_processor_id());
+	}
+
+	up_read(&pcpu->enable_sem);
+}
+
+static int cpufreq_interactive_x_speedchange_task(void *data)
+{
+	unsigned int cpu;
+	cpumask_t tmp_mask;
+	unsigned long flags;
+	struct cpufreq_interactive_x_cpuinfo *pcpu;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+		if (cpumask_empty(&speedchange_cpumask)) {
+			spin_unlock_irqrestore(&speedchange_cpumask_lock,
+					       flags);
+			schedule();
+
+			if (kthread_should_stop())
+				break;
+
+			spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+		}
+
+		set_current_state(TASK_RUNNING);
+		tmp_mask = speedchange_cpumask;
+		cpumask_clear(&speedchange_cpumask);
+		spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+		for_each_cpu(cpu, &tmp_mask) {
+			unsigned int j;
+			unsigned int max_freq = 0;
+			struct cpufreq_interactive_x_cpuinfo *pjcpu;
+			u64 hvt = 0;
+
+			pcpu = &per_cpu(cpuinfo, cpu);
+			if (!down_read_trylock(&pcpu->enable_sem))
+				continue;
+			if (!pcpu->governor_enabled) {
+				up_read(&pcpu->enable_sem);
+				continue;
+			}
+
+			for_each_cpu(j, pcpu->policy->cpus) {
+				pjcpu = &per_cpu(cpuinfo, j);
+
+				if (pjcpu->target_freq > max_freq) {
+					max_freq = pjcpu->target_freq;
+					hvt = pjcpu->local_hvtime;
+				} else if (pjcpu->target_freq == max_freq) {
+					hvt = min(hvt, pjcpu->local_hvtime);
+				}
+			}
+
+			if (max_freq != pcpu->policy->cur) {
+				__cpufreq_driver_target(pcpu->policy,
+							max_freq,
+							CPUFREQ_RELATION_H);
+				for_each_cpu(j, pcpu->policy->cpus) {
+					pjcpu = &per_cpu(cpuinfo, j);
+					pjcpu->hispeed_validate_time = hvt;
+				}
+			}
+			trace_cpufreq_interactive_x_setspeed(cpu,
+						     pcpu->target_freq,
+						     pcpu->policy->cur);
+
+			up_read(&pcpu->enable_sem);
+		}
+	}
+
+	return 0;
+}
+
+static void cpufreq_interactive_x_boost(void)
+{
+	int i;
+	int anyboost = 0;
+	unsigned long flags[2];
+	struct cpufreq_interactive_x_cpuinfo *pcpu;
+
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags[0]);
+
+	for_each_online_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+		spin_lock_irqsave(&pcpu->target_freq_lock, flags[1]);
+		if (pcpu->target_freq < hispeed_freq) {
+			pcpu->target_freq = hispeed_freq;
+			cpumask_set_cpu(i, &speedchange_cpumask);
+			pcpu->hispeed_validate_time =
+				ktime_to_us(ktime_get());
+			anyboost = 1;
+		}
+
+		/*
+		 * Set floor freq and (re)start timer for when last
+		 * validated.
+		 */
+
+		pcpu->floor_freq = hispeed_freq;
+		pcpu->floor_validate_time = ktime_to_us(ktime_get());
+		spin_unlock_irqrestore(&pcpu->target_freq_lock, flags[1]);
+	}
+
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags[0]);
+
+	if (anyboost)
+		wake_up_process(speedchange_task);
+}
+
+static int cpufreq_interactive_x_notifier(
+	struct notifier_block *nb, unsigned long val, void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpufreq_interactive_x_cpuinfo *pcpu;
+	int cpu;
+	unsigned long flags;
+
+	if (val == CPUFREQ_PRECHANGE) {
+		pcpu = &per_cpu(cpuinfo, freq->cpu);
+		if (!down_read_trylock(&pcpu->enable_sem))
+			return 0;
+		if (!pcpu->governor_enabled) {
+			up_read(&pcpu->enable_sem);
+			return 0;
+		}
+
+		for_each_cpu(cpu, pcpu->policy->cpus) {
+			struct cpufreq_interactive_x_cpuinfo *pjcpu =
+				&per_cpu(cpuinfo, cpu);
+			if (cpu != freq->cpu) {
+				if (!down_read_trylock(&pjcpu->enable_sem))
+					continue;
+				if (!pjcpu->governor_enabled) {
+					up_read(&pjcpu->enable_sem);
+					continue;
+				}
+			}
+			spin_lock_irqsave(&pjcpu->load_lock, flags);
+			update_load(cpu);
+			spin_unlock_irqrestore(&pjcpu->load_lock, flags);
+			if (cpu != freq->cpu)
+				up_read(&pjcpu->enable_sem);
+		}
+
+		up_read(&pcpu->enable_sem);
+	}
+	return 0;
+}
+
+static struct notifier_block cpufreq_notifier_block = {
+	.notifier_call = cpufreq_interactive_x_notifier,
+};
+
+static unsigned int *get_tokenized_data(const char *buf, int *num_tokens)
+{
+	const char *cp;
+	int i;
+	int ntokens = 1;
+	unsigned int *tokenized_data;
+	int err = -EINVAL;
+
+	cp = buf;
+	while ((cp = strpbrk(cp + 1, " :")))
+		ntokens++;
+
+	if (!(ntokens & 0x1))
+		goto err;
+
+	tokenized_data = kmalloc(ntokens * sizeof(unsigned int), GFP_KERNEL);
+	if (!tokenized_data) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	cp = buf;
+	i = 0;
+	while (i < ntokens) {
+		if (sscanf(cp, "%u", &tokenized_data[i++]) != 1)
+			goto err_kfree;
+
+		cp = strpbrk(cp, " :");
+		if (!cp)
+			break;
+		cp++;
+	}
+
+	if (i != ntokens)
+		goto err_kfree;
+
+	*num_tokens = ntokens;
+	return tokenized_data;
+
+err_kfree:
+	kfree(tokenized_data);
+err:
+	return ERR_PTR(err);
+}
+
+static ssize_t show_target_loads(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+
+	for (i = 0; i < ntarget_loads; i++)
+		ret += sprintf(buf + ret, "%u%s", target_loads[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+static ssize_t store_target_loads(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_target_loads = NULL;
+	unsigned long flags;
+
+	new_target_loads = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_target_loads))
+		return PTR_RET(new_target_loads);
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+	if (target_loads != default_target_loads)
+		kfree(target_loads);
+	target_loads = new_target_loads;
+	ntarget_loads = ntokens;
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return count;
+}
+
+static struct global_attr target_loads_attr =
+	__ATTR(target_loads, S_IRUGO | S_IWUSR,
+		show_target_loads, store_target_loads);
+
+static ssize_t show_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay; i++)
+		ret += sprintf(buf + ret, "%u%s", above_hispeed_delay[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static ssize_t store_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens, i;
+	unsigned int *new_above_hispeed_delay = NULL;
+	unsigned long flags;
+
+	new_above_hispeed_delay = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_above_hispeed_delay))
+		return PTR_RET(new_above_hispeed_delay);
+
+	/* Make sure frequencies are in ascending order. */
+	for (i = 3; i < ntokens; i += 2) {
+		if (new_above_hispeed_delay[i] <=
+		    new_above_hispeed_delay[i - 2]) {
+			kfree(new_above_hispeed_delay);
+			return -EINVAL;
+		}
+	}
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+	if (above_hispeed_delay != default_above_hispeed_delay)
+		kfree(above_hispeed_delay);
+	above_hispeed_delay = new_above_hispeed_delay;
+	nabove_hispeed_delay = ntokens;
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return count;
+
+}
+
+static struct global_attr above_hispeed_delay_attr =
+	__ATTR(above_hispeed_delay, S_IRUGO | S_IWUSR,
+		show_above_hispeed_delay, store_above_hispeed_delay);
+
+static ssize_t show_hispeed_freq(struct kobject *kobj,
+				 struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", hispeed_freq);
+}
+
+static ssize_t store_hispeed_freq(struct kobject *kobj,
+				  struct attribute *attr, const char *buf,
+				  size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	hispeed_freq = val;
+	return count;
+}
+
+static struct global_attr hispeed_freq_attr = __ATTR(hispeed_freq, 0644,
+		show_hispeed_freq, store_hispeed_freq);
+
+static ssize_t show_max_freq_hysteresis(struct kobject *kobj,
+				     struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", max_freq_hysteresis);
+}
+
+static ssize_t store_max_freq_hysteresis(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	max_freq_hysteresis = val;
+	return count;
+}
+
+static struct global_attr max_freq_hysteresis_attr =
+	__ATTR(max_freq_hysteresis, 0644, show_max_freq_hysteresis,
+		store_max_freq_hysteresis);
+
+static ssize_t show_align_windows(struct kobject *kobj,
+				     struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", align_windows);
+}
+
+static ssize_t store_align_windows(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	align_windows = val;
+	return count;
+}
+
+static struct global_attr align_windows_attr = __ATTR(align_windows, 0644,
+		show_align_windows, store_align_windows);
+
+static ssize_t show_go_hispeed_load(struct kobject *kobj,
+				     struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", go_hispeed_load);
+}
+
+static ssize_t store_go_hispeed_load(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	go_hispeed_load = val;
+	return count;
+}
+
+static struct global_attr go_hispeed_load_attr = __ATTR(go_hispeed_load, 0644,
+		show_go_hispeed_load, store_go_hispeed_load);
+
+static ssize_t show_min_sample_time(struct kobject *kobj,
+				struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", min_sample_time);
+}
+
+static ssize_t store_min_sample_time(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	min_sample_time = val;
+	return count;
+}
+
+static struct global_attr min_sample_time_attr = __ATTR(min_sample_time, 0644,
+		show_min_sample_time, store_min_sample_time);
+
+static ssize_t show_timer_rate(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", timer_rate);
+}
+
+static ssize_t store_timer_rate(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val, val_round;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	val_round = jiffies_to_usecs(usecs_to_jiffies(val));
+	if (val != val_round)
+		pr_warn("timer_rate not aligned to jiffy. Rounded up to %lu\n",
+				val_round);
+
+	timer_rate = val_round;
+	return count;
+}
+
+static struct global_attr timer_rate_attr = __ATTR(timer_rate, 0644,
+		show_timer_rate, store_timer_rate);
+
+static ssize_t show_timer_slack(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", timer_slack_val);
+}
+
+static ssize_t store_timer_slack(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret < 0)
+		return ret;
+
+	timer_slack_val = val;
+	return count;
+}
+
+define_one_global_rw(timer_slack);
+
+static ssize_t show_boost(struct kobject *kobj, struct attribute *attr,
+			  char *buf)
+{
+	return sprintf(buf, "%d\n", boost_val);
+}
+
+static ssize_t store_boost(struct kobject *kobj, struct attribute *attr,
+			   const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boost_val = val;
+
+	if (boost_val) {
+		trace_cpufreq_interactive_x_boost("on");
+		cpufreq_interactive_x_boost();
+	} else {
+		boostpulse_endtime = ktime_to_us(ktime_get());
+		trace_cpufreq_interactive_x_unboost("off");
+	}
+
+	return count;
+}
+
+define_one_global_rw(boost);
+
+static ssize_t store_boostpulse(struct kobject *kobj, struct attribute *attr,
+				const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_endtime = ktime_to_us(ktime_get()) + boostpulse_duration_val;
+	trace_cpufreq_interactive_x_boost("pulse");
+	cpufreq_interactive_x_boost();
+	return count;
+}
+
+static struct global_attr boostpulse =
+	__ATTR(boostpulse, 0200, NULL, store_boostpulse);
+
+static ssize_t show_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", boostpulse_duration_val);
+}
+
+static ssize_t store_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_duration_val = val;
+	return count;
+}
+
+define_one_global_rw(boostpulse_duration);
+
+static ssize_t show_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	io_is_busy = val;
+	return count;
+}
+
+static struct global_attr io_is_busy_attr = __ATTR(io_is_busy, 0644,
+		show_io_is_busy, store_io_is_busy);
+
+static ssize_t show_input_boost_freq(struct kobject *kobj,
+                        struct attribute *attr, char *buf)
+{
+        return sprintf(buf, "%u\n", input_boost_freq);
+}
+
+static ssize_t store_input_boost_freq(struct kobject *kobj,
+                        struct attribute *attr, const char *buf, size_t count)
+{
+        int ret;
+        unsigned long val;
+
+        ret = kstrtoul(buf, 0, &val);
+        if (ret < 0)
+                return ret;
+        input_boost_freq = val;
+        return count;
+}
+
+static struct global_attr input_boost_freq_attr = __ATTR(input_boost_freq, 0644,
+                show_input_boost_freq, store_input_boost_freq);
+
+static ssize_t show_down_low_load_threshold(struct kobject *kobj,
+                        struct attribute *attr, char *buf)
+{
+        return sprintf(buf, "%u\n", down_low_load_threshold);
+}
+
+static ssize_t store_down_low_load_threshold(struct kobject *kobj,
+                        struct attribute *attr, const char *buf, size_t count)
+{
+        int ret;
+        unsigned long val;
+
+        ret = kstrtoul(buf, 0, &val);
+        if (ret < 0)
+                return ret;
+        down_low_load_threshold = val;
+        return count;
+}
+
+static struct global_attr down_low_load_threshold_attr = __ATTR(down_low_load_threshold, 0644,
+                show_down_low_load_threshold, store_down_low_load_threshold);
+
+static struct attribute *interactive_x_attributes[] = {
+	&target_loads_attr.attr,
+	&above_hispeed_delay_attr.attr,
+	&hispeed_freq_attr.attr,
+	&go_hispeed_load_attr.attr,
+	&min_sample_time_attr.attr,
+	&timer_rate_attr.attr,
+	&timer_slack.attr,
+	&boost.attr,
+	&boostpulse.attr,
+	&boostpulse_duration.attr,
+	&io_is_busy_attr.attr,
+	&max_freq_hysteresis_attr.attr,
+	&align_windows_attr.attr,
+	&input_boost_freq_attr.attr,
+	&down_low_load_threshold_attr.attr,
+	NULL,
+};
+
+static struct attribute_group interactive_x_attr_group = {
+	.attrs = interactive_x_attributes,
+	.name = "interactive_x",
+};
+
+static int cpufreq_interactive_x_idle_notifier(struct notifier_block *nb,
+					     unsigned long val,
+					     void *data)
+{
+	if (val == IDLE_END)
+		cpufreq_interactive_x_idle_end();
+
+	return 0;
+}
+
+static struct notifier_block cpufreq_interactive_x_idle_nb = {
+	.notifier_call = cpufreq_interactive_x_idle_notifier,
+};
+
+static int cpufreq_governor_interactive_x(struct cpufreq_policy *policy,
+		unsigned int event)
+{
+	int rc;
+	unsigned int j;
+	struct cpufreq_interactive_x_cpuinfo *pcpu;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned long flags;
+	unsigned int anyboost;
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if (!cpu_online(policy->cpu))
+			return -EINVAL;
+
+		mutex_lock(&gov_lock);
+
+		freq_table =
+			cpufreq_frequency_get_table(policy->cpu);
+		if (!hispeed_freq)
+			hispeed_freq = policy->max;
+
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			pcpu->policy = policy;
+			pcpu->target_freq = policy->cur;
+			pcpu->freq_table = freq_table;
+			pcpu->floor_freq = pcpu->target_freq;
+			pcpu->floor_validate_time =
+				ktime_to_us(ktime_get());
+			pcpu->hispeed_validate_time =
+				pcpu->floor_validate_time;
+			pcpu->local_hvtime = pcpu->floor_validate_time;
+			pcpu->min_freq = policy->min;
+			down_write(&pcpu->enable_sem);
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			pcpu->last_evaluated_jiffy = get_jiffies_64();
+			cpufreq_interactive_x_timer_start(j);
+			pcpu->governor_enabled = 1;
+			up_write(&pcpu->enable_sem);
+		}
+
+		/*
+		 * Do not register the idle hook and create sysfs
+		 * entries if we have already done so.
+		 */
+		if (++active_count > 1) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		rc = sysfs_create_group(cpufreq_global_kobject,
+				&interactive_x_attr_group);
+		if (rc) {
+			mutex_unlock(&gov_lock);
+			return rc;
+		}
+
+		idle_notifier_register(&cpufreq_interactive_x_idle_nb);
+		cpufreq_register_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		mutex_unlock(&gov_lock);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		mutex_lock(&gov_lock);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			down_write(&pcpu->enable_sem);
+			pcpu->governor_enabled = 0;
+			pcpu->target_freq = 0;
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			up_write(&pcpu->enable_sem);
+		}
+
+		if (--active_count > 0) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		cpufreq_unregister_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		idle_notifier_unregister(&cpufreq_interactive_x_idle_nb);
+		sysfs_remove_group(cpufreq_global_kobject,
+				&interactive_x_attr_group);
+		mutex_unlock(&gov_lock);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		__cpufreq_driver_target(policy,
+				policy->cur, CPUFREQ_RELATION_L);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+
+			down_read(&pcpu->enable_sem);
+			if (pcpu->governor_enabled == 0) {
+				up_read(&pcpu->enable_sem);
+				continue;
+			}
+
+			spin_lock_irqsave(&pcpu->target_freq_lock, flags);
+			if (policy->max < pcpu->target_freq) {
+				pcpu->target_freq = policy->max;
+			} else if (policy->min >= pcpu->target_freq) {
+				pcpu->target_freq = policy->min;
+				anyboost = 1;
+			}
+
+			spin_unlock_irqrestore(&pcpu->target_freq_lock, flags);
+
+			if (policy->min < pcpu->min_freq)
+				cpufreq_interactive_x_timer_resched(j, true);
+			pcpu->min_freq = policy->min;
+
+			up_read(&pcpu->enable_sem);
+
+			if (anyboost) {
+				u64 now = ktime_to_us(ktime_get());
+
+				cpumask_set_cpu(j, &speedchange_cpumask);
+				pcpu->hispeed_validate_time = now;
+				pcpu->floor_freq = policy->min;
+				pcpu->floor_validate_time = now;
+			}
+		}
+		if (anyboost)
+			wake_up_process(speedchange_task);
+
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE_X
+static
+#endif
+struct cpufreq_governor cpufreq_gov_interactive_x = {
+	.name = "interactive_x",
+	.governor = cpufreq_governor_interactive_x,
+	.max_transition_latency = 10000000,
+	.owner = THIS_MODULE,
+};
+
+static void cpufreq_interactive_x_nop_timer(unsigned long data)
+{
+}
+
+static int __init cpufreq_interactive_x_init(void)
+{
+	unsigned int i;
+	struct cpufreq_interactive_x_cpuinfo *pcpu;
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+
+	/* Initalize per-cpu timers */
+	for_each_possible_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+		init_timer_deferrable(&pcpu->cpu_timer);
+		pcpu->cpu_timer.function = cpufreq_interactive_x_timer;
+		pcpu->cpu_timer.data = i;
+		init_timer(&pcpu->cpu_slack_timer);
+		pcpu->cpu_slack_timer.function = cpufreq_interactive_x_nop_timer;
+		spin_lock_init(&pcpu->load_lock);
+		spin_lock_init(&pcpu->target_freq_lock);
+		init_rwsem(&pcpu->enable_sem);
+	}
+
+	spin_lock_init(&target_loads_lock);
+	spin_lock_init(&speedchange_cpumask_lock);
+	spin_lock_init(&above_hispeed_delay_lock);
+	mutex_init(&gov_lock);
+	speedchange_task =
+		kthread_create(cpufreq_interactive_x_speedchange_task, NULL,
+			       "cfinteractive");
+	if (IS_ERR(speedchange_task))
+		return PTR_ERR(speedchange_task);
+
+	sched_setscheduler_nocheck(speedchange_task, SCHED_FIFO, &param);
+	get_task_struct(speedchange_task);
+
+	/* NB: wake up so the thread does not look hung to the freezer */
+	wake_up_process(speedchange_task);
+
+	return cpufreq_register_governor(&cpufreq_gov_interactive_x);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE_X
+fs_initcall(cpufreq_interactive_x_init);
+#else
+module_init(cpufreq_interactive_x_init);
+#endif
+
+static void __exit cpufreq_interactive_x_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_interactive_x);
+	kthread_stop(speedchange_task);
+	put_task_struct(speedchange_task);
+	if (above_hispeed_delay != default_above_hispeed_delay)
+		kfree(above_hispeed_delay);
+}
+
+module_exit(cpufreq_interactive_x_exit);
+
+MODULE_AUTHOR("Mike Chan <mike@android.com>");
+MODULE_DESCRIPTION("'cpufreq_interactive_x' - A cpufreq governor for "
+	"Latency sensitive workloads");
+MODULE_LICENSE("GPL");
diff --git a/drivers/cpufreq/cpufreq_ironactive.c b/drivers/cpufreq/cpufreq_ironactive.c
new file mode 100755
index 0000000..d34d89b
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_ironactive.c
@@ -0,0 +1,1757 @@
+/*
+ * drivers/cpufreq/cpufreq_ironactive.c
+ *
+ * Copyright (C) 2016 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * Authors: Mike Chan (mike@android.com) & Pranav Vashi <neobuddy89@gmail.com>
+ *
+ * Ironactive Version: 1.0
+ *
+ */
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/rwsem.h>
+#include <linux/sched.h>
+#include <linux/sched/rt.h>
+#include <linux/tick.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/kernel_stat.h>
+#ifdef CONFIG_STATE_NOTIFIER
+#include <linux/state_notifier.h>
+#endif
+#include <asm/cputime.h>
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/cpufreq_ironactive.h>
+
+struct cpufreq_ironactive_policyinfo {
+	struct timer_list policy_timer;
+	struct timer_list policy_slack_timer;
+	spinlock_t load_lock; /* protects load tracking stat */
+	u64 last_evaluated_jiffy;
+	struct cpufreq_policy *policy;
+	struct cpufreq_frequency_table *freq_table;
+	spinlock_t target_freq_lock; /*protects target freq */
+	unsigned int target_freq;
+	unsigned int floor_freq;
+	unsigned int min_freq;
+	u64 floor_validate_time;
+	u64 hispeed_validate_time;
+	u64 max_freq_hyst_start_time;
+	struct rw_semaphore enable_sem;
+	bool reject_notification;
+	int governor_enabled;
+	struct cpufreq_ironactive_tunables *cached_tunables;
+	unsigned long *cpu_busy_times;
+};
+
+/* Protected by per-policy load_lock */
+struct cpufreq_ironactive_cpuinfo {
+	u64 time_in_idle;
+	u64 time_in_idle_timestamp;
+	u64 cputime_speedadj;
+	u64 cputime_speedadj_timestamp;
+	unsigned int loadadjfreq;
+};
+
+static DEFINE_PER_CPU(struct cpufreq_ironactive_policyinfo *, polinfo);
+static DEFINE_PER_CPU(struct cpufreq_ironactive_cpuinfo, cpuinfo);
+
+/* realtime thread handles frequency scaling */
+static struct task_struct *speedchange_task;
+static cpumask_t speedchange_cpumask;
+static spinlock_t speedchange_cpumask_lock;
+static struct mutex gov_lock;
+
+static int set_window_count;
+static int migration_register_count;
+static struct mutex sched_lock;
+
+/* Target load.  Lower values result in higher CPU speeds. */
+#define DEFAULT_TARGET_LOAD 90
+static unsigned int default_target_loads[] = {DEFAULT_TARGET_LOAD};
+
+#define DEFAULT_TIMER_RATE (20 * USEC_PER_MSEC)
+#define DEFAULT_TIMER_RATE_SUSP ((unsigned long)(50 * USEC_PER_MSEC))
+#define DEFAULT_ABOVE_HISPEED_DELAY DEFAULT_TIMER_RATE
+static unsigned int default_above_hispeed_delay[] = {
+	DEFAULT_ABOVE_HISPEED_DELAY };
+
+struct cpufreq_ironactive_tunables {
+	int usage_count;
+	/* Hi speed to bump to from lo speed when load burst (default max) */
+	unsigned int hispeed_freq;
+	/* Go to hi speed when CPU load at or above this value. */
+#define DEFAULT_GO_HISPEED_LOAD 99
+	unsigned long go_hispeed_load;
+	/* Target load. Lower values result in higher CPU speeds. */
+	spinlock_t target_loads_lock;
+	unsigned int *target_loads;
+	int ntarget_loads;
+	/*
+	 * The minimum amount of time to spend at a frequency before we can ramp
+	 * down.
+	 */
+#define DEFAULT_MIN_SAMPLE_TIME (80 * USEC_PER_MSEC)
+	unsigned long min_sample_time;
+	/*
+	 * The sample rate of the timer used to increase frequency
+	 */
+	unsigned long timer_rate;
+#ifdef CONFIG_STATE_NOTIFIER
+	unsigned long timer_rate_prev;
+#endif
+	/*
+	 * Wait this long before raising speed above hispeed, by default a
+	 * single timer interval.
+	 */
+	spinlock_t above_hispeed_delay_lock;
+	unsigned int *above_hispeed_delay;
+	int nabove_hispeed_delay;
+	/* Non-zero means indefinite speed boost active */
+	int boost_val;
+	/* Duration of a boot pulse in usecs */
+	int boostpulse_duration_val;
+	/* End time of boost pulse in ktime converted to usecs */
+	u64 boostpulse_endtime;
+	bool boosted;
+	/*
+	 * Max additional time to wait in idle, beyond timer_rate, at speeds
+	 * above minimum before wakeup to reduce speed, or -1 if unnecessary.
+	 */
+#define DEFAULT_TIMER_SLACK (4 * DEFAULT_TIMER_RATE)
+	int timer_slack_val;
+	bool io_is_busy;
+
+	/* scheduler input related flags */
+	bool use_sched_load;
+	bool use_migration_notif;
+
+	/*
+	 * Whether to align timer windows across all CPUs. When
+	 * use_sched_load is true, this flag is ignored and windows
+	 * will always be aligned.
+	 */
+	bool align_windows;
+
+	/*
+	 * Stay at max freq for at least max_freq_hysteresis before dropping
+	 * frequency.
+	 */
+	unsigned int max_freq_hysteresis;
+};
+
+/* For cases where we have single governor instance for system */
+static struct cpufreq_ironactive_tunables *common_tunables;
+static struct cpufreq_ironactive_tunables *cached_common_tunables;
+
+static struct attribute_group *get_sysfs_attr(void);
+
+/* Round to starting jiffy of next evaluation window */
+static u64 round_to_nw_start(u64 jif,
+			     struct cpufreq_ironactive_tunables *tunables)
+{
+	unsigned long step = usecs_to_jiffies(tunables->timer_rate);
+	u64 ret;
+
+	if (tunables->use_sched_load || tunables->align_windows) {
+		do_div(jif, step);
+		ret = (jif + 1) * step;
+	} else {
+		ret = jiffies + usecs_to_jiffies(tunables->timer_rate);
+	}
+
+	return ret;
+}
+
+static inline int set_window_helper(
+			struct cpufreq_ironactive_tunables *tunables)
+{
+	return sched_set_window(round_to_nw_start(get_jiffies_64(), tunables),
+			 usecs_to_jiffies(tunables->timer_rate));
+}
+
+static void cpufreq_ironactive_timer_resched(unsigned long cpu,
+					      bool slack_only)
+{
+	struct cpufreq_ironactive_policyinfo *ppol = per_cpu(polinfo, cpu);
+	struct cpufreq_ironactive_cpuinfo *pcpu;
+	struct cpufreq_ironactive_tunables *tunables =
+		ppol->policy->governor_data;
+	u64 expires;
+	unsigned long flags;
+	int i;
+
+	spin_lock_irqsave(&ppol->load_lock, flags);
+	expires = round_to_nw_start(ppol->last_evaluated_jiffy, tunables);
+	if (!slack_only) {
+		for_each_cpu(i, ppol->policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, i);
+			pcpu->time_in_idle = get_cpu_idle_time(i,
+						&pcpu->time_in_idle_timestamp,
+						tunables->io_is_busy);
+			pcpu->cputime_speedadj = 0;
+			pcpu->cputime_speedadj_timestamp =
+						pcpu->time_in_idle_timestamp;
+		}
+		del_timer(&ppol->policy_timer);
+		ppol->policy_timer.expires = expires;
+		add_timer(&ppol->policy_timer);
+	}
+
+	if (tunables->timer_slack_val >= 0 &&
+	    ppol->target_freq > ppol->policy->min) {
+		expires += usecs_to_jiffies(tunables->timer_slack_val);
+		del_timer(&ppol->policy_slack_timer);
+		ppol->policy_slack_timer.expires = expires;
+		add_timer(&ppol->policy_slack_timer);
+	}
+
+	spin_unlock_irqrestore(&ppol->load_lock, flags);
+}
+
+/* The caller shall take enable_sem write semaphore to avoid any timer race.
+ * The policy_timer and policy_slack_timer must be deactivated when calling
+ * this function.
+ */
+static void cpufreq_ironactive_timer_start(
+	struct cpufreq_ironactive_tunables *tunables, int cpu)
+{
+	struct cpufreq_ironactive_policyinfo *ppol = per_cpu(polinfo, cpu);
+	struct cpufreq_ironactive_cpuinfo *pcpu;
+	u64 expires = round_to_nw_start(ppol->last_evaluated_jiffy, tunables);
+	unsigned long flags;
+	int i;
+
+	spin_lock_irqsave(&ppol->load_lock, flags);
+	ppol->policy_timer.expires = expires;
+	add_timer(&ppol->policy_timer);
+	if (tunables->timer_slack_val >= 0 &&
+	    ppol->target_freq > ppol->policy->min) {
+		expires += usecs_to_jiffies(tunables->timer_slack_val);
+		ppol->policy_slack_timer.expires = expires;
+		add_timer(&ppol->policy_slack_timer);
+	}
+
+	for_each_cpu(i, ppol->policy->cpus) {
+		pcpu = &per_cpu(cpuinfo, i);
+		pcpu->time_in_idle =
+			get_cpu_idle_time(i, &pcpu->time_in_idle_timestamp,
+					  tunables->io_is_busy);
+		pcpu->cputime_speedadj = 0;
+		pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	}
+	spin_unlock_irqrestore(&ppol->load_lock, flags);
+}
+
+static unsigned int freq_to_above_hispeed_delay(
+	struct cpufreq_ironactive_tunables *tunables,
+	unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&tunables->above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < tunables->nabove_hispeed_delay - 1 &&
+			freq >= tunables->above_hispeed_delay[i+1]; i += 2)
+		;
+
+	ret = tunables->above_hispeed_delay[i];
+	spin_unlock_irqrestore(&tunables->above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static unsigned int freq_to_targetload(
+	struct cpufreq_ironactive_tunables *tunables, unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&tunables->target_loads_lock, flags);
+
+	for (i = 0; i < tunables->ntarget_loads - 1 &&
+		    freq >= tunables->target_loads[i+1]; i += 2)
+		;
+
+	ret = tunables->target_loads[i];
+	spin_unlock_irqrestore(&tunables->target_loads_lock, flags);
+	return ret;
+}
+
+/*
+ * If increasing frequencies never map to a lower target load then
+ * choose_freq() will find the minimum frequency that does not exceed its
+ * target load given the current load.
+ */
+static unsigned int choose_freq(struct cpufreq_ironactive_policyinfo *pcpu,
+		unsigned int loadadjfreq)
+{
+	unsigned int freq = pcpu->policy->cur;
+	unsigned int prevfreq, freqmin, freqmax;
+	unsigned int tl;
+	int index;
+
+	freqmin = 0;
+	freqmax = UINT_MAX;
+
+	do {
+		prevfreq = freq;
+		tl = freq_to_targetload(pcpu->policy->governor_data, freq);
+
+		/*
+		 * Find the lowest frequency where the computed load is less
+		 * than or equal to the target load.
+		 */
+
+		if (cpufreq_frequency_table_target(
+			    pcpu->policy, pcpu->freq_table, loadadjfreq / tl,
+			    CPUFREQ_RELATION_L, &index))
+			break;
+		freq = pcpu->freq_table[index].frequency;
+
+		if (freq > prevfreq) {
+			/* The previous frequency is too low. */
+			freqmin = prevfreq;
+
+			if (freq >= freqmax) {
+				/*
+				 * Find the highest frequency that is less
+				 * than freqmax.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmax - 1, CPUFREQ_RELATION_H,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				if (freq == freqmin) {
+					/*
+					 * The first frequency below freqmax
+					 * has already been found to be too
+					 * low.  freqmax is the lowest speed
+					 * we found that is fast enough.
+					 */
+					freq = freqmax;
+					break;
+				}
+			}
+		} else if (freq < prevfreq) {
+			/* The previous frequency is high enough. */
+			freqmax = prevfreq;
+
+			if (freq <= freqmin) {
+				/*
+				 * Find the lowest frequency that is higher
+				 * than freqmin.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmin + 1, CPUFREQ_RELATION_L,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				/*
+				 * If freqmax is the first frequency above
+				 * freqmin then we have already found that
+				 * this speed is fast enough.
+				 */
+				if (freq == freqmax)
+					break;
+			}
+		}
+
+		/* If same frequency chosen as previous then done. */
+	} while (freq != prevfreq);
+
+	return freq;
+}
+
+static u64 update_load(int cpu)
+{
+	struct cpufreq_ironactive_policyinfo *ppol = per_cpu(polinfo, cpu);
+	struct cpufreq_ironactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	struct cpufreq_ironactive_tunables *tunables =
+		ppol->policy->governor_data;
+	u64 now;
+	u64 now_idle;
+	unsigned int delta_idle;
+	unsigned int delta_time;
+	u64 active_time;
+
+	now_idle = get_cpu_idle_time(cpu, &now, tunables->io_is_busy);
+	delta_idle = (unsigned int)(now_idle - pcpu->time_in_idle);
+	delta_time = (unsigned int)(now - pcpu->time_in_idle_timestamp);
+
+	if (delta_time <= delta_idle)
+		active_time = 0;
+	else
+		active_time = delta_time - delta_idle;
+
+	pcpu->cputime_speedadj += active_time * ppol->policy->cur;
+
+	pcpu->time_in_idle = now_idle;
+	pcpu->time_in_idle_timestamp = now;
+	return now;
+}
+
+#define MAX_LOCAL_LOAD 100
+static void cpufreq_ironactive_timer(unsigned long data)
+{
+	u64 now;
+	unsigned int delta_time;
+	u64 cputime_speedadj;
+	int cpu_load;
+	struct cpufreq_ironactive_policyinfo *ppol = per_cpu(polinfo, data);
+	struct cpufreq_ironactive_tunables *tunables =
+		ppol->policy->governor_data;
+	struct cpufreq_ironactive_cpuinfo *pcpu;
+	unsigned int new_freq;
+	unsigned int loadadjfreq = 0, tmploadadjfreq;
+	unsigned int index;
+	unsigned long flags;
+	unsigned long max_cpu;
+	int i, fcpu;
+	struct cpufreq_govinfo govinfo;
+
+	if (!down_read_trylock(&ppol->enable_sem))
+		return;
+	if (!ppol->governor_enabled)
+		goto exit;
+
+	fcpu = cpumask_first(ppol->policy->related_cpus);
+	now = ktime_to_us(ktime_get());
+	spin_lock_irqsave(&ppol->load_lock, flags);
+	ppol->last_evaluated_jiffy = get_jiffies_64();
+
+#ifdef CONFIG_STATE_NOTIFIER
+	if (!state_suspended &&
+		tunables->timer_rate != tunables->timer_rate_prev)
+		tunables->timer_rate = tunables->timer_rate_prev;
+	else if (state_suspended &&
+		tunables->timer_rate != DEFAULT_TIMER_RATE_SUSP) {
+		tunables->timer_rate_prev = tunables->timer_rate;
+		tunables->timer_rate
+			= max(tunables->timer_rate,
+				DEFAULT_TIMER_RATE_SUSP);
+	}
+#endif
+
+	if (tunables->use_sched_load)
+		sched_get_cpus_busy(ppol->cpu_busy_times,
+				    ppol->policy->related_cpus);
+	max_cpu = cpumask_first(ppol->policy->cpus);
+	for_each_cpu(i, ppol->policy->cpus) {
+		pcpu = &per_cpu(cpuinfo, i);
+		if (tunables->use_sched_load) {
+			cputime_speedadj = (u64)ppol->cpu_busy_times[i - fcpu]
+					* ppol->policy->cpuinfo.max_freq;
+			do_div(cputime_speedadj, tunables->timer_rate);
+		} else {
+			now = update_load(i);
+			delta_time = (unsigned int)
+				(now - pcpu->cputime_speedadj_timestamp);
+			if (WARN_ON_ONCE(!delta_time))
+				continue;
+			cputime_speedadj = pcpu->cputime_speedadj;
+			do_div(cputime_speedadj, delta_time);
+		}
+		tmploadadjfreq = (unsigned int)cputime_speedadj * 100;
+		pcpu->loadadjfreq = tmploadadjfreq;
+		trace_cpufreq_ironactive_cpuload(i, tmploadadjfreq /
+						  ppol->policy->cur);
+
+		if (tmploadadjfreq > loadadjfreq) {
+			loadadjfreq = tmploadadjfreq;
+			max_cpu = i;
+		}
+	}
+	spin_unlock_irqrestore(&ppol->load_lock, flags);
+
+	/*
+	 * Send govinfo notification.
+	 * Govinfo notification could potentially wake up another thread
+	 * managed by its clients. Thread wakeups might trigger a load
+	 * change callback that executes this function again. Therefore
+	 * no spinlock could be held when sending the notification.
+	 */
+	for_each_cpu(i, ppol->policy->cpus) {
+		pcpu = &per_cpu(cpuinfo, i);
+		govinfo.cpu = i;
+		govinfo.load = pcpu->loadadjfreq / ppol->policy->max;
+		govinfo.sampling_rate_us = tunables->timer_rate;
+		atomic_notifier_call_chain(&cpufreq_govinfo_notifier_list,
+					   CPUFREQ_LOAD_CHANGE, &govinfo);
+	}
+
+	spin_lock_irqsave(&ppol->target_freq_lock, flags);
+	cpu_load = loadadjfreq / ppol->policy->cur;
+	tunables->boosted = tunables->boost_val || now < tunables->boostpulse_endtime;
+
+	if (cpu_load >= tunables->go_hispeed_load || tunables->boosted) {
+		if (ppol->policy->cur < tunables->hispeed_freq &&
+		    cpu_load <= MAX_LOCAL_LOAD) {
+			new_freq = tunables->hispeed_freq;
+		} else {
+			new_freq = choose_freq(ppol, loadadjfreq);
+
+			if (new_freq < tunables->hispeed_freq)
+				new_freq = tunables->hispeed_freq;
+		}
+	} else {
+		new_freq = choose_freq(ppol, loadadjfreq);
+	}
+
+	if (cpu_load <= MAX_LOCAL_LOAD &&
+	    ppol->policy->cur >= tunables->hispeed_freq &&
+	    new_freq > ppol->policy->cur &&
+	    now - ppol->hispeed_validate_time <
+	    freq_to_above_hispeed_delay(tunables, ppol->policy->cur)) {
+		trace_cpufreq_ironactive_notyet(
+			max_cpu, cpu_load, ppol->target_freq,
+			ppol->policy->cur, new_freq);
+		spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	ppol->hispeed_validate_time = now;
+
+	if (cpufreq_frequency_table_target(ppol->policy, ppol->freq_table,
+					   new_freq, CPUFREQ_RELATION_L,
+					   &index)) {
+		spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	new_freq = ppol->freq_table[index].frequency;
+
+	if (new_freq < ppol->target_freq &&
+	    now - ppol->max_freq_hyst_start_time <
+	    tunables->max_freq_hysteresis) {
+		trace_cpufreq_ironactive_notyet(max_cpu, cpu_load,
+			ppol->target_freq, ppol->policy->cur, new_freq);
+		spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	/*
+	 * Do not scale below floor_freq unless we have been at or above the
+	 * floor frequency for the minimum sample time since last validated.
+	 */
+	if (new_freq < ppol->floor_freq) {
+		if (now - ppol->floor_validate_time <
+				tunables->min_sample_time) {
+			trace_cpufreq_ironactive_notyet(
+				max_cpu, cpu_load, ppol->target_freq,
+				ppol->policy->cur, new_freq);
+			spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+			goto rearm;
+		}
+	}
+
+	/*
+	 * Update the timestamp for checking whether speed has been held at
+	 * or above the selected frequency for a minimum of min_sample_time,
+	 * if not boosted to hispeed_freq.  If boosted to hispeed_freq then we
+	 * allow the speed to drop as soon as the boostpulse duration expires
+	 * (or the indefinite boost is turned off).
+	 */
+
+	if (!tunables->boosted || new_freq > tunables->hispeed_freq) {
+		ppol->floor_freq = new_freq;
+		ppol->floor_validate_time = now;
+	}
+
+	if (new_freq == ppol->policy->max)
+		ppol->max_freq_hyst_start_time = now;
+
+	if (ppol->target_freq == new_freq) {
+		trace_cpufreq_ironactive_already(
+			max_cpu, cpu_load, ppol->target_freq,
+			ppol->policy->cur, new_freq);
+		spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+		goto rearm;
+	}
+
+	trace_cpufreq_ironactive_target(max_cpu, cpu_load, ppol->target_freq,
+					 ppol->policy->cur, new_freq);
+
+	ppol->target_freq = new_freq;
+	spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+	cpumask_set_cpu(max_cpu, &speedchange_cpumask);
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+	wake_up_process(speedchange_task);
+
+rearm:
+	if (!timer_pending(&ppol->policy_timer))
+		cpufreq_ironactive_timer_resched(data, false);
+
+exit:
+	up_read(&ppol->enable_sem);
+	return;
+}
+
+
+static int cpufreq_ironactive_speedchange_task(void *data)
+{
+	unsigned int cpu;
+	cpumask_t tmp_mask;
+	unsigned long flags;
+	struct cpufreq_ironactive_policyinfo *ppol;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+		if (cpumask_empty(&speedchange_cpumask)) {
+			spin_unlock_irqrestore(&speedchange_cpumask_lock,
+					       flags);
+			schedule();
+
+			if (kthread_should_stop())
+				break;
+
+			spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+		}
+
+		set_current_state(TASK_RUNNING);
+		tmp_mask = speedchange_cpumask;
+		cpumask_clear(&speedchange_cpumask);
+		spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+		for_each_cpu(cpu, &tmp_mask) {
+			ppol = per_cpu(polinfo, cpu);
+			if (!down_read_trylock(&ppol->enable_sem))
+				continue;
+			if (!ppol->governor_enabled) {
+				up_read(&ppol->enable_sem);
+				continue;
+			}
+
+			if (ppol->target_freq != ppol->policy->cur)
+				__cpufreq_driver_target(ppol->policy,
+							ppol->target_freq,
+							CPUFREQ_RELATION_H);
+			trace_cpufreq_ironactive_setspeed(cpu,
+						     ppol->target_freq,
+						     ppol->policy->cur);
+			up_read(&ppol->enable_sem);
+		}
+	}
+
+	return 0;
+}
+
+static void cpufreq_ironactive_boost(struct cpufreq_ironactive_tunables *tunables)
+{
+	int i;
+	int anyboost = 0;
+	unsigned long flags[2];
+	struct cpufreq_ironactive_policyinfo *ppol;
+
+	tunables->boosted = true;
+
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags[0]);
+
+	for_each_online_cpu(i) {
+		ppol = per_cpu(polinfo, i);
+		if (!ppol || tunables != ppol->policy->governor_data)
+			continue;
+
+		spin_lock_irqsave(&ppol->target_freq_lock, flags[1]);
+		if (ppol->target_freq < tunables->hispeed_freq) {
+			ppol->target_freq = tunables->hispeed_freq;
+			cpumask_set_cpu(i, &speedchange_cpumask);
+			ppol->hispeed_validate_time =
+				ktime_to_us(ktime_get());
+			anyboost = 1;
+		}
+
+		/*
+		 * Set floor freq and (re)start timer for when last
+		 * validated.
+		 */
+
+		ppol->floor_freq = tunables->hispeed_freq;
+		ppol->floor_validate_time = ktime_to_us(ktime_get());
+		spin_unlock_irqrestore(&ppol->target_freq_lock, flags[1]);
+		break;
+	}
+
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags[0]);
+
+	if (anyboost)
+		wake_up_process(speedchange_task);
+}
+
+static int load_change_callback(struct notifier_block *nb, unsigned long val,
+				void *data)
+{
+	unsigned long cpu = (unsigned long) data;
+	struct cpufreq_ironactive_policyinfo *ppol = per_cpu(polinfo, cpu);
+	struct cpufreq_ironactive_tunables *tunables;
+
+	if (speedchange_task == current)
+		return 0;
+	if (!ppol || ppol->reject_notification)
+		return 0;
+
+	if (!down_read_trylock(&ppol->enable_sem))
+		return 0;
+	if (!ppol->governor_enabled) {
+		up_read(&ppol->enable_sem);
+		return 0;
+	}
+	tunables = ppol->policy->governor_data;
+	if (!tunables->use_sched_load || !tunables->use_migration_notif) {
+		up_read(&ppol->enable_sem);
+		return 0;
+	}
+
+	trace_cpufreq_ironactive_load_change(cpu);
+	del_timer(&ppol->policy_timer);
+	del_timer(&ppol->policy_slack_timer);
+	cpufreq_ironactive_timer(cpu);
+
+	up_read(&ppol->enable_sem);
+	return 0;
+}
+
+static struct notifier_block load_notifier_block = {
+	.notifier_call = load_change_callback,
+};
+
+static int cpufreq_ironactive_notifier(
+	struct notifier_block *nb, unsigned long val, void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpufreq_ironactive_policyinfo *ppol;
+	int cpu;
+	unsigned long flags;
+
+	if (val == CPUFREQ_POSTCHANGE) {
+		ppol = per_cpu(polinfo, freq->cpu);
+		if (!ppol)
+			return 0;
+		if (!down_read_trylock(&ppol->enable_sem))
+			return 0;
+		if (!ppol->governor_enabled) {
+			up_read(&ppol->enable_sem);
+			return 0;
+		}
+
+		if (cpumask_first(ppol->policy->cpus) != freq->cpu) {
+			up_read(&ppol->enable_sem);
+			return 0;
+		}
+		spin_lock_irqsave(&ppol->load_lock, flags);
+		for_each_cpu(cpu, ppol->policy->cpus)
+			update_load(cpu);
+		spin_unlock_irqrestore(&ppol->load_lock, flags);
+
+		up_read(&ppol->enable_sem);
+	}
+	return 0;
+}
+
+static struct notifier_block cpufreq_notifier_block = {
+	.notifier_call = cpufreq_ironactive_notifier,
+};
+
+static unsigned int *get_tokenized_data(const char *buf, int *num_tokens)
+{
+	const char *cp;
+	int i;
+	int ntokens = 1;
+	unsigned int *tokenized_data;
+	int err = -EINVAL;
+
+	cp = buf;
+	while ((cp = strpbrk(cp + 1, " :")))
+		ntokens++;
+
+	if (!(ntokens & 0x1))
+		goto err;
+
+	tokenized_data = kmalloc(ntokens * sizeof(unsigned int), GFP_KERNEL);
+	if (!tokenized_data) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	cp = buf;
+	i = 0;
+	while (i < ntokens) {
+		if (sscanf(cp, "%u", &tokenized_data[i++]) != 1)
+			goto err_kfree;
+
+		cp = strpbrk(cp, " :");
+		if (!cp)
+			break;
+		cp++;
+	}
+
+	if (i != ntokens)
+		goto err_kfree;
+
+	*num_tokens = ntokens;
+	return tokenized_data;
+
+err_kfree:
+	kfree(tokenized_data);
+err:
+	return ERR_PTR(err);
+}
+
+static ssize_t show_target_loads(
+	struct cpufreq_ironactive_tunables *tunables,
+	char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&tunables->target_loads_lock, flags);
+
+	for (i = 0; i < tunables->ntarget_loads; i++)
+		ret += sprintf(buf + ret, "%u%s", tunables->target_loads[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&tunables->target_loads_lock, flags);
+	return ret;
+}
+
+static ssize_t store_target_loads(
+	struct cpufreq_ironactive_tunables *tunables,
+	const char *buf, size_t count)
+{
+	int ntokens;
+	unsigned int *new_target_loads = NULL;
+	unsigned long flags;
+
+	new_target_loads = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_target_loads))
+		return PTR_RET(new_target_loads);
+
+	spin_lock_irqsave(&tunables->target_loads_lock, flags);
+	if (tunables->target_loads != default_target_loads)
+		kfree(tunables->target_loads);
+	tunables->target_loads = new_target_loads;
+	tunables->ntarget_loads = ntokens;
+	spin_unlock_irqrestore(&tunables->target_loads_lock, flags);
+	return count;
+}
+
+static ssize_t show_above_hispeed_delay(
+	struct cpufreq_ironactive_tunables *tunables, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&tunables->above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < tunables->nabove_hispeed_delay; i++)
+		ret += sprintf(buf + ret, "%u%s",
+			       tunables->above_hispeed_delay[i],
+			       i & 0x1 ? ":" : " ");
+
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&tunables->above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static ssize_t store_above_hispeed_delay(
+	struct cpufreq_ironactive_tunables *tunables,
+	const char *buf, size_t count)
+{
+	int ntokens;
+	unsigned int *new_above_hispeed_delay = NULL;
+	unsigned long flags;
+
+	new_above_hispeed_delay = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_above_hispeed_delay))
+		return PTR_RET(new_above_hispeed_delay);
+
+	spin_lock_irqsave(&tunables->above_hispeed_delay_lock, flags);
+	if (tunables->above_hispeed_delay != default_above_hispeed_delay)
+		kfree(tunables->above_hispeed_delay);
+	tunables->above_hispeed_delay = new_above_hispeed_delay;
+	tunables->nabove_hispeed_delay = ntokens;
+	spin_unlock_irqrestore(&tunables->above_hispeed_delay_lock, flags);
+	return count;
+
+}
+
+static ssize_t show_hispeed_freq(struct cpufreq_ironactive_tunables *tunables,
+		char *buf)
+{
+	return sprintf(buf, "%u\n", tunables->hispeed_freq);
+}
+
+static ssize_t store_hispeed_freq(struct cpufreq_ironactive_tunables *tunables,
+		const char *buf, size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	tunables->hispeed_freq = val;
+	return count;
+}
+
+#define show_store_one(file_name)					\
+static ssize_t show_##file_name(					\
+	struct cpufreq_ironactive_tunables *tunables, char *buf)	\
+{									\
+	return snprintf(buf, PAGE_SIZE, "%u\n", tunables->file_name);	\
+}									\
+static ssize_t store_##file_name(					\
+		struct cpufreq_ironactive_tunables *tunables,		\
+		const char *buf, size_t count)				\
+{									\
+	int ret;							\
+	long unsigned int val;						\
+									\
+	ret = kstrtoul(buf, 0, &val);				\
+	if (ret < 0)							\
+		return ret;						\
+	tunables->file_name = val;					\
+	return count;							\
+}
+show_store_one(max_freq_hysteresis);
+show_store_one(align_windows);
+
+static ssize_t show_go_hispeed_load(struct cpufreq_ironactive_tunables
+		*tunables, char *buf)
+{
+	return sprintf(buf, "%lu\n", tunables->go_hispeed_load);
+}
+
+static ssize_t store_go_hispeed_load(struct cpufreq_ironactive_tunables
+		*tunables, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	tunables->go_hispeed_load = val;
+	return count;
+}
+
+static ssize_t show_min_sample_time(struct cpufreq_ironactive_tunables
+		*tunables, char *buf)
+{
+	return sprintf(buf, "%lu\n", tunables->min_sample_time);
+}
+
+static ssize_t store_min_sample_time(struct cpufreq_ironactive_tunables
+		*tunables, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	tunables->min_sample_time = val;
+	return count;
+}
+
+static ssize_t show_timer_rate(struct cpufreq_ironactive_tunables *tunables,
+		char *buf)
+{
+	return sprintf(buf, "%lu\n", tunables->timer_rate);
+}
+
+static ssize_t store_timer_rate(struct cpufreq_ironactive_tunables *tunables,
+		const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val, val_round;
+	struct cpufreq_ironactive_tunables *t;
+	int cpu;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	val_round = jiffies_to_usecs(usecs_to_jiffies(val));
+	if (val != val_round)
+		pr_warn("timer_rate not aligned to jiffy. Rounded up to %lu\n",
+			val_round);
+	tunables->timer_rate = val_round;
+#ifdef CONFIG_STATE_NOTIFIER
+	tunables->timer_rate_prev = val_round;
+#endif
+
+	if (!tunables->use_sched_load)
+		return count;
+
+	for_each_possible_cpu(cpu) {
+		if (!per_cpu(polinfo, cpu))
+			continue;
+		t = per_cpu(polinfo, cpu)->cached_tunables;
+		if (t && t->use_sched_load) {
+			t->timer_rate = val_round;
+#ifdef CONFIG_STATE_NOTIFIER
+			t->timer_rate_prev = val_round;
+#endif
+		}
+	}
+	set_window_helper(tunables);
+
+	return count;
+}
+
+static ssize_t show_timer_slack(struct cpufreq_ironactive_tunables *tunables,
+		char *buf)
+{
+	return sprintf(buf, "%d\n", tunables->timer_slack_val);
+}
+
+static ssize_t store_timer_slack(struct cpufreq_ironactive_tunables *tunables,
+		const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret < 0)
+		return ret;
+
+	tunables->timer_slack_val = val;
+	return count;
+}
+
+static ssize_t show_boost(struct cpufreq_ironactive_tunables *tunables,
+			  char *buf)
+{
+	return sprintf(buf, "%d\n", tunables->boost_val);
+}
+
+static ssize_t store_boost(struct cpufreq_ironactive_tunables *tunables,
+			   const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	tunables->boost_val = val;
+
+	if (tunables->boost_val) {
+		trace_cpufreq_ironactive_boost("on");
+		if (!tunables->boosted)
+			cpufreq_ironactive_boost(tunables);
+	} else {
+		tunables->boostpulse_endtime = ktime_to_us(ktime_get());
+		trace_cpufreq_ironactive_unboost("off");
+	}
+
+	return count;
+}
+
+static ssize_t store_boostpulse(struct cpufreq_ironactive_tunables *tunables,
+				const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	tunables->boostpulse_endtime = ktime_to_us(ktime_get()) +
+		tunables->boostpulse_duration_val;
+	trace_cpufreq_ironactive_boost("pulse");
+	if (!tunables->boosted)
+		cpufreq_ironactive_boost(tunables);
+	return count;
+}
+
+static ssize_t show_boostpulse_duration(struct cpufreq_ironactive_tunables
+		*tunables, char *buf)
+{
+	return sprintf(buf, "%d\n", tunables->boostpulse_duration_val);
+}
+
+static ssize_t store_boostpulse_duration(struct cpufreq_ironactive_tunables
+		*tunables, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	tunables->boostpulse_duration_val = val;
+	return count;
+}
+
+static ssize_t show_io_is_busy(struct cpufreq_ironactive_tunables *tunables,
+		char *buf)
+{
+	return sprintf(buf, "%u\n", tunables->io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct cpufreq_ironactive_tunables *tunables,
+		const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+	struct cpufreq_ironactive_tunables *t;
+	int cpu;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	tunables->io_is_busy = val;
+
+	if (!tunables->use_sched_load)
+		return count;
+
+	for_each_possible_cpu(cpu) {
+		if (!per_cpu(polinfo, cpu))
+			continue;
+		t = per_cpu(polinfo, cpu)->cached_tunables;
+		if (t && t->use_sched_load)
+			t->io_is_busy = val;
+	}
+	sched_set_io_is_busy(val);
+
+	return count;
+}
+
+static int cpufreq_ironactive_enable_sched_input(
+			struct cpufreq_ironactive_tunables *tunables)
+{
+	int rc = 0, j;
+	struct cpufreq_ironactive_tunables *t;
+
+	mutex_lock(&sched_lock);
+
+	set_window_count++;
+	if (set_window_count > 1) {
+		for_each_possible_cpu(j) {
+			if (!per_cpu(polinfo, j))
+				continue;
+			t = per_cpu(polinfo, j)->cached_tunables;
+			if (t && t->use_sched_load) {
+				tunables->timer_rate = t->timer_rate;
+				tunables->io_is_busy = t->io_is_busy;
+				break;
+			}
+		}
+	} else {
+		rc = set_window_helper(tunables);
+		if (rc) {
+			pr_err("%s: Failed to set sched window\n", __func__);
+			set_window_count--;
+			goto out;
+		}
+		sched_set_io_is_busy(tunables->io_is_busy);
+	}
+
+	if (!tunables->use_migration_notif)
+		goto out;
+
+	migration_register_count++;
+	if (migration_register_count > 1)
+		goto out;
+	else
+		atomic_notifier_chain_register(&load_alert_notifier_head,
+						&load_notifier_block);
+out:
+	mutex_unlock(&sched_lock);
+	return rc;
+}
+
+static int cpufreq_ironactive_disable_sched_input(
+			struct cpufreq_ironactive_tunables *tunables)
+{
+	mutex_lock(&sched_lock);
+
+	if (tunables->use_migration_notif) {
+		migration_register_count--;
+		if (migration_register_count < 1)
+			atomic_notifier_chain_unregister(
+					&load_alert_notifier_head,
+					&load_notifier_block);
+	}
+	set_window_count--;
+
+	mutex_unlock(&sched_lock);
+	return 0;
+}
+
+static ssize_t show_use_sched_load(
+		struct cpufreq_ironactive_tunables *tunables, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%d\n", tunables->use_sched_load);
+}
+
+static ssize_t store_use_sched_load(
+			struct cpufreq_ironactive_tunables *tunables,
+			const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	if (tunables->use_sched_load == (bool) val)
+		return count;
+
+	tunables->use_sched_load = val;
+
+	if (val)
+		ret = cpufreq_ironactive_enable_sched_input(tunables);
+	else
+		ret = cpufreq_ironactive_disable_sched_input(tunables);
+
+	if (ret) {
+		tunables->use_sched_load = !val;
+		return ret;
+	}
+
+	return count;
+}
+
+static ssize_t show_use_migration_notif(
+		struct cpufreq_ironactive_tunables *tunables, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%d\n",
+			tunables->use_migration_notif);
+}
+
+static ssize_t store_use_migration_notif(
+			struct cpufreq_ironactive_tunables *tunables,
+			const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	if (tunables->use_migration_notif == (bool) val)
+		return count;
+	tunables->use_migration_notif = val;
+
+	if (!tunables->use_sched_load)
+		return count;
+
+	mutex_lock(&sched_lock);
+	if (val) {
+		migration_register_count++;
+		if (migration_register_count == 1)
+			atomic_notifier_chain_register(
+					&load_alert_notifier_head,
+					&load_notifier_block);
+	} else {
+		migration_register_count--;
+		if (!migration_register_count)
+			atomic_notifier_chain_unregister(
+					&load_alert_notifier_head,
+					&load_notifier_block);
+	}
+	mutex_unlock(&sched_lock);
+
+	return count;
+}
+
+/*
+ * Create show/store routines
+ * - sys: One governor instance for complete SYSTEM
+ * - pol: One governor instance per struct cpufreq_policy
+ */
+#define show_gov_pol_sys(file_name)					\
+static ssize_t show_##file_name##_gov_sys				\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return show_##file_name(common_tunables, buf);			\
+}									\
+									\
+static ssize_t show_##file_name##_gov_pol				\
+(struct cpufreq_policy *policy, char *buf)				\
+{									\
+	return show_##file_name(policy->governor_data, buf);		\
+}
+
+#define store_gov_pol_sys(file_name)					\
+static ssize_t store_##file_name##_gov_sys				\
+(struct kobject *kobj, struct attribute *attr, const char *buf,		\
+	size_t count)							\
+{									\
+	return store_##file_name(common_tunables, buf, count);		\
+}									\
+									\
+static ssize_t store_##file_name##_gov_pol				\
+(struct cpufreq_policy *policy, const char *buf, size_t count)		\
+{									\
+	return store_##file_name(policy->governor_data, buf, count);	\
+}
+
+#define show_store_gov_pol_sys(file_name)				\
+show_gov_pol_sys(file_name);						\
+store_gov_pol_sys(file_name)
+
+show_store_gov_pol_sys(target_loads);
+show_store_gov_pol_sys(above_hispeed_delay);
+show_store_gov_pol_sys(hispeed_freq);
+show_store_gov_pol_sys(go_hispeed_load);
+show_store_gov_pol_sys(min_sample_time);
+show_store_gov_pol_sys(timer_rate);
+show_store_gov_pol_sys(timer_slack);
+show_store_gov_pol_sys(boost);
+store_gov_pol_sys(boostpulse);
+show_store_gov_pol_sys(boostpulse_duration);
+show_store_gov_pol_sys(io_is_busy);
+show_store_gov_pol_sys(use_sched_load);
+show_store_gov_pol_sys(use_migration_notif);
+show_store_gov_pol_sys(max_freq_hysteresis);
+show_store_gov_pol_sys(align_windows);
+
+#define gov_sys_attr_rw(_name)						\
+static struct global_attr _name##_gov_sys =				\
+__ATTR(_name, 0644, show_##_name##_gov_sys, store_##_name##_gov_sys)
+
+#define gov_pol_attr_rw(_name)						\
+static struct freq_attr _name##_gov_pol =				\
+__ATTR(_name, 0644, show_##_name##_gov_pol, store_##_name##_gov_pol)
+
+#define gov_sys_pol_attr_rw(_name)					\
+	gov_sys_attr_rw(_name);						\
+	gov_pol_attr_rw(_name)
+
+gov_sys_pol_attr_rw(target_loads);
+gov_sys_pol_attr_rw(above_hispeed_delay);
+gov_sys_pol_attr_rw(hispeed_freq);
+gov_sys_pol_attr_rw(go_hispeed_load);
+gov_sys_pol_attr_rw(min_sample_time);
+gov_sys_pol_attr_rw(timer_rate);
+gov_sys_pol_attr_rw(timer_slack);
+gov_sys_pol_attr_rw(boost);
+gov_sys_pol_attr_rw(boostpulse_duration);
+gov_sys_pol_attr_rw(io_is_busy);
+gov_sys_pol_attr_rw(use_sched_load);
+gov_sys_pol_attr_rw(use_migration_notif);
+gov_sys_pol_attr_rw(max_freq_hysteresis);
+gov_sys_pol_attr_rw(align_windows);
+
+static struct global_attr boostpulse_gov_sys =
+	__ATTR(boostpulse, 0200, NULL, store_boostpulse_gov_sys);
+
+static struct freq_attr boostpulse_gov_pol =
+	__ATTR(boostpulse, 0200, NULL, store_boostpulse_gov_pol);
+
+/* One Governor instance for entire system */
+static struct attribute *ironactive_attributes_gov_sys[] = {
+	&target_loads_gov_sys.attr,
+	&above_hispeed_delay_gov_sys.attr,
+	&hispeed_freq_gov_sys.attr,
+	&go_hispeed_load_gov_sys.attr,
+	&min_sample_time_gov_sys.attr,
+	&timer_rate_gov_sys.attr,
+	&timer_slack_gov_sys.attr,
+	&boost_gov_sys.attr,
+	&boostpulse_gov_sys.attr,
+	&boostpulse_duration_gov_sys.attr,
+	&io_is_busy_gov_sys.attr,
+	&use_sched_load_gov_sys.attr,
+	&use_migration_notif_gov_sys.attr,
+	&max_freq_hysteresis_gov_sys.attr,
+	&align_windows_gov_sys.attr,
+	NULL,
+};
+
+static struct attribute_group ironactive_attr_group_gov_sys = {
+	.attrs = ironactive_attributes_gov_sys,
+	.name = "ironactive",
+};
+
+/* Per policy governor instance */
+static struct attribute *ironactive_attributes_gov_pol[] = {
+	&target_loads_gov_pol.attr,
+	&above_hispeed_delay_gov_pol.attr,
+	&hispeed_freq_gov_pol.attr,
+	&go_hispeed_load_gov_pol.attr,
+	&min_sample_time_gov_pol.attr,
+	&timer_rate_gov_pol.attr,
+	&timer_slack_gov_pol.attr,
+	&boost_gov_pol.attr,
+	&boostpulse_gov_pol.attr,
+	&boostpulse_duration_gov_pol.attr,
+	&io_is_busy_gov_pol.attr,
+	&use_sched_load_gov_pol.attr,
+	&use_migration_notif_gov_pol.attr,
+	&max_freq_hysteresis_gov_pol.attr,
+	&align_windows_gov_pol.attr,
+	NULL,
+};
+
+static struct attribute_group ironactive_attr_group_gov_pol = {
+	.attrs = ironactive_attributes_gov_pol,
+	.name = "ironactive",
+};
+
+static struct attribute_group *get_sysfs_attr(void)
+{
+	if (have_governor_per_policy())
+		return &ironactive_attr_group_gov_pol;
+	else
+		return &ironactive_attr_group_gov_sys;
+}
+
+static void cpufreq_ironactive_nop_timer(unsigned long data)
+{
+}
+
+static struct cpufreq_ironactive_tunables *alloc_tunable(
+					struct cpufreq_policy *policy)
+{
+	struct cpufreq_ironactive_tunables *tunables;
+
+	tunables = kzalloc(sizeof(*tunables), GFP_KERNEL);
+	if (!tunables)
+		return ERR_PTR(-ENOMEM);
+
+	tunables->above_hispeed_delay = default_above_hispeed_delay;
+	tunables->nabove_hispeed_delay =
+		ARRAY_SIZE(default_above_hispeed_delay);
+	tunables->go_hispeed_load = DEFAULT_GO_HISPEED_LOAD;
+	tunables->target_loads = default_target_loads;
+	tunables->ntarget_loads = ARRAY_SIZE(default_target_loads);
+	tunables->min_sample_time = DEFAULT_MIN_SAMPLE_TIME;
+	tunables->timer_rate = DEFAULT_TIMER_RATE;
+#ifdef CONFIG_STATE_NOTIFIER
+	tunables->timer_rate_prev = DEFAULT_TIMER_RATE;
+#endif
+	tunables->boostpulse_duration_val = DEFAULT_MIN_SAMPLE_TIME;
+	tunables->timer_slack_val = DEFAULT_TIMER_SLACK;
+
+	spin_lock_init(&tunables->target_loads_lock);
+	spin_lock_init(&tunables->above_hispeed_delay_lock);
+
+	return tunables;
+}
+
+static struct cpufreq_ironactive_policyinfo *get_policyinfo(
+					struct cpufreq_policy *policy)
+{
+	struct cpufreq_ironactive_policyinfo *ppol =
+				per_cpu(polinfo, policy->cpu);
+	int i;
+	unsigned long *busy;
+
+	/* polinfo already allocated for policy, return */
+	if (ppol)
+		return ppol;
+
+	ppol = kzalloc(sizeof(*ppol), GFP_KERNEL);
+	if (!ppol)
+		return ERR_PTR(-ENOMEM);
+
+	busy = kcalloc(cpumask_weight(policy->related_cpus), sizeof(*busy),
+		       GFP_KERNEL);
+	if (!busy) {
+		kfree(ppol);
+		return ERR_PTR(-ENOMEM);
+	}
+	ppol->cpu_busy_times = busy;
+
+	init_timer_deferrable(&ppol->policy_timer);
+	ppol->policy_timer.function = cpufreq_ironactive_timer;
+	init_timer(&ppol->policy_slack_timer);
+	ppol->policy_slack_timer.function = cpufreq_ironactive_nop_timer;
+	spin_lock_init(&ppol->load_lock);
+	spin_lock_init(&ppol->target_freq_lock);
+	init_rwsem(&ppol->enable_sem);
+
+	for_each_cpu(i, policy->related_cpus)
+		per_cpu(polinfo, i) = ppol;
+	return ppol;
+}
+
+/* This function is not multithread-safe. */
+static void free_policyinfo(int cpu)
+{
+	struct cpufreq_ironactive_policyinfo *ppol = per_cpu(polinfo, cpu);
+	int j;
+
+	if (!ppol)
+		return;
+
+	for_each_possible_cpu(j)
+		if (per_cpu(polinfo, j) == ppol)
+			per_cpu(polinfo, cpu) = NULL;
+	kfree(ppol->cached_tunables);
+	kfree(ppol->cpu_busy_times);
+	kfree(ppol);
+}
+
+static struct cpufreq_ironactive_tunables *get_tunables(
+				struct cpufreq_ironactive_policyinfo *ppol)
+{
+	if (have_governor_per_policy())
+		return ppol->cached_tunables;
+	else
+		return cached_common_tunables;
+}
+
+static int cpufreq_governor_ironactive(struct cpufreq_policy *policy,
+		unsigned int event)
+{
+	int rc;
+	struct cpufreq_ironactive_policyinfo *ppol;
+	struct cpufreq_frequency_table *freq_table;
+	struct cpufreq_ironactive_tunables *tunables;
+	unsigned long flags;
+
+	if (have_governor_per_policy())
+		tunables = policy->governor_data;
+	else
+		tunables = common_tunables;
+
+	BUG_ON(!tunables && (event != CPUFREQ_GOV_POLICY_INIT));
+
+	switch (event) {
+	case CPUFREQ_GOV_POLICY_INIT:
+		ppol = get_policyinfo(policy);
+		if (IS_ERR(ppol))
+			return PTR_ERR(ppol);
+
+		if (have_governor_per_policy()) {
+			WARN_ON(tunables);
+		} else if (tunables) {
+			tunables->usage_count++;
+			policy->governor_data = tunables;
+			return 0;
+		}
+
+		tunables = get_tunables(ppol);
+		if (!tunables) {
+			tunables = alloc_tunable(policy);
+			if (IS_ERR(tunables))
+				return PTR_ERR(tunables);
+		}
+
+		tunables->usage_count = 1;
+		policy->governor_data = tunables;
+		if (!have_governor_per_policy()) {
+			WARN_ON(cpufreq_get_global_kobject());
+			common_tunables = tunables;
+		}
+
+		rc = sysfs_create_group(get_governor_parent_kobj(policy),
+				get_sysfs_attr());
+		if (rc) {
+			kfree(tunables);
+			policy->governor_data = NULL;
+			if (!have_governor_per_policy()) {
+				common_tunables = NULL;
+				cpufreq_put_global_kobject();
+			}
+			return rc;
+		}
+
+		if (!policy->governor->initialized)
+			cpufreq_register_notifier(&cpufreq_notifier_block,
+					CPUFREQ_TRANSITION_NOTIFIER);
+
+		if (tunables->use_sched_load)
+			cpufreq_ironactive_enable_sched_input(tunables);
+
+		if (have_governor_per_policy())
+			ppol->cached_tunables = tunables;
+		else
+			cached_common_tunables = tunables;
+
+		break;
+
+	case CPUFREQ_GOV_POLICY_EXIT:
+		if (!--tunables->usage_count) {
+			if (policy->governor->initialized == 1)
+				cpufreq_unregister_notifier(&cpufreq_notifier_block,
+						CPUFREQ_TRANSITION_NOTIFIER);
+
+			sysfs_remove_group(get_governor_parent_kobj(policy),
+					get_sysfs_attr());
+			if (!have_governor_per_policy())
+				cpufreq_put_global_kobject();
+			common_tunables = NULL;
+		}
+
+		policy->governor_data = NULL;
+
+		if (tunables->use_sched_load)
+			cpufreq_ironactive_disable_sched_input(tunables);
+
+		break;
+
+	case CPUFREQ_GOV_START:
+		mutex_lock(&gov_lock);
+
+		freq_table = cpufreq_frequency_get_table(policy->cpu);
+		if (!tunables->hispeed_freq)
+			tunables->hispeed_freq = policy->max;
+
+		ppol = per_cpu(polinfo, policy->cpu);
+		ppol->policy = policy;
+		ppol->target_freq = policy->cur;
+		ppol->freq_table = freq_table;
+		ppol->floor_freq = ppol->target_freq;
+		ppol->floor_validate_time = ktime_to_us(ktime_get());
+		ppol->hispeed_validate_time = ppol->floor_validate_time;
+		ppol->min_freq = policy->min;
+		ppol->reject_notification = true;
+		down_write(&ppol->enable_sem);
+		del_timer_sync(&ppol->policy_timer);
+		del_timer_sync(&ppol->policy_slack_timer);
+		ppol->policy_timer.data = policy->cpu;
+		ppol->last_evaluated_jiffy = get_jiffies_64();
+		if (ppol->governor_enabled != 1)
+				cpufreq_ironactive_timer_start(tunables, policy->cpu);
+			else
+				WARN(1, "GOV_START is called without GOV_STOP\n");
+		ppol->governor_enabled = 1;
+		up_write(&ppol->enable_sem);
+		ppol->reject_notification = false;
+
+		mutex_unlock(&gov_lock);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		mutex_lock(&gov_lock);
+
+		ppol = per_cpu(polinfo, policy->cpu);
+		ppol->reject_notification = true;
+		down_write(&ppol->enable_sem);
+		ppol->governor_enabled = 0;
+		ppol->target_freq = 0;
+		del_timer_sync(&ppol->policy_timer);
+		del_timer_sync(&ppol->policy_slack_timer);
+		up_write(&ppol->enable_sem);
+		ppol->reject_notification = false;
+
+		mutex_unlock(&gov_lock);
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		__cpufreq_driver_target(policy,
+				policy->cur, CPUFREQ_RELATION_L);
+
+		ppol = per_cpu(polinfo, policy->cpu);
+
+		down_read(&ppol->enable_sem);
+		if (ppol->governor_enabled) {
+			spin_lock_irqsave(&ppol->target_freq_lock, flags);
+			if (policy->max < ppol->target_freq)
+				ppol->target_freq = policy->max;
+			else if (policy->min > ppol->target_freq)
+				ppol->target_freq = policy->min;
+			spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+
+			if (policy->min < ppol->min_freq)
+				cpufreq_ironactive_timer_resched(policy->cpu,
+								  true);
+			ppol->min_freq = policy->min;
+		}
+
+		up_read(&ppol->enable_sem);
+
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_IRONACTIVE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_ironactive = {
+	.name = "ironactive",
+	.governor = cpufreq_governor_ironactive,
+	.max_transition_latency = 10000000,
+	.owner = THIS_MODULE,
+};
+
+static int __init cpufreq_ironactive_init(void)
+{
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+
+	spin_lock_init(&speedchange_cpumask_lock);
+	mutex_init(&gov_lock);
+	mutex_init(&sched_lock);
+	speedchange_task =
+		kthread_create(cpufreq_ironactive_speedchange_task, NULL,
+			       "cfironactive");
+	if (IS_ERR(speedchange_task))
+		return PTR_ERR(speedchange_task);
+
+	sched_setscheduler_nocheck(speedchange_task, SCHED_FIFO, &param);
+	get_task_struct(speedchange_task);
+
+	/* NB: wake up so the thread does not look hung to the freezer */
+	wake_up_process(speedchange_task);
+
+	return cpufreq_register_governor(&cpufreq_gov_ironactive);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_IRONACTIVE
+fs_initcall(cpufreq_ironactive_init);
+#else
+module_init(cpufreq_ironactive_init);
+#endif
+
+static void __exit cpufreq_ironactive_exit(void)
+{
+	int cpu;
+
+	cpufreq_unregister_governor(&cpufreq_gov_ironactive);
+	kthread_stop(speedchange_task);
+	put_task_struct(speedchange_task);
+
+	for_each_possible_cpu(cpu)
+		free_policyinfo(cpu);
+}
+
+module_exit(cpufreq_ironactive_exit);
+
+MODULE_AUTHOR("Pranav Vashi <neobuddy89@gmail.com>");
+MODULE_DESCRIPTION("'cpufreq_ironactive' - A cpufreq governor for "
+	"Latency sensitive workloads");
+MODULE_LICENSE("GPLv2");
diff --git a/drivers/cpufreq/cpufreq_lionheart.c b/drivers/cpufreq/cpufreq_lionheart.c
new file mode 100755
index 0000000..c02c1a6
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_lionheart.c
@@ -0,0 +1,550 @@
+/*
+ * drivers/cpufreq/cpufreq_lionheart.c
+ *
+ * Patched & tweaked: knzo
+ *
+ * Based on the Conservative governor by:
+ *
+ *    Copyright (C)  2001 Russell King
+ *              (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                        Jun Nakajima <jun.nakajima@intel.com>
+ *              (C)  2009 Alexander Clouter <alex@digriz.org.uk>
+ *              (C)  2014 LoungeKatt <twistedumbrella@gmail.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <asm/cputime.h>
+#include <linux/kthread.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/cpumask.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/input.h>
+#include <linux/workqueue.h>
+#include <linux/slab.h>
+
+#define DEF_FREQUENCY_UP_THRESHOLD		(99)
+#define DEF_FREQUENCY_DOWN_THRESHOLD		(40)
+#define MIN_SAMPLING_RATE_RATIO			(2)
+
+static unsigned int min_sampling_rate;
+
+#define LATENCY_MULTIPLIER			(1000)
+#define MIN_LATENCY_MULTIPLIER			(100)
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define MAX_SAMPLING_DOWN_FACTOR		(10)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+static void do_dbs_timer(struct work_struct *work);
+
+struct cpu_dbs_info_s {
+	cputime64_t prev_cpu_idle;
+	cputime64_t prev_cpu_wall;
+	cputime64_t prev_cpu_nice;
+	struct cpufreq_policy *cur_policy;
+	struct delayed_work work;
+	unsigned int down_skip;
+	unsigned int requested_freq;
+	int cpu;
+	unsigned int enable:1;
+
+	struct mutex timer_mutex;
+};
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, cs_cpu_dbs_info);
+
+static unsigned int dbs_enable;	
+
+static DEFINE_MUTEX(dbs_mutex);
+
+static struct dbs_tuners {
+	unsigned int sampling_rate;
+	unsigned int sampling_down_factor;
+	unsigned int up_threshold;
+	unsigned int down_threshold;
+	unsigned int ignore_nice;
+	unsigned int freq_step;
+} dbs_tuners_ins = {
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.down_threshold = DEF_FREQUENCY_DOWN_THRESHOLD,
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.ignore_nice = 0,
+	.freq_step = 5,
+};
+
+//static inline cputime64_t get_cpu_idle_time_jiffy(unsigned int cpu,
+//							cputime64_t *wall)
+//{
+//	cputime64_t idle_time;
+//	cputime64_t cur_wall_time;
+//	cputime64_t busy_time;
+//
+//	cur_wall_time = jiffies64_to_cputime64(get_jiffies_64());
+//
+//	busy_time  = kcpustat_cpu(cpu).cpustat[CPUTIME_USER];
+//	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SYSTEM];
+//	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_IRQ];
+//	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SOFTIRQ];
+//	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_STEAL];
+//	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_NICE];
+//
+//	idle_time = (cur_wall_time - busy_time);
+//	if (wall)
+//		*wall = (cputime64_t)jiffies_to_usecs(cur_wall_time);
+//
+//	return (cputime64_t)jiffies_to_usecs(idle_time);
+//}
+//
+//static inline cputime64_t get_cpu_idle_time(unsigned int cpu, cputime64_t *wall)
+//{
+//	u64 idle_time = get_cpu_idle_time_us(cpu, wall);
+//
+//	if (idle_time == -1ULL)
+//		return get_cpu_idle_time_jiffy(cpu, wall);
+//
+//	return idle_time;
+//}
+
+static int
+dbs_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
+		     void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpu_dbs_info_s *this_dbs_info = &per_cpu(cs_cpu_dbs_info,
+							freq->cpu);
+
+	struct cpufreq_policy *policy;
+
+	if (!this_dbs_info->enable)
+		return 0;
+
+	policy = this_dbs_info->cur_policy;
+
+	if (this_dbs_info->requested_freq > policy->max
+			|| this_dbs_info->requested_freq < policy->min)
+		this_dbs_info->requested_freq = freq->new;
+
+	return 0;
+}
+
+static struct notifier_block dbs_cpufreq_notifier_block = {
+	.notifier_call = dbs_cpufreq_notifier
+};
+
+static ssize_t show_sampling_rate_min(struct kobject *kobj,
+				      struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", min_sampling_rate);
+}
+
+define_one_global_ro(sampling_rate_min);
+
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+
+show_one(sampling_rate, sampling_rate);
+show_one(sampling_down_factor, sampling_down_factor);
+show_one(up_threshold, up_threshold);
+show_one(down_threshold, down_threshold);
+show_one(ignore_nice_load, ignore_nice);
+show_one(freq_step, freq_step);
+
+static ssize_t store_sampling_down_factor(struct kobject *a,
+					  struct attribute *b,
+					  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_down_factor = input;
+	return count;
+}
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_rate = max(input, min_sampling_rate);
+	return count;
+}
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 ||
+			input <= dbs_tuners_ins.down_threshold)
+		return -EINVAL;
+
+	dbs_tuners_ins.up_threshold = input;
+	return count;
+}
+
+static ssize_t store_down_threshold(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 11 || input > 100 ||
+			input >= dbs_tuners_ins.up_threshold)
+		return -EINVAL;
+
+	dbs_tuners_ins.down_threshold = input;
+	return count;
+}
+
+static ssize_t store_ignore_nice_load(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	unsigned int j;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == dbs_tuners_ins.ignore_nice)
+		return count;
+
+	dbs_tuners_ins.ignore_nice = input;
+
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+		dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&dbs_info->prev_cpu_wall, 0);
+		if (dbs_tuners_ins.ignore_nice)
+			dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+	}
+	return count;
+}
+
+static ssize_t store_freq_step(struct kobject *a, struct attribute *b,
+			       const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 100)
+		input = 100;
+
+	dbs_tuners_ins.freq_step = input;
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(sampling_down_factor);
+define_one_global_rw(up_threshold);
+define_one_global_rw(down_threshold);
+define_one_global_rw(ignore_nice_load);
+define_one_global_rw(freq_step);
+
+static struct attribute *dbs_attributes[] = {
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&sampling_down_factor.attr,
+	&up_threshold.attr,
+	&down_threshold.attr,
+	&ignore_nice_load.attr,
+	&freq_step.attr,
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "Lionheart",
+};
+
+static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
+{
+	unsigned int load = 0;
+	unsigned int max_load = 0;
+	unsigned int freq_target;
+
+	struct cpufreq_policy *policy;
+	unsigned int j;
+
+	policy = this_dbs_info->cur_policy;
+
+	for_each_cpu(j, policy->cpus) {
+		struct cpu_dbs_info_s *j_dbs_info;
+		cputime64_t cur_wall_time, cur_idle_time;
+		unsigned int idle_time, wall_time;
+
+		j_dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time, 0);
+
+		wall_time = (unsigned int) (cur_wall_time - j_dbs_info->prev_cpu_wall);
+		j_dbs_info->prev_cpu_wall = cur_wall_time;
+
+		idle_time = (unsigned int) (cur_idle_time - j_dbs_info->prev_cpu_idle);
+		j_dbs_info->prev_cpu_idle = cur_idle_time;
+
+		if (dbs_tuners_ins.ignore_nice) {
+			cputime64_t cur_nice;
+			unsigned long cur_nice_jiffies;
+
+			cur_nice = (kcpustat_cpu(j).cpustat[CPUTIME_NICE] - j_dbs_info->prev_cpu_nice);
+
+			cur_nice_jiffies = (unsigned long)
+					cputime64_to_jiffies64(cur_nice);
+
+			j_dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			idle_time += jiffies_to_usecs(cur_nice_jiffies);
+		}
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		load = 100 * (wall_time - idle_time) / wall_time;
+
+		if (load > max_load)
+			max_load = load;
+	}
+
+	if (dbs_tuners_ins.freq_step == 0)
+		return;
+
+	if (max_load > dbs_tuners_ins.up_threshold) {
+		this_dbs_info->down_skip = 0;
+
+		if (this_dbs_info->requested_freq == policy->max)
+			return;
+
+		freq_target = (dbs_tuners_ins.freq_step * policy->max) / 100;
+
+		if (unlikely(freq_target == 0))
+			freq_target = 5;
+
+		this_dbs_info->requested_freq += freq_target;
+		if (this_dbs_info->requested_freq > policy->max)
+			this_dbs_info->requested_freq = policy->max;
+
+		__cpufreq_driver_target(policy, this_dbs_info->requested_freq,
+			CPUFREQ_RELATION_H);
+		return;
+	}
+
+	if (max_load < (dbs_tuners_ins.down_threshold - 10)) {
+		freq_target = (dbs_tuners_ins.freq_step * policy->max) / 100;
+
+		this_dbs_info->requested_freq -= freq_target;
+		if (this_dbs_info->requested_freq < policy->min)
+			this_dbs_info->requested_freq = policy->min;
+
+		if (policy->cur == policy->min)
+			return;
+
+		__cpufreq_driver_target(policy, this_dbs_info->requested_freq,
+				CPUFREQ_RELATION_H);
+		return;
+	}
+}
+
+static void do_dbs_timer(struct work_struct *work)
+{
+	struct cpu_dbs_info_s *dbs_info =
+		container_of(work, struct cpu_dbs_info_s, work.work);
+	unsigned int cpu = dbs_info->cpu;
+
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+	// delay -= jiffies % delay;
+
+	mutex_lock(&dbs_info->timer_mutex);
+
+	dbs_check_cpu(dbs_info);
+
+	schedule_delayed_work_on(cpu, &dbs_info->work, delay);
+	mutex_unlock(&dbs_info->timer_mutex);
+}
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
+{
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+	// delay -= jiffies % delay;
+
+	dbs_info->enable = 1;
+	INIT_DEFERRABLE_WORK(&dbs_info->work, do_dbs_timer);
+	schedule_delayed_work_on(dbs_info->cpu, &dbs_info->work, delay);
+}
+
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
+{
+	dbs_info->enable = 0;
+	cancel_delayed_work_sync(&dbs_info->work);
+}
+
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int rc;
+
+	this_dbs_info = &per_cpu(cs_cpu_dbs_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy->cur))
+			return -EINVAL;
+
+		mutex_lock(&dbs_mutex);
+
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			j_dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&j_dbs_info->prev_cpu_wall, 0);
+			if (dbs_tuners_ins.ignore_nice) {
+				j_dbs_info->prev_cpu_nice =
+						kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			}
+		}
+		this_dbs_info->down_skip = 0;
+		this_dbs_info->requested_freq = policy->cur;
+
+		mutex_init(&this_dbs_info->timer_mutex);
+		dbs_enable++;
+
+		if (dbs_enable == 1) {
+			unsigned int latency;
+
+			latency = policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&dbs_attr_group);
+			if (rc) {
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+
+			min_sampling_rate = 10000;
+			dbs_tuners_ins.sampling_rate = 10000;
+
+			cpufreq_register_notifier(
+					&dbs_cpufreq_notifier_block,
+					CPUFREQ_TRANSITION_NOTIFIER);
+		}
+		mutex_unlock(&dbs_mutex);
+
+		dbs_timer_init(this_dbs_info);
+
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		dbs_timer_exit(this_dbs_info);
+
+		mutex_lock(&dbs_mutex);
+		dbs_enable--;
+		mutex_destroy(&this_dbs_info->timer_mutex);
+
+		if (dbs_enable == 0)
+			cpufreq_unregister_notifier(
+					&dbs_cpufreq_notifier_block,
+					CPUFREQ_TRANSITION_NOTIFIER);
+
+		mutex_unlock(&dbs_mutex);
+		if (!dbs_enable)
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &dbs_attr_group);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&this_dbs_info->timer_mutex);
+		if (policy->max < this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(
+					this_dbs_info->cur_policy,
+					policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(
+					this_dbs_info->cur_policy,
+					policy->min, CPUFREQ_RELATION_C);
+		mutex_unlock(&this_dbs_info->timer_mutex);
+
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_LIONHEART
+static
+#endif
+struct cpufreq_governor cpufreq_gov_lionheart = {
+	.name			= "Lionheart",
+	.governor		= cpufreq_governor_dbs,
+	.max_transition_latency	= TRANSITION_LATENCY_LIMIT,
+	.owner			= THIS_MODULE,
+};
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	return cpufreq_register_governor(&cpufreq_gov_lionheart);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_lionheart);
+}
+
+MODULE_AUTHOR("knzo");
+MODULE_DESCRIPTION("'cpufreq_lionheart' - A brave and agile conservative-based governor.");
+MODULE_LICENSE("GPL");
+
+fs_initcall(cpufreq_gov_dbs_init);
+module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_nightmare.c b/drivers/cpufreq/cpufreq_nightmare.c
new file mode 100644
index 0000000..cda598a
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_nightmare.c
@@ -0,0 +1,726 @@
+/*
+ *  drivers/cpufreq/cpufreq_nightmare.c
+ *
+ *  Copyright (C)  2011 Samsung Electronics co. ltd
+ *    ByungChang Cha <bc.cha@samsung.com>
+ *
+ *  Based on ondemand governor
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * Created by Alucard_24@xda
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+
+#define MIN_SAMPLING_RATE	10000
+
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+static void do_nightmare_timer(struct work_struct *work);
+
+struct cpufreq_nightmare_cpuinfo {
+	u64 prev_cpu_wall;
+	u64 prev_cpu_idle;
+	struct cpufreq_frequency_table *freq_table;
+	struct delayed_work work;
+	struct cpufreq_policy *cur_policy;
+	bool governor_enabled;
+	unsigned int cpu;
+	unsigned int prev_load;
+	/*
+	 * mutex that serializes governor limit change with
+	 * do_nightmare_timer invocation. We do not want do_nightmare_timer to run
+	 * when user is changing the governor or limits.
+	 */
+	struct mutex timer_mutex;
+};
+
+static DEFINE_PER_CPU(struct cpufreq_nightmare_cpuinfo, od_nightmare_cpuinfo);
+
+static unsigned int nightmare_enable;	/* number of CPUs using this policy */
+/*
+ * nightmare_mutex protects nightmare_enable in governor start/stop.
+ */
+static DEFINE_MUTEX(nightmare_mutex);
+
+static struct workqueue_struct *nightmare_wq;
+
+/* nightmare tuners */
+static struct nightmare_tuners {
+	unsigned int sampling_rate;
+	int inc_cpu_load_at_min_freq;
+	int inc_cpu_load;
+	int dec_cpu_load;
+	int freq_for_responsiveness;
+	int freq_for_responsiveness_max;
+	int freq_up_brake_at_min_freq;
+	int freq_up_brake;
+	int freq_step_at_min_freq;
+	int freq_step;
+	int freq_step_dec;
+	int freq_step_dec_at_max_freq;
+
+} nightmare_tuners_ins = {
+	.sampling_rate = 50000,
+	.inc_cpu_load_at_min_freq = 40,
+	.inc_cpu_load = 60,
+	.dec_cpu_load = 60,
+	.freq_for_responsiveness = 1728000,
+	.freq_for_responsiveness_max = 24570000,
+	.freq_step_at_min_freq = 40,
+	.freq_step = 50,
+	.freq_up_brake_at_min_freq = 40,
+	.freq_up_brake = 30,
+	.freq_step_dec = 10,
+	.freq_step_dec_at_max_freq = 10,
+};
+
+/************************** sysfs interface ************************/
+
+/* cpufreq_nightmare Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return sprintf(buf, "%d\n", nightmare_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(inc_cpu_load_at_min_freq, inc_cpu_load_at_min_freq);
+show_one(inc_cpu_load, inc_cpu_load);
+show_one(dec_cpu_load, dec_cpu_load);
+show_one(freq_for_responsiveness, freq_for_responsiveness);
+show_one(freq_for_responsiveness_max, freq_for_responsiveness_max);
+show_one(freq_step_at_min_freq, freq_step_at_min_freq);
+show_one(freq_step, freq_step);
+show_one(freq_up_brake_at_min_freq, freq_up_brake_at_min_freq);
+show_one(freq_up_brake, freq_up_brake);
+show_one(freq_step_dec, freq_step_dec);
+show_one(freq_step_dec_at_max_freq, freq_step_dec_at_max_freq);
+
+/* sampling_rate */
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret = 0;
+	int mpd = strcmp(current->comm, "mpdecision");
+
+	if (mpd == 0)
+		return ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(input,10000);
+
+	if (input == nightmare_tuners_ins.sampling_rate)
+		return count;
+
+	nightmare_tuners_ins.sampling_rate = input;
+
+	return count;
+}
+
+/* inc_cpu_load_at_min_freq */
+static ssize_t store_inc_cpu_load_at_min_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1) {
+		return -EINVAL;
+	}
+
+	input = min(input,nightmare_tuners_ins.inc_cpu_load);
+
+	if (input == nightmare_tuners_ins.inc_cpu_load_at_min_freq)
+		return count;
+
+	nightmare_tuners_ins.inc_cpu_load_at_min_freq = input;
+
+	return count;
+}
+
+/* inc_cpu_load */
+static ssize_t store_inc_cpu_load(struct kobject *a, struct attribute *b,
+					const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == nightmare_tuners_ins.inc_cpu_load)
+		return count;
+
+	nightmare_tuners_ins.inc_cpu_load = input;
+
+	return count;
+}
+
+/* dec_cpu_load */
+static ssize_t store_dec_cpu_load(struct kobject *a, struct attribute *b,
+					const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,95),5);
+
+	if (input == nightmare_tuners_ins.dec_cpu_load)
+		return count;
+
+	nightmare_tuners_ins.dec_cpu_load = input;
+
+	return count;
+}
+
+/* freq_for_responsiveness */
+static ssize_t store_freq_for_responsiveness(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input == nightmare_tuners_ins.freq_for_responsiveness)
+		return count;
+
+	nightmare_tuners_ins.freq_for_responsiveness = input;
+
+	return count;
+}
+
+/* freq_for_responsiveness_max */
+static ssize_t store_freq_for_responsiveness_max(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input == nightmare_tuners_ins.freq_for_responsiveness_max)
+		return count;
+
+	nightmare_tuners_ins.freq_for_responsiveness_max = input;
+
+	return count;
+}
+
+/* freq_step_at_min_freq */
+static ssize_t store_freq_step_at_min_freq(struct kobject *a, struct attribute *b,
+			       const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == nightmare_tuners_ins.freq_step_at_min_freq)
+		return count;
+
+	nightmare_tuners_ins.freq_step_at_min_freq = input;
+
+	return count;
+}
+
+/* freq_step */
+static ssize_t store_freq_step(struct kobject *a, struct attribute *b,
+			       const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == nightmare_tuners_ins.freq_step)
+		return count;
+
+	nightmare_tuners_ins.freq_step = input;
+
+	return count;
+}
+
+/* freq_up_brake_at_min_freq */
+static ssize_t store_freq_up_brake_at_min_freq(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == nightmare_tuners_ins.freq_up_brake_at_min_freq)
+		return count;
+
+	nightmare_tuners_ins.freq_up_brake_at_min_freq = input;
+
+	return count;
+}
+
+/* freq_up_brake */
+static ssize_t store_freq_up_brake(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == nightmare_tuners_ins.freq_up_brake)
+		return count;
+
+	nightmare_tuners_ins.freq_up_brake = input;
+
+	return count;
+}
+
+/* freq_step_dec */
+static ssize_t store_freq_step_dec(struct kobject *a, struct attribute *b,
+				       const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == nightmare_tuners_ins.freq_step_dec)
+		return count;
+
+	nightmare_tuners_ins.freq_step_dec = input;
+
+	return count;
+}
+
+/* freq_step_dec_at_max_freq */
+static ssize_t store_freq_step_dec_at_max_freq(struct kobject *a, struct attribute *b,
+				       const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == nightmare_tuners_ins.freq_step_dec_at_max_freq)
+		return count;
+
+	nightmare_tuners_ins.freq_step_dec_at_max_freq = input;
+
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(inc_cpu_load_at_min_freq);
+define_one_global_rw(inc_cpu_load);
+define_one_global_rw(dec_cpu_load);
+define_one_global_rw(freq_for_responsiveness);
+define_one_global_rw(freq_for_responsiveness_max);
+define_one_global_rw(freq_step_at_min_freq);
+define_one_global_rw(freq_step);
+define_one_global_rw(freq_up_brake_at_min_freq);
+define_one_global_rw(freq_up_brake);
+define_one_global_rw(freq_step_dec);
+define_one_global_rw(freq_step_dec_at_max_freq);
+
+static struct attribute *nightmare_attributes[] = {
+	&sampling_rate.attr,
+	&inc_cpu_load_at_min_freq.attr,
+	&inc_cpu_load.attr,
+	&dec_cpu_load.attr,
+	&freq_for_responsiveness.attr,
+	&freq_for_responsiveness_max.attr,
+	&freq_step_at_min_freq.attr,
+	&freq_step.attr,
+	&freq_up_brake_at_min_freq.attr,
+	&freq_up_brake.attr,
+	&freq_step_dec.attr,
+	&freq_step_dec_at_max_freq.attr,
+	NULL
+};
+
+static struct attribute_group nightmare_attr_group = {
+	.attrs = nightmare_attributes,
+	.name = "nightmare",
+};
+
+/************************** sysfs end ************************/
+
+static unsigned int adjust_cpufreq_frequency_target(struct cpufreq_policy *policy,
+					struct cpufreq_frequency_table *table,
+					unsigned int tmp_freq)
+{
+	unsigned int i = 0, l_freq = 0, h_freq = 0, target_freq = 0;
+
+	if (tmp_freq < policy->min)
+		tmp_freq = policy->min;
+	if (tmp_freq > policy->max)
+		tmp_freq = policy->max;
+
+	for (i = 0; (table[i].frequency != CPUFREQ_TABLE_END); i++) {
+		unsigned int freq = table[i].frequency;
+		if (freq == CPUFREQ_ENTRY_INVALID) {
+			continue;
+		}
+		if (freq < tmp_freq) {
+			h_freq = freq;
+		}
+		if (freq == tmp_freq) {
+			target_freq = freq;
+			break;
+		}
+		if (freq > tmp_freq) {
+			l_freq = freq;
+			break;
+		}
+	}
+	if (!target_freq) {
+		if (policy->cur >= h_freq
+			 && policy->cur <= l_freq)
+			target_freq = policy->cur;
+		else
+			target_freq = l_freq;
+	}
+
+	return target_freq;
+}
+
+static void nightmare_check_cpu(struct cpufreq_nightmare_cpuinfo *this_nightmare_cpuinfo)
+{
+	struct cpufreq_policy *policy;
+	unsigned int freq_for_responsiveness = nightmare_tuners_ins.freq_for_responsiveness;
+	unsigned int freq_for_responsiveness_max = nightmare_tuners_ins.freq_for_responsiveness_max;
+	int dec_cpu_load = nightmare_tuners_ins.dec_cpu_load;
+	int inc_cpu_load = nightmare_tuners_ins.inc_cpu_load;
+	int freq_step = nightmare_tuners_ins.freq_step;
+	int freq_up_brake = nightmare_tuners_ins.freq_up_brake;
+	int freq_step_dec = nightmare_tuners_ins.freq_step_dec;
+	unsigned int max_load = 0;
+	unsigned int tmp_freq = 0;
+	unsigned int j;
+	unsigned int sampling_rate = nightmare_tuners_ins.sampling_rate;
+
+	policy = this_nightmare_cpuinfo->cur_policy;
+	if (!policy)
+		return;
+
+	for_each_cpu(j, policy->cpus) {
+		struct cpufreq_nightmare_cpuinfo *j_nightmare_cpuinfo = &per_cpu(od_nightmare_cpuinfo, j);
+		u64 cur_wall_time, cur_idle_time;
+		unsigned int idle_time, wall_time;
+		unsigned int cur_load;
+		
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time, 0);
+
+		wall_time = (unsigned int)
+			(cur_wall_time - j_nightmare_cpuinfo->prev_cpu_wall);
+		j_nightmare_cpuinfo->prev_cpu_wall = cur_wall_time;
+
+		idle_time = (unsigned int)
+			(cur_idle_time - j_nightmare_cpuinfo->prev_cpu_idle);
+		j_nightmare_cpuinfo->prev_cpu_idle = cur_idle_time;
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		/*
+		 * If the CPU had gone completely idle, and a task just woke up
+		 * on this CPU now, it would be unfair to calculate 'load' the
+		 * usual way for this elapsed time-window, because it will show
+		 * near-zero load, irrespective of how CPU intensive that task
+		 * actually is. This is undesirable for latency-sensitive bursty
+		 * workloads.
+		 *
+		 * To avoid this, we reuse the 'load' from the previous
+		 * time-window and give this task a chance to start with a
+		 * reasonably high CPU frequency. (However, we shouldn't over-do
+		 * this copy, lest we get stuck at a high load (high frequency)
+		 * for too long, even when the current system load has actually
+		 * dropped down. So we perform the copy only once, upon the
+		 * first wake-up from idle.)
+		 *
+		 * Detecting this situation is easy: the governor's deferrable
+		 * timer would not have fired during CPU-idle periods. Hence
+		 * an unusually large 'wall_time' (as compared to the sampling
+		 * rate) indicates this scenario.
+		 *
+		 * prev_load can be zero in two cases and we must recalculate it
+		 * for both cases:
+		 * - during long idle intervals
+		 * - explicitly set to zero
+		 */
+		if (unlikely(wall_time > (2 * sampling_rate) &&
+			     j_nightmare_cpuinfo->prev_load)) {
+			cur_load = j_nightmare_cpuinfo->prev_load;
+
+			/*
+			 * Perform a destructive copy, to ensure that we copy
+			 * the previous load only once, upon the first wake-up
+			 * from idle.
+			 */
+			j_nightmare_cpuinfo->prev_load = 0;
+		} else {
+			cur_load = 100 * (wall_time - idle_time) / wall_time;
+			j_nightmare_cpuinfo->prev_load = cur_load;
+		}
+
+		if (cur_load > max_load)
+			max_load = cur_load;
+	}
+
+	cpufreq_notify_utilization(policy, max_load);
+
+	/* CPUs Online Scale Frequency*/
+	if (policy->cur < freq_for_responsiveness) {
+		inc_cpu_load = nightmare_tuners_ins.inc_cpu_load_at_min_freq;
+		freq_step = nightmare_tuners_ins.freq_step_at_min_freq;
+		freq_up_brake = nightmare_tuners_ins.freq_up_brake_at_min_freq;
+	} else if (policy->cur > freq_for_responsiveness_max) {
+		freq_step_dec = nightmare_tuners_ins.freq_step_dec_at_max_freq;
+	}
+	/* check if policy is valid */
+	if (!policy)
+		return;
+	/* Check for frequency increase or for frequency decrease */
+	if (max_load >= inc_cpu_load
+		 && policy->cur < policy->max) {
+		tmp_freq = adjust_cpufreq_frequency_target(policy,
+												   this_nightmare_cpuinfo->freq_table,
+												   (policy->cur + ((max_load + freq_step - freq_up_brake == 0 ? 1 : max_load + freq_step - freq_up_brake) * 3780)));
+
+		__cpufreq_driver_target(policy, tmp_freq, CPUFREQ_RELATION_L);
+	} else if (max_load < dec_cpu_load
+				&& policy->cur > policy->min) {
+		tmp_freq = adjust_cpufreq_frequency_target(policy,
+												   this_nightmare_cpuinfo->freq_table,
+												   (policy->cur - ((100 - max_load + freq_step_dec == 0 ? 1 : 100 - max_load + freq_step_dec) * 3780)));
+
+		__cpufreq_driver_target(policy, tmp_freq, CPUFREQ_RELATION_L);
+	}
+}
+
+static void do_nightmare_timer(struct work_struct *work)
+{
+	struct cpufreq_nightmare_cpuinfo *this_nightmare_cpuinfo =
+		container_of(work, struct cpufreq_nightmare_cpuinfo, work.work);
+	int delay;
+
+	if (unlikely(!cpu_online(this_nightmare_cpuinfo->cpu) ||
+				!this_nightmare_cpuinfo->cur_policy))
+		return;
+
+	mutex_lock(&this_nightmare_cpuinfo->timer_mutex);
+
+	nightmare_check_cpu(this_nightmare_cpuinfo);
+
+	delay = usecs_to_jiffies(nightmare_tuners_ins.sampling_rate);
+	/* We want all CPUs to do sampling nearly on
+	 * same jiffy
+	 */
+	if (num_online_cpus() > 1) {
+		delay -= jiffies % delay;
+	}
+
+	queue_delayed_work_on(this_nightmare_cpuinfo->cpu, nightmare_wq,
+			&this_nightmare_cpuinfo->work, delay);
+	mutex_unlock(&this_nightmare_cpuinfo->timer_mutex);
+}
+
+static int cpufreq_governor_nightmare(struct cpufreq_policy *policy,
+				unsigned int event)
+{
+	struct cpufreq_nightmare_cpuinfo *this_nightmare_cpuinfo;
+	unsigned int cpu = policy->cpu, j;
+	int rc, delay;
+
+	this_nightmare_cpuinfo = &per_cpu(od_nightmare_cpuinfo, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy))
+			return -EINVAL;
+
+		mutex_lock(&nightmare_mutex);
+		this_nightmare_cpuinfo->freq_table = cpufreq_frequency_get_table(cpu);
+		if (!this_nightmare_cpuinfo->freq_table) {
+			mutex_unlock(&nightmare_mutex);
+			return -EINVAL;
+		}
+
+		for_each_cpu(j, policy->cpus) {
+			struct cpufreq_nightmare_cpuinfo *j_nightmare_cpuinfo = &per_cpu(od_nightmare_cpuinfo, j);
+			unsigned int prev_load;
+
+			j_nightmare_cpuinfo->prev_cpu_idle = get_cpu_idle_time(j,
+				&j_nightmare_cpuinfo->prev_cpu_wall, 0);
+
+			prev_load = (unsigned int)
+				(j_nightmare_cpuinfo->prev_cpu_wall -
+				j_nightmare_cpuinfo->prev_cpu_idle);
+			j_nightmare_cpuinfo->prev_load = 100 * prev_load /
+				(unsigned int) j_nightmare_cpuinfo->prev_cpu_wall;
+		}
+
+		nightmare_enable++;
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (nightmare_enable == 1) {
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&nightmare_attr_group);
+			if (rc) {
+				nightmare_enable--;
+				mutex_unlock(&nightmare_mutex);
+				return rc;
+			}
+		}
+		cpu = policy->cpu;
+		this_nightmare_cpuinfo->cpu = cpu;
+		this_nightmare_cpuinfo->cur_policy = policy;
+		this_nightmare_cpuinfo->governor_enabled = true;
+		mutex_unlock(&nightmare_mutex);
+
+		mutex_init(&this_nightmare_cpuinfo->timer_mutex);
+
+		delay=usecs_to_jiffies(nightmare_tuners_ins.sampling_rate);
+		/* We want all CPUs to do sampling nearly on same jiffy */
+		if (num_online_cpus() > 1) {
+			delay -= jiffies % delay;
+		}
+
+		INIT_DEFERRABLE_WORK(&this_nightmare_cpuinfo->work, do_nightmare_timer);
+		queue_delayed_work_on(cpu,
+			nightmare_wq, &this_nightmare_cpuinfo->work, delay);
+
+		break;
+	case CPUFREQ_GOV_STOP:
+		cancel_delayed_work_sync(&this_nightmare_cpuinfo->work);
+
+		mutex_lock(&nightmare_mutex);
+		mutex_destroy(&this_nightmare_cpuinfo->timer_mutex);
+
+		this_nightmare_cpuinfo->governor_enabled = false;
+
+		this_nightmare_cpuinfo->cur_policy = NULL;
+
+		nightmare_enable--;
+		if (!nightmare_enable) {
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &nightmare_attr_group);
+		}
+		mutex_unlock(&nightmare_mutex);
+
+		break;
+	case CPUFREQ_GOV_LIMITS:
+		if (!this_nightmare_cpuinfo->cur_policy
+			 || !policy) {
+			pr_debug("Unable to limit cpu freq due to cur_policy == NULL\n");
+			return -EPERM;
+		}
+		mutex_lock(&this_nightmare_cpuinfo->timer_mutex);
+		__cpufreq_driver_target(this_nightmare_cpuinfo->cur_policy,
+				policy->cur, CPUFREQ_RELATION_L);
+		mutex_unlock(&this_nightmare_cpuinfo->timer_mutex);
+
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_nightmare = {
+	.name                   = "nightmare",
+	.governor               = cpufreq_governor_nightmare,
+	.owner                  = THIS_MODULE,
+};
+
+static int __init cpufreq_gov_nightmare_init(void)
+{
+	nightmare_wq = alloc_workqueue("nightmare_wq", WQ_HIGHPRI, 0);
+	if (!nightmare_wq) {
+		printk(KERN_ERR "Failed to create nightmare_wq workqueue\n");
+		return -EFAULT;
+	}
+
+	return cpufreq_register_governor(&cpufreq_gov_nightmare);
+}
+
+static void __exit cpufreq_gov_nightmare_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_nightmare);
+}
+
+MODULE_AUTHOR("Alucard24@XDA");
+MODULE_DESCRIPTION("'cpufreq_nightmare' - A dynamic cpufreq/cpuhotplug governor v5.0 (SnapDragon)");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE
+fs_initcall(cpufreq_gov_nightmare_init);
+#else
+module_init(cpufreq_gov_nightmare_init);
+#endif
+module_exit(cpufreq_gov_nightmare_exit);
diff --git a/drivers/cpufreq/cpufreq_ondemand.c b/drivers/cpufreq/cpufreq_ondemand.c
index 9e1a9eb..b95bef2 100755
--- a/drivers/cpufreq/cpufreq_ondemand.c
+++ b/drivers/cpufreq/cpufreq_ondemand.c
@@ -170,21 +170,24 @@ static void od_check_cpu(int cpu, unsigned int load)
 		dbs_freq_increase(policy, policy->max);
 	} else {
 		/* Calculate the next frequency proportional to load */
-		unsigned int freq_next;
-		freq_next = load * policy->cpuinfo.max_freq / 100;
+		unsigned int freq_next, min_f, max_f;
+
+		min_f = policy->cpuinfo.min_freq;
+		max_f = policy->cpuinfo.max_freq;
+		freq_next = min_f + load * (max_f - min_f) / 100;
 
 		/* No longer fully busy, reset rate_mult */
 		dbs_info->rate_mult = 1;
 
 		if (!od_tuners->powersave_bias) {
 			__cpufreq_driver_target(policy, freq_next,
-					CPUFREQ_RELATION_L);
+					CPUFREQ_RELATION_C);
 			return;
 		}
 
 		freq_next = od_ops.powersave_bias_target(policy, freq_next,
 					CPUFREQ_RELATION_L);
-		__cpufreq_driver_target(policy, freq_next, CPUFREQ_RELATION_L);
+		__cpufreq_driver_target(policy, freq_next, CPUFREQ_RELATION_C);
 	}
 }
 
diff --git a/drivers/cpufreq/cpufreq_ondemand_x.c b/drivers/cpufreq/cpufreq_ondemand_x.c
new file mode 100755
index 0000000..c06677e
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_ondemand_x.c
@@ -0,0 +1,895 @@
+/*
+ *  drivers/cpufreq/cpufreq_ondemand_x.c
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *            (c)  2013 The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/workqueue.h>
+#include <linux/slab.h>
+
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+/* User tunabble controls */
+#define DEF_FREQUENCY_UP_THRESHOLD		(70)
+#define ANY_CPU_DEF_FREQUENCY_UP_THRESHOLD	(70)
+#define MULTI_CORE_DEF_FREQUENCY_UP_THRESHOLD	(70)
+#define MICRO_FREQUENCY_UP_THRESHOLD		(95)
+
+#define DEF_MIDDLE_GRID_STEP			(14)
+#define DEF_HIGH_GRID_STEP			(20)
+#define DEF_MIDDLE_GRID_LOAD			(55)
+#define DEF_HIGH_GRID_LOAD			(79)
+
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define DEF_SAMPLING_RATE			(20000)
+
+#define DEF_SYNC_FREQUENCY			(1267200)
+#define DEF_OPTIMAL_FREQUENCY			(1574400)
+#define DEF_OPTIMAL_MAX_FREQ			(1728000)
+
+/* Kernel tunabble controls */
+#define MICRO_FREQUENCY_MIN_SAMPLE_RATE		(10000)
+#define MAX_SAMPLING_DOWN_FACTOR		(3)
+#define MIN_FREQUENCY_UP_THRESHOLD		(11)
+#define MAX_FREQUENCY_UP_THRESHOLD		(100)
+#define MIN_FREQUENCY_DOWN_DIFFERENTIAL		(1)
+
+/*
+ * The polling frequency of this governor depends on the capability of
+ * the processor. Default polling frequency is 1000 times the transition
+ * latency of the processor. The governor will work on any processor with
+ * transition latency <= 10mS, using appropriate sampling
+ * rate.
+ * For CPUs with transition latency > 10mS (mostly drivers with CPUFREQ_ETERNAL)
+ * this governor will not work.
+ * All times here are in uS.
+ */
+#define MIN_SAMPLING_RATE_RATIO			(2)
+
+static unsigned int min_sampling_rate;
+
+#define LATENCY_MULTIPLIER			(1000)
+#define MIN_LATENCY_MULTIPLIER			(100)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+static void do_dbs_timer(struct work_struct *work);
+
+/* Sampling types */
+enum {DBS_NORMAL_SAMPLE, DBS_SUB_SAMPLE};
+
+struct cpu_dbs_info_s {
+	u64 prev_cpu_idle;
+	u64 prev_cpu_iowait;
+	u64 prev_cpu_wall;
+	u64 prev_cpu_nice;
+	/*
+	 * Used to keep track of load in the previous interval. However, when
+	 * explicitly set to zero, it is used as a flag to ensure that we copy
+	 * the previous load to the current interval only once, upon the first
+	 * wake-up from idle.
+	 */
+	unsigned int prev_load;
+	struct cpufreq_policy *cur_policy;
+	struct delayed_work work;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int freq_lo;
+	unsigned int freq_lo_jiffies;
+	unsigned int freq_hi_jiffies;
+	unsigned int rate_mult;
+	unsigned int max_load;
+	unsigned int cpu;
+	unsigned int sample_type:1;
+	/*
+	 * percpu mutex that serializes governor limit change with
+	 * do_dbs_timer invocation. We do not want do_dbs_timer to run
+	 * when user is changing the governor or limits.
+	 */
+	struct mutex timer_mutex;
+
+	wait_queue_head_t sync_wq;
+};
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, od_cpu_dbs_info);
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info);
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info);
+
+static unsigned int dbs_enable;	/* number of CPUs using this policy */
+
+/*
+ * dbs_mutex protects dbs_enable and dbs_info during start/stop.
+ */
+static DEFINE_MUTEX(dbs_mutex);
+
+static struct workqueue_struct *dbs_wq;
+
+static struct dbs_tuners {
+	unsigned int sampling_rate;
+	unsigned int up_threshold;
+	unsigned int up_threshold_multi_core;
+	unsigned int optimal_freq;
+	unsigned int up_threshold_any_cpu_load;
+	unsigned int micro_freq_up_threshold;
+	unsigned int sync_freq;
+	unsigned int sampling_down_factor;
+	unsigned int optimal_max_freq;
+	unsigned int middle_grid_step;
+	unsigned int high_grid_step;
+	unsigned int middle_grid_load;
+	unsigned int high_grid_load;
+} dbs_tuners_ins = {
+	.up_threshold_multi_core = MULTI_CORE_DEF_FREQUENCY_UP_THRESHOLD,
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.up_threshold_any_cpu_load = ANY_CPU_DEF_FREQUENCY_UP_THRESHOLD,
+	.micro_freq_up_threshold = MICRO_FREQUENCY_UP_THRESHOLD,
+	.middle_grid_step = DEF_MIDDLE_GRID_STEP,
+	.high_grid_step = DEF_HIGH_GRID_STEP,
+	.middle_grid_load = DEF_MIDDLE_GRID_LOAD,
+	.high_grid_load = DEF_HIGH_GRID_LOAD,
+	.sync_freq = DEF_SYNC_FREQUENCY,
+	.optimal_freq = DEF_OPTIMAL_FREQUENCY,
+	.optimal_max_freq = DEF_OPTIMAL_MAX_FREQ,
+	.sampling_rate = DEF_SAMPLING_RATE,
+};
+
+
+/************************** sysfs interface ************************/
+
+static ssize_t show_sampling_rate_min(struct kobject *kobj,
+				      struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", min_sampling_rate);
+}
+
+define_one_global_ro(sampling_rate_min);
+
+/* cpufreq_ondemand_x Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)              \
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(up_threshold, up_threshold);
+show_one(up_threshold_multi_core, up_threshold_multi_core);
+show_one(sampling_down_factor, sampling_down_factor);
+show_one(optimal_freq, optimal_freq);
+show_one(up_threshold_any_cpu_load, up_threshold_any_cpu_load);
+show_one(micro_freq_up_threshold, micro_freq_up_threshold);
+show_one(middle_grid_step, middle_grid_step);
+show_one(high_grid_step, high_grid_step);
+show_one(middle_grid_load, middle_grid_load);
+show_one(high_grid_load, high_grid_load);
+show_one(sync_freq, sync_freq);
+show_one(optimal_max_freq, optimal_max_freq);
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret = 0;
+	int mpd = strcmp(current->comm, "mpdecision");
+
+	if (mpd == 0)
+		return ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_rate = max(input, min_sampling_rate);
+	return count;
+}
+
+static ssize_t store_sync_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret = 0;
+	int mpd = strcmp(current->comm, "mpdecision");
+
+	if (mpd == 0)
+		return ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sync_freq = input;
+	return count;
+}
+
+static ssize_t store_optimal_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret = 0;
+	int mpd = strcmp(current->comm, "mpdecision");
+
+	if (mpd == 0)
+		return ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.optimal_freq = input;
+	return count;
+}
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold = input;
+	return count;
+}
+
+static ssize_t store_up_threshold_multi_core(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold_multi_core = input;
+	return count;
+}
+
+static ssize_t store_up_threshold_any_cpu_load(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold_any_cpu_load = input;
+	return count;
+}
+
+static ssize_t store_middle_grid_step(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.middle_grid_step = input;
+	return count;
+}
+
+static ssize_t store_high_grid_step(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.high_grid_step = input;
+
+	return count;
+}
+
+static ssize_t store_optimal_max_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret = 0;
+	int mpd = strcmp(current->comm, "mpdecision");
+
+	if (mpd == 0)
+		return ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.optimal_max_freq = input;
+
+	return count;
+}
+
+static ssize_t store_middle_grid_load(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.middle_grid_load = input;
+
+	return count;
+}
+
+static ssize_t store_high_grid_load(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.high_grid_load = input;
+
+	return count;
+}
+
+static ssize_t store_micro_freq_up_threshold(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.micro_freq_up_threshold = input;
+
+	return count;
+}
+
+static ssize_t store_sampling_down_factor(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input, j;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+	dbs_tuners_ins.sampling_down_factor = input;
+
+	/* Reset down sampling multiplier in case it was active */
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(od_cpu_dbs_info, j);
+		dbs_info->rate_mult = 1;
+	}
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(up_threshold);
+define_one_global_rw(sampling_down_factor);
+define_one_global_rw(up_threshold_multi_core);
+define_one_global_rw(optimal_freq);
+define_one_global_rw(up_threshold_any_cpu_load);
+define_one_global_rw(micro_freq_up_threshold);
+define_one_global_rw(sync_freq);
+define_one_global_rw(optimal_max_freq);
+define_one_global_rw(middle_grid_step);
+define_one_global_rw(high_grid_step);
+define_one_global_rw(middle_grid_load);
+define_one_global_rw(high_grid_load);
+
+static struct attribute *dbs_attributes[] = {
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&up_threshold.attr,
+	&sampling_down_factor.attr,
+	&up_threshold_multi_core.attr,
+	&optimal_freq.attr,
+	&optimal_max_freq.attr,
+	&up_threshold_any_cpu_load.attr,
+	&micro_freq_up_threshold.attr,
+	&sync_freq.attr,
+	&middle_grid_step.attr,
+	&high_grid_step.attr,
+	&middle_grid_load.attr,
+	&high_grid_load.attr,
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "ondemand_x",
+};
+
+/************************** sysfs end ************************/
+
+static void dbs_freq_increase(struct cpufreq_policy *p, unsigned int freq)
+{
+	if (p->cur == p->max)
+		return;
+
+	__cpufreq_driver_target(p, freq, CPUFREQ_RELATION_L);
+}
+
+static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
+{
+	/* Extrapolated load of this CPU */
+	unsigned int load_at_max_freq = 0;
+	unsigned int max_load_freq;
+	/* Current load across this CPU */
+	unsigned int cur_load = 0;
+	unsigned int max_load_other_cpu = 0;
+	unsigned int sampling_rate;
+	struct cpufreq_policy *policy;
+	unsigned int j;
+
+	sampling_rate = dbs_tuners_ins.sampling_rate * this_dbs_info->rate_mult;
+	this_dbs_info->freq_lo = 0;
+
+	policy = this_dbs_info->cur_policy;
+	if (policy == NULL)
+		return;
+
+	/*
+	 * Every sampling_rate, we check, if current idle time is less
+	 * than 20% (default), then we try to increase frequency
+	 * Every sampling_rate, we look for a the lowest
+	 * frequency which can sustain the load while keeping idle time over
+	 * 30%. If such a frequency exist, we try to decrease to this frequency.
+	 *
+	 * Any frequency increase takes it to the maximum frequency.
+	 * Frequency reduction happens at minimum steps of
+	 * 5% (default) of current frequency
+	 */
+
+	/* Get Absolute Load - in terms of freq */
+	max_load_freq = 0;
+
+	for_each_cpu(j, policy->cpus) {
+		struct cpu_dbs_info_s *j_dbs_info;
+		u64 cur_wall_time, cur_idle_time;
+		unsigned int idle_time, wall_time;
+		unsigned int load_freq;
+		int freq_avg;
+
+		j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+
+		/*
+		 * For the purpose of ondemand_x, waiting for disk IO is an
+		 * indication that you're performance critical, and not that
+		 * the system is actually idle. So subtract the iowait time
+		 * from the cpu idle time.
+		 */
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time, 0);
+
+		wall_time = (unsigned int)
+			(cur_wall_time - j_dbs_info->prev_cpu_wall);
+		j_dbs_info->prev_cpu_wall = cur_wall_time;
+
+		idle_time = (unsigned int)
+			(cur_idle_time - j_dbs_info->prev_cpu_idle);
+		j_dbs_info->prev_cpu_idle = cur_idle_time;
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		/*
+		 * If the CPU had gone completely idle, and a task just woke up
+		 * on this CPU now, it would be unfair to calculate 'load' the
+		 * usual way for this elapsed time-window, because it will show
+		 * near-zero load, irrespective of how CPU intensive the new
+		 * task is. This is undesirable for latency-sensitive bursty
+		 * workloads.
+		 *
+		 * To avoid this, we reuse the 'load' from the previous
+		 * time-window and give this task a chance to start with a
+		 * reasonably high CPU frequency. (However, we shouldn't over-do
+		 * this copy, lest we get stuck at a high load (high frequency)
+		 * for too long, even when the current system load has actually
+		 * dropped down. So we perform the copy only once, upon the
+		 * first wake-up from idle.)
+		 *
+		 *
+		 * Detecting this situation is easy: the governor's deferrable
+		 * timer would not have fired during CPU-idle periods. Hence
+		 * an unusually large 'wall_time' (as compared to the sampling
+		 * rate) indicates this scenario.
+		 *
+		 * prev_load can be zero in two cases and we must recalculate it
+		 * for both cases:
+		 * - during long idle intervals
+		 * - explicitly set to zero
+		 */
+		if (unlikely(wall_time > (2 * sampling_rate) &&
+					j_dbs_info->prev_load)) {
+			cur_load = j_dbs_info->prev_load;
+			j_dbs_info->max_load = cur_load;
+			/*
+			 * Perform a destructive copy, to ensure that we copy
+			 * the previous load only once, upon the first wake-up
+			 * from idle.
+			 */
+			j_dbs_info->prev_load = 0;
+		} else {
+			cur_load = 100 * (wall_time - idle_time) / wall_time;
+			j_dbs_info->max_load = max(cur_load, j_dbs_info->prev_load);
+			j_dbs_info->prev_load = cur_load;
+		}
+
+		freq_avg = __cpufreq_driver_getavg(policy, j);
+		if (policy == NULL)
+			return;
+		if (freq_avg <= 0)
+			freq_avg = policy->cur;
+
+		load_freq = cur_load * freq_avg;
+		if (load_freq > max_load_freq)
+			max_load_freq = load_freq;
+	}
+
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *j_dbs_info;
+		j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+
+		if (j == policy->cpu)
+			continue;
+
+		if (max_load_other_cpu < j_dbs_info->max_load)
+			max_load_other_cpu = j_dbs_info->max_load;
+	}
+
+	/* calculate the scaled load across CPU */
+	load_at_max_freq = (cur_load * policy->cur)/policy->max;
+
+	cpufreq_notify_utilization(policy, load_at_max_freq);
+
+	/* Check for frequency increase */
+	if (max_load_freq > (dbs_tuners_ins.up_threshold * policy->cur)) {
+		int freq_target, freq_div;
+		freq_target = 0; freq_div = 0;
+
+		if (load_at_max_freq > dbs_tuners_ins.high_grid_load) {
+			freq_div = (policy->max * dbs_tuners_ins.high_grid_step) / 100;
+			freq_target = min(policy->max, policy->cur + freq_div);
+		} else if (load_at_max_freq > dbs_tuners_ins.middle_grid_load) {
+			freq_div = (policy->max * dbs_tuners_ins.middle_grid_step) / 100;
+			freq_target = min(policy->max, policy->cur + freq_div);
+		} else {
+			if (policy->max < dbs_tuners_ins.optimal_max_freq)
+				freq_target = policy->max;
+			else
+				freq_target = dbs_tuners_ins.optimal_max_freq;
+		}
+
+		/* If switching to max speed, apply sampling_down_factor */
+		if (policy->cur < policy->max)
+			this_dbs_info->rate_mult =
+				dbs_tuners_ins.sampling_down_factor;
+
+		dbs_freq_increase(policy, freq_target);
+		return;
+	}
+
+	if (num_online_cpus() > 1) {
+		if (max_load_other_cpu >
+				dbs_tuners_ins.up_threshold_any_cpu_load) {
+			if (policy->cur < dbs_tuners_ins.sync_freq)
+				dbs_freq_increase(policy,
+						dbs_tuners_ins.sync_freq);
+			return;
+		}
+
+		if (max_load_freq > (dbs_tuners_ins.up_threshold_multi_core *
+								policy->cur)) {
+			if (policy->cur < dbs_tuners_ins.optimal_freq)
+				dbs_freq_increase(policy,
+						dbs_tuners_ins.optimal_freq);
+			return;
+		}
+	}
+
+	/* Check for frequency decrease */
+	/* if we cannot reduce the frequency anymore, break out early */
+	if (policy->cur == policy->min)
+		return;
+
+	/*
+	 * The optimal frequency is the frequency that is the lowest that
+	 * can support the current CPU usage without triggering the up
+	 * policy.
+	 */
+	if (max_load_freq <
+	    (dbs_tuners_ins.up_threshold * policy->cur)) {
+		unsigned int freq_next;
+		freq_next = max_load_freq / dbs_tuners_ins.up_threshold;
+
+		/* No longer fully busy, reset rate_mult */
+		this_dbs_info->rate_mult = 1;
+
+		if (freq_next < policy->min)
+			freq_next = policy->min;
+
+		if (num_online_cpus() > 1) {
+			if (max_load_other_cpu >
+					dbs_tuners_ins.up_threshold_multi_core
+					&& freq_next <
+					dbs_tuners_ins.sync_freq)
+				freq_next = dbs_tuners_ins.sync_freq;
+
+			if (max_load_freq >
+					(dbs_tuners_ins.up_threshold_multi_core *
+					policy->cur) &&
+					freq_next < dbs_tuners_ins.optimal_freq)
+				freq_next = dbs_tuners_ins.optimal_freq;
+
+		}
+		__cpufreq_driver_target(policy, freq_next,
+				CPUFREQ_RELATION_L);
+	}
+}
+
+static void do_dbs_timer(struct work_struct *work)
+{
+	struct cpu_dbs_info_s *dbs_info =
+		container_of(work, struct cpu_dbs_info_s, work.work);
+	int sample_type = dbs_info->sample_type;
+	int delay;
+
+	if (unlikely(!cpu_online(dbs_info->cpu) || !dbs_info->cur_policy))
+		return;
+
+	mutex_lock(&dbs_info->timer_mutex);
+
+	/* Common NORMAL_SAMPLE setup */
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	if (sample_type == DBS_NORMAL_SAMPLE) {
+		dbs_check_cpu(dbs_info);
+		if (dbs_info->freq_lo) {
+			/* Setup timer for SUB_SAMPLE */
+			dbs_info->sample_type = DBS_SUB_SAMPLE;
+			delay = dbs_info->freq_hi_jiffies;
+		} else {
+			/* We want all CPUs to do sampling nearly on
+			 * same jiffy
+			 */
+			delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate
+				* dbs_info->rate_mult);
+
+			if (num_online_cpus() > 1)
+				delay -= jiffies % delay;
+		}
+	} else {
+		__cpufreq_driver_target(dbs_info->cur_policy,
+			dbs_info->freq_lo, CPUFREQ_RELATION_H);
+		delay = dbs_info->freq_lo_jiffies;
+	}
+	queue_delayed_work_on(dbs_info->cpu, dbs_wq, &dbs_info->work, delay);
+	mutex_unlock(&dbs_info->timer_mutex);
+}
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
+{
+	/* We want all CPUs to do sampling nearly on same jiffy */
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+	if (num_online_cpus() > 1)
+		delay -= jiffies % delay;
+
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	INIT_DEFERRABLE_WORK(&dbs_info->work, do_dbs_timer);
+	queue_delayed_work_on(dbs_info->cpu, dbs_wq, &dbs_info->work, delay);
+}
+
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
+{
+	cancel_delayed_work_sync(&dbs_info->work);
+}
+
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int rc;
+
+	this_dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy))
+			return -EINVAL;
+
+		mutex_lock(&dbs_mutex);
+
+		dbs_enable++;
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			unsigned int prev_load;
+
+			j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+					&j_dbs_info->prev_cpu_wall, 0);
+
+			prev_load = (unsigned int)
+				(j_dbs_info->prev_cpu_wall - j_dbs_info->prev_cpu_idle);
+			j_dbs_info->prev_load = 100 * prev_load /
+				(unsigned int) j_dbs_info->prev_cpu_wall;
+		}
+		cpu = policy->cpu;
+		this_dbs_info->cpu = cpu;
+		this_dbs_info->rate_mult = 1;
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 1) {
+			unsigned int latency;
+
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&dbs_attr_group);
+			if (rc) {
+				dbs_enable--;
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+
+			/* policy latency is in nS. Convert it to uS first */
+			latency = policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+			/* Bring kernel and HW constraints together */
+			min_sampling_rate = max(min_sampling_rate,
+					MIN_LATENCY_MULTIPLIER * latency);
+			if (latency != 1)
+				dbs_tuners_ins.sampling_rate =
+					max(dbs_tuners_ins.sampling_rate,
+						latency * LATENCY_MULTIPLIER);
+
+			if (dbs_tuners_ins.optimal_freq == 0)
+				dbs_tuners_ins.optimal_freq = policy->cpuinfo.min_freq;
+
+			if (dbs_tuners_ins.sync_freq == 0)
+				dbs_tuners_ins.sync_freq = policy->cpuinfo.min_freq;
+		}
+		mutex_unlock(&dbs_mutex);
+
+		dbs_timer_init(this_dbs_info);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		dbs_timer_exit(this_dbs_info);
+
+		mutex_lock(&dbs_mutex);
+
+		dbs_enable--;
+
+		/* If device is being removed, policy is no longer
+		 * valid. */
+		this_dbs_info->cur_policy = NULL;
+		if (!dbs_enable)
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &dbs_attr_group);
+
+		mutex_unlock(&dbs_mutex);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		/* If device is being removed, skip set limits */
+		if (!this_dbs_info->cur_policy || !policy)
+			break;
+		mutex_lock(&this_dbs_info->timer_mutex);
+		__cpufreq_driver_target(this_dbs_info->cur_policy,
+				policy->cur, CPUFREQ_RELATION_L);
+		dbs_check_cpu(this_dbs_info);
+		mutex_unlock(&this_dbs_info->timer_mutex);
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND_X
+static
+#endif
+struct cpufreq_governor cpufreq_gov_ondemand_x = {
+	.name					= "ondemand_x",
+	.governor				= cpufreq_governor_dbs,
+	.max_transition_latency	= TRANSITION_LATENCY_LIMIT,
+	.owner					= THIS_MODULE,
+};
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	u64 idle_time;
+	unsigned int i;
+	int cpu = get_cpu();
+
+	idle_time = get_cpu_idle_time_us(cpu, NULL);
+	put_cpu();
+	if (idle_time != -1ULL) {
+		/* Idle micro accounting is supported. Use finer thresholds */
+		dbs_tuners_ins.up_threshold = dbs_tuners_ins.micro_freq_up_threshold;
+		/*
+		 * In nohz/micro accounting case we set the minimum frequency
+		 * not depending on HZ, but fixed (very low). The deferred
+		 * timer might skip some samples if idle/sleeping as needed.
+		*/
+		min_sampling_rate = MICRO_FREQUENCY_MIN_SAMPLE_RATE;
+	} else {
+		/* For correct statistics, we need 10 ticks for each measure */
+		min_sampling_rate =
+			MIN_SAMPLING_RATE_RATIO * jiffies_to_usecs(10);
+	}
+
+	dbs_wq = alloc_workqueue("ondemand_x_dbs_wq", WQ_HIGHPRI, 0);
+	if (!dbs_wq) {
+		printk(KERN_ERR "Failed to create ondemand_x_dbs_wq workqueue\n");
+		return -EFAULT;
+	}
+	for_each_possible_cpu(i) {
+		struct cpu_dbs_info_s *this_dbs_info =
+			&per_cpu(od_cpu_dbs_info, i);
+		mutex_init(&this_dbs_info->timer_mutex);
+		init_waitqueue_head(&this_dbs_info->sync_wq);
+	}
+
+	return cpufreq_register_governor(&cpufreq_gov_ondemand_x);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	unsigned int i;
+
+	cpufreq_unregister_governor(&cpufreq_gov_ondemand_x);
+	for_each_possible_cpu(i) {
+		struct cpu_dbs_info_s *this_dbs_info =
+			&per_cpu(od_cpu_dbs_info, i);
+		mutex_destroy(&this_dbs_info->timer_mutex);
+	}
+	destroy_workqueue(dbs_wq);
+}
+
+MODULE_AUTHOR("Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>");
+MODULE_AUTHOR("Alexey Starikovskiy <alexey.y.starikovskiy@intel.com>");
+MODULE_DESCRIPTION("'cpufreq_ondemand_x' - A dynamic cpufreq governor for "
+	"Low Latency Frequency Transition capable processors");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND_X
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_ondemandplus.c b/drivers/cpufreq/cpufreq_ondemandplus.c
new file mode 100644
index 0000000..352731b
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_ondemandplus.c
@@ -0,0 +1,954 @@
+/*
+ * drivers/cpufreq/cpufreq_ondemandplus.c
+ * Copyright (C) 2013 Boy Petersen
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ *
+ * based upon:
+ *
+ *
+ *	drivers/cpufreq/cpufreq_interactive.c
+ *
+ *	Copyright (C) 2010 Google, Inc.
+ *
+ *	This software is licensed under the terms of the GNU General Public
+ *	License version 2, as published by the Free Software Foundation, and
+ *	may be copied, distributed, and modified under those terms.
+ *
+ *	This program is distributed in the hope that it will be useful,
+ *	but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *	GNU General Public License for more details.
+ *
+ *	Author: Mike Chan (mike@android.com)
+ *
+ *
+ * and:
+ *
+ *	drivers/cpufreq/cpufreq_ondemand.c
+ *
+ *	Copyright (C)  2001 Russell King
+ *		  (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *			    Jun Nakajima <jun.nakajima@intel.com>
+ *
+ *	This program is free software; you can redistribute it and/or modify
+ *	it under the terms of the GNU General Public License version 2 as
+ *	published by the Free Software Foundation.
+ */
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/sched.h>
+#include <linux/sched/rt.h>
+#include <linux/tick.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/kthread.h>
+#include <linux/mutex.h>
+#include <linux/slab.h>
+#include <linux/kernel_stat.h>
+#include <asm/cputime.h>
+#include <linux/module.h>
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/cpufreq_ondemandplus.h>
+
+static atomic_t active_count = ATOMIC_INIT(0);
+
+struct cpufreq_ondemandplus_cpuinfo {
+	struct timer_list cpu_timer;
+	struct mutex timer_mutex;
+	int timer_idlecancel;
+	u64 time_in_idle;
+	u64 idle_exit_time;
+	u64 timer_run_time;
+	int idling;
+	u64 target_set_time;
+	u64 target_set_time_in_idle;
+	struct cpufreq_policy *policy;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int target_freq;
+	int governor_enabled;
+	unsigned int cpu;
+};
+
+static DEFINE_PER_CPU(struct cpufreq_ondemandplus_cpuinfo, cpuinfo);
+
+/* realtime thread handles frequency scaling */
+static struct task_struct *speedchange_task;
+static cpumask_t speedchange_cpumask;
+static spinlock_t speedchange_cpumask_lock;
+
+/*
+ * Tunables start
+ */
+
+#define DEFAULT_TIMER_RATE (20 * USEC_PER_MSEC)
+static unsigned long timer_rate;
+
+#define DEFAULT_UP_THRESHOLD 90
+static unsigned long up_threshold;
+
+#define DEFAULT_DOWN_DIFFERENTIAL 62
+static unsigned long down_differential;
+
+#define DEFAULT_INTER_HIFREQ 1728000
+static u64 inter_hifreq;
+
+#define DEFAULT_INTER_LOFREQ 1036800
+static u64 inter_lofreq;
+
+#define DEFAULT_INTER_STAYCYCLES 2
+static unsigned long inter_staycycles;
+
+#define DEFAULT_STAYCYCLES_RESETFREQ 652800
+static u64 staycycles_resetfreq;
+
+#define DEFAULT_IO_IS_BUSY 0
+static unsigned int io_is_busy;
+
+static u64 screen_on_min_freq = 300000;
+
+/*
+ * Tunables end
+ */
+
+static void cpufreq_ondemandplus_timer(unsigned long data)
+{
+	unsigned int delta_idle;
+	unsigned int delta_time;
+	int cpu_load;
+	unsigned int load_freq;
+	int load_since_change;
+	u64 time_in_idle;
+	u64 idle_exit_time;
+	struct cpufreq_ondemandplus_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, data);
+	u64 now_idle;
+	unsigned int new_freq = 0;
+	unsigned int index;
+	static unsigned int stay_counter;
+	unsigned long flags;
+	static unsigned int i = 0;
+	static unsigned int last_cpu_freqs[5] = {0};
+	static unsigned int avg_cpu_freq = 0;
+	static unsigned int lo_avg_cpu_freq;
+	static unsigned int hi_avg_cpu_freq;
+	static unsigned int low_timer_rate = 0;
+
+	smp_rmb();
+
+	if (!pcpu->governor_enabled)
+		goto exit;
+
+	/*
+	 * Once pcpu->timer_run_time is updated to >= pcpu->idle_exit_time,
+	 * this lets idle exit know the current idle time sample has
+	 * been processed, and idle exit can generate a new sample and
+	 * re-arm the timer.  This prevents a concurrent idle
+	 * exit on that CPU from writing a new set of info at the same time
+	 * the timer function runs (the timer function can't use that info
+	 * until more time passes).
+	 */
+
+	time_in_idle = pcpu->time_in_idle;
+	idle_exit_time = pcpu->idle_exit_time;
+	now_idle = get_cpu_idle_time(data, &pcpu->timer_run_time, io_is_busy);
+	smp_wmb();
+
+	/* If we raced with cancelling a timer, skip. */
+	if (!idle_exit_time)
+		goto exit;
+
+	delta_idle = (unsigned int) (now_idle - time_in_idle);
+	delta_time = (unsigned int) (pcpu->timer_run_time - idle_exit_time);
+
+	/*
+	 * If timer ran less than 1ms after short-term sample started, retry.
+	 */
+	if (delta_time < 1000)
+		goto rearm;
+
+	if (delta_idle > delta_time)
+		cpu_load = 0;
+	else
+		cpu_load = 100 * (delta_time - delta_idle) / delta_time;
+
+	delta_idle = (unsigned int) (now_idle - pcpu->target_set_time_in_idle);
+	delta_time = (unsigned int) (pcpu->timer_run_time - pcpu->target_set_time);
+
+	if ((delta_time == 0) || (delta_idle > delta_time))
+		load_since_change = 0;
+	else
+		load_since_change =
+			100 * (delta_time - delta_idle) / delta_time;
+
+	/*
+	 * If short-term load (since last idle timer started or
+	 * timer function re-armed itself) is higher than long-term
+	 * load (since last frequency change), use short-term load
+	 * to be able to scale up quickly.
+	 * When long-term load is higher than short-term load,
+	 * use the average of short-term load and long-term load
+	 * (instead of just long-term load) to be able to scale
+	 * down faster, with the long-term load being able to delay
+	 * down scaling a little to maintain responsiveness.
+	 */
+	if (load_since_change > cpu_load) {
+		cpu_load = (cpu_load + load_since_change) / 2;
+	}
+
+	load_freq = cpu_load * pcpu->target_freq;
+
+	new_freq = pcpu->target_freq;
+
+	/* Check for frequency increase */
+	if (load_freq > up_threshold * pcpu->target_freq) {
+		/* if we are already at full speed then break out early */
+		if (pcpu->target_freq < pcpu->policy->max) {
+
+			if (stay_counter == 0 && inter_staycycles != 0) {
+				new_freq = inter_lofreq;
+				stay_counter++;
+			} else if (stay_counter == 1 && inter_staycycles != 1) {
+				new_freq = inter_hifreq;
+				stay_counter++;
+			} else if (stay_counter < inter_staycycles) {
+				stay_counter++;
+				goto rearm;
+			} else {
+				new_freq = pcpu->policy->max;
+			}
+		}
+	/* Check for frequency decrease */
+
+	/*
+	 * The optimal frequency is the frequency that is the lowest that
+	 * can support the current CPU usage without triggering the up
+	 * policy. To be safe, we focus 10 points under the threshold.
+	 */
+	} else if (load_freq < (up_threshold - down_differential) *
+			pcpu->target_freq) {
+
+		if (pcpu->target_freq != screen_on_min_freq) {
+			new_freq = load_freq /
+					(up_threshold - down_differential);
+
+			if (new_freq <= staycycles_resetfreq) {
+				stay_counter = 0;
+			}
+
+			if (new_freq < screen_on_min_freq) {
+				new_freq = screen_on_min_freq;
+			}
+		}
+	} else if (pcpu->target_freq == pcpu->policy->max &&
+			load_freq < (up_threshold - down_differential / 2) *
+			pcpu->target_freq) {
+		new_freq = load_freq / (up_threshold - down_differential * 2 / 3);
+	}
+
+	if (cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table,
+					   new_freq, CPUFREQ_RELATION_H,
+					   &index)) {
+		pr_warn_once("timer %d: cpufreq_frequency_table_target error\n",
+			     (int) data);
+		goto rearm;
+	}
+
+	new_freq = pcpu->freq_table[index].frequency;
+
+	if (pcpu->target_freq == new_freq) {
+		trace_cpufreq_ondemandplus_already(data, cpu_load,
+						  pcpu->target_freq, new_freq);
+		goto rearm_if_notmax;
+	}
+
+	trace_cpufreq_ondemandplus_target(data, cpu_load, pcpu->target_freq,
+					 new_freq);
+	pcpu->target_set_time_in_idle = now_idle;
+	pcpu->target_set_time = pcpu->timer_run_time;
+
+	pcpu->target_freq = new_freq;
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+	cpumask_set_cpu(data, &speedchange_cpumask);
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+	wake_up_process(speedchange_task);
+
+rearm_if_notmax:
+	/*
+	 * Already set max speed and don't see a need to change that,
+	 * wait until next idle to re-evaluate, don't need timer.
+	 */
+	if (pcpu->target_freq == pcpu->policy->max)
+		goto exit;
+
+rearm:
+	if (!timer_pending(&pcpu->cpu_timer)) {
+		/*
+		 * If already at min: if that CPU is idle, don't set timer.
+		 * Else cancel the timer if that CPU goes idle.  We don't
+		 * need to re-evaluate speed until the next idle exit.
+		 */
+		if (pcpu->target_freq == screen_on_min_freq) {
+			smp_rmb();
+
+			if (pcpu->idling)
+				goto exit;
+
+			pcpu->timer_idlecancel = 1;
+		}
+
+		/*
+		 * Calculate the average CPU frequency of the last 5 timer
+		 * cycles. Then check if the new to-be-requested frequency
+		 * is within a divergent range of 15% for lower frequencies,
+		 * or is equal to the average for higher frequencies.
+		 * If yes, slow down the timer.
+		 */
+		if (i >= 4 && !pcpu->idling) {
+			unsigned int k;
+			avg_cpu_freq = 0;
+			for (k = 0; k <= 4; k++) {
+				avg_cpu_freq += last_cpu_freqs[k];
+			}
+			avg_cpu_freq /= 5;
+			hi_avg_cpu_freq = (avg_cpu_freq * 115) / 100;
+			lo_avg_cpu_freq = (avg_cpu_freq * 100) / 115;
+
+			if ((new_freq < inter_lofreq && hi_avg_cpu_freq > new_freq &&
+						lo_avg_cpu_freq < new_freq) || (new_freq >=
+						inter_lofreq && avg_cpu_freq == new_freq)) {
+				low_timer_rate = timer_rate * 2;
+			} else {
+				low_timer_rate = 0;
+			}
+			i = 0;
+		} else if ((new_freq < inter_lofreq && (hi_avg_cpu_freq < new_freq ||
+					lo_avg_cpu_freq > new_freq)) || (new_freq >=
+					inter_lofreq && avg_cpu_freq != new_freq)) {
+			low_timer_rate = 0;
+		}
+
+		/*
+		 * Re-arm timer
+		 */
+		pcpu->time_in_idle = get_cpu_idle_time(
+			data, &pcpu->idle_exit_time, io_is_busy);
+		if (!low_timer_rate) {
+			mod_timer(&pcpu->cpu_timer,
+				jiffies + usecs_to_jiffies(timer_rate));
+		} else {
+			mod_timer(&pcpu->cpu_timer,
+				jiffies + usecs_to_jiffies(low_timer_rate));
+		}
+	}
+
+exit:
+	/*
+	 * Write CPU frequency of new timer cycle into the correct
+	 * last_cpu_freqs array-field
+	 */
+	if (pcpu->idling) {
+		memset(last_cpu_freqs, 0, sizeof(last_cpu_freqs));
+		i = 0;
+	}
+	return;
+}
+
+static void cpufreq_ondemandplus_idle_start(void)
+{
+	struct cpufreq_ondemandplus_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+	int pending;
+
+	if (!pcpu->governor_enabled)
+		return;
+
+	pcpu->idling = 1;
+	smp_wmb();
+	pending = timer_pending(&pcpu->cpu_timer);
+
+	if (pcpu->target_freq != pcpu->policy->min) {
+#ifdef CONFIG_SMP
+		/*
+		 * Entering idle while not at lowest speed.  On some
+		 * platforms this can hold the other CPU(s) at that speed
+		 * even though the CPU is idle. Set a timer to re-evaluate
+		 * speed so this idle CPU doesn't hold the other CPUs above
+		 * min indefinitely.  This should probably be a quirk of
+		 * the CPUFreq driver.
+		 */
+		if (!pending) {
+			pcpu->time_in_idle = get_cpu_idle_time(
+				smp_processor_id(), &pcpu->idle_exit_time, io_is_busy);
+			pcpu->timer_idlecancel = 0;
+			mod_timer(&pcpu->cpu_timer,
+				  jiffies + usecs_to_jiffies(timer_rate));
+		}
+#endif
+	} else {
+		/*
+		 * If at min speed and entering idle after load has
+		 * already been evaluated, and a timer has been set just in
+		 * case the CPU suddenly goes busy, cancel that timer.  The
+		 * CPU didn't go busy; we'll recheck things upon idle exit.
+		 */
+		if (pending && pcpu->timer_idlecancel) {
+			del_timer(&pcpu->cpu_timer);
+			/*
+			 * Ensure last timer run time is after current idle
+			 * sample start time, so next idle exit will always
+			 * start a new idle sampling period.
+			 */
+			pcpu->idle_exit_time = 0;
+			pcpu->timer_idlecancel = 0;
+		}
+	}
+
+}
+
+static void cpufreq_ondemandplus_idle_end(void)
+{
+	struct cpufreq_ondemandplus_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+
+	pcpu->idling = 0;
+	smp_wmb();
+
+	/*
+	 * Arm the timer for 1-2 ticks later if not already, and if the timer
+	 * function has already processed the previous load sampling
+	 * interval.  (If the timer is not pending but has not processed
+	 * the previous interval, it is probably racing with us on another
+	 * CPU.  Let it compute load based on the previous sample and then
+	 * re-arm the timer for another interval when it's done, rather
+	 * than updating the interval start time to be "now", which doesn't
+	 * give the timer function enough time to make a decision on this
+	 * run.)
+	 */
+	if (timer_pending(&pcpu->cpu_timer) == 0 &&
+	    pcpu->timer_run_time >= pcpu->idle_exit_time &&
+	    pcpu->governor_enabled) {
+		pcpu->time_in_idle =
+			get_cpu_idle_time(smp_processor_id(),
+					     &pcpu->idle_exit_time, io_is_busy);
+		pcpu->timer_idlecancel = 0;
+		mod_timer(&pcpu->cpu_timer,
+			  jiffies + usecs_to_jiffies(timer_rate));
+	}
+
+}
+
+static int cpufreq_ondemandplus_speedchange_task(void *data)
+{
+	unsigned int cpu;
+	cpumask_t tmp_mask;
+	unsigned long flags;
+	struct cpufreq_ondemandplus_cpuinfo *pcpu;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+		if (cpumask_empty(&speedchange_cpumask)) {
+			spin_unlock_irqrestore(&speedchange_cpumask_lock,
+						flags);
+			schedule();
+
+			if (kthread_should_stop())
+				break;
+
+			spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+		}
+
+		set_current_state(TASK_RUNNING);
+		tmp_mask = speedchange_cpumask;
+		cpumask_clear(&speedchange_cpumask);
+		spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+		for_each_cpu(cpu, &tmp_mask) {
+			unsigned int j;
+			unsigned int max_freq = 0;
+
+			pcpu = &per_cpu(cpuinfo, cpu);
+			smp_rmb();
+
+			if (!pcpu->governor_enabled)
+				continue;
+
+			for_each_cpu(j, pcpu->policy->cpus) {
+				struct cpufreq_ondemandplus_cpuinfo *pjcpu =
+					&per_cpu(cpuinfo, j);
+
+				if (pjcpu->target_freq > max_freq)
+					max_freq = pjcpu->target_freq;
+			}
+
+			if (max_freq != pcpu->policy->cur)
+				__cpufreq_driver_target(pcpu->policy,
+							max_freq,
+							CPUFREQ_RELATION_H);
+			trace_cpufreq_ondemandplus_setspeed(cpu,
+						pcpu->target_freq,
+						pcpu->policy->cur);
+		}
+	}
+
+	return 0;
+}
+
+static ssize_t show_timer_rate(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", timer_rate);
+}
+
+static ssize_t store_timer_rate(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	timer_rate = val;
+	return count;
+}
+
+static struct global_attr timer_rate_attr = __ATTR(timer_rate, 0644,
+		show_timer_rate, store_timer_rate);
+
+static ssize_t show_up_threshold(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", up_threshold);
+}
+
+static ssize_t store_up_threshold(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	if (val > 100)
+		val = 100;
+
+	if (val < 1)
+		val = 1;
+
+	up_threshold = val;
+	return count;
+}
+
+static struct global_attr up_threshold_attr = __ATTR(up_threshold, 0644,
+		show_up_threshold, store_up_threshold);
+
+static ssize_t show_down_differential(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", down_differential);
+}
+
+static ssize_t store_down_differential(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	if (val > 100)
+		val = 100;
+
+	down_differential = val;
+	return count;
+}
+
+static struct global_attr down_differential_attr = __ATTR(down_differential, 0644,
+		show_down_differential, store_down_differential);
+
+static ssize_t show_inter_hifreq(struct kobject *kobj,
+				 struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%llu\n", inter_hifreq);
+}
+
+static ssize_t store_inter_hifreq(struct kobject *kobj,
+				  struct attribute *attr, const char *buf,
+				  size_t count)
+{
+	int ret;
+	u64 val;
+	struct cpufreq_ondemandplus_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+	unsigned int index;
+
+	ret = strict_strtoull(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	index = 0;
+	cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table, val,
+		CPUFREQ_RELATION_L, &index);
+	val = pcpu->freq_table[index].frequency;
+
+	if (val > pcpu->policy->max)
+		val = pcpu->policy->max;
+
+	if (val < screen_on_min_freq)
+		val = screen_on_min_freq;
+
+	inter_hifreq = val;
+	return count;
+}
+
+static struct global_attr inter_hifreq_attr = __ATTR(inter_hifreq, 0644,
+		show_inter_hifreq, store_inter_hifreq);
+
+static ssize_t show_inter_lofreq(struct kobject *kobj,
+				 struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%llu\n", inter_lofreq);
+}
+
+static ssize_t store_inter_lofreq(struct kobject *kobj,
+				  struct attribute *attr, const char *buf,
+				  size_t count)
+{
+	int ret;
+	u64 val;
+	struct cpufreq_ondemandplus_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+	unsigned int index;
+
+	ret = strict_strtoull(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	index = 0;
+	cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table, val,
+			CPUFREQ_RELATION_H, &index);
+	val = pcpu->freq_table[index].frequency;
+
+	if (val > pcpu->policy->max)
+		val = pcpu->policy->max;
+
+	if (val < screen_on_min_freq)
+		val = screen_on_min_freq;
+
+	inter_lofreq = val;
+	return count;
+}
+
+static struct global_attr inter_lofreq_attr = __ATTR(inter_lofreq, 0644,
+		show_inter_lofreq, store_inter_lofreq);
+
+static ssize_t show_inter_staycycles(struct kobject *kobj,
+					struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", inter_staycycles);
+}
+
+static ssize_t store_inter_staycycles(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	if (val > 10)
+		val = 10;
+
+	inter_staycycles = val;
+	return count;
+}
+
+static struct global_attr inter_staycycles_attr = __ATTR(inter_staycycles, 0644,
+		show_inter_staycycles, store_inter_staycycles);
+
+static ssize_t show_staycycles_resetfreq(struct kobject *kobj,
+				 struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%llu\n", staycycles_resetfreq);
+}
+
+static ssize_t store_staycycles_resetfreq(struct kobject *kobj,
+				  struct attribute *attr, const char *buf,
+				  size_t count)
+{
+	int ret;
+	u64 val;
+	struct cpufreq_ondemandplus_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+
+	ret = strict_strtoull(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	if (val > pcpu->policy->max)
+		val = pcpu->policy->max;
+
+	if (val < screen_on_min_freq)
+		val = screen_on_min_freq;
+
+	staycycles_resetfreq = val;
+	return count;
+}
+
+static struct global_attr staycycles_resetfreq_attr = __ATTR(staycycles_resetfreq, 0644,
+		show_staycycles_resetfreq, store_staycycles_resetfreq);
+
+static ssize_t show_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	io_is_busy = val;
+	return count;
+}
+
+static struct global_attr io_is_busy_attr = __ATTR(io_is_busy, 0644,
+		show_io_is_busy, store_io_is_busy);
+
+static struct attribute *ondemandplus_attributes[] = {
+	&timer_rate_attr.attr,
+	&up_threshold_attr.attr,
+	&down_differential_attr.attr,
+	&inter_hifreq_attr.attr,
+	&inter_lofreq_attr.attr,
+	&inter_staycycles_attr.attr,
+	&staycycles_resetfreq_attr.attr,
+	&io_is_busy_attr.attr,
+	NULL,
+};
+
+static struct attribute_group ondemandplus_attr_group = {
+	.attrs = ondemandplus_attributes,
+	.name = "ondemandplus",
+};
+
+static int cpufreq_ondemandplus_idle_notifier(struct notifier_block *nb,
+					     unsigned long val,
+					     void *data)
+{
+	switch (val) {
+	case IDLE_START:
+		cpufreq_ondemandplus_idle_start();
+		break;
+	case IDLE_END:
+		cpufreq_ondemandplus_idle_end();
+		break;
+	}
+
+	return 0;
+}
+
+static struct notifier_block cpufreq_ondemandplus_idle_nb = {
+	.notifier_call = cpufreq_ondemandplus_idle_notifier,
+};
+
+static int cpufreq_governor_ondemandplus(struct cpufreq_policy *policy,
+		unsigned int event)
+{
+	int rc;
+	unsigned int cpu;
+	unsigned int j;
+	struct cpufreq_ondemandplus_cpuinfo *pcpu;
+	struct cpufreq_frequency_table *freq_table;
+
+	pcpu = &per_cpu(cpuinfo, policy->cpu);
+	cpu = pcpu->cpu;
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if (!cpu_online(cpu) || (!policy->cur))
+			return -EINVAL;
+
+		freq_table =
+			cpufreq_frequency_get_table(cpu);
+
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			pcpu->policy = policy;
+			pcpu->target_freq = policy->cur;
+			pcpu->freq_table = freq_table;
+			pcpu->target_set_time_in_idle =
+				get_cpu_idle_time(j,
+					     &pcpu->target_set_time, io_is_busy);
+			/* update min freq static value */
+			if (policy->cpu == 0)
+				screen_on_min_freq = policy->min;
+			pcpu->governor_enabled = 1;
+			smp_wmb();
+		}
+
+		/*
+		 * Do not register the idle hook and create sysfs
+		 * entries if we have already done so.
+		 */
+		if (atomic_inc_return(&active_count) > 1)
+			return 0;
+
+		rc = sysfs_create_group(cpufreq_global_kobject,
+				&ondemandplus_attr_group);
+		if (rc)
+			return rc;
+
+		mutex_init(&pcpu->timer_mutex);
+
+		idle_notifier_register(&cpufreq_ondemandplus_idle_nb);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			pcpu->governor_enabled = 0;
+			smp_wmb();
+			del_timer_sync(&pcpu->cpu_timer);
+
+			/*
+			 * Reset idle exit time since we may cancel the timer
+			 * before it can run after the last idle exit time,
+			 * to avoid tripping the check in idle exit for a timer
+			 * that is trying to run.
+			 */
+			pcpu->idle_exit_time = 0;
+		}
+
+		if (atomic_dec_return(&active_count) > 0)
+			return 0;
+
+		mutex_destroy(&pcpu->timer_mutex);
+
+		idle_notifier_unregister(&cpufreq_ondemandplus_idle_nb);
+		sysfs_remove_group(cpufreq_global_kobject,
+				&ondemandplus_attr_group);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		/* If device is being removed, skip set limits */
+		if (!pcpu->policy->cur)
+			break;
+		mutex_lock(&pcpu->timer_mutex);
+		if (policy->max < policy->cur)
+			__cpufreq_driver_target(policy,
+					policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > policy->cur)
+			__cpufreq_driver_target(policy,
+					policy->min, CPUFREQ_RELATION_L);
+		/* update min freq static value */
+		if (cpu == 0)
+			screen_on_min_freq = policy->min;
+		mutex_unlock(&pcpu->timer_mutex);
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS
+static
+#endif
+struct cpufreq_governor cpufreq_gov_ondemandplus = {
+	.name = "ondemandplus",
+	.governor = cpufreq_governor_ondemandplus,
+	.max_transition_latency = 10000000,
+	.owner = THIS_MODULE,
+};
+
+static int __init cpufreq_ondemandplus_init(void)
+{
+	unsigned int i;
+	struct cpufreq_ondemandplus_cpuinfo *pcpu;
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+
+	timer_rate = DEFAULT_TIMER_RATE;
+	up_threshold = DEFAULT_UP_THRESHOLD;
+	down_differential = DEFAULT_DOWN_DIFFERENTIAL;
+	inter_hifreq = DEFAULT_INTER_HIFREQ;
+	inter_lofreq = DEFAULT_INTER_LOFREQ;
+	inter_staycycles = DEFAULT_INTER_STAYCYCLES;
+	staycycles_resetfreq = DEFAULT_STAYCYCLES_RESETFREQ;
+	io_is_busy = DEFAULT_IO_IS_BUSY;
+
+	/* Initalize per-cpu timers */
+	for_each_possible_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+		init_timer(&pcpu->cpu_timer);
+		pcpu->cpu_timer.function = cpufreq_ondemandplus_timer;
+		pcpu->cpu_timer.data = i;
+		mutex_init(&pcpu->timer_mutex);
+	}
+
+	spin_lock_init(&speedchange_cpumask_lock);
+	speedchange_task =
+		kthread_create(cpufreq_ondemandplus_speedchange_task, NULL,
+				"cfondemandplus");
+	if (IS_ERR(speedchange_task))
+		return PTR_ERR(speedchange_task);
+
+	sched_setscheduler_nocheck(speedchange_task, SCHED_FIFO, &param);
+	get_task_struct(speedchange_task);
+
+	/* NB: wake up so the thread does not look hung to the freezer */
+	wake_up_process(speedchange_task);
+
+	return cpufreq_register_governor(&cpufreq_gov_ondemandplus);
+
+	put_task_struct(speedchange_task);
+	return -ENOMEM;
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS
+fs_initcall(cpufreq_ondemandplus_init);
+#else
+module_init(cpufreq_ondemandplus_init);
+#endif
+
+static void __exit cpufreq_ondemandplus_exit(void)
+{
+	unsigned int i;
+
+	cpufreq_unregister_governor(&cpufreq_gov_ondemandplus);
+	for_each_possible_cpu(i) {
+		struct cpufreq_ondemandplus_cpuinfo *pcpu =
+			&per_cpu(cpuinfo, i);
+		mutex_destroy(&pcpu->timer_mutex);
+	}
+	kthread_stop(speedchange_task);
+	put_task_struct(speedchange_task);
+}
+
+module_exit(cpufreq_ondemandplus_exit);
+
+MODULE_AUTHOR("Mike Chan <mike@android.com>");
+MODULE_DESCRIPTION("'cpufreq_ondemandplus' - A cpufreq governor for "
+        "semi-aggressive scaling");
+MODULE_LICENSE("GPL");
diff --git a/drivers/cpufreq/cpufreq_pegasusq.c b/drivers/cpufreq/cpufreq_pegasusq.c
new file mode 100755
index 0000000..aee27de
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_pegasusq.c
@@ -0,0 +1,645 @@
+/*
+ *  drivers/cpufreq/cpufreq_pegasusq.c
+ *
+ *  Copyright (C)  2011 Samsung Electronics co. ltd
+ *    ByungChang Cha <bc.cha@samsung.com>
+ *
+ *  Based on ondemand governor
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/suspend.h>
+#include <linux/reboot.h>
+
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define MAX_SAMPLING_DOWN_FACTOR		(3)
+#define DEF_FREQUENCY_DOWN_DIFFERENTIAL		(10)
+#define DEF_FREQUENCY_UP_THRESHOLD		(70)
+
+/* for multiple freq_step */
+#define DEF_UP_THRESHOLD_DIFF			(5)
+
+#define DEF_FREQUENCY_MIN_SAMPLE_RATE		(5000)
+#define MIN_FREQUENCY_UP_THRESHOLD		(11)
+#define MAX_FREQUENCY_UP_THRESHOLD		(100)
+#define DEF_SAMPLING_RATE			(40000)
+#define MIN_SAMPLING_RATE			(10000)
+
+#define DEF_FREQ_STEP				(55)
+/* for multiple freq_step */
+#define DEF_FREQ_STEP_DEC			(13)
+
+#define DEF_START_DELAY				(0)
+
+#define UP_THRESHOLD_AT_MIN_FREQ		(40)
+/* for fast decrease */
+#define UP_THRESHOLD_AT_FAST_DOWN		(80)
+
+static unsigned int min_sampling_rate;
+
+static void do_dbs_timer(struct work_struct *work);
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ
+static
+#endif
+struct cpufreq_governor cpufreq_gov_pegasusq = {
+	.name                   = "pegasusq",
+	.governor               = cpufreq_governor_dbs,
+	.owner                  = THIS_MODULE,
+};
+
+/* Sampling types */
+enum {DBS_NORMAL_SAMPLE, DBS_SUB_SAMPLE};
+
+struct cpu_dbs_info_s {
+	u64 prev_cpu_idle;
+	u64 prev_cpu_iowait;
+	u64 prev_cpu_wall;
+	unsigned int prev_cpu_wall_delta;
+	u64 prev_cpu_nice;
+	struct cpufreq_policy *cur_policy;
+	struct delayed_work work;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int rate_mult;
+	int cpu;
+	unsigned int prev_load;
+	/*
+	 * percpu mutex that serializes governor limit change with
+	 * do_dbs_timer invocation. We do not want do_dbs_timer to run
+	 * when user is changing the governor or limits.
+	 */
+	struct mutex timer_mutex;
+};
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, od_cpu_dbs_info);
+
+static unsigned int dbs_enable;	/* number of CPUs using this policy */
+
+/*
+ * dbs_mutex protects dbs_enable in governor start/stop.
+ */
+static DEFINE_MUTEX(dbs_mutex);
+
+static struct workqueue_struct *dbs_wq;
+
+static struct dbs_tuners {
+	unsigned int sampling_rate;
+	unsigned int up_threshold;
+	unsigned int down_differential;
+	unsigned int sampling_down_factor;
+	/* pegasusq tuners */
+	unsigned int freq_step;
+	unsigned int max_freq;
+	unsigned int min_freq;
+	unsigned int up_threshold_at_min_freq;
+} dbs_tuners_ins = {
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.down_differential = DEF_FREQUENCY_DOWN_DIFFERENTIAL,
+	.freq_step = DEF_FREQ_STEP,
+	.up_threshold_at_min_freq = UP_THRESHOLD_AT_MIN_FREQ,
+};
+
+static inline u64 get_cpu_iowait_time(unsigned int cpu, u64 *wall)
+{
+	u64 iowait_time = get_cpu_iowait_time_us(cpu, wall);
+
+	if (iowait_time == -1ULL)
+		return 0;
+
+	return iowait_time;
+}
+
+/************************** sysfs interface ************************/
+
+static ssize_t show_sampling_rate_min(struct kobject *kobj,
+				      struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", min_sampling_rate);
+}
+
+define_one_global_ro(sampling_rate_min);
+
+/* cpufreq_pegasusq Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)              \
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(up_threshold, up_threshold);
+show_one(sampling_down_factor, sampling_down_factor);
+show_one(down_differential, down_differential);
+show_one(freq_step, freq_step);
+show_one(up_threshold_at_min_freq, up_threshold_at_min_freq);
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret = 0;
+	int mpd = strcmp(current->comm, "mpdecision");
+
+	if (mpd == 0)
+		return ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_rate = max(input, min_sampling_rate);
+
+	return count;
+}
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold = input;
+
+	return count;
+}
+
+static ssize_t store_sampling_down_factor(struct kobject *a,
+					  struct attribute *b,
+					  const char *buf, size_t count)
+{
+	unsigned int input, j;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_down_factor = input;
+
+	/* Reset down sampling multiplier in case it was active */
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(od_cpu_dbs_info, j);
+		dbs_info->rate_mult = 1;
+	}
+	return count;
+}
+
+static ssize_t store_down_differential(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.down_differential = min(input, 100u);
+
+	return count;
+}
+
+static ssize_t store_freq_step(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.freq_step = min(input, 100u);
+	return count;
+}
+
+static ssize_t store_up_threshold_at_min_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+	    input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold_at_min_freq = input;
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(up_threshold);
+define_one_global_rw(sampling_down_factor);
+define_one_global_rw(down_differential);
+define_one_global_rw(freq_step);
+define_one_global_rw(up_threshold_at_min_freq);
+
+static struct attribute *dbs_attributes[] = {
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&up_threshold.attr,
+	&sampling_down_factor.attr,
+	&down_differential.attr,
+	&freq_step.attr,
+	&up_threshold_at_min_freq.attr,
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "pegasusq",
+};
+
+/************************** sysfs end ************************/
+
+static void dbs_freq_increase(struct cpufreq_policy *p, unsigned int freq)
+{
+	if (p->cur == p->max)
+		return;
+
+	__cpufreq_driver_target(p, freq, CPUFREQ_RELATION_L);
+}
+
+static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
+{
+	unsigned int max_load_freq;
+	unsigned int max_load = 0;
+	unsigned int cur_load = 0;
+	unsigned int sampling_rate;
+
+	struct cpufreq_policy *policy;
+	unsigned int j;
+	int up_threshold = dbs_tuners_ins.up_threshold;
+
+	sampling_rate = dbs_tuners_ins.sampling_rate * this_dbs_info->rate_mult;
+	policy = this_dbs_info->cur_policy;
+
+	/* Get Absolute Load - in terms of freq */
+	max_load_freq = 0;
+
+	for_each_cpu(j, policy->cpus) {
+		struct cpu_dbs_info_s *j_dbs_info;
+		u64 cur_wall_time, cur_idle_time, cur_iowait_time;
+		u64 prev_wall_time, prev_idle_time, prev_iowait_time;
+		unsigned int idle_time, wall_time, iowait_time;
+		unsigned int load_freq;
+		int freq_avg;
+
+		j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+		prev_wall_time = j_dbs_info->prev_cpu_wall;
+		prev_idle_time = j_dbs_info->prev_cpu_idle;
+		prev_iowait_time = j_dbs_info->prev_cpu_iowait;
+
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time,
+				0);
+		cur_iowait_time = get_cpu_iowait_time(j, &cur_wall_time);
+
+		wall_time = (unsigned int)
+				(cur_wall_time - prev_wall_time);
+		j_dbs_info->prev_cpu_wall = cur_wall_time;
+
+		idle_time = (unsigned int)
+				(cur_idle_time - prev_idle_time);
+		j_dbs_info->prev_cpu_idle = cur_idle_time;
+
+		iowait_time = (unsigned int)
+				(cur_iowait_time - prev_iowait_time);
+		j_dbs_info->prev_cpu_iowait = cur_iowait_time;
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		/*
+		 * If the CPU had gone completely idle, and a task just woke up
+		 * on this CPU now, it would be unfair to calculate 'load' the
+		 * usual way for this elapsed time-window, because it will show
+		 * near-zero load, irrespective of how CPU intensive that task
+		 * actually is. This is undesirable for latency-sensitive bursty
+		 * workloads.
+		 *
+		 * To avoid this, we reuse the 'load' from the previous
+		 * time-window and give this task a chance to start with a
+		 * reasonably high CPU frequency. (However, we shouldn't over-do
+		 * this copy, lest we get stuck at a high load (high frequency)
+		 * for too long, even when the current system load has actually
+		 * dropped down. So we perform the copy only once, upon the
+		 * first wake-up from idle.)
+		 *
+		 * Detecting this situation is easy: the governor's deferrable
+		 * timer would not have fired during CPU-idle periods. Hence
+		 * an unusually large 'wall_time' (as compared to the sampling
+		 * rate) indicates this scenario.
+		 *
+		 * prev_load can be zero in two cases and we must recalculate it
+		 * for both cases:
+		 * - during long idle intervals
+		 * - explicitly set to zero
+		 */
+		if (unlikely(wall_time > (2 * sampling_rate) &&
+			     j_dbs_info->prev_load)) {
+			cur_load = j_dbs_info->prev_load;
+
+			/*
+			 * Perform a destructive copy, to ensure that we copy
+			 * the previous load only once, upon the first wake-up
+			 * from idle.
+			 */
+			j_dbs_info->prev_load = 0;
+		} else {
+			cur_load = 100 * (wall_time - idle_time) / wall_time;
+			j_dbs_info->prev_load = cur_load;
+		}
+
+		if (cur_load > max_load)
+			max_load = cur_load;
+
+		freq_avg = __cpufreq_driver_getavg(policy, j);
+		if (policy == NULL)
+			return;
+		if (freq_avg <= 0)
+			freq_avg = policy->cur;
+
+		load_freq = cur_load * freq_avg;
+		if (load_freq > max_load_freq)
+			max_load_freq = load_freq;
+	}
+
+	cpufreq_notify_utilization(policy, max_load);
+
+	/* Check for frequency increase */
+	up_threshold = dbs_tuners_ins.up_threshold;
+
+	if (max_load_freq > up_threshold * policy->cur) {
+		/* for multiple freq_step */
+		int inc = policy->max * (dbs_tuners_ins.freq_step
+					- DEF_FREQ_STEP_DEC * 2) / 100;
+		int target = 0;
+
+		/* for multiple freq_step */
+		if (max_load_freq > (up_threshold + DEF_UP_THRESHOLD_DIFF * 2)
+			* policy->cur)
+			inc = policy->max * dbs_tuners_ins.freq_step / 100;
+		else if (max_load_freq > (up_threshold + DEF_UP_THRESHOLD_DIFF)
+			* policy->cur)
+			inc = policy->max * (dbs_tuners_ins.freq_step
+					- DEF_FREQ_STEP_DEC) / 100;
+
+		target = min(policy->max, policy->cur + inc);
+
+		/* If switching to max speed, apply sampling_down_factor */
+		if (policy->cur < policy->max && target == policy->max)
+			this_dbs_info->rate_mult =
+				dbs_tuners_ins.sampling_down_factor;
+		dbs_freq_increase(policy, target);
+		return;
+	}
+
+	/* Check for frequency decrease */
+	/* if we cannot reduce the frequency anymore, break out early */
+	if (policy->cur == policy->min)
+		return;
+
+	/*
+	 * The optimal frequency is the frequency that is the lowest that
+	 * can support the current CPU usage without triggering the up
+	 * policy. To be safe, we focus DOWN_DIFFERENTIAL points under
+	 * the threshold.
+	 */
+	if (max_load_freq <
+	    (dbs_tuners_ins.up_threshold - dbs_tuners_ins.down_differential) *
+			policy->cur) {
+		unsigned int freq_next;
+		unsigned int down_thres;
+
+		freq_next = max_load_freq /
+			(dbs_tuners_ins.up_threshold -
+				dbs_tuners_ins.down_differential);
+
+		/* No longer fully busy, reset rate_mult */
+		this_dbs_info->rate_mult = 1;
+
+		if (freq_next < policy->min)
+			freq_next = policy->min;
+
+		down_thres = dbs_tuners_ins.up_threshold_at_min_freq
+			- dbs_tuners_ins.down_differential;
+
+		if (policy->cur == freq_next)
+			return;
+
+		__cpufreq_driver_target(policy, freq_next,
+					CPUFREQ_RELATION_L);
+	}
+}
+
+static void do_dbs_timer(struct work_struct *work)
+{
+	struct cpu_dbs_info_s *dbs_info =
+		container_of(work, struct cpu_dbs_info_s, work.work);
+	int delay;
+
+	if (unlikely(!cpu_online(dbs_info->cpu) || !dbs_info->cur_policy))
+		return;
+
+	mutex_lock(&dbs_info->timer_mutex);
+
+	dbs_check_cpu(dbs_info);
+	/* We want all CPUs to do sampling nearly on
+	 * same jiffy
+	 */
+	delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate
+				 * dbs_info->rate_mult);
+
+	if (num_online_cpus() > 1)
+		delay -= jiffies % delay;
+
+	queue_delayed_work_on(dbs_info->cpu, dbs_wq, &dbs_info->work, delay);
+	mutex_unlock(&dbs_info->timer_mutex);
+}
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
+{
+	/* We want all CPUs to do sampling nearly on same jiffy */
+	int delay = usecs_to_jiffies(DEF_START_DELAY * 1000 * 1000
+				     + dbs_tuners_ins.sampling_rate);
+
+	if (num_online_cpus() > 1)
+		delay -= jiffies % delay;
+
+	INIT_DEFERRABLE_WORK(&dbs_info->work, do_dbs_timer);
+	queue_delayed_work_on(dbs_info->cpu, dbs_wq, &dbs_info->work, delay);
+}
+
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
+{
+	cancel_delayed_work_sync(&dbs_info->work);
+}
+
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int rc;
+
+	this_dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy))
+			return -EINVAL;
+
+		dbs_tuners_ins.max_freq = policy->max;
+		dbs_tuners_ins.min_freq = policy->min;
+
+		mutex_lock(&dbs_mutex);
+
+		dbs_enable++;
+
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			unsigned int prev_load;
+
+			j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&j_dbs_info->prev_cpu_wall,
+						0);
+
+			prev_load = (unsigned int)
+				(j_dbs_info->prev_cpu_wall - j_dbs_info->prev_cpu_idle);
+			j_dbs_info->prev_load = 100 * prev_load /
+				(unsigned int) j_dbs_info->prev_cpu_wall;
+		}
+		cpu = policy->cpu;
+		this_dbs_info->cpu = cpu;
+		this_dbs_info->rate_mult = 1;
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 1) {
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&dbs_attr_group);
+			if (rc) {
+				dbs_enable--;
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+
+			min_sampling_rate = MIN_SAMPLING_RATE;
+			dbs_tuners_ins.sampling_rate = DEF_SAMPLING_RATE;
+		}
+		mutex_init(&this_dbs_info->timer_mutex);
+
+		mutex_unlock(&dbs_mutex);
+
+		dbs_timer_init(this_dbs_info);
+
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		dbs_timer_exit(this_dbs_info);
+
+		mutex_lock(&dbs_mutex);
+		mutex_destroy(&this_dbs_info->timer_mutex);
+
+		dbs_enable--;
+
+		if (!dbs_enable) {
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &dbs_attr_group);
+		}
+		mutex_unlock(&dbs_mutex);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		/* If device is being removed, skip set limits */
+		if (!this_dbs_info->cur_policy)
+			break;
+		mutex_lock(&this_dbs_info->timer_mutex);
+		__cpufreq_driver_target(this_dbs_info->cur_policy,
+				policy->cur, CPUFREQ_RELATION_L);
+		dbs_check_cpu(this_dbs_info);
+		mutex_unlock(&this_dbs_info->timer_mutex);
+
+		break;
+	}
+	return 0;
+}
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	int ret;
+
+	dbs_wq = alloc_workqueue("pegasusq_dbs_wq", WQ_HIGHPRI, 0);
+	if (!dbs_wq) {
+		printk(KERN_ERR "Failed to create pegasusq_dbs_wq workqueue\n");
+		return -EFAULT;
+	}
+
+	ret = cpufreq_register_governor(&cpufreq_gov_pegasusq);
+	if (ret)
+		goto err_reg;
+
+	return ret;
+
+err_reg:
+	kfree(&dbs_tuners_ins);
+	return ret;
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_pegasusq);
+	destroy_workqueue(dbs_wq);
+	kfree(&dbs_tuners_ins);
+}
+
+MODULE_AUTHOR("ByungChang Cha <bc.cha@samsung.com>");
+MODULE_DESCRIPTION("'cpufreq_pegasusq' - A dynamic cpufreq governor");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_raccoon_city.c b/drivers/cpufreq/cpufreq_raccoon_city.c
new file mode 100755
index 0000000..955b122
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_raccoon_city.c
@@ -0,0 +1,176 @@
+/*
+ *  linux/drivers/cpufreq/cpufreq_raccoon_city.c
+ *
+ *  Copyright (C) 2002 - 2003 Dominik Brodowski <linux@brodo.de>
+ *            (C) 2014 LoungeKatt <twistedumbrella@gmail.com>
+ *
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/cpufreq.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#ifdef CONFIG_POWERSUSPEND
+#include <linux/powersuspend.h>
+#endif
+
+static int active_count;
+static struct mutex gov_lock;
+
+#define DEFAULT_GOVERNOR_FREQ_OFF   1190400
+unsigned int max_governor_freq;
+unsigned int max_freq_screen_off = DEFAULT_GOVERNOR_FREQ_OFF;
+
+#ifdef CONFIG_POWERSUSPEND
+static ssize_t max_freq_screen_off_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
+{
+    return sprintf(buf, "%d\n", max_freq_screen_off);
+}
+
+static ssize_t max_freq_screen_off_store(struct kobject *kobj, struct kobj_attribute *attr, const char *buf, size_t count)
+{
+    unsigned int new_max_freq_screen_off;
+    
+    if (!sscanf(buf, "%du", &new_max_freq_screen_off))
+        return -EINVAL;
+    
+    if (new_max_freq_screen_off == max_freq_screen_off)
+        return count;
+    
+    max_freq_screen_off = new_max_freq_screen_off;
+    return count;
+}
+
+static struct kobj_attribute max_freq_screen_off_attr = __ATTR(max_freq_screen_off, 0666, max_freq_screen_off_show, max_freq_screen_off_store);
+
+static struct attribute *raccoon_city_attributes[] = {
+    &max_freq_screen_off_attr.attr,
+    NULL,
+};
+
+static struct attribute_group raccoon_city_attr_group = {
+    .attrs = raccoon_city_attributes,
+    .name = "raccoon_city",
+};
+#endif
+
+static int cpufreq_governor_raccoon_city(struct cpufreq_policy *policy,
+					unsigned int event)
+{
+    int rc;
+	switch (event) {
+	case CPUFREQ_GOV_START:
+        mutex_lock(&gov_lock);
+        /*
+         * Do not register the idle hook and create sysfs
+         * entries if we have already done so.
+         */
+        if (++active_count > 1) {
+            mutex_unlock(&gov_lock);
+            return 0;
+        }
+#ifdef CONFIG_POWERSUSPEND
+        if (!have_governor_per_policy())
+            WARN_ON(cpufreq_get_global_kobject());
+        
+        rc = sysfs_create_group(get_governor_parent_kobj(policy),
+                                &raccoon_city_attr_group);
+        if (rc) {
+            mutex_unlock(&gov_lock);
+            return rc;
+        }
+#endif
+        mutex_unlock(&gov_lock);
+        break;
+    case CPUFREQ_GOV_STOP:
+        mutex_lock(&gov_lock);
+        
+        if (--active_count > 0) {
+            mutex_unlock(&gov_lock);
+            return 0;
+        }
+#ifdef CONFIG_POWERSUSPEND
+        sysfs_remove_group(get_governor_parent_kobj(policy),
+                           &raccoon_city_attr_group);
+        if (!have_governor_per_policy())
+            cpufreq_put_global_kobject();
+#endif
+        mutex_unlock(&gov_lock);
+        break;
+	case CPUFREQ_GOV_LIMITS:
+		pr_debug("setting to %u kHz because of event %u\n",
+						max_governor_freq, event);
+		__cpufreq_driver_target(policy, max_governor_freq,
+						CPUFREQ_RELATION_H);
+		break;
+	default:
+		break;
+	}
+	return 0;
+}
+
+#ifdef CONFIG_POWERSUSPEND
+static void cpufreq_raccoon_city_power_suspend(struct power_suspend *h)
+{
+    mutex_lock(&gov_lock);
+    if (max_governor_freq != max_freq_screen_off) {
+        max_governor_freq = max_freq_screen_off;
+    }
+    mutex_unlock(&gov_lock);
+}
+
+static void cpufreq_raccoon_city_power_resume(struct power_suspend *h)
+{
+    struct cpufreq_policy *policy = cpufreq_cpu_get(0);
+    mutex_lock(&gov_lock);
+    max_governor_freq = policy->max;
+    mutex_unlock(&gov_lock);
+}
+
+static struct power_suspend cpufreq_raccoon_city_power_suspend_info = {
+    .suspend = cpufreq_raccoon_city_power_suspend,
+    .resume = cpufreq_raccoon_city_power_resume,
+};
+#endif
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_RACCOON_CITY
+static
+#endif
+struct cpufreq_governor cpufreq_gov_raccoon_city = {
+	.name		= "raccoon_city",
+	.governor	= cpufreq_governor_raccoon_city,
+	.owner		= THIS_MODULE,
+};
+
+static int __init cpufreq_gov_raccoon_city_init(void)
+{
+    struct cpufreq_policy *policy = cpufreq_cpu_get(0);
+    max_governor_freq = policy->max;
+#ifdef CONFIG_POWERSUSPEND
+    register_power_suspend(&cpufreq_raccoon_city_power_suspend_info);
+#endif
+	return cpufreq_register_governor(&cpufreq_gov_raccoon_city);
+}
+
+static void __exit cpufreq_gov_raccoon_city_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_raccoon_city);
+}
+
+MODULE_AUTHOR("LoungeKatt <twistedumbrella@gmail.com>");
+MODULE_DESCRIPTION("CPUfreq policy governor 'raccoon_city'");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_RACCOON_CITY
+fs_initcall(cpufreq_gov_raccoon_city_init);
+#else
+module_init(cpufreq_gov_raccoon_city_init);
+#endif
+module_exit(cpufreq_gov_raccoon_city_exit);
diff --git a/drivers/cpufreq/cpufreq_smartass2.c b/drivers/cpufreq/cpufreq_smartass2.c
new file mode 100644
index 0000000..1b903ae
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_smartass2.c
@@ -0,0 +1,925 @@
+/*
+ * drivers/cpufreq/cpufreq_smartass2.c
+ *
+ * Copyright (C) 2010 Google, Inc.
+ *           (C) 2014 LoungeKatt <twistedumbrella@gmail.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * Author: Erasmux
+ *
+ * Based on the interactive governor By Mike Chan (mike@android.com)
+ * which was adaptated to 2.6.29 kernel by Nadlabak (pavel@doshaska.net)
+ *
+ * SMP support based on mod by faux123
+ *
+ * For a general overview of smartassV2 see the relavent part in
+ * Documentation/cpu-freq/governors.txt
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/sched.h>
+#include <linux/tick.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/moduleparam.h>
+#include <asm/cputime.h>
+
+#ifdef CONFIG_HAS_EARLYSUSPEND
+#include <linux/earlysuspend.h>
+#endif
+
+#ifdef CONFIG_POWERSUSPEND
+#include <linux/powersuspend.h>
+#endif
+
+#define cputime64_sub(__a, __b)    ((__a) - (__b))
+
+/******************** Tunable parameters: ********************/
+
+/*
+ * The "ideal" frequency to use when awake. The governor will ramp up faster
+ * towards the ideal frequency and slower after it has passed it. Similarly,
+ * lowering the frequency towards the ideal frequency is faster than below it.
+ */
+#define DEFAULT_AWAKE_IDEAL_FREQ 300000
+static unsigned int awake_ideal_freq;
+
+/*
+ * The "ideal" frequency to use when suspended.
+ * When set to 0, the governor will not track the suspended state (meaning
+ * that practically when sleep_ideal_freq==0 the awake_ideal_freq is used
+ * also when suspended).
+ */
+#define DEFAULT_SLEEP_IDEAL_FREQ 300000
+static unsigned int sleep_ideal_freq;
+
+/*
+ * Freqeuncy delta when ramping up above the ideal freqeuncy.
+ * Zero disables and causes to always jump straight to max frequency.
+ * When below the ideal freqeuncy we always ramp up to the ideal freq.
+ */
+#define DEFAULT_RAMP_UP_STEP 128000
+static unsigned int ramp_up_step;
+
+/*
+ * Freqeuncy delta when ramping down below the ideal freqeuncy.
+ * Zero disables and will calculate ramp down according to load heuristic.
+ * When above the ideal freqeuncy we always ramp down to the ideal freq.
+ */
+#define DEFAULT_RAMP_DOWN_STEP 256000
+static unsigned int ramp_down_step;
+
+/*
+ * CPU freq will be increased if measured load > max_cpu_load;
+ */
+#define DEFAULT_MAX_CPU_LOAD 75
+static unsigned long max_cpu_load;
+
+/*
+ * CPU freq will be decreased if measured load < min_cpu_load;
+ */
+#define DEFAULT_MIN_CPU_LOAD 35
+static unsigned long min_cpu_load;
+
+/*
+ * The minimum amount of time to spend at a frequency before we can ramp up.
+ * Notice we ignore this when we are below the ideal frequency.
+ */
+#define DEFAULT_UP_RATE_US 48000;
+static unsigned long up_rate_us;
+
+/*
+ * The minimum amount of time to spend at a frequency before we can ramp down.
+ * Notice we ignore this when we are above the ideal frequency.
+ */
+#define DEFAULT_DOWN_RATE_US 99000;
+static unsigned long down_rate_us;
+
+/*
+ * The frequency to set when waking up from sleep.
+ * When sleep_ideal_freq=0 this will have no effect.
+ */
+#define DEFAULT_SLEEP_WAKEUP_FREQ 1190000
+static unsigned int sleep_wakeup_freq;
+
+/*
+ * Sampling rate, I highly recommend to leave it at 2.
+ */
+#define DEFAULT_SAMPLE_RATE_JIFFIES 2
+static unsigned int sample_rate_jiffies;
+
+
+/*************** End of tunables ***************/
+
+void (*pm_idle)(void);
+static void (*pm_idle_old)(void);
+static atomic_t active_count = ATOMIC_INIT(0);
+
+struct smartass_info_s {
+	struct cpufreq_policy *cur_policy;
+	struct cpufreq_frequency_table *freq_table;
+	struct timer_list timer;
+	u64 time_in_idle;
+	u64 idle_exit_time;
+	u64 freq_change_time;
+	u64 freq_change_time_in_idle;
+	int cur_cpu_load;
+	int old_freq;
+	int ramp_dir;
+	unsigned int enable;
+	int ideal_speed;
+};
+static DEFINE_PER_CPU(struct smartass_info_s, smartass_info);
+
+/* Workqueues handle frequency scaling */
+static struct workqueue_struct *up_wq;
+static struct workqueue_struct *down_wq;
+static struct work_struct freq_scale_work;
+
+static cpumask_t work_cpumask;
+static spinlock_t cpumask_lock;
+
+static unsigned int suspended;
+
+#define dprintk(flag,msg...) do { \
+	if (debug_mask & flag) printk(KERN_DEBUG msg); \
+	} while (0)
+
+enum {
+	SMARTASS_DEBUG_JUMPS=1,
+	SMARTASS_DEBUG_LOAD=2,
+	SMARTASS_DEBUG_ALG=4
+};
+
+/*
+ * Combination of the above debug flags.
+ */
+static unsigned long debug_mask;
+
+static int cpufreq_governor_smartass(struct cpufreq_policy *policy,
+		unsigned int event);
+
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2
+static
+#endif
+struct cpufreq_governor cpufreq_gov_smartass2 = {
+	.name = "smartassV2",
+	.governor = cpufreq_governor_smartass,
+	.max_transition_latency = TRANSITION_LATENCY_LIMIT,
+	.owner = THIS_MODULE,
+};
+
+inline static void smartass_update_min_max(struct smartass_info_s *this_smartass, struct cpufreq_policy *policy, int suspend) {
+	if (suspend) {
+		this_smartass->ideal_speed = // sleep_ideal_freq; but make sure it obeys the policy min/max
+			policy->max > sleep_ideal_freq ?
+			(sleep_ideal_freq > policy->min ? sleep_ideal_freq : policy->min) : policy->max;
+	} else {
+		this_smartass->ideal_speed = // awake_ideal_freq; but make sure it obeys the policy min/max
+			policy->min < awake_ideal_freq ?
+			(awake_ideal_freq < policy->max ? awake_ideal_freq : policy->max) : policy->min;
+	}
+}
+
+inline static void smartass_update_min_max_allcpus(void) {
+	unsigned int i;
+	for_each_online_cpu(i) {
+		struct smartass_info_s *this_smartass = &per_cpu(smartass_info, i);
+		if (this_smartass->enable)
+			smartass_update_min_max(this_smartass,this_smartass->cur_policy,suspended);
+	}
+}
+
+inline static unsigned int validate_freq(struct cpufreq_policy *policy, int freq) {
+	if (freq > (int)policy->max)
+		return policy->max;
+	if (freq < (int)policy->min)
+		return policy->min;
+	return freq;
+}
+
+inline static void reset_timer(unsigned long cpu, struct smartass_info_s *this_smartass) {
+	this_smartass->time_in_idle = get_cpu_idle_time_us(cpu, &this_smartass->idle_exit_time);
+	mod_timer(&this_smartass->timer, jiffies + sample_rate_jiffies);
+}
+
+inline static void work_cpumask_set(unsigned long cpu) {
+	unsigned long flags;
+	spin_lock_irqsave(&cpumask_lock, flags);
+	cpumask_set_cpu(cpu, &work_cpumask);
+	spin_unlock_irqrestore(&cpumask_lock, flags);
+}
+
+inline static int work_cpumask_test_and_clear(unsigned long cpu) {
+	unsigned long flags;
+	int res = 0;
+	spin_lock_irqsave(&cpumask_lock, flags);
+	res = cpumask_test_and_clear_cpu(cpu, &work_cpumask);
+	spin_unlock_irqrestore(&cpumask_lock, flags);
+	return res;
+}
+
+inline static int target_freq(struct cpufreq_policy *policy, struct smartass_info_s *this_smartass,
+			      int new_freq, int old_freq, int prefered_relation) {
+	int index, target;
+	struct cpufreq_frequency_table *table = this_smartass->freq_table;
+
+	if (new_freq == old_freq)
+		return 0;
+	new_freq = validate_freq(policy,new_freq);
+	if (new_freq == old_freq)
+		return 0;
+
+	if (table &&
+	    !cpufreq_frequency_table_target(policy,table,new_freq,prefered_relation,&index))
+	{
+		target = table[index].frequency;
+		if (target == old_freq) {
+			// if for example we are ramping up to *at most* current + ramp_up_step
+			// but there is no such frequency higher than the current, try also
+			// to ramp up to *at least* current + ramp_up_step.
+			if (new_freq > old_freq && prefered_relation==CPUFREQ_RELATION_H
+			    && !cpufreq_frequency_table_target(policy,table,new_freq,
+							       CPUFREQ_RELATION_C,&index))
+				target = table[index].frequency;
+			// simlarly for ramping down:
+			else if (new_freq < old_freq && prefered_relation==CPUFREQ_RELATION_C
+				&& !cpufreq_frequency_table_target(policy,table,new_freq,
+								   CPUFREQ_RELATION_H,&index))
+				target = table[index].frequency;
+		}
+
+		if (target == old_freq) {
+			// We should not get here:
+			// If we got here we tried to change to a validated new_freq which is different
+			// from old_freq, so there is no reason for us to remain at same frequency.
+			printk(KERN_WARNING "Smartass: frequency change failed: %d to %d => %d\n",
+			       old_freq,new_freq,target);
+			return 0;
+		}
+	}
+	else target = new_freq;
+
+	__cpufreq_driver_target(policy, target, prefered_relation);
+
+	dprintk(SMARTASS_DEBUG_JUMPS,"SmartassQ: jumping from %d to %d => %d (%d)\n",
+		old_freq,new_freq,target,policy->cur);
+
+	return target;
+}
+
+static void cpufreq_smartass_timer(unsigned long cpu)
+{
+	u64 delta_idle;
+	u64 delta_time;
+	int cpu_load;
+	int old_freq;
+	u64 update_time;
+	u64 now_idle;
+	int queued_work = 0;
+	struct smartass_info_s *this_smartass = &per_cpu(smartass_info, cpu);
+	struct cpufreq_policy *policy = this_smartass->cur_policy;
+
+	now_idle = get_cpu_idle_time_us(cpu, &update_time);
+	old_freq = policy->cur;
+
+	if (this_smartass->idle_exit_time == 0 || update_time == this_smartass->idle_exit_time)
+		return;
+
+	delta_idle = cputime64_sub(now_idle, this_smartass->time_in_idle);
+	delta_time = cputime64_sub(update_time, this_smartass->idle_exit_time);
+
+	// If timer ran less than 1ms after short-term sample started, retry.
+	if (delta_time < 1000) {
+		if (!timer_pending(&this_smartass->timer))
+			reset_timer(cpu,this_smartass);
+		return;
+	}
+
+	if (delta_idle > delta_time)
+		cpu_load = 0;
+	else
+		cpu_load = 100 * (unsigned int)(delta_time - delta_idle) / (unsigned int)delta_time;
+
+	dprintk(SMARTASS_DEBUG_LOAD,"smartassT @ %d: load %d (delta_time %llu)\n",
+		old_freq,cpu_load,delta_time);
+
+	this_smartass->cur_cpu_load = cpu_load;
+	this_smartass->old_freq = old_freq;
+
+	// Scale up if load is above max or if there where no idle cycles since coming out of idle,
+	// additionally, if we are at or above the ideal_speed, verify we have been at this frequency
+	// for at least up_rate_us:
+	if (cpu_load > max_cpu_load || delta_idle == 0)
+	{
+		if (old_freq < policy->max &&
+			 (old_freq < this_smartass->ideal_speed || delta_idle == 0 ||
+			  cputime64_sub(update_time, this_smartass->freq_change_time) >= up_rate_us))
+		{
+			dprintk(SMARTASS_DEBUG_ALG,"smartassT @ %d ramp up: load %d (delta_idle %llu)\n",
+				old_freq,cpu_load,delta_idle);
+			this_smartass->ramp_dir = 1;
+			work_cpumask_set(cpu);
+			queue_work(up_wq, &freq_scale_work);
+			queued_work = 1;
+		}
+		else this_smartass->ramp_dir = 0;
+	}
+	// Similarly for scale down: load should be below min and if we are at or below ideal
+	// frequency we require that we have been at this frequency for at least down_rate_us:
+	else if (cpu_load < min_cpu_load && old_freq > policy->min &&
+		 (old_freq > this_smartass->ideal_speed ||
+		  cputime64_sub(update_time, this_smartass->freq_change_time) >= down_rate_us))
+	{
+		dprintk(SMARTASS_DEBUG_ALG,"smartassT @ %d ramp down: load %d (delta_idle %llu)\n",
+			old_freq,cpu_load,delta_idle);
+		this_smartass->ramp_dir = -1;
+		work_cpumask_set(cpu);
+		queue_work(down_wq, &freq_scale_work);
+		queued_work = 1;
+	}
+	else this_smartass->ramp_dir = 0;
+
+	// To avoid unnecessary load when the CPU is already at high load, we don't
+	// reset ourselves if we are at max speed. If and when there are idle cycles,
+	// the idle loop will activate the timer.
+	// Additionally, if we queued some work, the work task will reset the timer
+	// after it has done its adjustments.
+	if (!queued_work && old_freq < policy->max)
+		reset_timer(cpu,this_smartass);
+}
+
+static void cpufreq_idle(void)
+{
+	struct smartass_info_s *this_smartass = &per_cpu(smartass_info, smp_processor_id());
+	struct cpufreq_policy *policy = this_smartass->cur_policy;
+
+	if (!this_smartass->enable) {
+		pm_idle_old();
+		return;
+	}
+
+	if (policy->cur == policy->min && timer_pending(&this_smartass->timer))
+		del_timer(&this_smartass->timer);
+
+	pm_idle_old();
+
+	if (!timer_pending(&this_smartass->timer))
+		reset_timer(smp_processor_id(), this_smartass);
+}
+
+/* We use the same work function to sale up and down */
+static void cpufreq_smartass_freq_change_time_work(struct work_struct *work)
+{
+	unsigned int cpu;
+	int new_freq;
+	int old_freq;
+	int ramp_dir;
+	struct smartass_info_s *this_smartass;
+	struct cpufreq_policy *policy;
+	unsigned int relation = CPUFREQ_RELATION_C;
+	for_each_possible_cpu(cpu) {
+		this_smartass = &per_cpu(smartass_info, cpu);
+		if (!work_cpumask_test_and_clear(cpu))
+			continue;
+
+		ramp_dir = this_smartass->ramp_dir;
+		this_smartass->ramp_dir = 0;
+
+		old_freq = this_smartass->old_freq;
+		policy = this_smartass->cur_policy;
+
+		if (old_freq != policy->cur) {
+			// frequency was changed by someone else?
+			printk(KERN_WARNING "Smartass: frequency changed by 3rd party: %d to %d\n",
+			       old_freq,policy->cur);
+			new_freq = old_freq;
+		}
+		else if (ramp_dir > 0 && nr_running() > 1) {
+			// ramp up logic:
+			if (old_freq < this_smartass->ideal_speed)
+				new_freq = this_smartass->ideal_speed;
+			else if (ramp_up_step) {
+				new_freq = old_freq + ramp_up_step;
+				relation = CPUFREQ_RELATION_H;
+			}
+			else {
+				new_freq = policy->max;
+				relation = CPUFREQ_RELATION_H;
+			}
+			dprintk(SMARTASS_DEBUG_ALG,"smartassQ @ %d ramp up: ramp_dir=%d ideal=%d\n",
+				old_freq,ramp_dir,this_smartass->ideal_speed);
+		}
+		else if (ramp_dir < 0) {
+			// ramp down logic:
+			if (old_freq > this_smartass->ideal_speed) {
+				new_freq = this_smartass->ideal_speed;
+				relation = CPUFREQ_RELATION_H;
+			}
+			else if (ramp_down_step)
+				new_freq = old_freq - ramp_down_step;
+			else {
+				// Load heuristics: Adjust new_freq such that, assuming a linear
+				// scaling of load vs. frequency, the load in the new frequency
+				// will be max_cpu_load:
+				new_freq = old_freq * this_smartass->cur_cpu_load / max_cpu_load;
+				if (new_freq > old_freq) // min_cpu_load > max_cpu_load ?!
+					new_freq = old_freq -1;
+			}
+			dprintk(SMARTASS_DEBUG_ALG,"smartassQ @ %d ramp down: ramp_dir=%d ideal=%d\n",
+				old_freq,ramp_dir,this_smartass->ideal_speed);
+		}
+		else { // ramp_dir==0 ?! Could the timer change its mind about a queued ramp up/down
+		       // before the work task gets to run?
+		       // This may also happen if we refused to ramp up because the nr_running()==1
+			new_freq = old_freq;
+			dprintk(SMARTASS_DEBUG_ALG,"smartassQ @ %d nothing: ramp_dir=%d nr_running=%lu\n",
+				old_freq,ramp_dir,nr_running());
+		}
+
+		// do actual ramp up (returns 0, if frequency change failed):
+		new_freq = target_freq(policy,this_smartass,new_freq,old_freq,relation);
+		if (new_freq)
+			this_smartass->freq_change_time_in_idle =
+				get_cpu_idle_time_us(cpu,&this_smartass->freq_change_time);
+
+		// reset timer:
+		if (new_freq < policy->max)
+			reset_timer(cpu,this_smartass);
+		// if we are maxed out, it is pointless to use the timer
+		// (idle cycles wake up the timer when the timer comes)
+		else if (timer_pending(&this_smartass->timer))
+			del_timer(&this_smartass->timer);
+	}
+}
+
+static ssize_t show_debug_mask(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", debug_mask);
+}
+
+static ssize_t store_debug_mask(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0)
+		debug_mask = input;
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_up_rate_us(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", up_rate_us);
+}
+
+static ssize_t store_up_rate_us(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0 && input <= 100000000)
+		up_rate_us = input;
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_down_rate_us(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", down_rate_us);
+}
+
+static ssize_t store_down_rate_us(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0 && input <= 100000000)
+		down_rate_us = input;
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_sleep_ideal_freq(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sleep_ideal_freq);
+}
+
+static ssize_t store_sleep_ideal_freq(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0) {
+		sleep_ideal_freq = input;
+		if (suspended)
+			smartass_update_min_max_allcpus();
+	}
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_sleep_wakeup_freq(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sleep_wakeup_freq);
+}
+
+static ssize_t store_sleep_wakeup_freq(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0)
+		sleep_wakeup_freq = input;
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_awake_ideal_freq(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", awake_ideal_freq);
+}
+
+static ssize_t store_awake_ideal_freq(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0) {
+		awake_ideal_freq = input;
+		if (!suspended)
+			smartass_update_min_max_allcpus();
+	}
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_sample_rate_jiffies(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sample_rate_jiffies);
+}
+
+static ssize_t store_sample_rate_jiffies(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input <= 1000)
+		sample_rate_jiffies = input;
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_ramp_up_step(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", ramp_up_step);
+}
+
+static ssize_t store_ramp_up_step(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0)
+		ramp_up_step = input;
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_ramp_down_step(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", ramp_down_step);
+}
+
+static ssize_t store_ramp_down_step(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0)
+		ramp_down_step = input;
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_max_cpu_load(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", max_cpu_load);
+}
+
+static ssize_t store_max_cpu_load(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input <= 100)
+		max_cpu_load = input;
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_min_cpu_load(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", min_cpu_load);
+}
+
+static ssize_t store_min_cpu_load(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input < 100)
+		min_cpu_load = input;
+	else return -EINVAL;
+	return count;
+}
+
+#define define_global_rw_attr(_name)		\
+static struct global_attr _name##_attr =	\
+	__ATTR(_name, 0644, show_##_name, store_##_name)
+
+define_global_rw_attr(debug_mask);
+define_global_rw_attr(up_rate_us);
+define_global_rw_attr(down_rate_us);
+define_global_rw_attr(sleep_ideal_freq);
+define_global_rw_attr(sleep_wakeup_freq);
+define_global_rw_attr(awake_ideal_freq);
+define_global_rw_attr(sample_rate_jiffies);
+define_global_rw_attr(ramp_up_step);
+define_global_rw_attr(ramp_down_step);
+define_global_rw_attr(max_cpu_load);
+define_global_rw_attr(min_cpu_load);
+
+static struct attribute * smartass_attributes[] = {
+	&debug_mask_attr.attr,
+	&up_rate_us_attr.attr,
+	&down_rate_us_attr.attr,
+	&sleep_ideal_freq_attr.attr,
+	&sleep_wakeup_freq_attr.attr,
+	&awake_ideal_freq_attr.attr,
+	&sample_rate_jiffies_attr.attr,
+	&ramp_up_step_attr.attr,
+	&ramp_down_step_attr.attr,
+	&max_cpu_load_attr.attr,
+	&min_cpu_load_attr.attr,
+	NULL,
+};
+
+static struct attribute_group smartass_attr_group = {
+	.attrs = smartass_attributes,
+	.name = "smartassV2",
+};
+
+static int cpufreq_governor_smartass(struct cpufreq_policy *new_policy,
+		unsigned int event)
+{
+	unsigned int cpu = new_policy->cpu;
+	int rc;
+	struct smartass_info_s *this_smartass = &per_cpu(smartass_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!new_policy->cur))
+			return -EINVAL;
+
+		this_smartass->cur_policy = new_policy;
+
+		this_smartass->enable = 1;
+
+		smartass_update_min_max(this_smartass,new_policy,suspended);
+
+		this_smartass->freq_table = cpufreq_frequency_get_table(cpu);
+		if (!this_smartass->freq_table)
+			printk(KERN_WARNING "Smartass: no frequency table for cpu %d?!\n",cpu);
+
+		smp_wmb();
+
+		// Do not register the idle hook and create sysfs
+		// entries if we have already done so.
+		if (atomic_inc_return(&active_count) <= 1) {
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&smartass_attr_group);
+			if (rc)
+				return rc;
+
+			pm_idle_old = pm_idle;
+			pm_idle = cpufreq_idle;
+		}
+
+		if (this_smartass->cur_policy->cur < new_policy->max && !timer_pending(&this_smartass->timer))
+			reset_timer(cpu,this_smartass);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		smartass_update_min_max(this_smartass,new_policy,suspended);
+
+		if (this_smartass->cur_policy->cur > new_policy->max) {
+			dprintk(SMARTASS_DEBUG_JUMPS,"SmartassI: jumping to new max freq: %d\n",new_policy->max);
+			__cpufreq_driver_target(this_smartass->cur_policy,
+						new_policy->max, CPUFREQ_RELATION_H);
+		}
+		else if (this_smartass->cur_policy->cur < new_policy->min) {
+			dprintk(SMARTASS_DEBUG_JUMPS,"SmartassI: jumping to new min freq: %d\n",new_policy->min);
+			__cpufreq_driver_target(this_smartass->cur_policy,
+						new_policy->min, CPUFREQ_RELATION_C);
+		}
+
+		if (this_smartass->cur_policy->cur < new_policy->max && !timer_pending(&this_smartass->timer))
+			reset_timer(cpu,this_smartass);
+
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		this_smartass->enable = 0;
+		smp_wmb();
+		del_timer(&this_smartass->timer);
+		flush_work(&freq_scale_work);
+		this_smartass->idle_exit_time = 0;
+
+		if (atomic_dec_return(&active_count) <= 1) {
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &smartass_attr_group);
+			pm_idle = pm_idle_old;
+		}
+		break;
+	}
+
+	return 0;
+}
+
+static void smartass_suspend(int cpu, int suspend)
+{
+	struct smartass_info_s *this_smartass = &per_cpu(smartass_info, smp_processor_id());
+	struct cpufreq_policy *policy = this_smartass->cur_policy;
+	unsigned int new_freq;
+
+	if (!this_smartass->enable)
+		return;
+
+	smartass_update_min_max(this_smartass,policy,suspend);
+	if (!suspend) { // resume at max speed:
+		new_freq = validate_freq(policy,sleep_wakeup_freq);
+
+		dprintk(SMARTASS_DEBUG_JUMPS,"SmartassS: awaking at %d\n",new_freq);
+
+		__cpufreq_driver_target(policy, new_freq,
+					CPUFREQ_RELATION_C);
+	} else {
+		// to avoid wakeup issues with quick sleep/wakeup don't change actual frequency when entering sleep
+		// to allow some time to settle down. Instead we just reset our statistics (and reset the timer).
+		// Eventually, the timer will adjust the frequency if necessary.
+
+		this_smartass->freq_change_time_in_idle =
+			get_cpu_idle_time_us(cpu,&this_smartass->freq_change_time);
+
+		dprintk(SMARTASS_DEBUG_JUMPS,"SmartassS: suspending at %d\n",policy->cur);
+	}
+
+	reset_timer(smp_processor_id(),this_smartass);
+}
+
+#ifdef CONFIG_HAS_EARLYSUSPEND
+static void smartass_early_suspend(struct early_suspend *handler) {
+	int i;
+	if (suspended || sleep_ideal_freq==0) // disable behavior for sleep_ideal_freq==0
+		return;
+	suspended = 1;
+	for_each_online_cpu(i)
+		smartass_suspend(i,1);
+}
+
+static void smartass_late_resume(struct early_suspend *handler) {
+	int i;
+	if (!suspended) // already not suspended so nothing to do
+		return;
+	suspended = 0;
+	for_each_online_cpu(i)
+		smartass_suspend(i,0);
+}
+
+static struct early_suspend smartass_power_suspend = {
+	.suspend = smartass_early_suspend,
+	.resume = smartass_late_resume,
+#ifdef CONFIG_MACH_HERO
+	.level = EARLY_SUSPEND_LEVEL_DISABLE_FB + 1,
+#endif
+};
+#endif
+
+#ifdef CONFIG_POWERSUSPEND
+static void cpufreq_smartass_power_suspend(struct power_suspend *h)
+{
+	int i;
+	if (suspended || sleep_ideal_freq==0) // disable behavior for sleep_ideal_freq==0
+		return;
+	suspended = 1;
+	for_each_online_cpu(i)
+	smartass_suspend(i,1);
+}
+
+static void cpufreq_smartass_power_resume(struct power_suspend *h)
+{
+	int i;
+	if (!suspended) // already not suspended so nothing to do
+		return;
+	suspended = 0;
+	for_each_online_cpu(i)
+	smartass_suspend(i,0);
+}
+
+static struct power_suspend smartass_power_suspend = {
+	.suspend = cpufreq_smartass_power_suspend,
+	.resume = cpufreq_smartass_power_resume,
+};
+#endif
+
+static int __init cpufreq_smartass_init(void)
+{
+	unsigned int i;
+	struct smartass_info_s *this_smartass;
+	debug_mask = 0;
+	up_rate_us = DEFAULT_UP_RATE_US;
+	down_rate_us = DEFAULT_DOWN_RATE_US;
+	sleep_ideal_freq = DEFAULT_SLEEP_IDEAL_FREQ;
+	sleep_wakeup_freq = DEFAULT_SLEEP_WAKEUP_FREQ;
+	awake_ideal_freq = DEFAULT_AWAKE_IDEAL_FREQ;
+	sample_rate_jiffies = DEFAULT_SAMPLE_RATE_JIFFIES;
+	ramp_up_step = DEFAULT_RAMP_UP_STEP;
+	ramp_down_step = DEFAULT_RAMP_DOWN_STEP;
+	max_cpu_load = DEFAULT_MAX_CPU_LOAD;
+	min_cpu_load = DEFAULT_MIN_CPU_LOAD;
+
+	spin_lock_init(&cpumask_lock);
+
+	suspended = 0;
+
+	/* Initalize per-cpu data: */
+	for_each_possible_cpu(i) {
+		this_smartass = &per_cpu(smartass_info, i);
+		this_smartass->enable = 0;
+		this_smartass->cur_policy = 0;
+		this_smartass->ramp_dir = 0;
+		this_smartass->time_in_idle = 0;
+		this_smartass->idle_exit_time = 0;
+		this_smartass->freq_change_time = 0;
+		this_smartass->freq_change_time_in_idle = 0;
+		this_smartass->cur_cpu_load = 0;
+		// intialize timer:
+		init_timer_deferrable(&this_smartass->timer);
+		this_smartass->timer.function = cpufreq_smartass_timer;
+		this_smartass->timer.data = i;
+		work_cpumask_test_and_clear(i);
+	}
+
+	// Scale up is high priority
+	up_wq = alloc_workqueue("ksmartass_up", WQ_HIGHPRI, 1);
+	down_wq = alloc_workqueue("ksmartass_down", 0, 1);
+	if (!up_wq || !down_wq)
+		return -ENOMEM;
+
+	INIT_WORK(&freq_scale_work, cpufreq_smartass_freq_change_time_work);
+
+#ifdef CONFIG_HAS_EARLYSUSPEND
+	register_early_suspend(&smartass_power_suspend);
+#endif
+
+#ifdef CONFIG_POWERSUSPEND
+	register_power_suspend(&smartass_power_suspend);
+#endif
+
+	return cpufreq_register_governor(&cpufreq_gov_smartass2);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2
+fs_initcall(cpufreq_smartass_init);
+#else
+module_init(cpufreq_smartass_init);
+#endif
+
+static void __exit cpufreq_smartass_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_smartass2);
+	destroy_workqueue(up_wq);
+	destroy_workqueue(down_wq);
+}
+
+module_exit(cpufreq_smartass_exit);
+
+MODULE_AUTHOR ("Erasmux");
+MODULE_DESCRIPTION ("'cpufreq_smartass2' - A smart cpufreq governor");
+MODULE_LICENSE ("GPL");
diff --git a/drivers/cpufreq/cpufreq_smartmax.c b/drivers/cpufreq/cpufreq_smartmax.c
new file mode 100644
index 0000000..dc51b86
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_smartmax.c
@@ -0,0 +1,1282 @@
+/*
+ * drivers/cpufreq/cpufreq_smartmax.c
+ *
+ * Copyright (C) 2013 maxwen
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * Author: maxwen
+ *
+ * Based on the ondemand and smartassV2 governor
+ *
+ * ondemand:
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *            (C)  2014 LoungeKatt <twistedumbrella@gmail.com>
+ *
+ * smartassV2:
+ * Author: Erasmux
+ *
+ * For a general overview of CPU governors see the relavent part in
+ * Documentation/cpu-freq/governors.txt
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/sched.h>
+#include <linux/tick.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/moduleparam.h>
+#include <linux/jiffies.h>
+#include <linux/input.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/kernel_stat.h>
+
+#ifdef CONFIG_HAS_EARLYSUSPEND
+#include <linux/earlysuspend.h>
+#endif
+
+/******************** Tunable parameters: ********************/
+
+/*
+ * The "ideal" frequency to use. The governor will ramp up faster
+ * towards the ideal frequency and slower after it has passed it. Similarly,
+ * lowering the frequency towards the ideal frequency is faster than below it.
+ */
+
+#define GOV_IDLE_FREQ 300000
+
+#define DEFAULT_SUSPEND_IDEAL_FREQ GOV_IDLE_FREQ
+static unsigned int suspend_ideal_freq;
+
+#define DEFAULT_AWAKE_IDEAL_FREQ GOV_IDLE_FREQ
+static unsigned int awake_ideal_freq;
+
+/*
+ * Freqeuncy delta when ramping up above the ideal freqeuncy.
+ * Zero disables and causes to always jump straight to max frequency.
+ * When below the ideal freqeuncy we always ramp up to the ideal freq.
+ */
+#define DEFAULT_RAMP_UP_STEP 300000
+static unsigned int ramp_up_step;
+
+/*
+ * Freqeuncy delta when ramping down below the ideal freqeuncy.
+ * Zero disables and will calculate ramp down according to load heuristic.
+ * When above the ideal freqeuncy we always ramp down to the ideal freq.
+ */
+#define DEFAULT_RAMP_DOWN_STEP 150000
+static unsigned int ramp_down_step;
+
+/*
+ * CPU freq will be increased if measured load > max_cpu_load;
+ */
+#define DEFAULT_MAX_CPU_LOAD 80
+static unsigned int max_cpu_load;
+
+/*
+ * CPU freq will be decreased if measured load < min_cpu_load;
+ */
+#define DEFAULT_MIN_CPU_LOAD 50
+static unsigned int min_cpu_load;
+
+/*
+ * The minimum amount of time in nsecs to spend at a frequency before we can ramp up.
+ * Notice we ignore this when we are below the ideal frequency.
+ */
+#define DEFAULT_UP_RATE 40000
+static unsigned int up_rate;
+
+/*
+ * The minimum amount of time in nsecs to spend at a frequency before we can ramp down.
+ * Notice we ignore this when we are above the ideal frequency.
+ */
+#define DEFAULT_DOWN_RATE 80000
+static unsigned int down_rate;
+
+/* in nsecs */
+#define DEFAULT_SAMPLING_RATE 40000
+static unsigned int sampling_rate;
+
+/* in nsecs */
+#define DEFAULT_INPUT_BOOST_DURATION 50000000
+static unsigned int input_boost_duration;
+
+static unsigned int touch_poke_freq = 1574000;
+static bool touch_poke = true;
+
+/*
+ * should ramp_up steps during boost be possible
+ */
+static bool ramp_up_during_boost = true;
+
+/*
+ * external boost interface - boost if duration is written
+ * to sysfs for boost_duration
+ */
+static unsigned int boost_freq = 1728000;
+static bool boost = true;
+
+/* in nsecs */
+static unsigned int boost_duration = 0;
+
+/* Consider IO as busy */
+#define DEFAULT_IO_IS_BUSY 1
+static unsigned int io_is_busy;
+
+#define DEFAULT_IGNORE_NICE 1
+static unsigned int ignore_nice;
+
+/*************** End of tunables ***************/
+
+static unsigned int dbs_enable; /* number of CPUs using this policy */
+
+static void do_dbs_timer(struct work_struct *work);
+
+struct smartmax_info_s {
+	struct cpufreq_policy *cur_policy;
+	struct cpufreq_frequency_table *freq_table;
+	struct delayed_work work;
+	cputime64_t prev_cpu_idle;
+	cputime64_t prev_cpu_iowait;
+	cputime64_t prev_cpu_wall;
+	cputime64_t prev_cpu_nice;
+	cputime64_t freq_change_time;
+	unsigned int cur_cpu_load;
+	unsigned int old_freq;
+	int ramp_dir;
+	bool enable;
+	unsigned int ideal_speed;
+	unsigned int cpu;
+	struct mutex timer_mutex;
+};
+static DEFINE_PER_CPU(struct smartmax_info_s, smartmax_info);
+
+#define SMARTMAX_DEBUG 0
+
+#if SMARTMAX_DEBUG
+#define dprintk(flag,msg...) do { \
+	if (debug_mask & flag) printk(KERN_DEBUG "[smartmax]" ":" msg); \
+	} while (0)
+#else
+#define dprintk(flag,msg...)
+#endif
+
+enum {
+	SMARTMAX_DEBUG_JUMPS = 1,
+	SMARTMAX_DEBUG_LOAD = 2,
+	SMARTMAX_DEBUG_ALG = 4,
+	SMARTMAX_DEBUG_BOOST = 8,
+	SMARTMAX_DEBUG_INPUT = 16,
+	SMARTMAX_DEBUG_SUSPEND = 32
+};
+
+/*
+ * Combination of the above debug flags.
+ */
+#if SMARTMAX_DEBUG
+static unsigned long debug_mask = SMARTMAX_DEBUG_LOAD|SMARTMAX_DEBUG_JUMPS|SMARTMAX_DEBUG_ALG|SMARTMAX_DEBUG_BOOST|SMARTMAX_DEBUG_INPUT|SMARTMAX_DEBUG_SUSPEND;
+#else
+static unsigned long debug_mask;
+#endif
+
+/*
+ * dbs_mutex protects dbs_enable in governor start/stop.
+ */
+static DEFINE_MUTEX(dbs_mutex);
+
+extern int tegra_input_boost(int cpu, unsigned int target_freq);
+
+static bool boost_task_alive = false;
+static struct task_struct *boost_task;
+static cputime64_t boost_end_time = 0ULL;
+static unsigned int cur_boost_freq = 0;
+static unsigned int cur_boost_duration = 0;
+static bool boost_running = false;
+static unsigned int ideal_freq;
+static bool is_suspended = false;
+
+#ifdef CONFIG_HAS_EARLYSUSPEND
+static struct early_suspend smartmax_early_suspend_handler;
+#endif
+
+#define LATENCY_MULTIPLIER			(1000)
+#define MIN_LATENCY_MULTIPLIER			(100)
+
+static int cpufreq_governor_smartmax(struct cpufreq_policy *policy,
+		unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX
+static
+#endif
+struct cpufreq_governor cpufreq_gov_smartmax = { .name = "smartmax", .governor =
+		cpufreq_governor_smartmax, .max_transition_latency = 9000000, .owner =
+		THIS_MODULE , };
+
+//static inline cputime64_t get_cpu_idle_time_jiffy(unsigned int cpu,
+//		cputime64_t *wall) {
+//	u64 idle_time;
+//	u64 cur_wall_time;
+//	u64 busy_time;
+//
+//	cur_wall_time = jiffies64_to_cputime64(get_jiffies_64());
+//
+//	busy_time  = kcpustat_cpu(cpu).cpustat[CPUTIME_USER];
+//	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SYSTEM];
+//	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_IRQ];
+//	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SOFTIRQ];
+//	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_STEAL];
+//	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_NICE];
+//
+//	idle_time = cur_wall_time - busy_time;
+//	if (wall)
+//		*wall = jiffies_to_usecs(cur_wall_time);
+//
+//	return jiffies_to_usecs(idle_time);
+//}
+//
+//static inline cputime64_t get_cpu_idle_time(unsigned int cpu, cputime64_t *wall) {
+//	u64 idle_time = get_cpu_idle_time_us(cpu, wall);
+//
+//	if (idle_time == -1ULL)
+//		return get_cpu_idle_time_jiffy(cpu, wall);
+//
+//	return idle_time;
+//}
+
+static inline cputime64_t get_cpu_iowait_time(unsigned int cpu,
+		cputime64_t *wall) {
+	u64 iowait_time = get_cpu_iowait_time_us(cpu, wall);
+
+	if (iowait_time == -1ULL)
+		return 0;
+
+	return iowait_time;
+}
+
+inline static void smartmax_update_min_max(
+		struct smartmax_info_s *this_smartmax, struct cpufreq_policy *policy) {
+	this_smartmax->ideal_speed = // ideal_freq; but make sure it obeys the policy min/max
+			policy->min < ideal_freq ?
+					(ideal_freq < policy->max ? ideal_freq : policy->max) :
+					policy->min;
+
+}
+
+inline static void smartmax_update_min_max_allcpus(void) {
+	unsigned int i;
+
+	for_each_online_cpu(i)
+	{
+		struct smartmax_info_s *this_smartmax = &per_cpu(smartmax_info, i);
+		if (this_smartmax->enable)
+			smartmax_update_min_max(this_smartmax, this_smartmax->cur_policy);
+	}
+}
+
+inline static unsigned int validate_freq(struct cpufreq_policy *policy,
+		int freq) {
+	if (freq > (int) policy->max)
+		return policy->max;
+	if (freq < (int) policy->min)
+		return policy->min;
+	return freq;
+}
+
+/* We want all CPUs to do sampling nearly on same jiffy */
+static inline unsigned int get_timer_delay(void) {
+	unsigned int delay = usecs_to_jiffies(sampling_rate);
+
+	if (num_online_cpus() > 1)
+		delay -= jiffies % delay;
+	return delay;
+}
+
+static inline void dbs_timer_init(struct smartmax_info_s *this_smartmax) {
+	int delay = get_timer_delay();
+
+	INIT_DEFERRABLE_WORK(&this_smartmax->work, do_dbs_timer);
+	schedule_delayed_work_on(this_smartmax->cpu, &this_smartmax->work, delay);
+}
+
+static inline void dbs_timer_exit(struct smartmax_info_s *this_smartmax) {
+	cancel_delayed_work_sync(&this_smartmax->work);
+}
+
+inline static void target_freq(struct cpufreq_policy *policy,
+		struct smartmax_info_s *this_smartmax, int new_freq, int old_freq,
+		int prefered_relation) {
+	int index, target;
+	struct cpufreq_frequency_table *table = this_smartmax->freq_table;
+#if SMARTMAX_DEBUG
+	unsigned int cpu = this_smartmax->cpu;
+#endif
+
+	dprintk(SMARTMAX_DEBUG_ALG, "%d: %s\n", old_freq, __func__);
+
+	if (new_freq == old_freq)
+		return;
+	new_freq = validate_freq(policy, new_freq);
+	if (new_freq == old_freq)
+		return;
+
+	if (table
+			&& !cpufreq_frequency_table_target(policy, table, new_freq,
+					prefered_relation, &index)) {
+		target = table[index].frequency;
+		if (target == old_freq) {
+			// if for example we are ramping up to *at most* current + ramp_up_step
+			// but there is no such frequency higher than the current, try also
+			// to ramp up to *at least* current + ramp_up_step.
+			if (new_freq > old_freq && prefered_relation == CPUFREQ_RELATION_H
+					&& !cpufreq_frequency_table_target(policy, table, new_freq,
+							CPUFREQ_RELATION_C, &index))
+				target = table[index].frequency;
+			// simlarly for ramping down:
+			else if (new_freq < old_freq
+					&& prefered_relation == CPUFREQ_RELATION_C
+					&& !cpufreq_frequency_table_target(policy, table, new_freq,
+							CPUFREQ_RELATION_H, &index))
+				target = table[index].frequency;
+		}
+
+		if (target == old_freq) {
+			// We should not get here:
+			// If we got here we tried to change to a validated new_freq which is different
+			// from old_freq, so there is no reason for us to remain at same frequency.
+			dprintk(SMARTMAX_DEBUG_ALG, "%d: frequency change failed to %d (%d)\n",
+					old_freq, new_freq, target);
+			return;
+		}
+	} else
+		target = new_freq;
+
+	dprintk(SMARTMAX_DEBUG_JUMPS, "%d: jumping to %d (%d) cpu %d\n", old_freq, new_freq, target, cpu);
+
+	__cpufreq_driver_target(policy, target, prefered_relation);
+
+	// remember last time we changed frequency
+	this_smartmax->freq_change_time = ktime_to_ns(ktime_get());
+}
+
+/* We use the same work function to sale up and down */
+static void cpufreq_smartmax_freq_change(struct smartmax_info_s *this_smartmax) {
+	unsigned int cpu;
+	unsigned int new_freq = 0;
+	unsigned int old_freq;
+	int ramp_dir;
+	struct cpufreq_policy *policy;
+	unsigned int relation = CPUFREQ_RELATION_C;
+
+	ramp_dir = this_smartmax->ramp_dir;
+	old_freq = this_smartmax->old_freq;
+	policy = this_smartmax->cur_policy;
+	cpu = this_smartmax->cpu;
+
+	dprintk(SMARTMAX_DEBUG_ALG, "%d: %s\n", old_freq, __func__);
+	
+	if (old_freq != policy->cur) {
+		// frequency was changed by someone else?
+		dprintk(SMARTMAX_DEBUG_ALG, "%d: frequency changed by 3rd party to %d\n",
+				old_freq, policy->cur);
+		new_freq = old_freq;
+	} else if (ramp_dir > 0 && nr_running() > 1) {
+		// ramp up logic:
+		if (old_freq < this_smartmax->ideal_speed)
+			new_freq = this_smartmax->ideal_speed;
+		else if (ramp_up_step) {
+			new_freq = old_freq + ramp_up_step;
+			relation = CPUFREQ_RELATION_H;
+		} else {
+			new_freq = policy->max;
+			relation = CPUFREQ_RELATION_H;
+		}
+	} else if (ramp_dir < 0) {
+		// ramp down logic:
+		if (old_freq > this_smartmax->ideal_speed) {
+			new_freq = this_smartmax->ideal_speed;
+			relation = CPUFREQ_RELATION_H;
+		} else if (ramp_down_step)
+			new_freq = old_freq - ramp_down_step;
+		else {
+			// Load heuristics: Adjust new_freq such that, assuming a linear
+			// scaling of load vs. frequency, the load in the new frequency
+			// will be max_cpu_load:
+			new_freq = old_freq * this_smartmax->cur_cpu_load / max_cpu_load;
+			if (new_freq > old_freq) // min_cpu_load > max_cpu_load ?!
+				new_freq = old_freq - 1;
+		}
+	}
+
+	if (new_freq!=0){
+		target_freq(policy, this_smartmax, new_freq, old_freq, relation);
+	}
+	
+	this_smartmax->ramp_dir = 0;
+}
+
+static inline void cpufreq_smartmax_get_ramp_direction(unsigned int debug_load, unsigned int cur, struct smartmax_info_s *this_smartmax, struct cpufreq_policy *policy, cputime64_t now)
+{
+	// Scale up if load is above max or if there where no idle cycles since coming out of idle,
+	// additionally, if we are at or above the ideal_speed, verify we have been at this frequency
+	// for at least up_rate:
+	if (debug_load > max_cpu_load && cur < policy->max
+			&& (cur < this_smartmax->ideal_speed
+				|| (now - this_smartmax->freq_change_time) >= up_rate)) {
+		dprintk(SMARTMAX_DEBUG_ALG,
+				"%d: ramp up: load %d\n", cur, debug_load);
+		this_smartmax->ramp_dir = 1;
+	}
+	// Similarly for scale down: load should be below min and if we are at or below ideal
+	// frequency we require that we have been at this frequency for at least down_rate:
+	else if (debug_load < min_cpu_load && cur > policy->min
+			&& (cur > this_smartmax->ideal_speed
+				|| (now - this_smartmax->freq_change_time) >= down_rate)) {
+		dprintk(SMARTMAX_DEBUG_ALG,
+				"%d: ramp down: load %d\n", cur, debug_load);
+		this_smartmax->ramp_dir = -1;
+	}
+}
+
+static void cpufreq_smartmax_timer(struct smartmax_info_s *this_smartmax) {
+	unsigned int cur;
+	struct cpufreq_policy *policy = this_smartmax->cur_policy;
+	cputime64_t now = ktime_to_ns(ktime_get());
+	unsigned int max_load_freq;
+	unsigned int debug_load = 0;
+	unsigned int debug_iowait = 0;
+	unsigned int j = 0;
+#if SMARTMAX_DEBUG
+	unsigned int cpu = this_smartmax->cpu;
+#endif
+
+	cur = policy->cur;
+		
+	dprintk(SMARTMAX_DEBUG_ALG, "%d: %s cpu %d %lld\n", cur, __func__, cpu, now);
+
+
+	/* Get Absolute Load - in terms of freq */
+	max_load_freq = 0;
+
+	for_each_cpu(j, policy->cpus)
+	{
+		struct smartmax_info_s *j_this_smartmax;
+		cputime64_t cur_wall_time, cur_idle_time, cur_iowait_time;
+		unsigned int idle_time, wall_time, iowait_time;
+		unsigned int load, load_freq;
+		int freq_avg;
+
+		j_this_smartmax = &per_cpu(smartmax_info, j);
+
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time, io_is_busy);
+		cur_iowait_time = get_cpu_iowait_time(j, &cur_wall_time);
+
+		wall_time = cur_wall_time - j_this_smartmax->prev_cpu_wall;
+		j_this_smartmax->prev_cpu_wall = cur_wall_time;
+
+		idle_time = cur_idle_time - j_this_smartmax->prev_cpu_idle;
+		j_this_smartmax->prev_cpu_idle = cur_idle_time;
+
+		iowait_time = cur_iowait_time - j_this_smartmax->prev_cpu_iowait;
+		j_this_smartmax->prev_cpu_iowait = cur_iowait_time;
+
+		if (ignore_nice) {
+			cputime64_t cur_nice;
+			unsigned long cur_nice_jiffies;
+
+			cur_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE] -
+					 j_this_smartmax->prev_cpu_nice;
+			cur_nice_jiffies = (unsigned long)
+					cputime64_to_jiffies64(cur_nice);
+
+			j_this_smartmax->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			idle_time += jiffies_to_usecs(cur_nice_jiffies);
+		}
+
+		/*
+		 * For the purpose of ondemand, waiting for disk IO is an
+		 * indication that you're performance critical, and not that
+		 * the system is actually idle. So subtract the iowait time
+		 * from the cpu idle time.
+		 */
+
+		if (io_is_busy && idle_time >= iowait_time)
+			idle_time -= iowait_time;
+
+		if (idle_time >= iowait_time)
+			idle_time -= iowait_time;
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		load = 100 * (wall_time - idle_time) / wall_time;
+
+		freq_avg = __cpufreq_driver_getavg(policy, j);
+		if (freq_avg <= 0)
+			freq_avg = policy->cur;
+
+		load_freq = load * freq_avg;
+		if (load_freq > max_load_freq) {
+			max_load_freq = load_freq;
+			debug_load = load;
+			debug_iowait = 100 * iowait_time / wall_time;
+		}
+	}
+
+	dprintk(SMARTMAX_DEBUG_LOAD, "%d: load %d\n", cur, debug_load);
+
+	this_smartmax->cur_cpu_load = debug_load;
+	this_smartmax->old_freq = cur;
+	this_smartmax->ramp_dir = 0;
+
+	cpufreq_smartmax_get_ramp_direction(debug_load, cur, this_smartmax, policy, now);
+
+	// no changes
+	if (this_smartmax->ramp_dir == 0)		
+		return;
+
+	// boost - but not block ramp up steps based on load if requested
+	if (boost_running && time_before64 (now, boost_end_time)) {
+		dprintk(SMARTMAX_DEBUG_BOOST, "%d: boost running %llu %llu\n", cur, now, boost_end_time);
+		
+		if (this_smartmax->ramp_dir == -1)
+			return;
+		else {
+			if (ramp_up_during_boost)
+				dprintk(SMARTMAX_DEBUG_BOOST, "%d: boost running but ramp_up above boost freq requested\n", cur);
+			else
+				return;
+		}
+	} else
+		boost_running = false;
+
+	cpufreq_smartmax_freq_change(this_smartmax);
+}
+
+static void do_dbs_timer(struct work_struct *work) {
+	struct smartmax_info_s *this_smartmax =
+			container_of(work, struct smartmax_info_s, work.work);
+	unsigned int cpu = this_smartmax->cpu;
+	int delay = get_timer_delay();
+
+	mutex_lock(&this_smartmax->timer_mutex);
+
+	cpufreq_smartmax_timer(this_smartmax);
+
+	schedule_delayed_work_on(cpu, &this_smartmax->work, delay);
+	mutex_unlock(&this_smartmax->timer_mutex);
+}
+
+static void update_idle_time(bool online) {
+	int j = 0;
+
+	for_each_possible_cpu(j)
+	{
+		struct smartmax_info_s *j_this_smartmax;
+
+		if (online && !cpu_online(j)) {
+			continue;
+		}
+		j_this_smartmax = &per_cpu(smartmax_info, j);
+
+		j_this_smartmax->prev_cpu_idle = get_cpu_idle_time(j,
+				&j_this_smartmax->prev_cpu_wall, io_is_busy);
+		if (ignore_nice)
+			j_this_smartmax->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+
+	}
+}
+
+static ssize_t show_debug_mask(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%lu\n", debug_mask);
+}
+
+static ssize_t store_debug_mask(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0)
+		debug_mask = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_up_rate(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", up_rate);
+}
+
+static ssize_t store_up_rate(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0 && input <= 100000000)
+		up_rate = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_down_rate(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", down_rate);
+}
+
+static ssize_t store_down_rate(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0 && input <= 100000000)
+		down_rate = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_awake_ideal_freq(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", awake_ideal_freq);
+}
+
+static ssize_t store_awake_ideal_freq(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0) {
+		awake_ideal_freq = input;
+		if (!is_suspended){
+			ideal_freq = awake_ideal_freq;
+			smartmax_update_min_max_allcpus();
+		}
+	} else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_suspend_ideal_freq(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", suspend_ideal_freq);
+}
+
+static ssize_t store_suspend_ideal_freq(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0) {
+		suspend_ideal_freq = input;
+		if (is_suspended){
+			ideal_freq = suspend_ideal_freq;
+			smartmax_update_min_max_allcpus();
+		}
+	} else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_ramp_up_step(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", ramp_up_step);
+}
+
+static ssize_t store_ramp_up_step(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0)
+		ramp_up_step = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_ramp_down_step(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", ramp_down_step);
+}
+
+static ssize_t store_ramp_down_step(struct kobject *kobj,
+		struct attribute *attr, const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0)
+		ramp_down_step = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_max_cpu_load(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", max_cpu_load);
+}
+
+static ssize_t store_max_cpu_load(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input <= 100)
+		max_cpu_load = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_min_cpu_load(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", min_cpu_load);
+}
+
+static ssize_t store_min_cpu_load(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input < 100)
+		min_cpu_load = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_sampling_rate(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", sampling_rate);
+}
+
+static ssize_t store_sampling_rate(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 10000)
+		sampling_rate = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_touch_poke_freq(struct kobject *kobj,
+		struct attribute *attr, char *buf) {
+	return sprintf(buf, "%u\n", touch_poke_freq);
+}
+
+static ssize_t store_touch_poke_freq(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0){
+		touch_poke_freq = input;
+	
+		if (touch_poke_freq == 0)
+			touch_poke = false;
+		else
+			touch_poke = true;
+	} else
+		return -EINVAL;	
+	return count;
+}
+
+static ssize_t show_input_boost_duration(struct kobject *kobj,
+		struct attribute *attr, char *buf) {
+	return sprintf(buf, "%u\n", input_boost_duration);
+}
+
+static ssize_t store_input_boost_duration(struct kobject *a,
+		struct attribute *b, const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 10000)
+		input_boost_duration = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_ramp_up_during_boost(struct kobject *kobj,
+		struct attribute *attr, char *buf) {
+	return sprintf(buf, "%d\n", ramp_up_during_boost);
+}
+
+static ssize_t store_ramp_up_during_boost(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0) {
+		if (input == 0)
+			ramp_up_during_boost = false;
+		else if (input == 1)
+			ramp_up_during_boost = true;
+		else
+			return -EINVAL;
+	} else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_boost_freq(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", boost_freq);
+}
+
+static ssize_t store_boost_freq(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0) {
+		boost_freq = input;
+		if (boost_freq == 0)
+			boost = false;
+		else
+			boost = true;
+	} else
+		return -EINVAL;	
+	return count;
+}
+
+static ssize_t show_boost_duration(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%d\n", boost_running);
+}
+
+static ssize_t store_boost_duration(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 10000){
+		boost_duration = input;
+		if (boost) {
+			// no need to bother if currently a boost is running anyway
+			if (boost_task_alive && boost_running)
+				return count;
+
+			if (boost_task_alive) {
+				cur_boost_freq = boost_freq;
+				cur_boost_duration = boost_duration;
+				wake_up_process(boost_task);
+			}
+		}
+	} else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_io_is_busy(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%d\n", io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0) {
+		if (input > 1)
+			input = 1;
+		if (input == io_is_busy) { /* nothing to do */
+			return count;
+		}
+		io_is_busy = input;
+	} else
+		return -EINVAL;	
+	return count;
+}
+
+static ssize_t show_ignore_nice(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%d\n", ignore_nice);
+}
+
+static ssize_t store_ignore_nice(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0) {
+		if (input > 1)
+			input = 1;
+		if (input == ignore_nice) { /* nothing to do */
+			return count;
+		}
+		ignore_nice = input;
+		/* we need to re-evaluate prev_cpu_idle */
+		update_idle_time(true);
+	} else
+		return -EINVAL;	
+	return count;
+}
+
+#define define_global_rw_attr(_name)		\
+static struct global_attr _name##_attr =	\
+	__ATTR(_name, 0644, show_##_name, store_##_name)
+
+define_global_rw_attr(debug_mask);
+define_global_rw_attr(up_rate);
+define_global_rw_attr(down_rate);
+define_global_rw_attr(ramp_up_step);
+define_global_rw_attr(ramp_down_step);
+define_global_rw_attr(max_cpu_load);
+define_global_rw_attr(min_cpu_load);
+define_global_rw_attr(sampling_rate);
+define_global_rw_attr(touch_poke_freq);
+define_global_rw_attr(input_boost_duration);
+define_global_rw_attr(boost_freq);
+define_global_rw_attr(boost_duration);
+define_global_rw_attr(io_is_busy);
+define_global_rw_attr(ignore_nice);
+define_global_rw_attr(ramp_up_during_boost);
+define_global_rw_attr(awake_ideal_freq);
+define_global_rw_attr(suspend_ideal_freq);
+
+static struct attribute * smartmax_attributes[] = { 
+	&debug_mask_attr.attr,
+	&up_rate_attr.attr, 
+	&down_rate_attr.attr, 
+	&ramp_up_step_attr.attr, 
+	&ramp_down_step_attr.attr,
+	&max_cpu_load_attr.attr, 
+	&min_cpu_load_attr.attr,
+	&sampling_rate_attr.attr, 
+	&touch_poke_freq_attr.attr,
+	&input_boost_duration_attr.attr, 
+	&boost_freq_attr.attr, 
+	&boost_duration_attr.attr, 
+	&io_is_busy_attr.attr,
+	&ignore_nice_attr.attr, 
+	&ramp_up_during_boost_attr.attr, 
+	&awake_ideal_freq_attr.attr,
+	&suspend_ideal_freq_attr.attr,		
+	NULL , };
+
+static struct attribute_group smartmax_attr_group = { .attrs =
+		smartmax_attributes, .name = "smartmax", };
+
+static int cpufreq_smartmax_boost_task(void *data) {
+	struct cpufreq_policy *policy;
+	struct smartmax_info_s *this_smartmax;
+	cputime64_t now;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule();
+
+		if (kthread_should_stop())
+			break;
+
+		set_current_state(TASK_RUNNING);
+
+		if (boost_running)
+			continue;
+		
+		/* we always boost cpu 0 */
+		this_smartmax = &per_cpu(smartmax_info, 0);
+		if (!this_smartmax)
+			continue;
+
+		policy = this_smartmax->cur_policy;
+		if (!policy)
+			continue;
+
+//		if (lock_policy_rwsem_write(0) < 0)
+//			continue;
+
+		mutex_lock(&this_smartmax->timer_mutex);
+
+		if (policy->cur < cur_boost_freq) {
+			boost_running = true;
+		
+			now = ktime_to_ns(ktime_get());
+			boost_end_time = now + cur_boost_duration;
+			dprintk(SMARTMAX_DEBUG_BOOST, "%s %llu %llu\n", __func__, now, boost_end_time);
+
+			target_freq(policy, this_smartmax, cur_boost_freq, this_smartmax->old_freq, CPUFREQ_RELATION_H);
+			this_smartmax->prev_cpu_idle = get_cpu_idle_time(0, &this_smartmax->prev_cpu_wall, io_is_busy);
+		}
+		mutex_unlock(&this_smartmax->timer_mutex);
+				
+//		unlock_policy_rwsem_write(0);
+	}
+
+	return 0;
+}
+
+static void dbs_input_event(struct input_handle *handle, unsigned int type,
+		unsigned int code, int value) {
+	if (touch_poke && type == EV_SYN && code == SYN_REPORT) {
+		// no need to bother if currently a boost is running anyway
+		if (boost_task_alive && boost_running)
+			return;
+
+		if (boost_task_alive) {
+			cur_boost_freq = touch_poke_freq;
+			cur_boost_duration = input_boost_duration;
+			wake_up_process(boost_task);
+		}
+	}
+}
+
+static int input_dev_filter(const char* input_dev_name) {
+	int ret = 0;
+	if (strstr(input_dev_name, "touchscreen")
+			|| strstr(input_dev_name, "-keypad")
+			|| strstr(input_dev_name, "-nav")
+			|| strstr(input_dev_name, "-oj")) {
+	} else {
+		ret = 1;
+	}
+	return ret;
+}
+
+static int dbs_input_connect(struct input_handler *handler,
+		struct input_dev *dev, const struct input_device_id *id) {
+	struct input_handle *handle;
+	int error;
+
+	/* filter out those input_dev that we don't care */
+	if (input_dev_filter(dev->name))
+		return 0;
+
+	dprintk(SMARTMAX_DEBUG_INPUT, "%s\n", __func__);
+
+	handle = kzalloc(sizeof(struct input_handle), GFP_KERNEL);
+	if (!handle)
+		return -ENOMEM;
+
+	handle->dev = dev;
+	handle->handler = handler;
+	handle->name = "cpufreq";
+
+	error = input_register_handle(handle);
+	if (error)
+		goto err2;
+
+	error = input_open_device(handle);
+	if (error)
+		goto err1;
+
+	return 0;
+	err1: input_unregister_handle(handle);
+	err2: kfree(handle);
+	return error;
+}
+
+static void dbs_input_disconnect(struct input_handle *handle) {
+	input_close_device(handle);
+	input_unregister_handle(handle);
+	kfree(handle);
+}
+
+static const struct input_device_id dbs_ids[] = { { .driver_info = 1 }, { }, };
+
+static struct input_handler dbs_input_handler = { .event = dbs_input_event,
+		.connect = dbs_input_connect, .disconnect = dbs_input_disconnect,
+		.name = "cpufreq_smartmax", .id_table = dbs_ids, };
+
+#ifdef CONFIG_HAS_EARLYSUSPEND
+static void smartmax_early_suspend(struct early_suspend *h)
+{
+	dprintk(SMARTMAX_DEBUG_SUSPEND, "%s\n", __func__);
+	ideal_freq = suspend_ideal_freq;
+	is_suspended = true;
+	smartmax_update_min_max_allcpus();
+}
+
+static void smartmax_late_resume(struct early_suspend *h)
+{
+	dprintk(SMARTMAX_DEBUG_SUSPEND, "%s\n", __func__);
+	ideal_freq = awake_ideal_freq;
+	is_suspended = false;
+	smartmax_update_min_max_allcpus();
+}
+#endif
+
+static int cpufreq_governor_smartmax(struct cpufreq_policy *new_policy,
+		unsigned int event) {
+	unsigned int cpu = new_policy->cpu;
+	int rc;
+	struct smartmax_info_s *this_smartmax = &per_cpu(smartmax_info, cpu);
+	struct sched_param param = { .sched_priority = 1 };
+    unsigned int latency;
+    unsigned int min_sampling_rate;
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!new_policy->cur))return -EINVAL;
+
+		mutex_lock(&dbs_mutex);
+
+		this_smartmax->cur_policy = new_policy;
+		this_smartmax->cpu = cpu;
+		this_smartmax->enable = true;
+
+		smartmax_update_min_max(this_smartmax,new_policy);
+
+		this_smartmax->freq_table = cpufreq_frequency_get_table(cpu);
+
+		update_idle_time(false);
+
+		dbs_enable++;
+		
+		if (dbs_enable == 1) {
+			if (!boost_task_alive) {
+				boost_task = kthread_create (
+						cpufreq_smartmax_boost_task,
+						NULL,
+						"kinputboostd"
+				);
+
+				if (IS_ERR(boost_task)) {
+					dbs_enable--;
+					mutex_unlock(&dbs_mutex);
+					return PTR_ERR(boost_task);
+				}
+
+				sched_setscheduler_nocheck(boost_task, SCHED_RR, &param);
+				get_task_struct(boost_task);
+				boost_task_alive = true;
+			}
+			rc = input_register_handler(&dbs_input_handler);
+			if (rc) {
+				dbs_enable--;
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+			rc = sysfs_create_group(cpufreq_global_kobject,
+					&smartmax_attr_group);
+			if (rc) {
+				dbs_enable--;
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+#ifdef CONFIG_HAS_EARLYSUSPEND
+			register_early_suspend(&smartmax_early_suspend_handler);
+#endif
+			latency = new_policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+			
+			min_sampling_rate = max(sampling_rate, MIN_LATENCY_MULTIPLIER * latency);
+			sampling_rate = max(min_sampling_rate, latency * LATENCY_MULTIPLIER);
+		}
+
+		mutex_unlock(&dbs_mutex);
+		mutex_init(&this_smartmax->timer_mutex);
+		dbs_timer_init(this_smartmax);
+
+		break;
+	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&this_smartmax->timer_mutex);
+		smartmax_update_min_max(this_smartmax,new_policy);
+
+		if (this_smartmax->cur_policy->cur > new_policy->max) {
+			dprintk(SMARTMAX_DEBUG_JUMPS,"jumping to new max freq: %d\n",new_policy->max);
+			__cpufreq_driver_target(this_smartmax->cur_policy,
+					new_policy->max, CPUFREQ_RELATION_H);
+		}
+		else if (this_smartmax->cur_policy->cur < new_policy->min) {
+			dprintk(SMARTMAX_DEBUG_JUMPS,"jumping to new min freq: %d\n",new_policy->min);
+			__cpufreq_driver_target(this_smartmax->cur_policy,
+					new_policy->min, CPUFREQ_RELATION_C);
+		}
+		mutex_unlock(&this_smartmax->timer_mutex);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		dbs_timer_exit(this_smartmax);
+
+		mutex_lock(&dbs_mutex);
+		mutex_destroy(&this_smartmax->timer_mutex);
+		this_smartmax->enable = false;
+		dbs_enable--;
+
+		if (!dbs_enable){
+			sysfs_remove_group(cpufreq_global_kobject, &smartmax_attr_group);
+			input_unregister_handler(&dbs_input_handler);
+#ifdef CONFIG_HAS_EARLYSUSPEND
+			unregister_early_suspend(&smartmax_early_suspend_handler);
+#endif
+		}
+		
+		mutex_unlock(&dbs_mutex);
+		break;
+	}
+
+	return 0;
+}
+
+static int __init cpufreq_smartmax_init(void) {
+	unsigned int i;
+	struct smartmax_info_s *this_smartmax;
+	up_rate = DEFAULT_UP_RATE;
+	down_rate = DEFAULT_DOWN_RATE;
+	suspend_ideal_freq = DEFAULT_SUSPEND_IDEAL_FREQ;
+	awake_ideal_freq = DEFAULT_AWAKE_IDEAL_FREQ;
+	ideal_freq = awake_ideal_freq;
+	ramp_up_step = DEFAULT_RAMP_UP_STEP;
+	ramp_down_step = DEFAULT_RAMP_DOWN_STEP;
+	max_cpu_load = DEFAULT_MAX_CPU_LOAD;
+	min_cpu_load = DEFAULT_MIN_CPU_LOAD;
+	sampling_rate = DEFAULT_SAMPLING_RATE;
+	input_boost_duration = DEFAULT_INPUT_BOOST_DURATION;
+	io_is_busy = DEFAULT_IO_IS_BUSY;
+	ignore_nice = DEFAULT_IGNORE_NICE;
+
+	/* Initalize per-cpu data: */for_each_possible_cpu(i)
+	{
+		this_smartmax = &per_cpu(smartmax_info, i);
+		this_smartmax->enable = false;
+		this_smartmax->cur_policy = 0;
+		this_smartmax->ramp_dir = 0;
+		this_smartmax->freq_change_time = 0;
+		this_smartmax->cur_cpu_load = 0;
+	}
+
+#ifdef CONFIG_HAS_EARLYSUSPEND
+	smartmax_early_suspend_handler.suspend = smartmax_early_suspend;
+	smartmax_early_suspend_handler.resume = smartmax_late_resume;
+	smartmax_early_suspend_handler.level = EARLY_SUSPEND_LEVEL_DISABLE_FB + 100;
+#endif
+	
+	return cpufreq_register_governor(&cpufreq_gov_smartmax);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX
+fs_initcall(cpufreq_smartmax_init);
+#else
+module_init(cpufreq_smartmax_init);
+#endif
+
+static void __exit cpufreq_smartmax_exit(void) {
+	cpufreq_unregister_governor(&cpufreq_gov_smartmax);
+}
+
+module_exit(cpufreq_smartmax_exit);
+
+MODULE_AUTHOR("maxwen");
+MODULE_DESCRIPTION("'cpufreq_smartmax' - A smart cpufreq governor");
+MODULE_LICENSE("GPL");
diff --git a/drivers/cpufreq/cpufreq_smartmax_eps.c b/drivers/cpufreq/cpufreq_smartmax_eps.c
new file mode 100644
index 0000000..114a263
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_smartmax_eps.c
@@ -0,0 +1,1439 @@
+/*
+ * drivers/cpufreq/cpufreq_smartmax_eps.c
+ *
+ * Copyright (C) 2013 maxwen
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * Author: maxwen
+ *
+ * Based on the ondemand and smartassV2 governor
+ *
+ * ondemand:
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *
+ * smartassV2:
+ * Author: Erasmux
+ *
+ * For a general overview of CPU governors see the relavent part in
+ * Documentation/cpu-freq/governors.txt
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/sched.h>
+#include <linux/tick.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/moduleparam.h>
+#include <linux/jiffies.h>
+#include <linux/input.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/kernel_stat.h>
+
+#ifdef CONFIG_CPU_FREQ_GOV_SMARTMAX_EPS_TEGRA
+extern int tegra_input_boost (struct cpufreq_policy *policy,
+		       unsigned int target_freq,
+		       unsigned int relation);
+#endif
+
+/******************** Tunable parameters: ********************/
+
+/*
+ * The "ideal" frequency to use. The governor will ramp up faster
+ * towards the ideal frequency and slower after it has passed it. Similarly,
+ * lowering the frequency towards the ideal frequency is faster than below it.
+ */
+
+
+#ifdef CONFIG_CPU_FREQ_GOV_SMARTMAX_EPS_ENRC2B
+#define DEFAULT_SUSPEND_IDEAL_FREQ 475000
+#define DEFAULT_AWAKE_IDEAL_FREQ 475000
+#define DEFAULT_RAMP_UP_STEP 300000
+#define DEFAULT_RAMP_DOWN_STEP 150000
+#define DEFAULT_MAX_CPU_LOAD 80
+#define DEFAULT_MIN_CPU_LOAD 50
+#define DEFAULT_UP_RATE 30000
+#define DEFAULT_DOWN_RATE 60000
+#define DEFAULT_SAMPLING_RATE 30000
+// default to 3 * sampling_rate
+#define DEFAULT_INPUT_BOOST_DURATION 90000
+#define DEFAULT_TOUCH_POKE_FREQ 910000
+#define DEFAULT_BOOST_FREQ 910000
+/*
+ * from cpufreq_wheatley.c
+ * Not all CPUs want IO time to be accounted as busy; this dependson how
+ * efficient idling at a higher frequency/voltage is.
+ * Pavel Machek says this is not so for various generations of AMD and old
+ * Intel systems.
+ * Mike Chan (androidlcom) calis this is also not true for ARM.
+ */
+#define DEFAULT_IO_IS_BUSY 0
+#define DEFAULT_IGNORE_NICE 1
+#endif
+
+// msm8974 platform
+#ifdef CONFIG_CPU_FREQ_GOV_SMARTMAX_EPS
+#define DEFAULT_SUSPEND_IDEAL_FREQ 300000
+#define DEFAULT_AWAKE_IDEAL_FREQ 652800
+#define DEFAULT_RAMP_UP_STEP 200000
+#define DEFAULT_RAMP_DOWN_STEP 200000
+#define DEFAULT_MAX_CPU_LOAD 70
+#define DEFAULT_MIN_CPU_LOAD 40
+#define DEFAULT_UP_RATE 30000
+#define DEFAULT_DOWN_RATE 60000
+#define DEFAULT_SAMPLING_RATE 30000
+#define DEFAULT_INPUT_BOOST_DURATION 90000
+#define DEFAULT_TOUCH_POKE_FREQ 1036800
+#define DEFAULT_BOOST_FREQ 1497600
+#define DEFAULT_IO_IS_BUSY 0
+#define DEFAULT_IGNORE_NICE 1
+#endif
+
+#ifdef CONFIG_CPU_FREQ_GOV_SMARTMAX_EPS_PRIMOU
+#define DEFAULT_SUSPEND_IDEAL_FREQ 368000
+#define DEFAULT_AWAKE_IDEAL_FREQ 806000
+#define DEFAULT_RAMP_UP_STEP 200000
+#define DEFAULT_RAMP_DOWN_STEP 200000
+#define DEFAULT_MAX_CPU_LOAD 60
+#define DEFAULT_MIN_CPU_LOAD 30
+#define DEFAULT_UP_RATE 30000
+#define DEFAULT_DOWN_RATE 60000
+#define DEFAULT_SAMPLING_RATE 30000
+#define DEFAULT_INPUT_BOOST_DURATION 90000
+#define DEFAULT_TOUCH_POKE_FREQ 1200000
+#define DEFAULT_BOOST_FREQ 1200000
+#define DEFAULT_IO_IS_BUSY 0
+#define DEFAULT_IGNORE_NICE 1
+#endif
+
+#ifdef CONFIG_CPU_FREQ_GOV_SMARTMAX_EPS_M7
+#define DEFAULT_SUSPEND_IDEAL_FREQ 384000
+#define DEFAULT_AWAKE_IDEAL_FREQ 594000
+#define DEFAULT_RAMP_UP_STEP 200000
+#define DEFAULT_RAMP_DOWN_STEP 200000
+#define DEFAULT_MAX_CPU_LOAD 70
+#define DEFAULT_MIN_CPU_LOAD 40
+#define DEFAULT_UP_RATE 30000
+#define DEFAULT_DOWN_RATE 60000
+#define DEFAULT_SAMPLING_RATE 30000
+#define DEFAULT_INPUT_BOOST_DURATION 90000
+#define DEFAULT_TOUCH_POKE_FREQ 1134000
+#define DEFAULT_BOOST_FREQ 1134000
+#define DEFAULT_IO_IS_BUSY 0
+#define DEFAULT_IGNORE_NICE 1
+#endif
+
+#ifdef CONFIG_CPU_FREQ_GOV_SMARTMAX_EPS_FIND5
+#define DEFAULT_SUSPEND_IDEAL_FREQ 384000
+#define DEFAULT_AWAKE_IDEAL_FREQ 594000
+#define DEFAULT_RAMP_UP_STEP 200000
+#define DEFAULT_RAMP_DOWN_STEP 200000
+#define DEFAULT_MAX_CPU_LOAD 90
+#define DEFAULT_MIN_CPU_LOAD 60
+#define DEFAULT_UP_RATE 30000
+#define DEFAULT_DOWN_RATE 60000
+#define DEFAULT_SAMPLING_RATE 30000
+#define DEFAULT_INPUT_BOOST_DURATION 900000
+#define DEFAULT_TOUCH_POKE_FREQ 1134000
+#define DEFAULT_BOOST_FREQ 1134000
+#define DEFAULT_IO_IS_BUSY 0
+#define DEFAULT_IGNORE_NICE 1
+#endif
+
+static unsigned int suspend_ideal_freq;
+static unsigned int awake_ideal_freq;
+/*
+ * Freqeuncy delta when ramping up above the ideal freqeuncy.
+ * Zero disables and causes to always jump straight to max frequency.
+ * When below the ideal freqeuncy we always ramp up to the ideal freq.
+ */
+static unsigned int ramp_up_step;
+
+/*
+ * Freqeuncy delta when ramping down below the ideal freqeuncy.
+ * Zero disables and will calculate ramp down according to load heuristic.
+ * When above the ideal freqeuncy we always ramp down to the ideal freq.
+ */
+static unsigned int ramp_down_step;
+
+/*
+ * CPU freq will be increased if measured load > max_cpu_load;
+ */
+static unsigned int max_cpu_load;
+
+/*
+ * CPU freq will be decreased if measured load < min_cpu_load;
+ */
+static unsigned int min_cpu_load;
+
+/*
+ * The minimum amount of time in usecs to spend at a frequency before we can ramp up.
+ * Notice we ignore this when we are below the ideal frequency.
+ */
+static unsigned int up_rate;
+
+/*
+ * The minimum amount of time in usecs to spend at a frequency before we can ramp down.
+ * Notice we ignore this when we are above the ideal frequency.
+ */
+static unsigned int down_rate;
+
+/* in usecs */
+static unsigned int sampling_rate;
+
+/* in usecs */
+static unsigned int input_boost_duration;
+
+static unsigned int touch_poke_freq;
+static bool touch_poke = true;
+
+/*
+ * should ramp_up steps during boost be possible
+ */
+static bool ramp_up_during_boost = true;
+
+/*
+ * external boost interface - boost if duration is written
+ * to sysfs for boost_duration
+ */
+static unsigned int boost_freq;
+static bool boost = true;
+
+/* in usecs */
+static unsigned int boost_duration = 0;
+
+/* Consider IO as busy */
+static unsigned int io_is_busy;
+
+static unsigned int ignore_nice;
+
+/*************** End of tunables ***************/
+
+static unsigned int dbs_enable; /* number of CPUs using this policy */
+
+static void do_dbs_timer(struct work_struct *work);
+
+struct smartmax_eps_info_s {
+	struct cpufreq_policy *cur_policy;
+	struct cpufreq_frequency_table *freq_table;
+	struct delayed_work work;
+	u64 prev_cpu_idle;
+	u64 prev_cpu_iowait;
+	u64 prev_cpu_wall;
+	u64 prev_cpu_nice;
+	u64 freq_change_time;
+	unsigned int cur_cpu_load;
+	unsigned int old_freq;
+	int ramp_dir;
+	unsigned int ideal_speed;
+	unsigned int cpu;
+	struct mutex timer_mutex;
+};
+static DEFINE_PER_CPU(struct smartmax_eps_info_s, smartmax_eps_info);
+
+#define dprintk(flag,msg...) do { \
+	if (debug_mask & flag) pr_info("[smartmax_eps]" ":" msg); \
+	} while (0)
+
+enum {
+	SMARTMAX_EPS_DEBUG_JUMPS = 1,
+	SMARTMAX_EPS_DEBUG_LOAD = 2,
+	SMARTMAX_EPS_DEBUG_ALG = 4,
+	SMARTMAX_EPS_DEBUG_BOOST = 8,
+	SMARTMAX_EPS_DEBUG_INPUT = 16,
+	SMARTMAX_EPS_DEBUG_SUSPEND = 32
+};
+
+/*
+ * Combination of the above debug flags.
+ */
+//static unsigned long debug_mask = SMARTMAX_EPS_DEBUG_LOAD|SMARTMAX_EPS_DEBUG_JUMPS|SMARTMAX_EPS_DEBUG_ALG|SMARTMAX_EPS_DEBUG_BOOST|SMARTMAX_EPS_DEBUG_INPUT|SMARTMAX_EPS_DEBUG_SUSPEND;
+static unsigned long debug_mask;
+
+#define SMARTMAX_EPS_STAT 0
+#if SMARTMAX_EPS_STAT
+static u64 timer_stat[4] = {0, 0, 0, 0};
+#endif
+
+/*
+ * dbs_mutex protects dbs_enable in governor start/stop.
+ */
+static DEFINE_MUTEX(dbs_mutex);
+static struct workqueue_struct *smartmax_eps_wq;
+
+static bool boost_task_alive = false;
+static struct task_struct *boost_task;
+static u64 boost_end_time = 0ULL;
+static unsigned int cur_boost_freq = 0;
+static unsigned int cur_boost_duration = 0;
+static bool boost_running = false;
+static unsigned int ideal_freq;
+static bool is_suspended = false;
+static unsigned int min_sampling_rate;
+
+#define LATENCY_MULTIPLIER			(1000)
+#define MIN_LATENCY_MULTIPLIER			(100)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+/*
+ * The polling frequency of this governor depends on the capability of
+ * the processor. Default polling frequency is 1000 times the transition
+ * latency of the processor. The governor will work on any processor with
+ * transition latency <= 10mS, using appropriate sampling
+ * rate.
+ * For CPUs with transition latency > 10mS (mostly drivers with CPUFREQ_ETERNAL)
+ * this governor will not work.
+ * All times here are in uS.
+ */
+#define MIN_SAMPLING_RATE_RATIO			(2)
+#define MICRO_FREQUENCY_MIN_SAMPLE_RATE		(10000)
+
+static int cpufreq_governor_smartmax_eps(struct cpufreq_policy *policy,
+		unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX_EPS
+static
+#endif
+struct cpufreq_governor cpufreq_gov_smartmax_eps = { 
+    .name = "smartmax_eps", 
+    .governor = cpufreq_governor_smartmax_eps, 
+    .max_transition_latency = TRANSITION_LATENCY_LIMIT, 
+    .owner = THIS_MODULE,
+    };
+
+static inline u64 get_cpu_iowait_time(unsigned int cpu, u64 *wall) {
+	u64 iowait_time = get_cpu_iowait_time_us(cpu, wall);
+
+	if (iowait_time == -1ULL)
+		return 0;
+
+	return iowait_time;
+}
+
+inline static void smartmax_eps_update_min_max(
+		struct smartmax_eps_info_s *this_smartmax_eps, struct cpufreq_policy *policy) {
+	this_smartmax_eps->ideal_speed = // ideal_freq; but make sure it obeys the policy min/max
+			policy->min < ideal_freq ?
+					(ideal_freq < policy->max ? ideal_freq : policy->max) :
+					policy->min;
+
+}
+
+inline static void smartmax_eps_update_min_max_allcpus(void) {
+	unsigned int cpu;
+
+	for_each_online_cpu(cpu)
+	{
+		struct smartmax_eps_info_s *this_smartmax_eps = &per_cpu(smartmax_eps_info, cpu);
+		if (this_smartmax_eps->cur_policy){
+//			if (lock_policy_rwsem_write(cpu) < 0)
+//				continue;
+
+			smartmax_eps_update_min_max(this_smartmax_eps, this_smartmax_eps->cur_policy);
+			
+//			unlock_policy_rwsem_write(cpu);
+		}
+	}
+}
+
+inline static unsigned int validate_freq(struct cpufreq_policy *policy,
+		int freq) {
+	if (freq > (int) policy->max)
+		return policy->max;
+	if (freq < (int) policy->min)
+		return policy->min;
+	return freq;
+}
+
+/* We want all CPUs to do sampling nearly on same jiffy */
+static inline unsigned int get_timer_delay(void) {
+	unsigned int delay = usecs_to_jiffies(sampling_rate);
+
+	if (num_online_cpus() > 1)
+		delay -= jiffies % delay;
+	return delay;
+}
+
+static inline void dbs_timer_init(struct smartmax_eps_info_s *this_smartmax_eps) {
+	int delay = get_timer_delay();
+
+	INIT_DEFERRABLE_WORK(&this_smartmax_eps->work, do_dbs_timer);
+	schedule_delayed_work_on(this_smartmax_eps->cpu, &this_smartmax_eps->work, delay);
+}
+
+static inline void dbs_timer_exit(struct smartmax_eps_info_s *this_smartmax_eps) {
+	cancel_delayed_work_sync(&this_smartmax_eps->work);
+}
+
+inline static void target_freq(struct cpufreq_policy *policy,
+		struct smartmax_eps_info_s *this_smartmax_eps, int new_freq, int old_freq,
+		int prefered_relation) {
+	int index, target;
+	struct cpufreq_frequency_table *table = this_smartmax_eps->freq_table;
+	unsigned int cpu = this_smartmax_eps->cpu;
+
+	dprintk(SMARTMAX_EPS_DEBUG_ALG, "%d: %s\n", old_freq, __func__);
+
+	// apply policy limits - just to be sure
+	new_freq = validate_freq(policy, new_freq);
+
+	if (!cpufreq_frequency_table_target(policy, table, new_freq,
+					prefered_relation, &index)) {
+		target = table[index].frequency;
+		if (target == old_freq) {
+			// if for example we are ramping up to *at most* current + ramp_up_step
+			// but there is no such frequency higher than the current, try also
+			// to ramp up to *at least* current + ramp_up_step.
+			if (new_freq > old_freq && prefered_relation == CPUFREQ_RELATION_H
+					&& !cpufreq_frequency_table_target(policy, table, new_freq,
+							CPUFREQ_RELATION_L, &index))
+				target = table[index].frequency;
+			// simlarly for ramping down:
+			else if (new_freq < old_freq
+					&& prefered_relation == CPUFREQ_RELATION_L
+					&& !cpufreq_frequency_table_target(policy, table, new_freq,
+							CPUFREQ_RELATION_H, &index))
+				target = table[index].frequency;
+		}
+
+		// no change
+		if (target == old_freq)
+			return;
+	} else {
+		dprintk(SMARTMAX_EPS_DEBUG_ALG, "frequency change failed\n");
+		return;
+	}
+
+	dprintk(SMARTMAX_EPS_DEBUG_JUMPS, "%d: jumping to %d (%d) cpu %d\n", old_freq, new_freq, target, cpu);
+
+	__cpufreq_driver_target(policy, target, prefered_relation);
+
+	// remember last time we changed frequency
+	this_smartmax_eps->freq_change_time = ktime_to_us(ktime_get());
+}
+
+/* We use the same work function to sale up and down */
+static void cpufreq_smartmax_eps_freq_change(struct smartmax_eps_info_s *this_smartmax_eps) {
+	unsigned int cpu;
+	unsigned int new_freq = 0;
+	unsigned int old_freq;
+	int ramp_dir;
+	struct cpufreq_policy *policy;
+	unsigned int relation = CPUFREQ_RELATION_L;
+
+	ramp_dir = this_smartmax_eps->ramp_dir;
+	old_freq = this_smartmax_eps->old_freq;
+	policy = this_smartmax_eps->cur_policy;
+	cpu = this_smartmax_eps->cpu;
+
+	dprintk(SMARTMAX_EPS_DEBUG_ALG, "%d: %s\n", old_freq, __func__);
+	
+	if (old_freq != policy->cur) {
+		// frequency was changed by someone else?
+		dprintk(SMARTMAX_EPS_DEBUG_ALG, "%d: frequency changed by 3rd party to %d\n",
+				old_freq, policy->cur);
+		new_freq = old_freq;
+	} else if (ramp_dir > 0 && nr_running() > 1) {
+		// ramp up logic:
+		if (old_freq < this_smartmax_eps->ideal_speed)
+			new_freq = this_smartmax_eps->ideal_speed;
+		else if (ramp_up_step) {
+			new_freq = old_freq + ramp_up_step;
+			relation = CPUFREQ_RELATION_H;
+		} else {
+			new_freq = policy->max;
+			relation = CPUFREQ_RELATION_H;
+		}
+	} else if (ramp_dir < 0) {
+		// ramp down logic:
+		if (old_freq > this_smartmax_eps->ideal_speed) {
+			new_freq = this_smartmax_eps->ideal_speed;
+			relation = CPUFREQ_RELATION_H;
+		} else if (ramp_down_step)
+			new_freq = old_freq - ramp_down_step;
+		else {
+			// Load heuristics: Adjust new_freq such that, assuming a linear
+			// scaling of load vs. frequency, the load in the new frequency
+			// will be max_cpu_load:
+			new_freq = old_freq * this_smartmax_eps->cur_cpu_load / max_cpu_load;
+			if (new_freq > old_freq) // min_cpu_load > max_cpu_load ?!
+				new_freq = old_freq - 1;
+		}
+	}
+
+	if (new_freq!=0){
+		target_freq(policy, this_smartmax_eps, new_freq, old_freq, relation);
+	}
+	
+	this_smartmax_eps->ramp_dir = 0;
+}
+
+static inline void cpufreq_smartmax_eps_get_ramp_direction(struct smartmax_eps_info_s *this_smartmax_eps, u64 now)
+{
+	unsigned int cur_load = this_smartmax_eps->cur_cpu_load;
+	unsigned int cur = this_smartmax_eps->old_freq;
+	struct cpufreq_policy *policy = this_smartmax_eps->cur_policy;
+	
+	// Scale up if load is above max or if there where no idle cycles since coming out of idle,
+	// additionally, if we are at or above the ideal_speed, verify we have been at this frequency
+	// for at least up_rate:
+	if (cur_load > max_cpu_load && cur < policy->max
+			&& (cur < this_smartmax_eps->ideal_speed
+				|| (now - this_smartmax_eps->freq_change_time) >= up_rate)) {
+		dprintk(SMARTMAX_EPS_DEBUG_ALG,
+				"%d: ramp up: load %d\n", cur, cur_load);
+		this_smartmax_eps->ramp_dir = 1;
+	}
+	// Similarly for scale down: load should be below min and if we are at or below ideal
+	// frequency we require that we have been at this frequency for at least down_rate:
+	else if (cur_load < min_cpu_load && cur > policy->min
+			&& (cur > this_smartmax_eps->ideal_speed
+				|| (now - this_smartmax_eps->freq_change_time) >= down_rate)) {
+		dprintk(SMARTMAX_EPS_DEBUG_ALG,
+				"%d: ramp down: load %d\n", cur, cur_load);
+		this_smartmax_eps->ramp_dir = -1;
+	}
+}
+
+static void inline cpufreq_smartmax_eps_calc_load(int j)
+{
+	struct smartmax_eps_info_s *j_this_smartmax_eps;
+	u64 cur_wall_time, cur_idle_time, cur_iowait_time;
+	unsigned int idle_time, wall_time, iowait_time;
+	unsigned int cur_load;
+		
+	j_this_smartmax_eps = &per_cpu(smartmax_eps_info, j);
+
+	cur_idle_time = get_cpu_idle_time(j, &cur_wall_time, 0);
+	cur_iowait_time = get_cpu_iowait_time(j, &cur_wall_time);
+
+	wall_time = cur_wall_time - j_this_smartmax_eps->prev_cpu_wall;
+	j_this_smartmax_eps->prev_cpu_wall = cur_wall_time;
+
+	idle_time = cur_idle_time - j_this_smartmax_eps->prev_cpu_idle;
+	j_this_smartmax_eps->prev_cpu_idle = cur_idle_time;
+
+	iowait_time = cur_iowait_time - j_this_smartmax_eps->prev_cpu_iowait;
+	j_this_smartmax_eps->prev_cpu_iowait = cur_iowait_time;
+
+	if (ignore_nice) {
+		u64 cur_nice;
+		unsigned long cur_nice_jiffies;
+
+#ifdef CONFIG_CPU_FREQ_GOV_SMARTMAX_EPS_30
+		cur_nice = kstat_cpu(j).cpustat.nice - j_this_smartmax_eps->prev_cpu_nice;
+		cur_nice_jiffies = (unsigned long) cputime64_to_jiffies64(cur_nice);
+
+		j_this_smartmax_eps->prev_cpu_nice = kstat_cpu(j).cpustat.nice;
+#else
+		cur_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE] - j_this_smartmax_eps->prev_cpu_nice;
+		cur_nice_jiffies = (unsigned long) cputime64_to_jiffies64(cur_nice);
+
+		j_this_smartmax_eps->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+
+#endif
+
+		idle_time += jiffies_to_usecs(cur_nice_jiffies);
+	}
+
+	/*
+	 * For the purpose of ondemand, waiting for disk IO is an
+	 * indication that you're performance critical, and not that
+	 * the system is actually idle. So subtract the iowait time
+	 * from the cpu idle time.
+	 */
+	if (io_is_busy && idle_time >= iowait_time)
+		idle_time -= iowait_time;
+
+	if (unlikely(!wall_time || wall_time < idle_time))
+		return;
+
+	cur_load = 100 * (wall_time - idle_time) / wall_time;
+	j_this_smartmax_eps->cur_cpu_load = cur_load;
+}
+
+static void cpufreq_smartmax_eps_timer(struct smartmax_eps_info_s *this_smartmax_eps) {
+	unsigned int cur;
+	struct cpufreq_policy *policy = this_smartmax_eps->cur_policy;
+	u64 now = ktime_to_us(ktime_get());
+	/* Extrapolated load of this CPU */
+	//unsigned int load_at_max_freq = 0;
+	unsigned int cpu = this_smartmax_eps->cpu;
+
+#if SMARTMAX_EPS_STAT 
+	u64 diff = 0;
+
+	if (timer_stat[cpu])
+		diff = now - timer_stat[cpu];
+
+	timer_stat[cpu] = now;
+	printk(KERN_DEBUG "[smartmax_eps]:cpu %d %lld\n", cpu, diff);
+#endif
+
+	cur = policy->cur;
+
+	dprintk(SMARTMAX_EPS_DEBUG_ALG, "%d: %s cpu %d %lld\n", cur, __func__, cpu, now);
+
+	cpufreq_smartmax_eps_calc_load(cpu);
+
+	/* calculate the scaled load across CPU */
+	//load_at_max_freq = (this_smartmax_eps->cur_cpu_load * policy->cur)/policy->cpuinfo.max_freq;
+
+	//cpufreq_notify_utilization(policy, load_at_max_freq);
+
+	dprintk(SMARTMAX_EPS_DEBUG_LOAD, "%d: load %d\n", cpu, this_smartmax_eps->cur_cpu_load);
+
+	this_smartmax_eps->old_freq = cur;
+	this_smartmax_eps->ramp_dir = 0;
+
+	cpufreq_smartmax_eps_get_ramp_direction(this_smartmax_eps, now);
+
+	// no changes
+	if (this_smartmax_eps->ramp_dir == 0)		
+		return;
+
+	// boost - but not block ramp up steps based on load if requested
+	if (boost_running){
+		if (now < boost_end_time) {
+			dprintk(SMARTMAX_EPS_DEBUG_BOOST, "%d: cpu %d boost running %llu %llu\n", cur, cpu, now, boost_end_time);
+		
+			if (this_smartmax_eps->ramp_dir == -1)
+				return;
+			else {
+				if (ramp_up_during_boost)
+					dprintk(SMARTMAX_EPS_DEBUG_BOOST, "%d: cpu %d boost running but ramp_up above boost freq requested\n", cur, cpu);
+				else
+					return;
+			}
+		} else
+			boost_running = false;
+	}
+
+	cpufreq_smartmax_eps_freq_change(this_smartmax_eps);
+}
+
+static void do_dbs_timer(struct work_struct *work) {
+	struct smartmax_eps_info_s *this_smartmax_eps =
+			container_of(work, struct smartmax_eps_info_s, work.work);
+	unsigned int cpu = this_smartmax_eps->cpu;
+	int delay = get_timer_delay();
+
+	mutex_lock(&this_smartmax_eps->timer_mutex);
+
+	cpufreq_smartmax_eps_timer(this_smartmax_eps);
+
+	queue_delayed_work_on(cpu, smartmax_eps_wq, &this_smartmax_eps->work, delay);
+	mutex_unlock(&this_smartmax_eps->timer_mutex);
+}
+
+static void update_idle_time(bool online) {
+	int j = 0;
+
+	for_each_possible_cpu(j)
+	{
+		struct smartmax_eps_info_s *j_this_smartmax_eps;
+
+		if (online && !cpu_online(j)) {
+			continue;
+		}
+		j_this_smartmax_eps = &per_cpu(smartmax_eps_info, j);
+
+		j_this_smartmax_eps->prev_cpu_idle = get_cpu_idle_time(j,
+				&j_this_smartmax_eps->prev_cpu_wall, 0);
+				
+		if (ignore_nice)
+#ifdef CONFIG_CPU_FREQ_GOV_SMARTMAX_EPS_30
+			j_this_smartmax_eps->prev_cpu_nice = kstat_cpu(j) .cpustat.nice;
+#else
+			j_this_smartmax_eps->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+#endif
+	}
+}
+
+static ssize_t show_debug_mask(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%lu\n", debug_mask);
+}
+
+static ssize_t store_debug_mask(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0)
+		debug_mask = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_up_rate(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", up_rate);
+}
+
+static ssize_t store_up_rate(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0 && input <= 100000000)
+		up_rate = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_down_rate(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", down_rate);
+}
+
+static ssize_t store_down_rate(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0 && input <= 100000000)
+		down_rate = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_awake_ideal_freq(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", awake_ideal_freq);
+}
+
+static ssize_t store_awake_ideal_freq(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0) {
+		awake_ideal_freq = input;
+		if (!is_suspended){
+			ideal_freq = awake_ideal_freq;
+			smartmax_eps_update_min_max_allcpus();
+		}
+	} else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_suspend_ideal_freq(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", suspend_ideal_freq);
+}
+
+static ssize_t store_suspend_ideal_freq(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0) {
+		suspend_ideal_freq = input;
+		if (is_suspended){
+			ideal_freq = suspend_ideal_freq;
+			smartmax_eps_update_min_max_allcpus();
+		}
+	} else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_ramp_up_step(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", ramp_up_step);
+}
+
+static ssize_t store_ramp_up_step(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0)
+		ramp_up_step = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_ramp_down_step(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", ramp_down_step);
+}
+
+static ssize_t store_ramp_down_step(struct kobject *kobj,
+		struct attribute *attr, const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0)
+		ramp_down_step = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_max_cpu_load(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", max_cpu_load);
+}
+
+static ssize_t store_max_cpu_load(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input <= 100)
+		max_cpu_load = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_min_cpu_load(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", min_cpu_load);
+}
+
+static ssize_t store_min_cpu_load(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input < 100)
+		min_cpu_load = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_sampling_rate(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", sampling_rate);
+}
+
+static ssize_t store_sampling_rate(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= min_sampling_rate)
+		sampling_rate = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_touch_poke_freq(struct kobject *kobj,
+		struct attribute *attr, char *buf) {
+	return sprintf(buf, "%u\n", touch_poke_freq);
+}
+
+static ssize_t store_touch_poke_freq(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0){
+		touch_poke_freq = input;
+	
+		if (touch_poke_freq == 0)
+			touch_poke = false;
+		else
+			touch_poke = true;
+	} else
+		return -EINVAL;	
+	return count;
+}
+
+static ssize_t show_input_boost_duration(struct kobject *kobj,
+		struct attribute *attr, char *buf) {
+	return sprintf(buf, "%u\n", input_boost_duration);
+}
+
+static ssize_t store_input_boost_duration(struct kobject *a,
+		struct attribute *b, const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 10000)
+		input_boost_duration = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_ramp_up_during_boost(struct kobject *kobj,
+		struct attribute *attr, char *buf) {
+	return sprintf(buf, "%d\n", ramp_up_during_boost);
+}
+
+static ssize_t store_ramp_up_during_boost(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0) {
+		if (input == 0)
+			ramp_up_during_boost = false;
+		else if (input == 1)
+			ramp_up_during_boost = true;
+		else
+			return -EINVAL;
+	} else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_boost_freq(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", boost_freq);
+}
+
+static ssize_t store_boost_freq(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0) {
+		boost_freq = input;
+		if (boost_freq == 0)
+			boost = false;
+		else
+			boost = true;
+	} else
+		return -EINVAL;	
+	return count;
+}
+
+static ssize_t show_boost_duration(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%d\n", boost_running);
+}
+
+static ssize_t store_boost_duration(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 10000){
+		boost_duration = input;
+		if (boost) {
+			// no need to bother if currently a boost is running anyway
+			if (boost_task_alive && boost_running)
+				return count;
+
+			if (boost_task_alive) {
+				cur_boost_freq = boost_freq;
+				cur_boost_duration = boost_duration;
+				wake_up_process(boost_task);
+			}
+		}
+	} else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_io_is_busy(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%d\n", io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0) {
+		if (input > 1)
+			input = 1;
+		if (input == io_is_busy) { /* nothing to do */
+			return count;
+		}
+		io_is_busy = input;
+	} else
+		return -EINVAL;	
+	return count;
+}
+
+static ssize_t show_ignore_nice(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%d\n", ignore_nice);
+}
+
+static ssize_t store_ignore_nice(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0) {
+		if (input > 1)
+			input = 1;
+		if (input == ignore_nice) { /* nothing to do */
+			return count;
+		}
+		ignore_nice = input;
+		/* we need to re-evaluate prev_cpu_idle */
+		update_idle_time(true);
+	} else
+		return -EINVAL;	
+	return count;
+}
+
+static ssize_t show_min_sampling_rate(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%d\n", min_sampling_rate);
+}
+
+static ssize_t store_min_sampling_rate(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	return -EINVAL;	
+}
+
+#define define_global_rw_attr(_name)		\
+static struct global_attr _name##_attr =	\
+	__ATTR(_name, 0644, show_##_name, store_##_name)
+
+#define define_global_ro_attr(_name)		\
+static struct global_attr _name##_attr =	\
+	__ATTR(_name, 0444, show_##_name, store_##_name)
+
+define_global_rw_attr(debug_mask);
+define_global_rw_attr(up_rate);
+define_global_rw_attr(down_rate);
+define_global_rw_attr(ramp_up_step);
+define_global_rw_attr(ramp_down_step);
+define_global_rw_attr(max_cpu_load);
+define_global_rw_attr(min_cpu_load);
+define_global_rw_attr(sampling_rate);
+define_global_rw_attr(touch_poke_freq);
+define_global_rw_attr(input_boost_duration);
+define_global_rw_attr(boost_freq);
+define_global_rw_attr(boost_duration);
+define_global_rw_attr(io_is_busy);
+define_global_rw_attr(ignore_nice);
+define_global_rw_attr(ramp_up_during_boost);
+define_global_rw_attr(awake_ideal_freq);
+define_global_rw_attr(suspend_ideal_freq);
+define_global_ro_attr(min_sampling_rate);
+
+static struct attribute * smartmax_eps_attributes[] = { 
+	&debug_mask_attr.attr,
+	&up_rate_attr.attr, 
+	&down_rate_attr.attr, 
+	&ramp_up_step_attr.attr, 
+	&ramp_down_step_attr.attr,
+	&max_cpu_load_attr.attr, 
+	&min_cpu_load_attr.attr,
+	&sampling_rate_attr.attr, 
+	&touch_poke_freq_attr.attr,
+	&input_boost_duration_attr.attr, 
+	&boost_freq_attr.attr, 
+	&boost_duration_attr.attr, 
+	&io_is_busy_attr.attr,
+	&ignore_nice_attr.attr, 
+	&ramp_up_during_boost_attr.attr, 
+	&awake_ideal_freq_attr.attr,
+	&suspend_ideal_freq_attr.attr,		
+	&min_sampling_rate_attr.attr,
+	NULL , };
+
+static struct attribute_group smartmax_eps_attr_group = { 
+	.attrs = smartmax_eps_attributes, 
+	.name = "smartmax_eps", 
+	};
+
+static int cpufreq_smartmax_eps_boost_task(void *data) {
+	struct smartmax_eps_info_s *this_smartmax_eps;
+	u64 now;
+	struct cpufreq_policy *policy;
+#ifndef CONFIG_CPU_FREQ_GOV_SMARTMAX_EPS_TEGRA
+	unsigned int cpu;
+	bool start_boost = false;
+#endif
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule();
+
+		if (kthread_should_stop())
+			break;
+
+		set_current_state(TASK_RUNNING);
+
+		if (boost_running)
+			continue;
+
+#ifdef CONFIG_CPU_FREQ_GOV_SMARTMAX_EPS_TEGRA
+		/* on tegra there is only one cpu clock so we only need to boost cpu 0 
+		   all others will run at the same speed */
+		this_smartmax_eps = &per_cpu(smartmax_eps_info, 0);
+		if (!this_smartmax_eps)
+			continue;
+
+		policy = this_smartmax_eps->cur_policy;
+		if (!policy)
+			continue;
+
+//		if (lock_policy_rwsem_write(0) < 0)
+//			continue;
+		
+		tegra_input_boost(policy, cur_boost_freq, CPUFREQ_RELATION_H);
+	
+        this_smartmax_eps->prev_cpu_idle = get_cpu_idle_time(0,
+						&this_smartmax_eps->prev_cpu_wall, 0);
+
+//	unlock_policy_rwsem_write(0);
+#else		
+		for_each_online_cpu(cpu){
+			this_smartmax_eps = &per_cpu(smartmax_eps_info, cpu);
+			if (!this_smartmax_eps)
+				continue;
+
+//			if (lock_policy_rwsem_write(cpu) < 0)
+//				continue;
+
+			policy = this_smartmax_eps->cur_policy;
+			if (!policy){
+//				unlock_policy_rwsem_write(cpu);
+				continue;
+			}
+
+			mutex_lock(&this_smartmax_eps->timer_mutex);
+
+			if (policy->cur < cur_boost_freq) {
+				start_boost = true;
+				dprintk(SMARTMAX_EPS_DEBUG_BOOST, "input boost cpu %d to %d\n", cpu, cur_boost_freq);
+				target_freq(policy, this_smartmax_eps, cur_boost_freq, this_smartmax_eps->old_freq, CPUFREQ_RELATION_H);
+				this_smartmax_eps->prev_cpu_idle = get_cpu_idle_time(cpu, &this_smartmax_eps->prev_cpu_wall, 0);
+			}
+			mutex_unlock(&this_smartmax_eps->timer_mutex);
+
+//			unlock_policy_rwsem_write(cpu);
+		}
+#endif
+
+#ifndef CONFIG_CPU_FREQ_GOV_SMARTMAX_EPS_TEGRA
+		if (start_boost) {
+#endif
+
+		boost_running = true;
+		now = ktime_to_us(ktime_get());
+		boost_end_time = now + (cur_boost_duration * num_online_cpus());
+		dprintk(SMARTMAX_EPS_DEBUG_BOOST, "%s %llu %llu\n", __func__, now, boost_end_time);
+		
+#ifndef CONFIG_CPU_FREQ_GOV_SMARTMAX_EPS_TEGRA
+		}
+#endif
+	}
+
+	pr_info("[smartmax_eps]:" "%s boost_thread stopped\n", __func__);
+	return 0;
+}
+
+static void smartmax_eps_input_event(struct input_handle *handle, unsigned int type,
+		unsigned int code, int value) {
+	if (!is_suspended && touch_poke && type == EV_SYN && code == SYN_REPORT) {
+		// no need to bother if currently a boost is running anyway
+		if (boost_task_alive && boost_running)
+			return;
+
+		if (boost_task_alive) {
+			cur_boost_freq = touch_poke_freq;
+			cur_boost_duration = input_boost_duration;
+			wake_up_process(boost_task);
+		}
+	}
+}
+
+#ifdef CONFIG_INPUT_MEDIATOR
+
+static struct input_mediator_handler smartmax_eps_input_mediator_handler = {
+	.event = smartmax_eps_input_event,
+	};
+
+#else
+
+static int dbs_input_connect(struct input_handler *handler,
+		struct input_dev *dev, const struct input_device_id *id) {
+	struct input_handle *handle;
+	int error;
+
+	pr_info("[smartmax_eps]:" "%s input connect to %s\n", __func__, dev->name);
+
+	handle = kzalloc(sizeof(struct input_handle), GFP_KERNEL);
+	if (!handle)
+		return -ENOMEM;
+
+	handle->dev = dev;
+	handle->handler = handler;
+	handle->name = "cpufreq";
+
+	error = input_register_handle(handle);
+	if (error)
+		goto err2;
+
+	error = input_open_device(handle);
+	if (error)
+		goto err1;
+
+	return 0;
+	err1: input_unregister_handle(handle);
+	err2: kfree(handle);
+	pr_err("[smartmax_eps]:" "%s faild to connect input handler %d\n", __func__, error);
+	return error;
+}
+
+static void dbs_input_disconnect(struct input_handle *handle) {
+	input_close_device(handle);
+	input_unregister_handle(handle);
+	kfree(handle);
+}
+
+static const struct input_device_id dbs_ids[] = {
+{
+		.flags = INPUT_DEVICE_ID_MATCH_EVBIT |
+			 INPUT_DEVICE_ID_MATCH_ABSBIT,
+		.evbit = { BIT_MASK(EV_ABS) },
+		.absbit = { [BIT_WORD(ABS_MT_POSITION_X)] =
+			    BIT_MASK(ABS_MT_POSITION_X) |
+			    BIT_MASK(ABS_MT_POSITION_Y) },
+	}, /* multi-touch touchscreen */
+	{
+		.flags = INPUT_DEVICE_ID_MATCH_KEYBIT |
+			 INPUT_DEVICE_ID_MATCH_ABSBIT,
+		.keybit = { [BIT_WORD(BTN_TOUCH)] = BIT_MASK(BTN_TOUCH) },
+		.absbit = { [BIT_WORD(ABS_X)] =
+			    BIT_MASK(ABS_X) | BIT_MASK(ABS_Y) },
+	}, /* touchpad */
+	{ },
+};
+
+static struct input_handler dbs_input_handler = { 
+	.event = smartmax_eps_input_event,
+	.connect = dbs_input_connect, 
+	.disconnect = dbs_input_disconnect,
+	.name = "cpufreq_smartmax_eps", 
+	.id_table = dbs_ids, 
+	};
+#endif
+
+static int cpufreq_governor_smartmax_eps(struct cpufreq_policy *new_policy,
+		unsigned int event) {
+	unsigned int cpu = new_policy->cpu;
+	int rc;
+	struct smartmax_eps_info_s *this_smartmax_eps = &per_cpu(smartmax_eps_info, cpu);
+	struct sched_param param = { .sched_priority = 1 };
+    unsigned int latency;
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!new_policy->cur))
+			return -EINVAL;
+
+		mutex_lock(&dbs_mutex);
+
+		this_smartmax_eps->cur_policy = new_policy;
+		this_smartmax_eps->cpu = cpu;
+
+		smartmax_eps_update_min_max(this_smartmax_eps,new_policy);
+
+		this_smartmax_eps->freq_table = cpufreq_frequency_get_table(cpu);
+		if (!this_smartmax_eps->freq_table){
+			mutex_unlock(&dbs_mutex);
+			return -EINVAL;
+		}
+
+		update_idle_time(false);
+
+		dbs_enable++;
+		
+		if (dbs_enable == 1) {
+			if (!boost_task_alive) {
+				boost_task = kthread_create (
+						cpufreq_smartmax_eps_boost_task,
+						NULL,
+						"smartmax_eps_input_boost_task"
+				);
+
+				if (IS_ERR(boost_task)) {
+					dbs_enable--;
+					mutex_unlock(&dbs_mutex);
+					return PTR_ERR(boost_task);
+				}
+
+				pr_info("[smartmax_eps]:" "%s input boost task created\n", __func__);
+				sched_setscheduler_nocheck(boost_task, SCHED_FIFO, &param);
+				get_task_struct(boost_task);
+				boost_task_alive = true;
+			}
+#ifdef CONFIG_INPUT_MEDIATOR
+			input_register_mediator_secondary(&smartmax_eps_input_mediator_handler);
+#else
+			rc = input_register_handler(&dbs_input_handler);
+			if (rc) {
+				dbs_enable--;
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+#endif
+			rc = sysfs_create_group(cpufreq_global_kobject,
+					&smartmax_eps_attr_group);
+			if (rc) {
+				dbs_enable--;
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+			/* policy latency is in nS. Convert it to uS first */
+			latency = new_policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+
+			/* Bring kernel and HW constraints together */
+			min_sampling_rate = max(min_sampling_rate, MIN_LATENCY_MULTIPLIER * latency);
+			sampling_rate = max(min_sampling_rate, sampling_rate);
+		}
+
+		mutex_unlock(&dbs_mutex);
+		dbs_timer_init(this_smartmax_eps);
+
+		break;
+	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&this_smartmax_eps->timer_mutex);
+		smartmax_eps_update_min_max(this_smartmax_eps,new_policy);
+
+		if (this_smartmax_eps->cur_policy->cur > new_policy->max) {
+			dprintk(SMARTMAX_EPS_DEBUG_JUMPS,"CPUFREQ_GOV_LIMITS jumping to new max freq: %d\n",new_policy->max);
+			__cpufreq_driver_target(this_smartmax_eps->cur_policy,
+					new_policy->max, CPUFREQ_RELATION_H);
+		}
+		else if (this_smartmax_eps->cur_policy->cur < new_policy->min) {
+			dprintk(SMARTMAX_EPS_DEBUG_JUMPS,"CPUFREQ_GOV_LIMITS jumping to new min freq: %d\n",new_policy->min);
+			__cpufreq_driver_target(this_smartmax_eps->cur_policy,
+					new_policy->min, CPUFREQ_RELATION_L);
+		}
+		mutex_unlock(&this_smartmax_eps->timer_mutex);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		dbs_timer_exit(this_smartmax_eps);
+
+		mutex_lock(&dbs_mutex);
+		this_smartmax_eps->cur_policy = NULL;
+		dbs_enable--;
+
+		if (!dbs_enable){
+			if (boost_task_alive)
+				kthread_stop(boost_task);
+
+			sysfs_remove_group(cpufreq_global_kobject, &smartmax_eps_attr_group);
+#ifdef CONFIG_INPUT_MEDIATOR
+			input_unregister_mediator_secondary(&smartmax_eps_input_mediator_handler);
+#else
+			input_unregister_handler(&dbs_input_handler);
+#endif
+		}
+		
+		mutex_unlock(&dbs_mutex);
+		break;
+	}
+
+	return 0;
+}
+
+static int __init cpufreq_smartmax_eps_init(void) {
+	unsigned int i;
+	struct smartmax_eps_info_s *this_smartmax_eps;
+	u64 wall;
+	u64 idle_time;
+	int cpu = get_cpu();
+
+	idle_time = get_cpu_idle_time_us(cpu, &wall);
+	put_cpu();
+	if (idle_time != -1ULL) {
+		/*
+		 * In no_hz/micro accounting case we set the minimum frequency
+		 * not depending on HZ, but fixed (very low). The deferred
+		 * timer might skip some samples if idle/sleeping as needed.
+		*/
+		min_sampling_rate = MICRO_FREQUENCY_MIN_SAMPLE_RATE;
+	} else {
+		/* For correct statistics, we need 10 ticks for each measure */
+		min_sampling_rate = MIN_SAMPLING_RATE_RATIO * jiffies_to_usecs(10);
+	}
+
+	smartmax_eps_wq = alloc_workqueue("smartmax_eps_wq", WQ_HIGHPRI, 0);
+	if (!smartmax_eps_wq) {
+		printk(KERN_ERR "Failed to create smartmax_eps_wq workqueue\n");
+		return -EFAULT;
+	}
+
+	up_rate = DEFAULT_UP_RATE;
+	down_rate = DEFAULT_DOWN_RATE;
+	suspend_ideal_freq = DEFAULT_SUSPEND_IDEAL_FREQ;
+	awake_ideal_freq = DEFAULT_AWAKE_IDEAL_FREQ;
+	ideal_freq = awake_ideal_freq;
+	ramp_up_step = DEFAULT_RAMP_UP_STEP;
+	ramp_down_step = DEFAULT_RAMP_DOWN_STEP;
+	max_cpu_load = DEFAULT_MAX_CPU_LOAD;
+	min_cpu_load = DEFAULT_MIN_CPU_LOAD;
+	sampling_rate = DEFAULT_SAMPLING_RATE;
+	input_boost_duration = DEFAULT_INPUT_BOOST_DURATION;
+	io_is_busy = DEFAULT_IO_IS_BUSY;
+	ignore_nice = DEFAULT_IGNORE_NICE;
+	touch_poke_freq = DEFAULT_TOUCH_POKE_FREQ;
+	boost_freq = DEFAULT_BOOST_FREQ;
+
+	/* Initalize per-cpu data: */
+	for_each_possible_cpu(i)
+	{
+		this_smartmax_eps = &per_cpu(smartmax_eps_info, i);
+		this_smartmax_eps->cur_policy = NULL;
+		this_smartmax_eps->ramp_dir = 0;
+		this_smartmax_eps->freq_change_time = 0;
+		this_smartmax_eps->cur_cpu_load = 0;
+		mutex_init(&this_smartmax_eps->timer_mutex);
+	}
+
+	return cpufreq_register_governor(&cpufreq_gov_smartmax_eps);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX_EPS
+fs_initcall(cpufreq_smartmax_eps_init);
+#else
+module_init(cpufreq_smartmax_eps_init);
+#endif
+
+static void __exit cpufreq_smartmax_eps_exit(void) {
+	unsigned int i;
+	struct smartmax_eps_info_s *this_smartmax_eps;
+
+	cpufreq_unregister_governor(&cpufreq_gov_smartmax_eps);
+
+	for_each_possible_cpu(i)
+	{
+		this_smartmax_eps = &per_cpu(smartmax_eps_info, i);
+		mutex_destroy(&this_smartmax_eps->timer_mutex);
+	}
+	destroy_workqueue(smartmax_eps_wq);
+}
+
+module_exit(cpufreq_smartmax_eps_exit);
+
+MODULE_AUTHOR("maxwen");
+MODULE_DESCRIPTION("'cpufreq_smartmax_eps' - A smart cpufreq governor");
+MODULE_LICENSE("GPL");
diff --git a/drivers/cpufreq/cpufreq_umbrella_core.c b/drivers/cpufreq/cpufreq_umbrella_core.c
new file mode 100755
index 0000000..a63aa65
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_umbrella_core.c
@@ -0,0 +1,2088 @@
+/*
+ * drivers/cpufreq/cpufreq_umbrella_core.c
+ *
+ * Copyright (C) 2010 Google, Inc.
+ *           (C) 2014 LoungeKatt <twistedumbrella@gmail.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/rwsem.h>
+#include <linux/sched.h>
+#include <linux/sched/rt.h>
+#include <linux/tick.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/kernel_stat.h>
+#include <asm/cputime.h>
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/cpufreq_umbrella_core.h>
+
+#ifdef CONFIG_POWERSUSPEND
+#include <linux/powersuspend.h>
+#endif
+
+#define CONFIG_UC_MODE_AUTO_CHANGE
+#define CONFIG_UC_MODE_AUTO_CHANGE_BOOST
+
+static int active_count;
+
+struct cpufreq_umbrella_core_cpuinfo {
+	struct timer_list cpu_timer;
+	struct timer_list cpu_slack_timer;
+	spinlock_t load_lock; /* protects the next 4 fields */
+	u64 time_in_idle;
+	u64 time_in_idle_timestamp;
+	u64 cputime_speedadj;
+	u64 cputime_speedadj_timestamp;
+	struct cpufreq_policy *policy;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int target_freq;
+	unsigned int floor_freq;
+	u64 floor_validate_time;
+	u64 hispeed_validate_time;
+	struct rw_semaphore enable_sem;
+	int governor_enabled;
+	int prev_load;
+	bool limits_changed;
+	unsigned int nr_timer_resched;
+};
+
+#define MIN_TIMER_JIFFIES 1UL
+
+static DEFINE_PER_CPU(struct cpufreq_umbrella_core_cpuinfo, cpuinfo);
+
+/* realtime thread handles frequency scaling */
+static struct task_struct *speedchange_task;
+static cpumask_t speedchange_cpumask;
+static spinlock_t speedchange_cpumask_lock;
+static struct mutex gov_lock;
+
+/* Hi speed to bump to from lo speed when load burst (default max) */
+static unsigned int hispeed_freq;
+
+/* Go to hi speed when CPU load at or above this value. */
+#define DEFAULT_GO_HISPEED_LOAD 95
+static unsigned long go_hispeed_load = DEFAULT_GO_HISPEED_LOAD;
+
+/* Sampling down factor to be applied to min_sample_time at max freq */
+static unsigned int sampling_down_factor;
+
+/* Target load.  Lower values result in higher CPU speeds. */
+#define DEFAULT_TARGET_LOAD 85
+static unsigned int default_target_loads[] = {DEFAULT_TARGET_LOAD};
+static spinlock_t target_loads_lock;
+static unsigned int *target_loads = default_target_loads;
+static int ntarget_loads = ARRAY_SIZE(default_target_loads);
+
+/*
+ * The minimum amount of time to spend at a frequency before we can ramp down.
+ */
+#define DEFAULT_MIN_SAMPLE_TIME (80 * USEC_PER_MSEC)
+static unsigned long min_sample_time = DEFAULT_MIN_SAMPLE_TIME;
+
+/*
+ * The sample rate of the timer used to increase frequency
+ */
+#define DEFAULT_TIMER_RATE (20 * USEC_PER_MSEC)
+static unsigned long timer_rate = DEFAULT_TIMER_RATE;
+
+/* Busy SDF parameters*/
+#define MIN_BUSY_TIME (100 * USEC_PER_MSEC)
+
+/*
+ * Wait this long before raising speed above hispeed, by default a single
+ * timer interval.
+ */
+#define DEFAULT_ABOVE_HISPEED_DELAY DEFAULT_TIMER_RATE
+static unsigned int default_above_hispeed_delay[] = {
+	DEFAULT_ABOVE_HISPEED_DELAY };
+static spinlock_t above_hispeed_delay_lock;
+static unsigned int *above_hispeed_delay = default_above_hispeed_delay;
+static int nabove_hispeed_delay = ARRAY_SIZE(default_above_hispeed_delay);
+
+/* Non-zero means indefinite speed boost active */
+static int boost_val;
+/* Duration of a boot pulse in usecs */
+static int boostpulse_duration_val = DEFAULT_MIN_SAMPLE_TIME;
+/* End time of boost pulse in ktime converted to usecs */
+static u64 boostpulse_endtime;
+
+/*
+ * Max additional time to wait in idle, beyond timer_rate, at speeds above
+ * minimum before wakeup to reduce speed, or -1 if unnecessary.
+ */
+#define DEFAULT_TIMER_SLACK (4 * DEFAULT_TIMER_RATE)
+static int timer_slack_val = DEFAULT_TIMER_SLACK;
+
+#define DEFAULT_INACTIVE_FREQ_ON    1958400
+#define DEFAULT_INACTIVE_FREQ_OFF   729600
+unsigned int max_inactive_freq = DEFAULT_INACTIVE_FREQ_ON;
+unsigned int max_inactive_freq_screen_on = DEFAULT_INACTIVE_FREQ_ON;
+unsigned int max_inactive_freq_screen_off = DEFAULT_INACTIVE_FREQ_OFF;
+
+static bool io_is_busy;
+
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+struct cpufreq_loadinfo {
+	unsigned int load;
+	unsigned int freq;
+	u64 timestamp;
+};
+
+static DEFINE_PER_CPU(struct cpufreq_loadinfo, loadinfo);
+
+static spinlock_t mode_lock;
+
+#define MULTI_MODE	2
+#define SINGLE_MODE	1
+#define NO_MODE	0
+
+static unsigned int mode = 0;
+static unsigned int enforced_mode = 0;
+static u64 mode_check_timestamp = 0;
+
+#define DEFAULT_MULTI_ENTER_TIME (4 * DEFAULT_TIMER_RATE)
+static unsigned long multi_enter_time = DEFAULT_MULTI_ENTER_TIME;
+static unsigned long time_in_multi_enter = 0;
+static unsigned int multi_enter_load = 4 * DEFAULT_TARGET_LOAD;
+
+#define DEFAULT_MULTI_EXIT_TIME (16 * DEFAULT_TIMER_RATE)
+static unsigned long multi_exit_time = DEFAULT_MULTI_EXIT_TIME;
+static unsigned long time_in_multi_exit = 0;
+static unsigned int multi_exit_load = 4 * DEFAULT_TARGET_LOAD;
+
+#define DEFAULT_SINGLE_ENTER_TIME (8 * DEFAULT_TIMER_RATE)
+static unsigned long single_enter_time = DEFAULT_SINGLE_ENTER_TIME;
+static unsigned long time_in_single_enter = 0;
+static unsigned int single_enter_load = DEFAULT_TARGET_LOAD;
+
+#define DEFAULT_SINGLE_EXIT_TIME (4 * DEFAULT_TIMER_RATE)
+static unsigned long single_exit_time = DEFAULT_SINGLE_EXIT_TIME;
+static unsigned long time_in_single_exit = 0;
+static unsigned int single_exit_load = DEFAULT_TARGET_LOAD;
+
+static unsigned int param_index = 0;
+static unsigned int cur_param_index = 0;
+
+#define MAX_PARAM_SET 4 /* ((MULTI_MODE | SINGLE_MODE | NO_MODE) + 1) */
+static unsigned int hispeed_freq_set[MAX_PARAM_SET];
+static unsigned long go_hispeed_load_set[MAX_PARAM_SET];
+static unsigned int *target_loads_set[MAX_PARAM_SET];
+static int ntarget_loads_set[MAX_PARAM_SET];
+static unsigned long min_sample_time_set[MAX_PARAM_SET];
+static unsigned long timer_rate_set[MAX_PARAM_SET];
+static unsigned int *above_hispeed_delay_set[MAX_PARAM_SET];
+static int nabove_hispeed_delay_set[MAX_PARAM_SET];
+static unsigned int sampling_down_factor_set[MAX_PARAM_SET];
+#endif /* CONFIG_UC_MODE_AUTO_CHANGE */
+
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE_BOOST
+// BIMC freq vs BW table
+// BW for 8084 : 762 1144 1525 2288 3051 3952 4684 5859 7019 8056 10101 12145 16250
+// Freq for 8974 (KHz) : 19200   37500   50000   75000  100000  150000  200000  307200  460800  614400  825600
+// Freq for 8084 (KHz) : 19200   37500   50000   75000  100000  150000  200000  307200  384000  460800  556800  691200  825600  931200
+static unsigned long bimc_hispeed_freq = 0;	// bimc hispeed freq on mode change. default : MHz
+static int mode_count = 0;
+extern int request_bimc_clk(unsigned long request_clk);
+extern void msm_pm_retention_mode_enable(bool enable);
+static void mode_auto_change_boost(struct work_struct *work);
+static struct workqueue_struct *mode_auto_change_boost_wq;
+static struct work_struct mode_auto_change_boost_work;
+#endif
+
+/*
+ * If the max load among other CPUs is higher than up_threshold_any_cpu_load
+ * and if the highest frequency among the other CPUs is higher than
+ * up_threshold_any_cpu_freq then do not let the frequency to drop below
+ * sync_freq
+ */
+static unsigned int up_threshold_any_cpu_load;
+static unsigned int sync_freq;
+static unsigned int up_threshold_any_cpu_freq;
+
+static int cpufreq_governor_umbrella_core(struct cpufreq_policy *policy,
+		unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_UMBRELLA_CORE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_umbrella_core = {
+	.name = "umbrella_core",
+	.governor = cpufreq_governor_umbrella_core,
+	.max_transition_latency = 10000000,
+	.owner = THIS_MODULE,
+};
+
+static void cpufreq_umbrella_core_timer_resched(
+	struct cpufreq_umbrella_core_cpuinfo *pcpu)
+{
+	unsigned long expires;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->time_in_idle =
+		get_cpu_idle_time(smp_processor_id(),
+				  &pcpu->time_in_idle_timestamp, io_is_busy);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	expires = jiffies + usecs_to_jiffies(timer_rate);
+	mod_timer_pinned(&pcpu->cpu_timer, expires);
+
+	if (timer_slack_val >= 0 && pcpu->target_freq > pcpu->policy->min) {
+		expires += usecs_to_jiffies(timer_slack_val);
+		mod_timer_pinned(&pcpu->cpu_slack_timer, expires);
+	}
+
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+/* The caller shall take enable_sem write semaphore to avoid any timer race.
+ * The cpu_timer and cpu_slack_timer must be deactivated when calling this
+ * function.
+ */
+static void cpufreq_umbrella_core_timer_start(int cpu, int time_override)
+{
+	struct cpufreq_umbrella_core_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	unsigned long flags;
+	unsigned long expires;
+	if (time_override)
+		expires = jiffies + time_override;
+	else
+		expires = jiffies + usecs_to_jiffies(timer_rate);
+
+	pcpu->cpu_timer.expires = expires;
+	add_timer_on(&pcpu->cpu_timer, cpu);
+	if (timer_slack_val >= 0 && pcpu->target_freq > pcpu->policy->min) {
+		expires += usecs_to_jiffies(timer_slack_val);
+		pcpu->cpu_slack_timer.expires = expires;
+		add_timer_on(&pcpu->cpu_slack_timer, cpu);
+	}
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->time_in_idle =
+		get_cpu_idle_time(cpu, &pcpu->time_in_idle_timestamp,
+				  io_is_busy);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+static unsigned int freq_to_above_hispeed_delay(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay - 1 &&
+			freq >= above_hispeed_delay[i+1]; i += 2)
+		;
+
+	ret = above_hispeed_delay[i];
+	ret = (ret > (1 * USEC_PER_MSEC)) ? (ret - (1 * USEC_PER_MSEC)) : ret;
+
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static unsigned int freq_to_targetload(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+
+	for (i = 0; i < ntarget_loads - 1 && freq >= target_loads[i+1]; i += 2)
+		;
+
+	ret = target_loads[i];
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+/*
+ * If increasing frequencies never map to a lower target load then
+ * choose_freq() will find the minimum frequency that does not exceed its
+ * target load given the current load.
+ */
+
+static unsigned int choose_freq(
+	struct cpufreq_umbrella_core_cpuinfo *pcpu, unsigned int loadadjfreq)
+{
+	unsigned int freq = pcpu->policy->cur;
+	unsigned int prevfreq, freqmin, freqmax;
+	unsigned int tl;
+	int index;
+
+	freqmin = 0;
+	freqmax = UINT_MAX;
+
+	do {
+		prevfreq = freq;
+		tl = freq_to_targetload(freq);
+
+		/*
+		 * Find the lowest frequency where the computed load is less
+		 * than or equal to the target load.
+		 */
+
+		if (cpufreq_frequency_table_target(
+			    pcpu->policy, pcpu->freq_table, loadadjfreq / tl,
+			    CPUFREQ_RELATION_C, &index))
+			break;
+		freq = pcpu->freq_table[index].frequency;
+
+		if (freq > prevfreq) {
+			/* The previous frequency is too low. */
+			freqmin = prevfreq;
+
+			if (freq >= freqmax) {
+				/*
+				 * Find the highest frequency that is less
+				 * than freqmax.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmax - 1, CPUFREQ_RELATION_H,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				if (freq == freqmin) {
+					/*
+					 * The first frequency below freqmax
+					 * has already been found to be too
+					 * low.  freqmax is the lowest speed
+					 * we found that is fast enough.
+					 */
+					freq = freqmax;
+					break;
+				}
+			}
+		} else if (freq < prevfreq) {
+			/* The previous frequency is high enough. */
+			freqmax = prevfreq;
+
+			if (freq <= freqmin) {
+				/*
+				 * Find the lowest frequency that is higher
+				 * than freqmin.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmin + 1, CPUFREQ_RELATION_C,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				/*
+				 * If freqmax is the first frequency above
+				 * freqmin then we have already found that
+				 * this speed is fast enough.
+				 */
+				if (freq == freqmax)
+					break;
+			}
+		}
+
+		/* If same frequency chosen as previous then done. */
+	} while (freq != prevfreq);
+
+	return freq;
+}
+
+static u64 update_load(int cpu)
+{
+	struct cpufreq_umbrella_core_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 now;
+	u64 now_idle;
+	unsigned int delta_idle;
+	unsigned int delta_time;
+	u64 active_time;
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	unsigned int cur_load = 0;
+	struct cpufreq_loadinfo *cur_loadinfo = &per_cpu(loadinfo, cpu);
+#endif
+	now_idle = get_cpu_idle_time(cpu, &now, io_is_busy);
+	delta_idle = (unsigned int)(now_idle - pcpu->time_in_idle);
+	delta_time = (unsigned int)(now - pcpu->time_in_idle_timestamp);
+
+	if (delta_time <= delta_idle)
+		active_time = 0;
+	else
+		active_time = delta_time - delta_idle;
+
+	pcpu->cputime_speedadj += active_time * pcpu->policy->cur;
+
+	pcpu->time_in_idle = now_idle;
+	pcpu->time_in_idle_timestamp = now;
+
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	cur_load = (unsigned int)(active_time * 100) / delta_time;
+	cur_loadinfo->load = (cur_load * pcpu->policy->cur) /
+									pcpu->policy->cpuinfo.max_freq;
+	pcpu->policy->load_at_max = cur_loadinfo->load;
+	cur_loadinfo->freq = pcpu->policy->cur;
+	cur_loadinfo->timestamp = now;
+#endif
+
+	return now;
+}
+
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+static unsigned int check_mode(int cpu, unsigned int cur_mode, u64 now)
+{
+	int i;
+	unsigned int ret=cur_mode, total_load=0, max_single_load=0;
+	struct cpufreq_loadinfo *cur_loadinfo;
+
+	if (now - mode_check_timestamp < timer_rate - 1000)
+		return ret;
+
+	if (now - mode_check_timestamp > timer_rate + 1000)
+		mode_check_timestamp = now - timer_rate;
+
+	for_each_online_cpu(i) {
+		cur_loadinfo = &per_cpu(loadinfo, i);
+		total_load += cur_loadinfo->load;
+		if (cur_loadinfo->load > max_single_load)
+			max_single_load = cur_loadinfo->load;
+	}
+
+	if (!(cur_mode & SINGLE_MODE)) {
+		if (max_single_load >= single_enter_load)
+			time_in_single_enter += now - mode_check_timestamp;
+		else
+			time_in_single_enter = 0;
+
+		if (time_in_single_enter >= single_enter_time)
+			ret |= SINGLE_MODE;
+	}
+
+	if (!(cur_mode & MULTI_MODE)) {
+		if (total_load >= multi_enter_load)
+			time_in_multi_enter += now - mode_check_timestamp;
+		else
+			time_in_multi_enter = 0;
+
+		if (time_in_multi_enter >= multi_enter_time)
+			ret |= MULTI_MODE;
+	}
+
+	if (cur_mode & SINGLE_MODE) {
+		if (max_single_load < single_exit_load)
+			time_in_single_exit += now - mode_check_timestamp;
+		else
+			time_in_single_exit = 0;
+
+		if (time_in_single_exit >= single_exit_time)
+			ret &= ~SINGLE_MODE;
+	}
+
+	if (cur_mode & MULTI_MODE) {
+		if (total_load < multi_exit_load)
+			time_in_multi_exit += now - mode_check_timestamp;
+		else
+			time_in_multi_exit = 0;
+
+		if (time_in_multi_exit >= multi_exit_time)
+			ret &= ~MULTI_MODE;
+	}
+
+	trace_cpufreq_umbrella_core_mode(cpu, total_load,
+		time_in_single_enter, time_in_multi_enter,
+		time_in_single_exit, time_in_multi_exit, ret);
+
+	if (time_in_single_enter >= single_enter_time)
+		time_in_single_enter = 0;
+	if (time_in_multi_enter >= multi_enter_time)
+		time_in_multi_enter = 0;
+	if (time_in_single_exit >= single_exit_time)
+		time_in_single_exit = 0;
+	if (time_in_multi_exit >= multi_exit_time)
+		time_in_multi_exit = 0;
+	mode_check_timestamp = now;
+
+	return ret;
+}
+
+static void set_new_param_set(unsigned int index)
+{
+	unsigned long flags;
+
+	hispeed_freq = hispeed_freq_set[index];
+	go_hispeed_load = go_hispeed_load_set[index];
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+	target_loads = target_loads_set[index];
+	ntarget_loads =	ntarget_loads_set[index];
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+
+	min_sample_time = min_sample_time_set[index];
+	timer_rate = timer_rate_set[index];
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+	above_hispeed_delay = above_hispeed_delay_set[index];
+	nabove_hispeed_delay = nabove_hispeed_delay_set[index];
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+
+	cur_param_index = index;
+}
+
+static void enter_mode(void)
+{
+#if 1
+	set_new_param_set(mode);
+#else
+	set_new_param_set(1);
+#endif
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE_BOOST
+	queue_work(mode_auto_change_boost_wq, &mode_auto_change_boost_work);
+#endif
+}
+
+static void exit_mode(void)
+{
+	set_new_param_set(0);
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE_BOOST
+	queue_work(mode_auto_change_boost_wq, &mode_auto_change_boost_work);
+#endif
+}
+#endif
+
+static void cpufreq_umbrella_core_timer(unsigned long data)
+{
+	u64 now;
+	unsigned int delta_time;
+	u64 cputime_speedadj;
+	int cpu_load;
+	struct cpufreq_umbrella_core_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, data);
+	unsigned int new_freq;
+	unsigned int loadadjfreq;
+	unsigned int index;
+	unsigned long flags;
+	bool boosted;
+	unsigned long mod_min_sample_time;
+	int i, max_load;
+	unsigned int max_freq;
+	struct cpufreq_umbrella_core_cpuinfo *picpu;
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	unsigned int new_mode;
+#endif
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled)
+		goto exit;
+
+	pcpu->nr_timer_resched = 0;
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	now = update_load(data);
+	delta_time = (unsigned int)(now - pcpu->cputime_speedadj_timestamp);
+	cputime_speedadj = pcpu->cputime_speedadj;
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+
+	if (WARN_ON_ONCE(!delta_time))
+		goto rearm;
+
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	spin_lock_irqsave(&mode_lock, flags);
+	if (enforced_mode)
+		new_mode = enforced_mode;
+	else
+		new_mode = check_mode(data, mode, now);
+	if (new_mode != mode) {
+		mode = new_mode;
+		if (new_mode & MULTI_MODE || new_mode & SINGLE_MODE) {
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE_BOOST
+			++mode_count;
+#endif
+			pr_info("Governor: enter mode 0x%x\n", mode);
+			enter_mode();
+		} else {
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE_BOOST
+			mode_count=0;
+#endif
+			pr_info("Governor: exit mode 0x%x\n", mode);
+			exit_mode();
+		}
+	}
+	spin_unlock_irqrestore(&mode_lock, flags);
+#endif
+
+	do_div(cputime_speedadj, delta_time);
+	loadadjfreq = (unsigned int)cputime_speedadj * 100;
+	cpu_load = loadadjfreq / pcpu->target_freq;
+	pcpu->prev_load = cpu_load;
+	boosted = boost_val || now < boostpulse_endtime;
+	pcpu->policy->util = cpu_load;
+
+	if (cpu_load >= go_hispeed_load || boosted) {
+		if (pcpu->target_freq < hispeed_freq) {
+			new_freq = hispeed_freq;
+		} else {
+			new_freq = choose_freq(pcpu, loadadjfreq);
+
+			if (new_freq < hispeed_freq)
+				new_freq = hispeed_freq;
+		}
+		if (new_freq > max_inactive_freq && cpu_load < 99)
+			new_freq = max_inactive_freq;
+	} else {
+		new_freq = choose_freq(pcpu, loadadjfreq);
+
+		if (sync_freq && new_freq < sync_freq) {
+
+			max_load = 0;
+			max_freq = 0;
+
+			for_each_online_cpu(i) {
+				picpu = &per_cpu(cpuinfo, i);
+
+				if (i == data || picpu->prev_load <
+						up_threshold_any_cpu_load)
+					continue;
+
+				max_load = max(max_load, picpu->prev_load);
+				max_freq = max(max_freq, picpu->target_freq);
+			}
+
+			if (max_freq > up_threshold_any_cpu_freq &&
+				max_load >= up_threshold_any_cpu_load)
+				new_freq = sync_freq;
+		}
+	}
+
+	if (pcpu->target_freq >= hispeed_freq &&
+	    new_freq > pcpu->target_freq &&
+	    now - pcpu->hispeed_validate_time <
+	    freq_to_above_hispeed_delay(pcpu->target_freq)) {
+		trace_cpufreq_umbrella_core_notyet(
+			data, cpu_load, pcpu->target_freq,
+			pcpu->policy->cur, new_freq);
+		goto rearm;
+	}
+
+	pcpu->hispeed_validate_time = now;
+
+	if (cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table,
+					   new_freq, CPUFREQ_RELATION_C,
+					   &index))
+		goto rearm;
+
+	new_freq = pcpu->freq_table[index].frequency;
+
+	/*
+	 * Do not scale below floor_freq unless we have been at or above the
+	 * floor frequency for the minimum sample time since last validated.
+	 */
+	if (sampling_down_factor && pcpu->policy->cur == pcpu->policy->max)
+		mod_min_sample_time = sampling_down_factor;
+	else
+		mod_min_sample_time = min_sample_time;
+
+	if (pcpu->limits_changed) {
+		if (sampling_down_factor &&
+			(pcpu->policy->cur != pcpu->policy->max))
+			mod_min_sample_time = 0;
+
+		pcpu->limits_changed = false;
+	}
+
+	if (new_freq < pcpu->floor_freq) {
+		if (now - pcpu->floor_validate_time < mod_min_sample_time) {
+			trace_cpufreq_umbrella_core_notyet(
+				data, cpu_load, pcpu->target_freq,
+				pcpu->policy->cur, new_freq);
+			goto rearm;
+		}
+	}
+
+	/*
+	 * Update the timestamp for checking whether speed has been held at
+	 * or above the selected frequency for a minimum of min_sample_time,
+	 * if not boosted to hispeed_freq.  If boosted to hispeed_freq then we
+	 * allow the speed to drop as soon as the boostpulse duration expires
+	 * (or the indefinite boost is turned off).
+	 */
+
+	if (!boosted || new_freq > hispeed_freq) {
+		pcpu->floor_freq = new_freq;
+		pcpu->floor_validate_time = now;
+	}
+
+	if (pcpu->target_freq == new_freq) {
+		trace_cpufreq_umbrella_core_already(
+			data, cpu_load, pcpu->target_freq,
+			pcpu->policy->cur, new_freq);
+		goto rearm_if_notmax;
+	}
+
+	trace_cpufreq_umbrella_core_target(data, cpu_load, pcpu->target_freq,
+					 pcpu->policy->cur, new_freq);
+
+	pcpu->target_freq = new_freq;
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+	cpumask_set_cpu(data, &speedchange_cpumask);
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+	wake_up_process(speedchange_task);
+
+rearm_if_notmax:
+	/*
+	 * Already set max speed and don't see a need to change that,
+	 * wait until next idle to re-evaluate, don't need timer.
+	 */
+	if (pcpu->target_freq == pcpu->policy->max)
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+		goto rearm;
+#else
+		goto exit;
+#endif
+
+rearm:
+	if (!timer_pending(&pcpu->cpu_timer))
+		cpufreq_umbrella_core_timer_resched(pcpu);
+
+exit:
+	up_read(&pcpu->enable_sem);
+	return;
+}
+
+static void cpufreq_umbrella_core_idle_start(void)
+{
+	struct cpufreq_umbrella_core_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+	int pending;
+	u64 now;
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled) {
+		up_read(&pcpu->enable_sem);
+		return;
+	}
+
+	pending = timer_pending(&pcpu->cpu_timer);
+
+	if (pcpu->target_freq != pcpu->policy->min) {
+		/*
+		 * Entering idle while not at lowest speed.  On some
+		 * platforms this can hold the other CPU(s) at that speed
+		 * even though the CPU is idle. Set a timer to re-evaluate
+		 * speed so this idle CPU doesn't hold the other CPUs above
+		 * min indefinitely.  This should probably be a quirk of
+		 * the CPUFreq driver.
+		 */
+		if (!pending) {
+			cpufreq_umbrella_core_timer_resched(pcpu);
+
+			now = ktime_to_us(ktime_get());
+			if ((pcpu->policy->cur == pcpu->policy->max) &&
+				(now - pcpu->hispeed_validate_time) >
+							MIN_BUSY_TIME) {
+				pcpu->floor_validate_time = now;
+			}
+
+		}
+	}
+
+	up_read(&pcpu->enable_sem);
+}
+
+static void cpufreq_umbrella_core_idle_end(void)
+{
+	struct cpufreq_umbrella_core_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled) {
+		up_read(&pcpu->enable_sem);
+		return;
+	}
+
+	/* Arm the timer for 1-2 ticks later if not already. */
+	if (!timer_pending(&pcpu->cpu_timer)) {
+		cpufreq_umbrella_core_timer_resched(pcpu);
+	} else if (time_after_eq(jiffies, pcpu->cpu_timer.expires)) {
+		del_timer(&pcpu->cpu_timer);
+		del_timer(&pcpu->cpu_slack_timer);
+		cpufreq_umbrella_core_timer(smp_processor_id());
+	}
+
+	up_read(&pcpu->enable_sem);
+}
+
+static int cpufreq_umbrella_core_speedchange_task(void *data)
+{
+	unsigned int cpu;
+	cpumask_t tmp_mask;
+	unsigned long flags;
+	struct cpufreq_umbrella_core_cpuinfo *pcpu;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+		if (cpumask_empty(&speedchange_cpumask)) {
+			spin_unlock_irqrestore(&speedchange_cpumask_lock,
+					       flags);
+			schedule();
+
+			if (kthread_should_stop())
+				break;
+
+			spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+		}
+
+		set_current_state(TASK_RUNNING);
+		tmp_mask = speedchange_cpumask;
+		cpumask_clear(&speedchange_cpumask);
+		spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+		for_each_cpu(cpu, &tmp_mask) {
+			unsigned int j;
+			unsigned int max_freq = 0;
+
+			pcpu = &per_cpu(cpuinfo, cpu);
+			if (!down_read_trylock(&pcpu->enable_sem))
+				continue;
+			if (!pcpu->governor_enabled) {
+				up_read(&pcpu->enable_sem);
+				continue;
+			}
+
+			for_each_cpu(j, pcpu->policy->cpus) {
+				struct cpufreq_umbrella_core_cpuinfo *pjcpu =
+					&per_cpu(cpuinfo, j);
+
+				if (pjcpu->target_freq > max_freq)
+					max_freq = pjcpu->target_freq;
+			}
+
+			if (max_freq != pcpu->policy->cur)
+				__cpufreq_driver_target(pcpu->policy,
+							max_freq,
+							CPUFREQ_RELATION_H);
+			trace_cpufreq_umbrella_core_setspeed(cpu,
+						     pcpu->target_freq,
+						     pcpu->policy->cur);
+
+			up_read(&pcpu->enable_sem);
+		}
+	}
+
+	return 0;
+}
+
+static void cpufreq_umbrella_core_boost(void)
+{
+	int i;
+	int anyboost = 0;
+	unsigned long flags;
+	struct cpufreq_umbrella_core_cpuinfo *pcpu;
+
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+	for_each_online_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+
+		if (pcpu->target_freq < hispeed_freq) {
+			pcpu->target_freq = hispeed_freq;
+			cpumask_set_cpu(i, &speedchange_cpumask);
+			pcpu->hispeed_validate_time =
+				ktime_to_us(ktime_get());
+			anyboost = 1;
+		}
+
+		/*
+		 * Set floor freq and (re)start timer for when last
+		 * validated.
+		 */
+
+		pcpu->floor_freq = hispeed_freq;
+		pcpu->floor_validate_time = ktime_to_us(ktime_get());
+	}
+
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+	if (anyboost)
+		wake_up_process(speedchange_task);
+}
+
+static int cpufreq_umbrella_core_notifier(
+	struct notifier_block *nb, unsigned long val, void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpufreq_umbrella_core_cpuinfo *pcpu;
+	int cpu;
+	unsigned long flags;
+
+	if (val == CPUFREQ_POSTCHANGE) {
+		pcpu = &per_cpu(cpuinfo, freq->cpu);
+		if (!down_read_trylock(&pcpu->enable_sem))
+			return 0;
+		if (!pcpu->governor_enabled) {
+			up_read(&pcpu->enable_sem);
+			return 0;
+		}
+
+		for_each_cpu(cpu, pcpu->policy->cpus) {
+			struct cpufreq_umbrella_core_cpuinfo *pjcpu =
+				&per_cpu(cpuinfo, cpu);
+			if (cpu != freq->cpu) {
+				if (!down_read_trylock(&pjcpu->enable_sem))
+					continue;
+				if (!pjcpu->governor_enabled) {
+					up_read(&pjcpu->enable_sem);
+					continue;
+				}
+			}
+			spin_lock_irqsave(&pjcpu->load_lock, flags);
+			update_load(cpu);
+			spin_unlock_irqrestore(&pjcpu->load_lock, flags);
+			if (cpu != freq->cpu)
+				up_read(&pjcpu->enable_sem);
+		}
+
+		up_read(&pcpu->enable_sem);
+	}
+	return 0;
+}
+
+static struct notifier_block cpufreq_notifier_block = {
+	.notifier_call = cpufreq_umbrella_core_notifier,
+};
+
+static unsigned int *get_tokenized_data(const char *buf, int *num_tokens)
+{
+	const char *cp;
+	int i;
+	int ntokens = 1;
+	unsigned int *tokenized_data;
+	int err = -EINVAL;
+
+	cp = buf;
+	while ((cp = strpbrk(cp + 1, " :")))
+		ntokens++;
+
+	if (!(ntokens & 0x1))
+		goto err;
+
+	tokenized_data = kmalloc(ntokens * sizeof(unsigned int), GFP_KERNEL);
+	if (!tokenized_data) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	cp = buf;
+	i = 0;
+	while (i < ntokens) {
+		if (sscanf(cp, "%u", &tokenized_data[i++]) != 1)
+			goto err_kfree;
+
+		cp = strpbrk(cp, " :");
+		if (!cp)
+			break;
+		cp++;
+	}
+
+	if (i != ntokens)
+		goto err_kfree;
+
+	*num_tokens = ntokens;
+	return tokenized_data;
+
+err_kfree:
+	kfree(tokenized_data);
+err:
+	return ERR_PTR(err);
+}
+
+static ssize_t show_target_loads(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	for (i = 0; i < ntarget_loads_set[param_index]; i++)
+		ret += sprintf(buf + ret, "%u%s", target_loads_set[param_index][i],
+			       i & 0x1 ? ":" : " ");
+#else
+	for (i = 0; i < ntarget_loads; i++)
+		ret += sprintf(buf + ret, "%u%s", target_loads[i],
+			       i & 0x1 ? ":" : " ");
+#endif
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+static ssize_t store_target_loads(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_target_loads = NULL;
+	unsigned long flags;
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	unsigned long flags2;
+#endif
+	new_target_loads = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_target_loads))
+		return PTR_RET(new_target_loads);
+
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	spin_lock_irqsave(&mode_lock, flags2);
+#endif
+	spin_lock_irqsave(&target_loads_lock, flags);
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	if (target_loads_set[param_index] != default_target_loads)
+		kfree(target_loads_set[param_index]);
+	target_loads_set[param_index] = new_target_loads;
+	ntarget_loads_set[param_index] = ntokens;
+	if (cur_param_index == param_index) {
+		target_loads = new_target_loads;
+		ntarget_loads = ntokens;
+	}
+#else
+	if (target_loads != default_target_loads)
+		kfree(target_loads);
+	target_loads = new_target_loads;
+	ntarget_loads = ntokens;
+#endif
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	spin_unlock_irqrestore(&mode_lock, flags2);
+#endif
+	return count;
+}
+
+static struct global_attr target_loads_attr =
+	__ATTR(target_loads, S_IRUGO | S_IWUSR,
+		show_target_loads, store_target_loads);
+
+static ssize_t show_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	for (i = 0; i < nabove_hispeed_delay_set[param_index]; i++)
+		ret += sprintf(buf + ret, "%u%s", above_hispeed_delay_set[param_index][i],
+			       i & 0x1 ? ":" : " ");
+#else
+	for (i = 0; i < nabove_hispeed_delay; i++)
+		ret += sprintf(buf + ret, "%u%s", above_hispeed_delay[i],
+			       i & 0x1 ? ":" : " ");
+#endif
+	sprintf(buf + ret - 1, "\n");
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static ssize_t store_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_above_hispeed_delay = NULL;
+	unsigned long flags;
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	unsigned long flags2;
+#endif
+
+	new_above_hispeed_delay = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_above_hispeed_delay))
+		return PTR_RET(new_above_hispeed_delay);
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	spin_lock_irqsave(&mode_lock, flags2);
+#endif
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	if (above_hispeed_delay_set[param_index] != default_above_hispeed_delay)
+		kfree(above_hispeed_delay_set[param_index]);
+	above_hispeed_delay_set[param_index] = new_above_hispeed_delay;
+	nabove_hispeed_delay_set[param_index] = ntokens;
+	if (cur_param_index == param_index) {
+		above_hispeed_delay = new_above_hispeed_delay;
+		nabove_hispeed_delay = ntokens;
+	}
+#else
+	if (above_hispeed_delay != default_above_hispeed_delay)
+		kfree(above_hispeed_delay);
+	above_hispeed_delay = new_above_hispeed_delay;
+	nabove_hispeed_delay = ntokens;
+#endif
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	spin_unlock_irqrestore(&mode_lock, flags2);
+#endif
+	return count;
+
+}
+
+static struct global_attr above_hispeed_delay_attr =
+	__ATTR(above_hispeed_delay, S_IRUGO | S_IWUSR,
+		show_above_hispeed_delay, store_above_hispeed_delay);
+
+static ssize_t show_hispeed_freq(struct kobject *kobj,
+				 struct attribute *attr, char *buf)
+{
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	return sprintf(buf, "%u\n", hispeed_freq_set[param_index]);
+#else
+	return sprintf(buf, "%u\n", hispeed_freq);
+#endif
+}
+
+static ssize_t store_hispeed_freq(struct kobject *kobj,
+				  struct attribute *attr, const char *buf,
+				  size_t count)
+{
+	int ret;
+	long unsigned int val;
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	unsigned long flags2;
+#endif
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	spin_lock_irqsave(&mode_lock, flags2);
+	hispeed_freq_set[param_index] = val;
+	if (cur_param_index == param_index)
+		hispeed_freq = val;
+	spin_unlock_irqrestore(&mode_lock, flags2);
+#else
+	hispeed_freq = val;
+#endif
+	return count;
+}
+
+static struct global_attr hispeed_freq_attr = __ATTR(hispeed_freq, 0644,
+		show_hispeed_freq, store_hispeed_freq);
+
+static ssize_t show_sampling_down_factor(struct kobject *kobj,
+				struct attribute *attr, char *buf)
+{
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	return sprintf(buf, "%u\n", sampling_down_factor_set[param_index]);
+#else
+	return sprintf(buf, "%u\n", sampling_down_factor);
+#endif
+}
+
+static ssize_t store_sampling_down_factor(struct kobject *kobj,
+				struct attribute *attr, const char *buf,
+				size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	unsigned long flags2;
+#endif
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	spin_lock_irqsave(&mode_lock, flags2);
+	sampling_down_factor_set[param_index] = val;
+	if (cur_param_index == param_index)
+		sampling_down_factor = val;
+	spin_unlock_irqrestore(&mode_lock, flags2);
+#else
+	sampling_down_factor = val;
+#endif
+	return count;
+}
+
+static struct global_attr sampling_down_factor_attr =
+				__ATTR(sampling_down_factor, 0644,
+		show_sampling_down_factor, store_sampling_down_factor);
+
+static ssize_t show_go_hispeed_load(struct kobject *kobj,
+				     struct attribute *attr, char *buf)
+{
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	return sprintf(buf, "%lu\n", go_hispeed_load_set[param_index]);
+#else
+	return sprintf(buf, "%lu\n", go_hispeed_load);
+#endif
+}
+
+static ssize_t store_go_hispeed_load(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	unsigned long flags2;
+#endif
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	spin_lock_irqsave(&mode_lock, flags2);
+	go_hispeed_load_set[param_index] = val;
+	if (cur_param_index == param_index)
+		go_hispeed_load = val;
+	spin_unlock_irqrestore(&mode_lock, flags2);
+#else
+	go_hispeed_load = val;
+#endif
+	return count;
+}
+
+static struct global_attr go_hispeed_load_attr = __ATTR(go_hispeed_load, 0644,
+		show_go_hispeed_load, store_go_hispeed_load);
+
+static ssize_t show_min_sample_time(struct kobject *kobj,
+				struct attribute *attr, char *buf)
+{
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	return sprintf(buf, "%lu\n", min_sample_time_set[param_index]);
+#else
+	return sprintf(buf, "%lu\n", min_sample_time);
+#endif
+}
+
+static ssize_t store_min_sample_time(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	unsigned long flags2;
+#endif
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	spin_lock_irqsave(&mode_lock, flags2);
+	min_sample_time_set[param_index] = val;
+	if (cur_param_index == param_index)
+		min_sample_time = val;
+	spin_unlock_irqrestore(&mode_lock, flags2);
+#else
+	min_sample_time = val;
+#endif
+	return count;
+}
+
+static struct global_attr min_sample_time_attr = __ATTR(min_sample_time, 0644,
+		show_min_sample_time, store_min_sample_time);
+
+static ssize_t show_timer_rate(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	return sprintf(buf, "%lu\n", timer_rate_set[param_index]);
+#else
+	return sprintf(buf, "%lu\n", timer_rate);
+#endif
+}
+
+static ssize_t store_timer_rate(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	unsigned long flags2;
+#endif
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	spin_lock_irqsave(&mode_lock, flags2);
+	timer_rate_set[param_index] = val;
+	if (cur_param_index == param_index)
+		timer_rate = val;
+	spin_unlock_irqrestore(&mode_lock, flags2);
+#else
+	timer_rate = val;
+#endif
+	return count;
+}
+
+static struct global_attr timer_rate_attr = __ATTR(timer_rate, 0644,
+		show_timer_rate, store_timer_rate);
+
+static ssize_t show_timer_slack(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", timer_slack_val);
+}
+
+static ssize_t store_timer_slack(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret < 0)
+		return ret;
+
+	timer_slack_val = val;
+	return count;
+}
+
+define_one_global_rw(timer_slack);
+
+static ssize_t show_boost(struct kobject *kobj, struct attribute *attr,
+			  char *buf)
+{
+	return sprintf(buf, "%d\n", boost_val);
+}
+
+static ssize_t store_boost(struct kobject *kobj, struct attribute *attr,
+			   const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boost_val = val;
+
+	if (boost_val) {
+		trace_cpufreq_umbrella_core_boost("on");
+		cpufreq_umbrella_core_boost();
+	} else {
+		trace_cpufreq_umbrella_core_unboost("off");
+	}
+
+	return count;
+}
+
+define_one_global_rw(boost);
+
+static ssize_t store_boostpulse(struct kobject *kobj, struct attribute *attr,
+				const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_endtime = ktime_to_us(ktime_get()) + boostpulse_duration_val;
+	trace_cpufreq_umbrella_core_boost("pulse");
+	cpufreq_umbrella_core_boost();
+	return count;
+}
+
+static struct global_attr boostpulse =
+	__ATTR(boostpulse, 0200, NULL, store_boostpulse);
+
+static ssize_t show_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", boostpulse_duration_val);
+}
+
+static ssize_t store_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_duration_val = val;
+	return count;
+}
+
+define_one_global_rw(boostpulse_duration);
+
+static ssize_t show_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	io_is_busy = val;
+	return count;
+}
+
+static struct global_attr io_is_busy_attr = __ATTR(io_is_busy, 0644,
+		show_io_is_busy, store_io_is_busy);
+
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE_BOOST
+static ssize_t show_bimc_hispeed_freq(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", bimc_hispeed_freq);
+}
+
+static ssize_t store_bimc_hispeed_freq(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	bimc_hispeed_freq = val;
+	pr_info("cpufreq-umbrella_core: bimc_hispeed_freq will be set to : (input)%lu \n", bimc_hispeed_freq);
+
+	return count;
+}
+
+static struct global_attr bimc_hispeed_freq_attr = __ATTR(bimc_hispeed_freq, 0666,
+		show_bimc_hispeed_freq, store_bimc_hispeed_freq);
+
+#endif	// CONFIG_UC_MODE_AUTO_CHANGE_BOOST
+
+static ssize_t show_sync_freq(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sync_freq);
+}
+
+static ssize_t store_sync_freq(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	sync_freq = val;
+	return count;
+}
+
+#ifdef CONFIG_POWERSUSPEND
+static ssize_t max_inactive_freq_screen_on_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
+{
+    return sprintf(buf, "%d\n", max_inactive_freq_screen_on);
+}
+
+static ssize_t max_inactive_freq_screen_on_store(struct kobject *kobj, struct kobj_attribute *attr, const char *buf, size_t count)
+{
+    unsigned int new_max_inactive_freq_screen_on;
+    
+    if (!sscanf(buf, "%du", &new_max_inactive_freq_screen_on))
+        return -EINVAL;
+    
+    if (new_max_inactive_freq_screen_on == max_inactive_freq_screen_on)
+        return count;
+
+    max_inactive_freq_screen_on = new_max_inactive_freq_screen_on;
+    if (max_inactive_freq_screen_on < max_inactive_freq) {
+        max_inactive_freq = max_inactive_freq_screen_on;
+    }
+    return count;
+}
+
+static struct kobj_attribute max_inactive_freq_screen_on_attr = __ATTR(max_inactive_freq_screen_on, 0666, max_inactive_freq_screen_on_show, max_inactive_freq_screen_on_store);
+
+static ssize_t max_inactive_freq_screen_off_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
+{
+    return sprintf(buf, "%d\n", max_inactive_freq_screen_off);
+}
+
+static ssize_t max_inactive_freq_screen_off_store(struct kobject *kobj, struct kobj_attribute *attr, const char *buf, size_t count)
+{
+    unsigned int new_max_inactive_freq_screen_off;
+    
+    if (!sscanf(buf, "%du", &new_max_inactive_freq_screen_off))
+        return -EINVAL;
+    
+    if (new_max_inactive_freq_screen_off == max_inactive_freq_screen_off)
+        return count;
+    
+    max_inactive_freq_screen_off = new_max_inactive_freq_screen_off;
+    return count;
+}
+
+static struct kobj_attribute max_inactive_freq_screen_off_attr = __ATTR(max_inactive_freq_screen_off, 0666, max_inactive_freq_screen_off_show, max_inactive_freq_screen_off_store);
+#else
+static ssize_t max_inactive_freq_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
+{
+    return sprintf(buf, "%d\n", max_inactive_freq);
+}
+
+static ssize_t max_inactive_freq_store(struct kobject *kobj, struct kobj_attribute *attr, const char *buf, size_t count)
+{
+    unsigned int new_max_inactive_freq;
+    
+    if (!sscanf(buf, "%du", &new_max_inactive_freq))
+        return -EINVAL;
+    
+    if (new_max_inactive_freq == max_inactive_freq)
+        return count;
+    
+    max_inactive_freq = new_max_inactive_freq;
+    return count;
+}
+
+static struct kobj_attribute max_inactive_freq_attr = __ATTR(max_inactive_freq, 0666, max_inactive_freq_show, max_inactive_freq_store);
+#endif
+
+static struct global_attr sync_freq_attr = __ATTR(sync_freq, 0644,
+		show_sync_freq, store_sync_freq);
+
+static ssize_t show_up_threshold_any_cpu_load(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", up_threshold_any_cpu_load);
+}
+
+static ssize_t store_up_threshold_any_cpu_load(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	up_threshold_any_cpu_load = val;
+	return count;
+}
+
+static struct global_attr up_threshold_any_cpu_load_attr =
+		__ATTR(up_threshold_any_cpu_load, 0644,
+		show_up_threshold_any_cpu_load,
+				store_up_threshold_any_cpu_load);
+
+static ssize_t show_up_threshold_any_cpu_freq(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", up_threshold_any_cpu_freq);
+}
+
+static ssize_t store_up_threshold_any_cpu_freq(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	up_threshold_any_cpu_freq = val;
+	return count;
+}
+
+static struct global_attr up_threshold_any_cpu_freq_attr =
+		__ATTR(up_threshold_any_cpu_freq, 0644,
+		show_up_threshold_any_cpu_freq,
+				store_up_threshold_any_cpu_freq);
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+#define index(obj_name, obj_attr)					\
+static ssize_t show_##obj_name(struct kobject *kobj,			\
+                              struct attribute *attr, char *buf)	\
+{									\
+        return sprintf(buf, "%u\n", obj_name);				\
+}									\
+									\
+static ssize_t store_##obj_name(struct kobject *kobj,			\
+                               struct attribute *attr, const char *buf,	\
+                               size_t count)				\
+{									\
+        int ret;							\
+        long unsigned int val;						\
+									\
+        ret = strict_strtoul(buf, 0, &val);				\
+        if (ret < 0)							\
+                return ret;						\
+									\
+	val &= MULTI_MODE | SINGLE_MODE | NO_MODE;			\
+        obj_name = val;							\
+        return count;							\
+}									\
+									\
+static struct global_attr obj_attr = __ATTR(obj_name, 0666,		\
+                show_##obj_name, store_##obj_name);			\
+
+index(mode, mode_attr);
+index(enforced_mode, enforced_mode_attr);
+index(param_index, param_index_attr);
+
+#define load(obj_name, obj_attr)					\
+static ssize_t show_##obj_name(struct kobject *kobj,			\
+                              struct attribute *attr, char *buf)	\
+{									\
+        return sprintf(buf, "%u\n", obj_name);				\
+}									\
+									\
+static ssize_t store_##obj_name(struct kobject *kobj,			\
+                               struct attribute *attr, const char *buf,	\
+                               size_t count)				\
+{									\
+        int ret;							\
+        long unsigned int val;						\
+									\
+        ret = strict_strtoul(buf, 0, &val);				\
+        if (ret < 0)							\
+                return ret;						\
+									\
+        obj_name = val;							\
+        return count;							\
+}									\
+									\
+static struct global_attr obj_attr = __ATTR(obj_name, 0644,		\
+                show_##obj_name, store_##obj_name);			\
+
+load(multi_enter_load, multi_enter_load_attr);
+load(multi_exit_load, multi_exit_load_attr);
+load(single_enter_load, single_enter_load_attr);
+load(single_exit_load, single_exit_load_attr);
+
+#define time(obj_name, obj_attr)					\
+static ssize_t show_##obj_name(struct kobject *kobj,			\
+                              struct attribute *attr, char *buf)	\
+{									\
+        return sprintf(buf, "%lu\n", obj_name);				\
+}									\
+									\
+static ssize_t store_##obj_name(struct kobject *kobj,			\
+                               struct attribute *attr, const char *buf,	\
+                               size_t count)				\
+{									\
+        int ret;							\
+        unsigned long val;						\
+									\
+        ret = strict_strtoul(buf, 0, &val);				\
+        if (ret < 0)							\
+                return ret;						\
+									\
+        obj_name = val;							\
+        return count;							\
+}									\
+									\
+static struct global_attr obj_attr = __ATTR(obj_name, 0644,		\
+                show_##obj_name, store_##obj_name);			\
+
+time(multi_enter_time, multi_enter_time_attr);
+time(multi_exit_time, multi_exit_time_attr);
+time(single_enter_time, single_enter_time_attr);
+time(single_exit_time, single_exit_time_attr);
+
+#endif
+static struct attribute *umbrella_core_attributes[] = {
+	&target_loads_attr.attr,
+	&above_hispeed_delay_attr.attr,
+	&hispeed_freq_attr.attr,
+	&go_hispeed_load_attr.attr,
+	&min_sample_time_attr.attr,
+	&timer_rate_attr.attr,
+	&timer_slack.attr,
+	&boost.attr,
+	&boostpulse.attr,
+	&boostpulse_duration.attr,
+	&io_is_busy_attr.attr,
+	&sampling_down_factor_attr.attr,
+	&sync_freq_attr.attr,
+	&up_threshold_any_cpu_load_attr.attr,
+	&up_threshold_any_cpu_freq_attr.attr,
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	&mode_attr.attr,
+	&enforced_mode_attr.attr,
+	&param_index_attr.attr,
+	&multi_enter_load_attr.attr,
+	&multi_exit_load_attr.attr,
+	&single_enter_load_attr.attr,
+	&single_exit_load_attr.attr,
+	&multi_enter_time_attr.attr,
+	&multi_exit_time_attr.attr,
+	&single_enter_time_attr.attr,
+	&single_exit_time_attr.attr,
+#endif
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE_BOOST
+	&bimc_hispeed_freq_attr.attr,
+#endif
+#ifdef CONFIG_POWERSUSPEND
+    &max_inactive_freq_screen_on_attr.attr,
+    &max_inactive_freq_screen_off_attr.attr,
+#else
+    &max_inactive_freq_attr.attr,
+#endif
+	NULL,
+};
+
+static struct attribute_group umbrella_core_attr_group = {
+	.attrs = umbrella_core_attributes,
+	.name = "umbrella_core",
+};
+
+static int cpufreq_umbrella_core_idle_notifier(struct notifier_block *nb,
+					     unsigned long val,
+					     void *data)
+{
+	switch (val) {
+	case IDLE_START:
+		cpufreq_umbrella_core_idle_start();
+		break;
+	case IDLE_END:
+		cpufreq_umbrella_core_idle_end();
+		break;
+	}
+
+	return 0;
+}
+
+static struct notifier_block cpufreq_umbrella_core_idle_nb = {
+	.notifier_call = cpufreq_umbrella_core_idle_notifier,
+};
+
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+static void cpufreq_param_set_init(void)
+{
+	unsigned int i;
+	unsigned long flags;
+
+	multi_enter_load = DEFAULT_TARGET_LOAD * num_possible_cpus();
+
+	spin_lock_irqsave(&mode_lock, flags);
+	for (i=0 ; i<MAX_PARAM_SET; i++) {
+		hispeed_freq_set[i] = 0;
+		go_hispeed_load_set[i] = go_hispeed_load;
+		target_loads_set[i] = target_loads;
+		ntarget_loads_set[i] = ntarget_loads;
+		min_sample_time_set[i] = min_sample_time;
+		timer_rate_set[i] = timer_rate;
+		above_hispeed_delay_set[i] = above_hispeed_delay;
+		nabove_hispeed_delay_set[i] = nabove_hispeed_delay;
+		sampling_down_factor_set[i] = sampling_down_factor;
+	}
+	spin_unlock_irqrestore(&mode_lock, flags);
+}
+#endif
+
+static int cpufreq_governor_umbrella_core(struct cpufreq_policy *policy,
+		unsigned int event)
+{
+	int rc;
+	unsigned int j;
+	struct cpufreq_umbrella_core_cpuinfo *pcpu;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned long expire_time;
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		mutex_lock(&gov_lock);
+
+		freq_table =
+			cpufreq_frequency_get_table(policy->cpu);
+		if (!hispeed_freq)
+			hispeed_freq = policy->max;
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+		for (j=0 ; j<MAX_PARAM_SET ; j++)
+			if (!hispeed_freq_set[j])
+				hispeed_freq_set[j] = policy->max;
+#endif
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			pcpu->policy = policy;
+			pcpu->target_freq = policy->cur;
+			pcpu->freq_table = freq_table;
+			pcpu->floor_freq = pcpu->target_freq;
+			pcpu->floor_validate_time =
+				ktime_to_us(ktime_get());
+			pcpu->hispeed_validate_time =
+				pcpu->floor_validate_time;
+			down_write(&pcpu->enable_sem);
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			cpufreq_umbrella_core_timer_start(j, 0);
+			pcpu->governor_enabled = 1;
+			up_write(&pcpu->enable_sem);
+		}
+
+		/*
+		 * Do not register the idle hook and create sysfs
+		 * entries if we have already done so.
+		 */
+		if (++active_count > 1) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		if (!have_governor_per_policy())
+			WARN_ON(cpufreq_get_global_kobject());
+
+		rc = sysfs_create_group(get_governor_parent_kobj(policy),
+				&umbrella_core_attr_group);
+		if (rc) {
+			mutex_unlock(&gov_lock);
+			return rc;
+		}
+
+		idle_notifier_register(&cpufreq_umbrella_core_idle_nb);
+		cpufreq_register_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		mutex_unlock(&gov_lock);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		mutex_lock(&gov_lock);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			down_write(&pcpu->enable_sem);
+			pcpu->governor_enabled = 0;
+			pcpu->target_freq = 0;
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			up_write(&pcpu->enable_sem);
+		}
+
+		if (--active_count > 0) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		cpufreq_unregister_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		idle_notifier_unregister(&cpufreq_umbrella_core_idle_nb);
+		sysfs_remove_group(get_governor_parent_kobj(policy),
+				&umbrella_core_attr_group);
+		if (!have_governor_per_policy())
+			cpufreq_put_global_kobject();
+		mutex_unlock(&gov_lock);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		if (policy->max < policy->cur)
+			__cpufreq_driver_target(policy,
+					policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > policy->cur)
+			__cpufreq_driver_target(policy,
+					policy->min, CPUFREQ_RELATION_C);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+
+			/* hold write semaphore to avoid race */
+			down_write(&pcpu->enable_sem);
+			if (pcpu->governor_enabled == 0) {
+				up_write(&pcpu->enable_sem);
+				continue;
+			}
+
+			/* update target_freq firstly */
+			if (policy->max < pcpu->target_freq)
+				pcpu->target_freq = policy->max;
+			/*
+			 * Delete and reschedule timer.
+			 * Else the timer callback may return without
+			 * re-arming the timer when it fails to acquire
+			 * the semaphore. This race condition may cause the
+			 * timer to stop unexpectedly.
+			 */
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+
+			if (pcpu->nr_timer_resched) {
+				if (pcpu->policy->max < pcpu->target_freq)
+					pcpu->target_freq = pcpu->policy->max;
+				if (pcpu->policy->min >= pcpu->target_freq)
+					pcpu->target_freq = pcpu->policy->min;
+				/*
+				 * To avoid deferring load evaluation for a
+				 * long time rearm the timer for the same jiffy
+				 * as it was supposed to fire at, if it has
+				 * already been rescheduled once. The timer
+				 * start and rescheduling functions aren't used
+				 * here so that the timestamps used for load
+				 * calculations do not get reset.
+				 */
+				add_timer_on(&pcpu->cpu_timer, j);
+				if (timer_slack_val >= 0 && pcpu->target_freq >
+							pcpu->policy->min)
+					add_timer_on(&pcpu->cpu_slack_timer, j);
+			} else if (policy->min >= pcpu->target_freq) {
+				pcpu->target_freq = policy->min;
+				/*
+				 * Reschedule timer.
+				 * The governor needs more time to evaluate
+				 * the load after changing policy parameters.
+				 */
+				cpufreq_umbrella_core_timer_start(j, 0);
+				pcpu->nr_timer_resched++;
+			} else {
+				/*
+				 * Reschedule timer with variable duration.
+				 * No boost was applied so the governor
+				 * doesn't need extra time to evaluate load.
+				 * The timer can be set to fire quicker if it
+				 * was already going to expire soon.
+				 */
+				expire_time = pcpu->cpu_timer.expires - jiffies;
+				expire_time = min(usecs_to_jiffies(timer_rate),
+						  expire_time);
+				expire_time = max(MIN_TIMER_JIFFIES,
+						  expire_time);
+
+				cpufreq_umbrella_core_timer_start(j, expire_time);
+				pcpu->nr_timer_resched++;
+			}
+			pcpu->limits_changed = true;
+			up_write(&pcpu->enable_sem);
+		}
+		break;
+	}
+	return 0;
+}
+
+#ifdef CONFIG_POWERSUSPEND
+static void cpufreq_umbrella_core_power_suspend(struct power_suspend *h)
+{
+    mutex_lock(&gov_lock);
+    if (max_inactive_freq_screen_off < max_inactive_freq) {
+        max_inactive_freq = max_inactive_freq_screen_off;
+    }
+    mutex_unlock(&gov_lock);
+}
+
+static void cpufreq_umbrella_core_power_resume(struct power_suspend *h)
+{
+    mutex_lock(&gov_lock);
+    if (max_inactive_freq_screen_on < max_inactive_freq) {
+        max_inactive_freq = max_inactive_freq_screen_on;
+    }
+    mutex_unlock(&gov_lock);
+}
+
+static struct power_suspend cpufreq_umbrella_core_power_suspend_info = {
+    .suspend = cpufreq_umbrella_core_power_suspend,
+    .resume = cpufreq_umbrella_core_power_resume,
+};
+#endif
+
+static void cpufreq_umbrella_core_nop_timer(unsigned long data)
+{
+}
+
+static int __init cpufreq_umbrella_core_init(void)
+{
+	unsigned int i;
+	struct cpufreq_umbrella_core_cpuinfo *pcpu;
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+
+	/* Initalize per-cpu timers */
+	for_each_possible_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+		init_timer_deferrable(&pcpu->cpu_timer);
+		pcpu->cpu_timer.function = cpufreq_umbrella_core_timer;
+		pcpu->cpu_timer.data = i;
+		init_timer(&pcpu->cpu_slack_timer);
+		pcpu->cpu_slack_timer.function = cpufreq_umbrella_core_nop_timer;
+		spin_lock_init(&pcpu->load_lock);
+		init_rwsem(&pcpu->enable_sem);
+	}
+
+	spin_lock_init(&target_loads_lock);
+	spin_lock_init(&speedchange_cpumask_lock);
+	spin_lock_init(&above_hispeed_delay_lock);
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE
+	spin_lock_init(&mode_lock);
+	cpufreq_param_set_init();
+#endif
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE_BOOST
+	mode_auto_change_boost_wq = alloc_workqueue("mode_auto_change_boost_wq", WQ_HIGHPRI, 0);
+	if(!mode_auto_change_boost_wq)
+		pr_info("mode auto change boost workqueue init error\n");
+	INIT_WORK(&mode_auto_change_boost_work, mode_auto_change_boost);
+#endif
+	mutex_init(&gov_lock);
+	speedchange_task =
+		kthread_create(cpufreq_umbrella_core_speedchange_task, NULL,
+			       "cfumbrella_core");
+	if (IS_ERR(speedchange_task))
+		return PTR_ERR(speedchange_task);
+
+	sched_setscheduler_nocheck(speedchange_task, SCHED_FIFO, &param);
+	get_task_struct(speedchange_task);
+
+	/* NB: wake up so the thread does not look hung to the freezer */
+	wake_up_process(speedchange_task);
+#ifdef CONFIG_POWERSUSPEND
+    register_power_suspend(&cpufreq_umbrella_core_power_suspend_info);
+#endif
+	return cpufreq_register_governor(&cpufreq_gov_umbrella_core);
+}
+
+#ifdef CONFIG_UC_MODE_AUTO_CHANGE_BOOST
+static void mode_auto_change_boost(struct work_struct *work)
+{
+	if(mode_count == 1) {
+		request_bimc_clk(bimc_hispeed_freq);
+		msm_pm_retention_mode_enable(0);
+	}
+	else if(mode_count == 0) {
+		request_bimc_clk(0);
+		msm_pm_retention_mode_enable(1);
+	}
+}
+#endif	// CONFIG_UC_MODE_AUTO_CHANGE_BOOST
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_UMBRELLA_CORE
+fs_initcall(cpufreq_umbrella_core_init);
+#else
+module_init(cpufreq_umbrella_core_init);
+#endif
+
+static void __exit cpufreq_umbrella_core_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_umbrella_core);
+	kthread_stop(speedchange_task);
+	put_task_struct(speedchange_task);
+}
+
+module_exit(cpufreq_umbrella_core_exit);
+
+MODULE_AUTHOR("LoungeKatt <twistedumbrella@gmail.com>");
+MODULE_DESCRIPTION("'cpufreq_umbrella_core' - A cpufreq governor for "
+	"Latency sensitive workloads");
+MODULE_LICENSE("GPL");
diff --git a/drivers/cpufreq/cpufreq_yankactive.c b/drivers/cpufreq/cpufreq_yankactive.c
new file mode 100644
index 0000000..efe82fd
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_yankactive.c
@@ -0,0 +1,1438 @@
+/*
+ * drivers/cpufreq/cpufreq_yankactive.c
+ *
+ * Copyright (C) 2010 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * Author: Mike Chan (mike@android.com)
+ *
+ */
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/rwsem.h>
+#include <linux/sched.h>
+#include <linux/sched/rt.h>
+#include <linux/tick.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/kernel_stat.h>
+#include <asm/cputime.h>
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/cpufreq_yankactive.h>
+
+#define DEFAULT_HISPEED_FREQ	1728000
+
+static int active_count;
+
+struct cpufreq_yankactive_cpuinfo {
+	struct timer_list cpu_timer;
+	struct timer_list cpu_slack_timer;
+	spinlock_t load_lock; /* protects the next 4 fields */
+	u64 time_in_idle;
+	u64 time_in_idle_timestamp;
+	u64 cputime_speedadj;
+	u64 cputime_speedadj_timestamp;
+	struct cpufreq_policy *policy;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int target_freq;
+	unsigned int floor_freq;
+	u64 floor_validate_time;
+	u64 hispeed_validate_time;
+	struct rw_semaphore enable_sem;
+	int governor_enabled;
+	int prev_load;
+	bool limits_changed;
+	unsigned int nr_timer_resched;
+};
+
+#define MIN_TIMER_JIFFIES 1UL
+
+static DEFINE_PER_CPU(struct cpufreq_yankactive_cpuinfo, cpuinfo);
+
+/* realtime thread handles frequency scaling */
+static struct task_struct *speedchange_task;
+static cpumask_t speedchange_cpumask;
+static spinlock_t speedchange_cpumask_lock;
+static struct mutex gov_lock;
+
+/* Hi speed to bump to from lo speed when load burst (default max) */
+static unsigned int hispeed_freq;
+
+/* Go to hi speed when CPU load at or above this value. */
+#define DEFAULT_GO_HISPEED_LOAD 99
+static unsigned long go_hispeed_load = DEFAULT_GO_HISPEED_LOAD;
+
+/* Sampling down factor to be applied to min_sample_time at max freq */
+static unsigned int sampling_down_factor;
+
+/* Target load.  Lower values result in higher CPU speeds. */
+#define DEFAULT_TARGET_LOAD 95
+static unsigned int default_target_loads[] = {DEFAULT_TARGET_LOAD};
+static spinlock_t target_loads_lock;
+static unsigned int *target_loads = default_target_loads;
+static int ntarget_loads = ARRAY_SIZE(default_target_loads);
+
+/*
+ * The minimum amount of time to spend at a frequency before we can ramp down.
+ */
+#define DEFAULT_MIN_SAMPLE_TIME (20 * USEC_PER_MSEC)
+static unsigned long min_sample_time = DEFAULT_MIN_SAMPLE_TIME;
+
+/*
+ * The sample rate of the timer used to increase frequency
+ */
+#define DEFAULT_TIMER_RATE (20 * USEC_PER_MSEC)
+static unsigned long timer_rate = DEFAULT_TIMER_RATE;
+
+/* Busy SDF parameters*/
+#define MIN_BUSY_TIME (100 * USEC_PER_MSEC)
+
+/*
+ * Wait this long before raising speed above hispeed, by default a single
+ * timer interval.
+ */
+#define DEFAULT_ABOVE_HISPEED_DELAY (80 * USEC_PER_MSEC)
+static unsigned int default_above_hispeed_delay[] = {
+	DEFAULT_ABOVE_HISPEED_DELAY };
+static spinlock_t above_hispeed_delay_lock;
+static unsigned int *above_hispeed_delay = default_above_hispeed_delay;
+static int nabove_hispeed_delay = ARRAY_SIZE(default_above_hispeed_delay);
+
+/* Non-zero means indefinite speed boost active */
+static int boost_val;
+/* Duration of a boot pulse in usecs */
+static int boostpulse_duration_val = DEFAULT_MIN_SAMPLE_TIME;
+/* End time of boost pulse in ktime converted to usecs */
+static u64 boostpulse_endtime;
+
+/*
+ * Max additional time to wait in idle, beyond timer_rate, at speeds above
+ * minimum before wakeup to reduce speed, or -1 if unnecessary.
+ */
+#define DEFAULT_TIMER_SLACK (4 * DEFAULT_TIMER_RATE)
+static int timer_slack_val = DEFAULT_TIMER_SLACK;
+
+static bool io_is_busy;
+
+/*
+ * If the max load among other CPUs is higher than up_threshold_any_cpu_load
+ * and if the highest frequency among the other CPUs is higher than
+ * up_threshold_any_cpu_freq then do not let the frequency to drop below
+ * sync_freq
+ */
+static unsigned int up_threshold_any_cpu_load;
+static unsigned int sync_freq;
+static unsigned int up_threshold_any_cpu_freq;
+
+static int cpufreq_governor_yankactive(struct cpufreq_policy *policy,
+		unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_YANKACTIVE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_yankactive = {
+	.name = "yankactive",
+	.governor = cpufreq_governor_yankactive,
+	.max_transition_latency = 10000000,
+	.owner = THIS_MODULE,
+};
+
+static void cpufreq_yankactive_timer_resched(
+	struct cpufreq_yankactive_cpuinfo *pcpu)
+{
+	unsigned long expires;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->time_in_idle =
+		get_cpu_idle_time(smp_processor_id(),
+				  &pcpu->time_in_idle_timestamp, io_is_busy);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	expires = jiffies + usecs_to_jiffies(timer_rate);
+	mod_timer_pinned(&pcpu->cpu_timer, expires);
+
+	if (timer_slack_val >= 0 && pcpu->target_freq > pcpu->policy->min) {
+		expires += usecs_to_jiffies(timer_slack_val);
+		mod_timer_pinned(&pcpu->cpu_slack_timer, expires);
+	}
+
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+/* The caller shall take enable_sem write semaphore to avoid any timer race.
+ * The cpu_timer and cpu_slack_timer must be deactivated when calling this
+ * function.
+ */
+static void cpufreq_yankactive_timer_start(int cpu, int time_override)
+{
+	struct cpufreq_yankactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	unsigned long flags;
+	unsigned long expires;
+	if (time_override)
+		expires = jiffies + time_override;
+	else
+		expires = jiffies + usecs_to_jiffies(timer_rate);
+
+	pcpu->cpu_timer.expires = expires;
+	add_timer_on(&pcpu->cpu_timer, cpu);
+	if (timer_slack_val >= 0 && pcpu->target_freq > pcpu->policy->min) {
+		expires += usecs_to_jiffies(timer_slack_val);
+		pcpu->cpu_slack_timer.expires = expires;
+		add_timer_on(&pcpu->cpu_slack_timer, cpu);
+	}
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->time_in_idle =
+		get_cpu_idle_time(cpu, &pcpu->time_in_idle_timestamp,
+				  io_is_busy);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+static unsigned int freq_to_above_hispeed_delay(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay - 1 &&
+			freq >= above_hispeed_delay[i+1]; i += 2)
+		;
+
+	ret = above_hispeed_delay[i];
+	ret = (ret > (1 * USEC_PER_MSEC)) ? (ret - (1 * USEC_PER_MSEC)) : ret;
+
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static unsigned int freq_to_targetload(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+
+	for (i = 0; i < ntarget_loads - 1 && freq >= target_loads[i+1]; i += 2)
+		;
+
+	ret = target_loads[i];
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+/*
+ * If increasing frequencies never map to a lower target load then
+ * choose_freq() will find the minimum frequency that does not exceed its
+ * target load given the current load.
+ */
+
+static unsigned int choose_freq(
+	struct cpufreq_yankactive_cpuinfo *pcpu, unsigned int loadadjfreq)
+{
+	unsigned int freq = pcpu->policy->cur;
+	unsigned int prevfreq, freqmin, freqmax;
+	unsigned int tl;
+	int index;
+
+	freqmin = 0;
+	freqmax = UINT_MAX;
+
+	do {
+		prevfreq = freq;
+		tl = freq_to_targetload(freq);
+
+		/*
+		 * Find the lowest frequency where the computed load is less
+		 * than or equal to the target load.
+		 */
+
+		if (cpufreq_frequency_table_target(
+			    pcpu->policy, pcpu->freq_table, loadadjfreq / tl,
+			    CPUFREQ_RELATION_L, &index))
+			break;
+		freq = pcpu->freq_table[index].frequency;
+
+		if (freq > prevfreq) {
+			/* The previous frequency is too low. */
+			freqmin = prevfreq;
+
+			if (freq >= freqmax) {
+				/*
+				 * Find the highest frequency that is less
+				 * than freqmax.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmax - 1, CPUFREQ_RELATION_H,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				if (freq == freqmin) {
+					/*
+					 * The first frequency below freqmax
+					 * has already been found to be too
+					 * low.  freqmax is the lowest speed
+					 * we found that is fast enough.
+					 */
+					freq = freqmax;
+					break;
+				}
+			}
+		} else if (freq < prevfreq) {
+			/* The previous frequency is high enough. */
+			freqmax = prevfreq;
+
+			if (freq <= freqmin) {
+				/*
+				 * Find the lowest frequency that is higher
+				 * than freqmin.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmin + 1, CPUFREQ_RELATION_L,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				/*
+				 * If freqmax is the first frequency above
+				 * freqmin then we have already found that
+				 * this speed is fast enough.
+				 */
+				if (freq == freqmax)
+					break;
+			}
+		}
+
+		/* If same frequency chosen as previous then done. */
+	} while (freq != prevfreq);
+
+	return freq;
+}
+
+static u64 update_load(int cpu)
+{
+	struct cpufreq_yankactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 now;
+	u64 now_idle;
+	unsigned int delta_idle;
+	unsigned int delta_time;
+	u64 active_time;
+
+	now_idle = get_cpu_idle_time(cpu, &now, io_is_busy);
+	delta_idle = (unsigned int)(now_idle - pcpu->time_in_idle);
+	delta_time = (unsigned int)(now - pcpu->time_in_idle_timestamp);
+
+	if (delta_time <= delta_idle)
+		active_time = 0;
+	else
+		active_time = delta_time - delta_idle;
+
+	pcpu->cputime_speedadj += active_time * pcpu->policy->cur;
+
+	pcpu->time_in_idle = now_idle;
+	pcpu->time_in_idle_timestamp = now;
+	return now;
+}
+
+static void cpufreq_yankactive_timer(unsigned long data)
+{
+	u64 now;
+	unsigned int delta_time;
+	u64 cputime_speedadj;
+	int cpu_load;
+	struct cpufreq_yankactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, data);
+	unsigned int new_freq;
+	unsigned int loadadjfreq;
+	unsigned int index;
+	unsigned long flags;
+	bool boosted;
+	unsigned long mod_min_sample_time;
+	int i, max_load;
+	unsigned int max_freq;
+	struct cpufreq_yankactive_cpuinfo *picpu;
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled)
+		goto exit;
+
+	pcpu->nr_timer_resched = 0;
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	now = update_load(data);
+	delta_time = (unsigned int)(now - pcpu->cputime_speedadj_timestamp);
+	cputime_speedadj = pcpu->cputime_speedadj;
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+
+	if (WARN_ON_ONCE(!delta_time))
+		goto rearm;
+
+	do_div(cputime_speedadj, delta_time);
+	loadadjfreq = (unsigned int)cputime_speedadj * 100;
+	cpu_load = loadadjfreq / pcpu->target_freq;
+	pcpu->prev_load = cpu_load;
+	boosted = boost_val || now < boostpulse_endtime;
+
+	if (cpu_load >= go_hispeed_load || boosted) {
+		if (pcpu->target_freq < hispeed_freq) {
+			new_freq = hispeed_freq;
+		} else {
+			new_freq = choose_freq(pcpu, loadadjfreq);
+
+			if (new_freq < hispeed_freq)
+				new_freq = hispeed_freq;
+		}
+	} else {
+		new_freq = choose_freq(pcpu, loadadjfreq);
+
+		if (sync_freq && new_freq < sync_freq) {
+
+			max_load = 0;
+			max_freq = 0;
+
+			for_each_online_cpu(i) {
+				picpu = &per_cpu(cpuinfo, i);
+
+				if (i == data || picpu->prev_load <
+						up_threshold_any_cpu_load)
+					continue;
+
+				max_load = max(max_load, picpu->prev_load);
+				max_freq = max(max_freq, picpu->target_freq);
+			}
+
+			if (max_freq > up_threshold_any_cpu_freq &&
+				max_load >= up_threshold_any_cpu_load)
+				new_freq = sync_freq;
+		}
+	}
+
+	if (pcpu->target_freq >= hispeed_freq &&
+	    new_freq > pcpu->target_freq &&
+	    now - pcpu->hispeed_validate_time <
+	    freq_to_above_hispeed_delay(pcpu->target_freq)) {
+		trace_cpufreq_yankactive_notyet(
+			data, cpu_load, pcpu->target_freq,
+			pcpu->policy->cur, new_freq);
+		goto rearm;
+	}
+
+	pcpu->hispeed_validate_time = now;
+
+	if (cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table,
+					   new_freq, CPUFREQ_RELATION_L,
+					   &index))
+		goto rearm;
+
+	new_freq = pcpu->freq_table[index].frequency;
+
+	/*
+	 * Do not scale below floor_freq unless we have been at or above the
+	 * floor frequency for the minimum sample time since last validated.
+	 */
+	if (sampling_down_factor && pcpu->policy->cur == pcpu->policy->max)
+		mod_min_sample_time = sampling_down_factor;
+	else
+		mod_min_sample_time = min_sample_time;
+
+	if (pcpu->limits_changed) {
+		if (sampling_down_factor &&
+			(pcpu->policy->cur != pcpu->policy->max))
+			mod_min_sample_time = 0;
+
+		pcpu->limits_changed = false;
+	}
+
+	if (new_freq < pcpu->floor_freq) {
+		if (now - pcpu->floor_validate_time < mod_min_sample_time) {
+			trace_cpufreq_yankactive_notyet(
+				data, cpu_load, pcpu->target_freq,
+				pcpu->policy->cur, new_freq);
+			goto rearm;
+		}
+	}
+
+	/*
+	 * Update the timestamp for checking whether speed has been held at
+	 * or above the selected frequency for a minimum of min_sample_time,
+	 * if not boosted to hispeed_freq.  If boosted to hispeed_freq then we
+	 * allow the speed to drop as soon as the boostpulse duration expires
+	 * (or the indefinite boost is turned off).
+	 */
+
+	if (!boosted || new_freq > hispeed_freq) {
+		pcpu->floor_freq = new_freq;
+		pcpu->floor_validate_time = now;
+	}
+
+	if (pcpu->target_freq == new_freq) {
+		trace_cpufreq_yankactive_already(
+			data, cpu_load, pcpu->target_freq,
+			pcpu->policy->cur, new_freq);
+		goto rearm_if_notmax;
+	}
+
+	trace_cpufreq_yankactive_target(data, cpu_load, pcpu->target_freq,
+					 pcpu->policy->cur, new_freq);
+
+	pcpu->target_freq = new_freq;
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+	cpumask_set_cpu(data, &speedchange_cpumask);
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+	wake_up_process(speedchange_task);
+
+rearm_if_notmax:
+	/*
+	 * Already set max speed and don't see a need to change that,
+	 * wait until next idle to re-evaluate, don't need timer.
+	 */
+	if (pcpu->target_freq == pcpu->policy->max)
+		goto exit;
+
+rearm:
+	if (!timer_pending(&pcpu->cpu_timer))
+		cpufreq_yankactive_timer_resched(pcpu);
+
+exit:
+	up_read(&pcpu->enable_sem);
+	return;
+}
+
+static void cpufreq_yankactive_idle_start(void)
+{
+	struct cpufreq_yankactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+	int pending;
+	u64 now;
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled) {
+		up_read(&pcpu->enable_sem);
+		return;
+	}
+
+	pending = timer_pending(&pcpu->cpu_timer);
+
+	if (pcpu->target_freq != pcpu->policy->min) {
+		/*
+		 * Entering idle while not at lowest speed.  On some
+		 * platforms this can hold the other CPU(s) at that speed
+		 * even though the CPU is idle. Set a timer to re-evaluate
+		 * speed so this idle CPU doesn't hold the other CPUs above
+		 * min indefinitely.  This should probably be a quirk of
+		 * the CPUFreq driver.
+		 */
+		if (!pending) {
+			cpufreq_yankactive_timer_resched(pcpu);
+
+			now = ktime_to_us(ktime_get());
+			if ((pcpu->policy->cur == pcpu->policy->max) &&
+				(now - pcpu->hispeed_validate_time) >
+							MIN_BUSY_TIME) {
+				pcpu->floor_validate_time = now;
+			}
+
+		}
+	}
+
+	up_read(&pcpu->enable_sem);
+}
+
+static void cpufreq_yankactive_idle_end(void)
+{
+	struct cpufreq_yankactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled) {
+		up_read(&pcpu->enable_sem);
+		return;
+	}
+
+	/* Arm the timer for 1-2 ticks later if not already. */
+	if (!timer_pending(&pcpu->cpu_timer)) {
+		cpufreq_yankactive_timer_resched(pcpu);
+	} else if (time_after_eq(jiffies, pcpu->cpu_timer.expires)) {
+		del_timer(&pcpu->cpu_timer);
+		del_timer(&pcpu->cpu_slack_timer);
+		cpufreq_yankactive_timer(smp_processor_id());
+	}
+
+	up_read(&pcpu->enable_sem);
+}
+
+static int cpufreq_yankactive_speedchange_task(void *data)
+{
+	unsigned int cpu;
+	cpumask_t tmp_mask;
+	unsigned long flags;
+	struct cpufreq_yankactive_cpuinfo *pcpu;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+		if (cpumask_empty(&speedchange_cpumask)) {
+			spin_unlock_irqrestore(&speedchange_cpumask_lock,
+					       flags);
+			schedule();
+
+			if (kthread_should_stop())
+				break;
+
+			spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+		}
+
+		set_current_state(TASK_RUNNING);
+		tmp_mask = speedchange_cpumask;
+		cpumask_clear(&speedchange_cpumask);
+		spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+		for_each_cpu(cpu, &tmp_mask) {
+			unsigned int j;
+			unsigned int max_freq = 0;
+
+			pcpu = &per_cpu(cpuinfo, cpu);
+			if (!down_read_trylock(&pcpu->enable_sem))
+				continue;
+			if (!pcpu->governor_enabled) {
+				up_read(&pcpu->enable_sem);
+				continue;
+			}
+
+			for_each_cpu(j, pcpu->policy->cpus) {
+				struct cpufreq_yankactive_cpuinfo *pjcpu =
+					&per_cpu(cpuinfo, j);
+
+				if (pjcpu->target_freq > max_freq)
+					max_freq = pjcpu->target_freq;
+			}
+
+			if (max_freq != pcpu->policy->cur)
+				__cpufreq_driver_target(pcpu->policy,
+							max_freq,
+							CPUFREQ_RELATION_H);
+			trace_cpufreq_yankactive_setspeed(cpu,
+						     pcpu->target_freq,
+						     pcpu->policy->cur);
+
+			up_read(&pcpu->enable_sem);
+		}
+	}
+
+	return 0;
+}
+
+static void cpufreq_yankactive_boost(void)
+{
+	int i;
+	int anyboost = 0;
+	unsigned long flags;
+	struct cpufreq_yankactive_cpuinfo *pcpu;
+
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+	for_each_online_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+
+		if (pcpu->target_freq < hispeed_freq) {
+			pcpu->target_freq = hispeed_freq;
+			cpumask_set_cpu(i, &speedchange_cpumask);
+			pcpu->hispeed_validate_time =
+				ktime_to_us(ktime_get());
+			anyboost = 1;
+		}
+
+		/*
+		 * Set floor freq and (re)start timer for when last
+		 * validated.
+		 */
+
+		pcpu->floor_freq = hispeed_freq;
+		pcpu->floor_validate_time = ktime_to_us(ktime_get());
+	}
+
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+	if (anyboost)
+		wake_up_process(speedchange_task);
+}
+
+static int cpufreq_yankactive_notifier(
+	struct notifier_block *nb, unsigned long val, void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpufreq_yankactive_cpuinfo *pcpu;
+	int cpu;
+	unsigned long flags;
+
+	if (val == CPUFREQ_POSTCHANGE) {
+		pcpu = &per_cpu(cpuinfo, freq->cpu);
+		if (!down_read_trylock(&pcpu->enable_sem))
+			return 0;
+		if (!pcpu->governor_enabled) {
+			up_read(&pcpu->enable_sem);
+			return 0;
+		}
+
+		for_each_cpu(cpu, pcpu->policy->cpus) {
+			struct cpufreq_yankactive_cpuinfo *pjcpu =
+				&per_cpu(cpuinfo, cpu);
+			if (cpu != freq->cpu) {
+				if (!down_read_trylock(&pjcpu->enable_sem))
+					continue;
+				if (!pjcpu->governor_enabled) {
+					up_read(&pjcpu->enable_sem);
+					continue;
+				}
+			}
+			spin_lock_irqsave(&pjcpu->load_lock, flags);
+			update_load(cpu);
+			spin_unlock_irqrestore(&pjcpu->load_lock, flags);
+			if (cpu != freq->cpu)
+				up_read(&pjcpu->enable_sem);
+		}
+
+		up_read(&pcpu->enable_sem);
+	}
+	return 0;
+}
+
+static struct notifier_block cpufreq_notifier_block = {
+	.notifier_call = cpufreq_yankactive_notifier,
+};
+
+static unsigned int *get_tokenized_data(const char *buf, int *num_tokens)
+{
+	const char *cp;
+	int i;
+	int ntokens = 1;
+	unsigned int *tokenized_data;
+	int err = -EINVAL;
+
+	cp = buf;
+	while ((cp = strpbrk(cp + 1, " :")))
+		ntokens++;
+
+	if (!(ntokens & 0x1))
+		goto err;
+
+	tokenized_data = kmalloc(ntokens * sizeof(unsigned int), GFP_KERNEL);
+	if (!tokenized_data) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	cp = buf;
+	i = 0;
+	while (i < ntokens) {
+		if (sscanf(cp, "%u", &tokenized_data[i++]) != 1)
+			goto err_kfree;
+
+		cp = strpbrk(cp, " :");
+		if (!cp)
+			break;
+		cp++;
+	}
+
+	if (i != ntokens)
+		goto err_kfree;
+
+	*num_tokens = ntokens;
+	return tokenized_data;
+
+err_kfree:
+	kfree(tokenized_data);
+err:
+	return ERR_PTR(err);
+}
+
+static ssize_t show_target_loads(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+
+	for (i = 0; i < ntarget_loads; i++)
+		ret += sprintf(buf + ret, "%u%s", target_loads[i],
+			       i & 0x1 ? ":" : " ");
+
+	ret += sprintf(buf + --ret, "\n");
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+static ssize_t store_target_loads(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_target_loads = NULL;
+	unsigned long flags;
+
+	new_target_loads = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_target_loads))
+		return PTR_RET(new_target_loads);
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+	if (target_loads != default_target_loads)
+		kfree(target_loads);
+	target_loads = new_target_loads;
+	ntarget_loads = ntokens;
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return count;
+}
+
+static struct global_attr target_loads_attr =
+	__ATTR(target_loads, S_IRUGO | S_IWUSR,
+		show_target_loads, store_target_loads);
+
+static ssize_t show_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay; i++)
+		ret += sprintf(buf + ret, "%u%s", above_hispeed_delay[i],
+			       i & 0x1 ? ":" : " ");
+
+	ret += sprintf(buf + --ret, "\n");
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static ssize_t store_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_above_hispeed_delay = NULL;
+	unsigned long flags;
+
+	new_above_hispeed_delay = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_above_hispeed_delay))
+		return PTR_RET(new_above_hispeed_delay);
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+	if (above_hispeed_delay != default_above_hispeed_delay)
+		kfree(above_hispeed_delay);
+	above_hispeed_delay = new_above_hispeed_delay;
+	nabove_hispeed_delay = ntokens;
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return count;
+
+}
+
+static struct global_attr above_hispeed_delay_attr =
+	__ATTR(above_hispeed_delay, S_IRUGO | S_IWUSR,
+		show_above_hispeed_delay, store_above_hispeed_delay);
+
+static ssize_t show_hispeed_freq(struct kobject *kobj,
+				 struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", hispeed_freq);
+}
+
+static ssize_t store_hispeed_freq(struct kobject *kobj,
+				  struct attribute *attr, const char *buf,
+				  size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	hispeed_freq = val;
+	return count;
+}
+
+static struct global_attr hispeed_freq_attr = __ATTR(hispeed_freq, 0644,
+		show_hispeed_freq, store_hispeed_freq);
+
+static ssize_t show_sampling_down_factor(struct kobject *kobj,
+				struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sampling_down_factor);
+}
+
+static ssize_t store_sampling_down_factor(struct kobject *kobj,
+				struct attribute *attr, const char *buf,
+				size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	sampling_down_factor = val;
+	return count;
+}
+
+static struct global_attr sampling_down_factor_attr =
+				__ATTR(sampling_down_factor, 0644,
+		show_sampling_down_factor, store_sampling_down_factor);
+
+static ssize_t show_go_hispeed_load(struct kobject *kobj,
+				     struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", go_hispeed_load);
+}
+
+static ssize_t store_go_hispeed_load(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	go_hispeed_load = val;
+	return count;
+}
+
+static struct global_attr go_hispeed_load_attr = __ATTR(go_hispeed_load, 0644,
+		show_go_hispeed_load, store_go_hispeed_load);
+
+static ssize_t show_min_sample_time(struct kobject *kobj,
+				struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", min_sample_time);
+}
+
+static ssize_t store_min_sample_time(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	min_sample_time = val;
+	return count;
+}
+
+static struct global_attr min_sample_time_attr = __ATTR(min_sample_time, 0644,
+		show_min_sample_time, store_min_sample_time);
+
+static ssize_t show_timer_rate(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", timer_rate);
+}
+
+static ssize_t store_timer_rate(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	timer_rate = val;
+	return count;
+}
+
+static struct global_attr timer_rate_attr = __ATTR(timer_rate, 0644,
+		show_timer_rate, store_timer_rate);
+
+static ssize_t show_timer_slack(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", timer_slack_val);
+}
+
+static ssize_t store_timer_slack(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret < 0)
+		return ret;
+
+	timer_slack_val = val;
+	return count;
+}
+
+define_one_global_rw(timer_slack);
+
+static ssize_t show_boost(struct kobject *kobj, struct attribute *attr,
+			  char *buf)
+{
+	return sprintf(buf, "%d\n", boost_val);
+}
+
+static ssize_t store_boost(struct kobject *kobj, struct attribute *attr,
+			   const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boost_val = val;
+
+	if (boost_val) {
+		trace_cpufreq_yankactive_boost("on");
+		cpufreq_yankactive_boost();
+	} else {
+		trace_cpufreq_yankactive_unboost("off");
+	}
+
+	return count;
+}
+
+define_one_global_rw(boost);
+
+static ssize_t store_boostpulse(struct kobject *kobj, struct attribute *attr,
+				const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_endtime = ktime_to_us(ktime_get()) + boostpulse_duration_val;
+	trace_cpufreq_yankactive_boost("pulse");
+	cpufreq_yankactive_boost();
+	return count;
+}
+
+static struct global_attr boostpulse =
+	__ATTR(boostpulse, 0200, NULL, store_boostpulse);
+
+static ssize_t show_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", boostpulse_duration_val);
+}
+
+static ssize_t store_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_duration_val = val;
+	return count;
+}
+
+define_one_global_rw(boostpulse_duration);
+
+static ssize_t show_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	io_is_busy = val;
+	return count;
+}
+
+static struct global_attr io_is_busy_attr = __ATTR(io_is_busy, 0644,
+		show_io_is_busy, store_io_is_busy);
+
+static ssize_t show_sync_freq(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sync_freq);
+}
+
+static ssize_t store_sync_freq(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	sync_freq = val;
+	return count;
+}
+
+static struct global_attr sync_freq_attr = __ATTR(sync_freq, 0644,
+		show_sync_freq, store_sync_freq);
+
+static ssize_t show_up_threshold_any_cpu_load(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", up_threshold_any_cpu_load);
+}
+
+static ssize_t store_up_threshold_any_cpu_load(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	up_threshold_any_cpu_load = val;
+	return count;
+}
+
+static struct global_attr up_threshold_any_cpu_load_attr =
+		__ATTR(up_threshold_any_cpu_load, 0644,
+		show_up_threshold_any_cpu_load,
+				store_up_threshold_any_cpu_load);
+
+static ssize_t show_up_threshold_any_cpu_freq(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", up_threshold_any_cpu_freq);
+}
+
+static ssize_t store_up_threshold_any_cpu_freq(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	up_threshold_any_cpu_freq = val;
+	return count;
+}
+
+static struct global_attr up_threshold_any_cpu_freq_attr =
+		__ATTR(up_threshold_any_cpu_freq, 0644,
+		show_up_threshold_any_cpu_freq,
+				store_up_threshold_any_cpu_freq);
+
+static struct attribute *yankactive_attributes[] = {
+	&target_loads_attr.attr,
+	&above_hispeed_delay_attr.attr,
+	&hispeed_freq_attr.attr,
+	&go_hispeed_load_attr.attr,
+	&min_sample_time_attr.attr,
+	&timer_rate_attr.attr,
+	&timer_slack.attr,
+	&boost.attr,
+	&boostpulse.attr,
+	&boostpulse_duration.attr,
+	&io_is_busy_attr.attr,
+	&sampling_down_factor_attr.attr,
+	&sync_freq_attr.attr,
+	&up_threshold_any_cpu_load_attr.attr,
+	&up_threshold_any_cpu_freq_attr.attr,
+	NULL,
+};
+
+static struct attribute_group yankactive_attr_group = {
+	.attrs = yankactive_attributes,
+	.name = "yankactive",
+};
+
+static int cpufreq_yankactive_idle_notifier(struct notifier_block *nb,
+					     unsigned long val,
+					     void *data)
+{
+	switch (val) {
+	case IDLE_START:
+		cpufreq_yankactive_idle_start();
+		break;
+	case IDLE_END:
+		cpufreq_yankactive_idle_end();
+		break;
+	}
+
+	return 0;
+}
+
+static struct notifier_block cpufreq_yankactive_idle_nb = {
+	.notifier_call = cpufreq_yankactive_idle_notifier,
+};
+
+static int cpufreq_governor_yankactive(struct cpufreq_policy *policy,
+		unsigned int event)
+{
+	int rc;
+	unsigned int j;
+	struct cpufreq_yankactive_cpuinfo *pcpu;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned long expire_time;
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if (!cpu_online(policy->cpu))
+			return -EINVAL;
+
+		mutex_lock(&gov_lock);
+
+		freq_table =
+			cpufreq_frequency_get_table(policy->cpu);
+		if (!hispeed_freq)
+			hispeed_freq = DEFAULT_HISPEED_FREQ;
+
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			pcpu->policy = policy;
+			pcpu->target_freq = policy->cur;
+			pcpu->freq_table = freq_table;
+			pcpu->floor_freq = pcpu->target_freq;
+			pcpu->floor_validate_time =
+				ktime_to_us(ktime_get());
+			pcpu->hispeed_validate_time =
+				pcpu->floor_validate_time;
+			down_write(&pcpu->enable_sem);
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			cpufreq_yankactive_timer_start(j, 0);
+			pcpu->governor_enabled = 1;
+			up_write(&pcpu->enable_sem);
+		}
+
+		/*
+		 * Do not register the idle hook and create sysfs
+		 * entries if we have already done so.
+		 */
+		if (++active_count > 1) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		if (!have_governor_per_policy())
+			WARN_ON(cpufreq_get_global_kobject());
+
+		rc = sysfs_create_group(get_governor_parent_kobj(policy),
+				&yankactive_attr_group);
+		if (rc) {
+			mutex_unlock(&gov_lock);
+			return rc;
+		}
+
+		idle_notifier_register(&cpufreq_yankactive_idle_nb);
+		cpufreq_register_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		mutex_unlock(&gov_lock);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		mutex_lock(&gov_lock);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			down_write(&pcpu->enable_sem);
+			pcpu->governor_enabled = 0;
+			pcpu->target_freq = 0;
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			up_write(&pcpu->enable_sem);
+		}
+
+		if (--active_count > 0) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		cpufreq_unregister_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		idle_notifier_unregister(&cpufreq_yankactive_idle_nb);
+		sysfs_remove_group(get_governor_parent_kobj(policy),
+				&yankactive_attr_group);
+		if (!have_governor_per_policy())
+			cpufreq_put_global_kobject();
+		mutex_unlock(&gov_lock);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		if (policy->max < policy->cur)
+			__cpufreq_driver_target(policy,
+					policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > policy->cur)
+			__cpufreq_driver_target(policy,
+					policy->min, CPUFREQ_RELATION_L);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+
+			/* hold write semaphore to avoid race */
+			down_write(&pcpu->enable_sem);
+			if (pcpu->governor_enabled == 0) {
+				up_write(&pcpu->enable_sem);
+				continue;
+			}
+
+			/* update target_freq firstly */
+			if (policy->max < pcpu->target_freq)
+				pcpu->target_freq = policy->max;
+			/*
+			 * Delete and reschedule timer.
+			 * Else the timer callback may return without
+			 * re-arming the timer when it fails to acquire
+			 * the semaphore. This race condition may cause the
+			 * timer to stop unexpectedly.
+			 */
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+
+			if (pcpu->nr_timer_resched) {
+				if (pcpu->policy->min >= pcpu->target_freq)
+					pcpu->target_freq = pcpu->policy->min;
+				/*
+				 * To avoid deferring load evaluation for a
+				 * long time rearm the timer for the same jiffy
+				 * as it was supposed to fire at, if it has
+				 * already been rescheduled once. The timer
+				 * start and rescheduling functions aren't used
+				 * here so that the timestamps used for load
+				 * calculations do not get reset.
+				 */
+				add_timer_on(&pcpu->cpu_timer, j);
+				if (timer_slack_val >= 0 && pcpu->target_freq >
+							pcpu->policy->min)
+					add_timer_on(&pcpu->cpu_slack_timer, j);
+			} else if (policy->min >= pcpu->target_freq) {
+				pcpu->target_freq = policy->min;
+				/*
+				 * Reschedule timer.
+				 * The governor needs more time to evaluate
+				 * the load after changing policy parameters.
+				 */
+				cpufreq_yankactive_timer_start(j, 0);
+				pcpu->nr_timer_resched++;
+			} else {
+				/*
+				 * Reschedule timer with variable duration.
+				 * No boost was applied so the governor
+				 * doesn't need extra time to evaluate load.
+				 * The timer can be set to fire quicker if it
+				 * was already going to expire soon.
+				 */
+				expire_time = pcpu->cpu_timer.expires - jiffies;
+				expire_time = min(usecs_to_jiffies(timer_rate),
+						  expire_time);
+				expire_time = max(MIN_TIMER_JIFFIES,
+						  expire_time);
+
+				cpufreq_yankactive_timer_start(j, expire_time);
+				pcpu->nr_timer_resched++;
+			}
+			pcpu->limits_changed = true;
+			up_write(&pcpu->enable_sem);
+		}
+		break;
+	}
+	return 0;
+}
+
+static void cpufreq_yankactive_nop_timer(unsigned long data)
+{
+}
+
+static int __init cpufreq_yankactive_init(void)
+{
+	unsigned int i;
+	struct cpufreq_yankactive_cpuinfo *pcpu;
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+
+	/* Initalize per-cpu timers */
+	for_each_possible_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+		init_timer_deferrable(&pcpu->cpu_timer);
+		pcpu->cpu_timer.function = cpufreq_yankactive_timer;
+		pcpu->cpu_timer.data = i;
+		init_timer(&pcpu->cpu_slack_timer);
+		pcpu->cpu_slack_timer.function = cpufreq_yankactive_nop_timer;
+		spin_lock_init(&pcpu->load_lock);
+		init_rwsem(&pcpu->enable_sem);
+	}
+
+	spin_lock_init(&target_loads_lock);
+	spin_lock_init(&speedchange_cpumask_lock);
+	spin_lock_init(&above_hispeed_delay_lock);
+	mutex_init(&gov_lock);
+	speedchange_task =
+		kthread_create(cpufreq_yankactive_speedchange_task, NULL,
+			       "cfyankactive");
+	if (IS_ERR(speedchange_task))
+		return PTR_ERR(speedchange_task);
+
+	sched_setscheduler_nocheck(speedchange_task, SCHED_FIFO, &param);
+	get_task_struct(speedchange_task);
+
+	/* NB: wake up so the thread does not look hung to the freezer */
+	wake_up_process(speedchange_task);
+
+	return cpufreq_register_governor(&cpufreq_gov_yankactive);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_YANKACTIVE
+fs_initcall(cpufreq_yankactive_init);
+#else
+module_init(cpufreq_yankactive_init);
+#endif
+
+static void __exit cpufreq_yankactive_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_yankactive);
+	kthread_stop(speedchange_task);
+	put_task_struct(speedchange_task);
+}
+
+module_exit(cpufreq_yankactive_exit);
+
+MODULE_AUTHOR("Mike Chan <mike@android.com>");
+MODULE_DESCRIPTION("'cpufreq_yankactive' - A cpufreq governor for "
+	"Latency sensitive workloads");
+MODULE_LICENSE("GPL");
diff --git a/drivers/cpufreq/cpufreq_zzmoove.c b/drivers/cpufreq/cpufreq_zzmoove.c
new file mode 100755
index 0000000..10dc4d8
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_zzmoove.c
@@ -0,0 +1,9146 @@
+/*
+ *  drivers/cpufreq/cpufreq_zzmoove.c
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *            (C)  2009 Alexander Clouter <alex@digriz.org.uk>
+ *            (C)  2012 Michael Weingaertner <mialwe@googlemail.com>
+ *                      Zane Zaminsky <cyxman@yahoo.com>
+ *                      Jean-Pierre Rasquin <yank555.lu@gmail.com>
+ *                      ffolkes <ffolkes@ffolkes.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * -------------------------------------------------------------------------------------------------------------------------------------------------------
+ * -  Description:																	 -
+ * -------------------------------------------------------------------------------------------------------------------------------------------------------
+ *
+ * 'ZZMoove' governor is based on the modified 'conservative' (original author Alexander Clouter <alex@digriz.org.uk>) 'smoove' governor from Michael
+ * Weingaertner <mialwe@googlemail.com> (source: https://github.com/mialwe/mngb/) ported/modified/optimzed for I9300 since November 2012 and further
+ * improved for exynos and snapdragon platform (but also working on other platforms like OMAP) by ZaneZam,Yank555 and ffolkes in 2013/14/15
+ * CPU Hotplug modifications partially taken from ktoonservative governor from ktoonsez KT747-JB kernel (https://github.com/ktoonsez/KT747-JB)
+ *
+ * --------------------------------------------------------------------------------------------------------------------------------------------------------
+ * -																			  -
+ * --------------------------------------------------------------------------------------------------------------------------------------------------------
+ */
+
+// ZZ: disable kernel power management
+// #define DISABLE_POWER_MANAGEMENT
+
+// AP: use msm8974 lcd status notifier
+// #define USE_LCD_NOTIFIER
+
+#include <linux/cpu.h>
+#ifdef USE_LCD_NOTIFIER
+#include <linux/lcd_notify.h>
+#endif /* USE_LCD_NOTIFIER */
+#include <linux/cpufreq.h>
+#if defined(CONFIG_HAS_EARLYSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)
+#include <linux/earlysuspend.h>
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+#include <linux/exynos4_export_temp.h>		// ZZ: Exynos4 temperatue reading support
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#include <linux/hrtimer.h>
+#include <linux/init.h>
+#include <linux/jiffies.h>
+#include <linux/kernel.h>
+#include <linux/kernel_stat.h>
+#include <linux/ktime.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#if defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)
+#include <linux/powersuspend.h>
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#include <linux/sched.h>
+#include <linux/tick.h>
+#include <linux/version.h>
+
+#define ENABLE_SNAP_THERMAL_SUPPORT		// ZZ: Snapdragon temperature tripping support
+
+#if defined(CONFIG_THERMAL_TSENS8974) || defined(CONFIG_THERMAL_TSENS8960) && defined(ENABLE_SNAP_THERMAL_SUPPORT) // ZZ: Snapdragon temperature sensor
+#include <linux/msm_tsens.h>
+#endif /* defined(CONFIG_THERMAL_TSENS8974)... */
+
+#define ENABLE_INPUTBOOSTER			// ZZ: enable/disable inputbooster support
+// #define ENABLE_WORK_RESTARTLOOP		// ZZ: enable/disable restart loop for touchboost (DO NOT ENABLE IN THIS VERSION -> NOT STABLE YET!)
+
+
+#ifdef ENABLE_INPUTBOOSTER
+#include <linux/slab.h>
+#include <linux/input.h>
+#endif /* ENABLE_INPUTBOOSTER */
+
+// Yank: enable/disable sysfs interface to display current zzmoove version
+#define ZZMOOVE_VERSION "develop-24.09.15"
+
+// ZZ: support for 2,4,6 or 8 cores (this will enable/disable hotplug threshold tuneables and limit hotplug max limit tuneable)
+#define MAX_CORES					(4)
+
+// ZZ: enable/disable hotplug support
+#define ENABLE_HOTPLUGGING
+
+// ZZ: enable support for native hotplugging on snapdragon platform
+#define SNAP_NATIVE_HOTPLUGGING
+
+// ZZ: enable for sources with backported cpufreq implementation of 3.10 kernel
+// #define CPU_IDLE_TIME_IN_CPUFREQ
+
+// ZZ: enable/disable music limits
+#define ENABLE_MUSIC_LIMITS
+
+// ZZ: enable/disable freq auto adjusting
+#define ENABLE_AUTO_ADJUST_FREQ
+
+// ZZ: enable/disable profiles support
+#define ENABLE_PROFILES_SUPPORT
+
+// ZZ: include profiles header file and set name for 'custom' profile (informational for a changed profile value)
+#ifdef ENABLE_PROFILES_SUPPORT
+#include "cpufreq_zzmoove_profiles.h"
+#define DEF_PROFILE_NUMBER				(0)	// ZZ: default profile number (profile = 0 = 'none' = tuneable mode)
+static char custom_profile[20] = "custom";			// ZZ: name to show in sysfs if any profile value has changed
+
+// ff: allows tuneables to be tweaked without reverting to "custom" profile
+#define DEF_PROFILE_STICKY_MODE				(1)
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+// Yank: enable/disable debugging code
+// #define ZZMOOVE_DEBUG
+
+/*
+ * The polling frequency of this governor depends on the capability of
+ * the processor. Default polling frequency is 1000 times the transition
+ * latency of the processor. The governor will work on any processor with
+ * transition latency <= 10mS, using appropriate samplingrate. For CPUs
+ * with transition latency > 10mS (mostly drivers with CPUFREQ_ETERNAL)
+ * this governor will not work. All times here are in uS.
+ */
+#define TRANSITION_LATENCY_LIMIT	    (10 * 1000 * 1000)	// ZZ: default transition latency limit
+#define LATENCY_MULTIPLIER				(1000)	// ZZ: default latency multiplier
+#define MIN_LATENCY_MULTIPLIER				(100)	// ZZ: default min latency multiplier
+#define MIN_SAMPLING_RATE_RATIO				(2)	// ZZ: default min sampling rate ratio
+
+// ZZ: general tuneable defaults
+#define DEF_FREQUENCY_UP_THRESHOLD			(70)	// ZZ: default regular scaling up threshold
+#ifdef ENABLE_HOTPLUGGING
+#define DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG		(68)	// ZZ: default hotplug up threshold for all cpus (cpu0 stays allways on)
+#define DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG_FREQ		(0)	// Yank: default hotplug up threshold frequency for all cpus (0 = disabled)
+#endif /* ENABLE_HOTPLUGGING */
+#define DEF_SMOOTH_UP					(75)	// ZZ: default cpu load trigger for 'boosting' scaling frequency
+#define DEF_FREQUENCY_DOWN_THRESHOLD			(52)	// ZZ: default regular scaling down threshold
+#ifdef ENABLE_HOTPLUGGING
+#define DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG		(55)	// ZZ: default hotplug down threshold for all cpus (cpu0 stays allways on)
+#define DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG_FREQ	(0)	// Yank: default hotplug down threshold frequency for all cpus (0 = disabled)
+#endif /* ENABLE_HOTPLUGGING */
+#define DEF_IGNORE_NICE					(0)	// ZZ: default ignore nice load
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+#define DEF_AUTO_ADJUST_FREQ				(0)	// ZZ: default auto adjust frequency thresholds
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+
+// ZZ: hotplug-switch, -block, -idle, -limit and scaling-block, -fastdown, -responiveness, -proportional tuneable defaults
+#ifdef ENABLE_HOTPLUGGING
+#define DEF_DISABLE_HOTPLUG				(0)	// ZZ: default hotplug switch
+#define DEF_HOTPLUG_BLOCK_UP_CYCLES			(0)	// ZZ: default hotplug up block cycles
+#define DEF_HOTPLUG_BLOCK_DOWN_CYCLES			(0)	// ZZ: default hotplug down block cycles
+#define DEF_BLOCK_UP_MULTIPLIER_HOTPLUG1		(0)	// ff: default hotplug up block multiplier for core 2
+#define DEF_BLOCK_UP_MULTIPLIER_HOTPLUG2		(0)	// ff: default hotplug up block multiplier for core 3
+#define DEF_BLOCK_UP_MULTIPLIER_HOTPLUG3		(0)	// ff: default hotplug up block multiplier for core 4
+#define DEF_BLOCK_UP_MULTIPLIER_HOTPLUG4		(0)	// ff: default hotplug up block multiplier for core 5
+#define DEF_BLOCK_UP_MULTIPLIER_HOTPLUG5		(0)	// ff: default hotplug up block multiplier for core 6
+#define DEF_BLOCK_UP_MULTIPLIER_HOTPLUG6		(0)	// ff: default hotplug up block multiplier for core 7
+#define DEF_BLOCK_UP_MULTIPLIER_HOTPLUG7		(0)	// ff: default hotplug up block multiplier for core 8
+#define DEF_BLOCK_DOWN_MULTIPLIER_HOTPLUG1		(0)	// ff: default hotplug down block multiplier for core 2
+#define DEF_BLOCK_DOWN_MULTIPLIER_HOTPLUG2		(0)	// ff: default hotplug down block multiplier for core 3
+#define DEF_BLOCK_DOWN_MULTIPLIER_HOTPLUG3		(0)	// ff: default hotplug down block multiplier for core 4
+#define DEF_BLOCK_DOWN_MULTIPLIER_HOTPLUG4		(0)	// ff: default hotplug down block multiplier for core 5
+#define DEF_BLOCK_DOWN_MULTIPLIER_HOTPLUG5		(0)	// ff: default hotplug down block multiplier for core 6
+#define DEF_BLOCK_DOWN_MULTIPLIER_HOTPLUG6		(0)	// ff: default hotplug down block multiplier for core 7
+#define DEF_BLOCK_DOWN_MULTIPLIER_HOTPLUG7		(0)	// ff: default hotplug down block multiplier for core 8
+#define DEF_HOTPLUG_STAGGER_UP				(0)	// ff: only bring one core up at a time when hotplug_online_work() called
+#define DEF_HOTPLUG_STAGGER_DOWN			(0)	// ff: only bring one core down at a time when hotplug_offline_work() called
+#define DEF_HOTPLUG_IDLE_THRESHOLD			(0)	// ZZ: default hotplug idle threshold
+#define DEF_HOTPLUG_IDLE_FREQ				(0)	// ZZ: default hotplug idle freq
+#define DEF_HOTPLUG_ENGAGE_FREQ				(0)	// ZZ: default hotplug engage freq. the frequency below which we run on only one core (0 = disabled) (ffolkes)
+#define DEF_HOTPLUG_MAX_LIMIT				(0)	// ff: default for hotplug_max_limit. the number of cores we allow to be online (0 = disabled)
+#define DEF_HOTPLUG_MIN_LIMIT				(0)	// ff: default for hotplug_min_limit. the number of cores we require to be online (0 = disabled)
+#define DEF_HOTPLUG_LOCK				(0)	// ff: default for hotplug_lock. the number of cores we require to be online (0 = disabled)
+#endif /* ENABLE_HOTPLUGGING */
+#define DEF_SCALING_BLOCK_THRESHOLD			(0)	// ZZ: default scaling block threshold
+#define DEF_SCALING_BLOCK_CYCLES			(0)	// ZZ: default scaling block cycles
+#define DEF_SCALING_BLOCK_FREQ				(0)	// ZZ: default scaling block freq
+#define DEF_SCALING_UP_BLOCK_CYCLES			(0)	// ff: default scaling-up block cycles
+#define DEF_SCALING_UP_BLOCK_FREQ			(0)	// ff: default scaling-up block frequency threshold
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+#define DEF_TMU_READ_DELAY				(1000)	// ZZ: delay for cpu temperature reading in ms (tmu driver polling intervall is 10 sec)
+#define DEF_SCALING_BLOCK_TEMP				(0)	// ZZ: default cpu temperature threshold in C
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT				// ff: snapdragon temperature tripping defaults
+#define DEF_SCALING_TRIP_TEMP				(60)	// ff: default trip cpu temp
+#define DEF_TMU_CHECK_DELAY				(2500)	// ZZ: default delay for snapdragon thermal tripping
+#define DEF_TMU_CHECK_DELAY_SLEEP			(10000)	// ZZ: default delay for snapdragon thermal tripping at sleep
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+#define DEF_SCALING_BLOCK_FORCE_DOWN			(2)	// ZZ: default scaling block force down
+#define DEF_SCALING_FASTDOWN_FREQ			(0)	// ZZ: default scaling fastdown freq. the frequency beyond which we apply a different up_threshold (ffolkes)
+#define DEF_SCALING_FASTDOWN_UP_THRESHOLD		(95)	// ZZ: default scaling fastdown up threshold. the up threshold when scaling fastdown freq has been exceeded (ffolkes)
+#define DEF_SCALING_FASTDOWN_DOWN_THRESHOLD		(90)	// ZZ: default scaling fastdown up threshold. the down threshold when scaling fastdown freq has been exceeded (ffolkes)
+#define DEF_SCALING_RESPONSIVENESS_FREQ			(0)	// ZZ: default frequency below which we use a lower up threshold (ffolkes)
+#define DEF_SCALING_RESPONSIVENESS_UP_THRESHOLD		(30)	// ZZ: default up threshold we use when below scaling responsiveness freq (ffolkes)
+#define DEF_SCALING_PROPORTIONAL			(0)	// ZZ: default proportional scaling
+
+// ZZ: sampling rate idle and sampling down momentum tuneable defaults
+#define DEF_SAMPLING_RATE_IDLE_THRESHOLD		(0)	// ZZ: default sampling rate idle threshold
+#define DEF_SAMPLING_RATE_IDLE				(180000)// ZZ: default sampling rate idle (must not be 0!)
+#define DEF_SAMPLING_RATE_IDLE_DELAY			(0)	// ZZ: default sampling rate idle delay
+#define DEF_SAMPLING_DOWN_FACTOR			(1)	// ZZ: default sampling down factor (stratosk default = 4) here disabled by default
+#define MAX_SAMPLING_DOWN_FACTOR			(100000)// ZZ: changed from 10 to 100000 for sampling down momentum implementation
+#define DEF_SAMPLING_DOWN_MOMENTUM			(0)	// ZZ: default sampling down momentum, here disabled by default
+#define DEF_SAMPLING_DOWN_MAX_MOMENTUM			(0)	// ZZ: default sampling down max momentum stratosk default=16, here disabled by default
+#define DEF_SAMPLING_DOWN_MOMENTUM_SENSITIVITY		(50)	// ZZ: default sampling down momentum sensitivity
+#define MAX_SAMPLING_DOWN_MOMENTUM_SENSITIVITY		(1000)	// ZZ: default maximum for sampling down momentum sensitivity
+
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+// ZZ: tuneable defaults for early suspend
+#define DEF_DISABLE_SLEEP_MODE				(1)	// ZZ: default for sleep mode switch
+#define MAX_SAMPLING_RATE_SLEEP_MULTIPLIER		(8)	// ZZ: default maximum for sampling rate sleep multiplier
+#define DEF_SAMPLING_RATE_SLEEP_MULTIPLIER		(2)	// ZZ: default sampling rate sleep multiplier
+#define DEF_UP_THRESHOLD_SLEEP				(90)	// ZZ: default up threshold sleep
+#define DEF_DOWN_THRESHOLD_SLEEP			(44)	// ZZ: default down threshold sleep
+#define DEF_SMOOTH_UP_SLEEP				(75)	// ZZ: default smooth up sleep
+#define DEF_EARLY_DEMAND_SLEEP				(1)	// ZZ: default early demand sleep
+#define DEF_GRAD_UP_THRESHOLD_SLEEP			(30)	// ZZ: default grad up sleep
+#define DEF_FAST_SCALING_SLEEP_UP			(0)	// Yank: default fast scaling sleep for upscaling
+#define DEF_FAST_SCALING_SLEEP_DOWN			(0)	// Yank: default fast scaling sleep for downscaling
+#define DEF_FREQ_LIMIT_SLEEP				(0)	// ZZ: default freq limit sleep
+#ifdef ENABLE_HOTPLUGGING
+#define DEF_DISABLE_HOTPLUG_SLEEP			(0)	// ZZ: default hotplug sleep switch
+#endif /* ENABLE_HOTPLUGGING */
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+
+/*
+ * ZZ: Hotplug Sleep: 0 do not touch hotplug settings at suspend, so all cores will stay online
+ * the value is equivalent to the amount of cores which should be online at suspend
+ */
+#ifdef ENABLE_HOTPLUGGING
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+#define DEF_HOTPLUG_SLEEP				(0)	// ZZ: default hotplug sleep
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#endif /* ENABLE_HOTPLUGGING */
+
+// ZZ: tuneable defaults for Early Demand
+#define DEF_GRAD_UP_THRESHOLD				(25)	// ZZ: default grad up threshold
+#define DEF_EARLY_DEMAND				(0)	// ZZ: default early demand, default off
+
+/*
+ * ZZ: Frequency Limit: 0 do not limit frequency and use the full range up to policy->max limit
+ * values policy->min to policy->max in khz
+ */
+#define DEF_FREQ_LIMIT					(0)	// ZZ: default freq limit
+
+/*
+ * ZZ: Fast Scaling: 0 do not activate fast scaling function
+ * values 1-4 to enable fast scaling and 5 for auto fast scaling (insane scaling)
+ */
+#define DEF_FAST_SCALING_UP				(0)	// Yank: default fast scaling for upscaling
+#define DEF_FAST_SCALING_DOWN				(0)	// Yank: default fast scaling for downscaling
+#define DEF_AFS_THRESHOLD1				(25)	// ZZ: default auto fast scaling step one
+#define DEF_AFS_THRESHOLD2				(50)	// ZZ: default auto fast scaling step two
+#define DEF_AFS_THRESHOLD3				(75)	// ZZ: default auto fast scaling step three
+#define DEF_AFS_THRESHOLD4				(90)	// ZZ: default auto fast scaling step four
+
+// ff: Input Booster defaults
+#ifdef ENABLE_INPUTBOOSTER
+#define DEF_INPUTBOOST_CYCLES				(0)	// ff: default number of cycles to boost up/down thresholds
+#define DEF_INPUTBOOST_UP_THRESHOLD			(80)	// ff: default up threshold for inputbooster
+#define DEF_INPUTBOOST_PUNCH_CYCLES			(20)	// ff: default number of cycles to meet or exceed punch freq
+#define DEF_INPUTBOOST_PUNCH_FREQ			(0)	// ff: default frequency to keep cur_freq at or above
+#define DEF_INPUTBOOST_PUNCH_ON_FINGERDOWN		(0)	// ff: default for constant punching (like a touchbooster)
+#define DEF_INPUTBOOST_PUNCH_ON_FINGERMOVE		(0)	// ff: default for constant punching (like a touchbooster)
+#define DEF_INPUTBOOST_PUNCH_ON_EPENMOVE		(0)	// ff: default for constant punching (like a touchbooster)
+#define DEF_INPUTBOOST_ON_TSP				(1)	// ff: default to boost on touchscreen input events
+#define DEF_INPUTBOOST_ON_TSP_HOVER			(1)	// ff: default to boost on touchscreen hovering input events
+#define DEF_INPUTBOOST_ON_GPIO				(1)	// ff: default to boost on gpio (button) input events
+#define DEF_INPUTBOOST_ON_TK				(1)	// ff: default to boost on touchkey input events
+#define DEF_INPUTBOOST_ON_EPEN				(1)	// ff: default to boost on e-pen input events
+#define DEF_INPUTBOOST_TYPINGBOOSTER_UP_THRESHOLD	(40)	// ff: default up threshold for typing booster
+#define DEF_INPUTBOOST_TYPINGBOOSTER_CORES		(3)	// ff: default cores for typing booster
+#endif /* ENABLE_INPUTBOOSTER */
+
+// ff: Music Detection defaults
+#ifdef ENABLE_MUSIC_LIMITS
+#define DEF_MUSIC_MAX_FREQ				(0)	// ff: default maximum freq to maintain while music is on
+#define DEF_MUSIC_MIN_FREQ				(0)	// ff: default minimum freq to maintain while music is on
+#ifdef ENABLE_HOTPLUGGING
+#define DEF_MUSIC_MIN_CORES				(2)	// ZZ: default minimum cores online while music is on
+#endif /* ENABLE_HOTPLUGGING */
+#endif /* ENABLE_MUSIC_LIMITS */
+
+// ZZ: Sampling Down Momentum variables
+static unsigned int min_sampling_rate;				// ZZ: minimal possible sampling rate
+static unsigned int orig_sampling_down_factor;			// ZZ: for saving previously set sampling down factor
+static unsigned int zz_sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR;		// ff: actual use variable, so dbs_tuner_ins version stays constant
+static unsigned int orig_sampling_down_max_mom;			// ZZ: for saving previously set smapling down max momentum
+static unsigned int zz_sampling_down_max_mom;			// ff: actual use variable, so dbs_tuner_ins version stays constant
+
+// ZZ: search limit for frequencies in scaling table, variables for scaling modes and state flag for suspend detection
+static struct cpufreq_frequency_table *system_freq_table;	// ZZ: static system frequency table
+static int scaling_mode_up;					// ZZ: fast scaling up mode holding up value during runtime
+static int scaling_mode_down;					// ZZ: fast scaling down mode holding down value during runtime
+static bool freq_table_desc = false;				// ZZ: true for descending order, false for ascending order
+static int freq_init_count = 0;					// ZZ: flag for executing freq table order and limit optimization code at gov start
+static unsigned int max_scaling_freq_soft = 0;			// ZZ: init value for 'soft' scaling limit, 0 = full range
+static unsigned int max_scaling_freq_hard = 0;			// ZZ: init value for 'hard' scaling limit, 0 = full range
+static unsigned int min_scaling_freq_soft = 0;			// ZZ: init value for 'soft' scaling limit, 0 = full range
+static unsigned int min_scaling_freq_hard = 0;			// ZZ: init value for 'hard' scaling limit, 0 = full range
+static unsigned int system_table_end = CPUFREQ_TABLE_END;	// ZZ: system freq table end for order detection, table size calculation and freq validations
+static unsigned int limit_table_end = CPUFREQ_TABLE_END;	// ZZ: initial (full range) search end limit for frequency table in descending ordered table
+static unsigned int limit_table_start = 0;			// ZZ: search start limit for frequency table in ascending order
+static unsigned int freq_table_size = 0;			// Yank: upper index limit of frequency table
+static unsigned int min_scaling_freq = 0;			// Yank: lowest frequency index in global frequency table
+static bool suspend_flag = false;				// ZZ: flag for suspend status, true = in early suspend
+
+// ZZ: hotplug-, scaling-block, scaling fastdown vars and sampling rate idle counters. flags for scaling, setting profile, cpu temp reading and hotplugging
+#ifdef ENABLE_HOTPLUGGING
+static int possible_cpus = 0;					// ZZ: for holding the maximal amount of cores for hotplugging
+static unsigned int hplg_down_block_cycles = 0;			// ZZ: delay cycles counter for hotplug down blocking
+static unsigned int hplg_up_block_cycles = 0;			// ZZ: delay cycles counter for hotplug up blocking
+static unsigned int num_online_cpus_last = 0;			// ff: how many cores were online last cycle
+static unsigned int zz_hotplug_block_up_cycles = 0;
+static unsigned int zz_hotplug_block_down_cycles = 0;
+#endif /* ENABLE_HOTPLUGGING */
+static unsigned int scaling_block_cycles_count = 0;		// ZZ: scaling block cycles counter
+static unsigned int sampling_rate_step_up_delay = 0;		// ZZ: sampling rate idle up delay cycles
+static unsigned int sampling_rate_step_down_delay = 0;		// ZZ: sampling rate idle down delay cycles
+static unsigned int scaling_up_threshold = 0;			// ZZ: scaling up threshold for fastdown/responsiveness functionality
+static unsigned int scaling_down_threshold = 0;			// ZZ: scaling down threshold for fastdown functionality
+#ifdef ENABLE_HOTPLUGGING
+static bool hotplug_idle_flag = false;				// ZZ: flag for hotplug idle mode
+static int __refdata enable_cores = 0;				// ZZ: mode for enabling offline cores for various functions in the governor
+static int __refdata disable_cores = 0;				// ZZ: mode for disabling online cores for various functions in the governor
+static bool hotplug_up_in_progress;				// ZZ: flag for hotplug up function call - block if hotplugging is in progress
+static bool hotplug_down_in_progress;				// ZZ: flag for hotplug down function call - block if hotplugging is in progress
+static bool boost_hotplug = false;				// ZZ: early demand boost hotplug flag
+#endif /* ENABLE_HOTPLUGGING */
+static bool boost_freq = false;					// ZZ: early demand boost freq flag
+static bool force_down_scaling = false;				// ZZ: force down scaling flag
+static bool cancel_up_scaling = false;				// ZZ: cancel up scaling flag
+#ifdef ENABLE_PROFILES_SUPPORT
+static bool set_profile_active = false;				// ZZ: flag to avoid changing of any tuneables during profile apply
+#endif
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+#ifdef ENABLE_HOTPLUGGING
+static bool hotplug_up_temp_block;				// ZZ: flag for blocking up hotplug work during temp freq blocking
+#endif /* ENABLE_HOTPLUGGING */
+static bool cancel_temp_reading = false;			// ZZ: flag for starting temp reading work
+static bool temp_reading_started = false;			// ZZ: flag for canceling temp reading work
+
+// ZZ: Exynos CPU temp reading work
+static void tmu_read_temperature(struct work_struct * tmu_read_work);	// ZZ: prepare temp reading work
+static DECLARE_DELAYED_WORK(tmu_read_work, tmu_read_temperature);	// ZZ: declare delayed work for temp reading
+static unsigned int cpu_temp;						// ZZ: static var for holding current cpu temp
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+
+// ZZ: current load & frequency for hotplugging work and scaling. max/min frequency for proportional scaling and auto freq threshold adjustment
+static unsigned int cur_load = 0;				// ZZ: current load for hotplugging functions
+static unsigned int cur_freq = 0;				// ZZ: current frequency for hotplugging functions
+static unsigned int pol_max = 0;				// ZZ: current max freq for proportional scaling and auto adjustment of freq thresholds
+static unsigned int pol_min = 0;				// ZZ: current min freq for auto adjustment of freq thresholds
+
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+static unsigned int old_pol_max = 0;				// ZZ: previous max freq for auto adjustment of freq thresholds
+static unsigned int pol_step = 0;				// ZZ: policy change step for auto adjustment of freq thresholds
+
+// ZZ: temp variables and flags to hold offset values for auto adjustment of freq thresholds
+#ifdef ENABLE_HOTPLUGGING
+static unsigned int temp_hotplug_engage_freq = 0;
+static bool temp_hotplug_engage_freq_flag = false;
+static unsigned int temp_hotplug_idle_freq = 0;
+static bool temp_hotplug_idle_freq_flag = false;
+#endif /* ENABLE_HOTPLUGGING */
+static unsigned int temp_scaling_block_freq = 0;
+static bool temp_scaling_block_freq_flag = false;
+static unsigned int temp_scaling_fastdown_freq = 0;
+static bool temp_scaling_fastdown_freq_flag = false;
+static unsigned int temp_scaling_responsiveness_freq = 0;
+static bool temp_scaling_responsiveness_freq_flag = false;
+#ifdef ENABLE_MUSIC_LIMITS
+static unsigned int temp_music_min_freq = 0;
+static bool temp_music_min_freq_flag = false;
+static unsigned int temp_music_max_freq = 0;
+static bool temp_music_max_freq_flag = false;
+#endif /* ENABLE_MUSIC_LIMITS */
+#ifdef ENABLE_HOTPLUGGING
+static unsigned int temp_up_threshold_hotplug_freq1 = 0;
+static bool temp_up_threshold_hotplug_freq1_flag = false;
+static unsigned int temp_down_threshold_hotplug_freq1 = 0;
+static bool temp_down_threshold_hotplug_freq1_flag = false;
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+static unsigned int temp_up_threshold_hotplug_freq2 = 0;
+static bool temp_up_threshold_hotplug_freq2_flag = false;
+static unsigned int temp_up_threshold_hotplug_freq3 = 0;
+static bool temp_up_threshold_hotplug_freq3_flag = false;
+static unsigned int temp_down_threshold_hotplug_freq2 = 0;
+static bool temp_down_threshold_hotplug_freq2_flag = false;
+static unsigned int temp_down_threshold_hotplug_freq3 = 0;
+static bool temp_down_threshold_hotplug_freq3_flag = false;
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+static unsigned int temp_up_threshold_hotplug_freq4 = 0;
+static bool temp_up_threshold_hotplug_freq4_flag = false;
+static unsigned int temp_up_threshold_hotplug_freq5 = 0;
+static bool temp_up_threshold_hotplug_freq5_flag = false;
+static unsigned int temp_down_threshold_hotplug_freq4 = 0;
+static bool temp_down_threshold_hotplug_freq4_flag = false;
+static unsigned int temp_down_threshold_hotplug_freq5 = 0;
+static bool temp_down_threshold_hotplug_freq5_flag = false;
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+static unsigned int temp_up_threshold_hotplug_freq6 = 0;
+static bool temp_up_threshold_hotplug_freq6_flag = false;
+static unsigned int temp_up_threshold_hotplug_freq7 = 0;
+static bool temp_up_threshold_hotplug_freq7_flag = false;
+static unsigned int temp_down_threshold_hotplug_freq6 = 0;
+static bool temp_down_threshold_hotplug_freq6_flag = false;
+static unsigned int temp_down_threshold_hotplug_freq7 = 0;
+static bool temp_down_threshold_hotplug_freq7_flag = false;
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#ifdef ENABLE_INPUTBOOSTER
+static unsigned int temp_inputboost_punch_freq = 0;
+static bool temp_inputboost_punch_freq_flag = false;
+#endif /* ENABLE_INPUTBOOSTER */
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+
+#ifdef ENABLE_HOTPLUGGING
+// ZZ: hotplug load thresholds array
+static int hotplug_thresholds[2][8] = {
+    { 0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 },
+    { 0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 }
+    };
+
+// ZZ: hotplug frequencies thresholds array
+static int hotplug_thresholds_freq[2][8] = {
+    { 0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 },
+    { 0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 }
+    };
+
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+// ZZ: hotplug frequencies out of range array
+static int hotplug_freq_threshold_out_of_range[2][8] = {
+    { 0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 },
+    { 0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 }
+    };
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+#endif /* ENABLE_HOTPLUGGING */
+
+// ZZ: core on which we currently run
+static unsigned int on_cpu = 0;
+
+// ZZ: Early Suspend variables
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+static unsigned int sampling_rate_awake;			// ZZ: for saving sampling rate awake value
+static unsigned int up_threshold_awake;				// ZZ: for saving up threshold awake value
+static unsigned int down_threshold_awake;			// ZZ: for saving down threshold awake value
+static unsigned int smooth_up_awake;				// ZZ: for saving smooth up awake value
+static unsigned int freq_limit_awake;				// ZZ: for saving frequency limit awake value
+static unsigned int fast_scaling_up_awake;			// Yank: for saving fast scaling awake value for upscaling
+static unsigned int fast_scaling_down_awake;			// Yank: for saving fast scaling awake value for downscaling
+#ifdef ENABLE_HOTPLUGGING
+static unsigned int disable_hotplug_awake;			// ZZ: for saving hotplug switch
+static unsigned int hotplug1_awake;				// ZZ: for saving hotplug1 threshold awake value
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+static unsigned int hotplug2_awake;				// ZZ: for saving hotplug2 threshold awake value
+static unsigned int hotplug3_awake;				// ZZ: for saving hotplug3 threshold awake value
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+static unsigned int hotplug4_awake;				// ZZ: for saving hotplug4 threshold awake value
+static unsigned int hotplug5_awake;				// ZZ: for saving hotplug5 threshold awake value
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+static unsigned int hotplug6_awake;				// ZZ: for saving hotplug6 threshold awake value
+static unsigned int hotplug7_awake;				// ZZ: for saving hotplug7 threshold awake value
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+static unsigned int sampling_rate_asleep;			// ZZ: for setting sampling rate value at early suspend
+static unsigned int up_threshold_asleep;			// ZZ: for setting up threshold value at early suspend
+static unsigned int down_threshold_asleep;			// ZZ: for setting down threshold value at early suspend
+static unsigned int smooth_up_asleep;				// ZZ: for setting smooth scaling value at early suspend
+static unsigned int freq_limit_asleep;				// ZZ: for setting frequency limit value at early suspend
+static unsigned int fast_scaling_up_asleep;			// Yank: for setting fast scaling value at early suspend for upscaling
+static unsigned int fast_scaling_down_asleep;			// Yank: for setting fast scaling value at early suspend for downscaling
+#ifdef ENABLE_HOTPLUGGING
+static unsigned int disable_hotplug_asleep;			// ZZ: for setting hotplug on/off at early suspend
+#endif /* ENABLE_HOTPLUGGING */
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+
+#if defined(USE_LCD_NOTIFIER) && !defined(CONFIG_POWERSUSPEND)
+static struct notifier_block zzmoove_lcd_notif;
+#endif /* defined(USE_LCD_NOTIFIER)... */
+
+#ifdef ENABLE_INPUTBOOSTER
+// ff: Input Booster variables
+static unsigned int boost_on_tsp = DEF_INPUTBOOST_ON_TSP;	// ff: hardcoded since it'd be silly not to use it
+static unsigned int boost_on_tsp_hover = DEF_INPUTBOOST_ON_TSP_HOVER;
+static unsigned int boost_on_gpio = DEF_INPUTBOOST_ON_GPIO;	// ff: hardcoded since it'd be silly not to use it
+static unsigned int boost_on_tk = DEF_INPUTBOOST_ON_TK;		// ff: hardcoded since it'd be silly not to use it
+static unsigned int boost_on_epen = DEF_INPUTBOOST_ON_EPEN;	// ff: hardcoded since it'd be silly not to use it
+static unsigned int inputboost_last_type = 0;
+static unsigned int inputboost_last_code = 0;
+static unsigned int inputboost_last_value = 0;
+static int inputboost_report_btn_touch = -1;
+static int inputboost_report_btn_toolfinger = -1;
+static int inputboost_report_mt_trackingid = 0;
+static bool flg_inputboost_report_mt_touchmajor = false;
+static bool flg_inputboost_report_abs_xy = false;
+int flg_ctr_cpuboost = 0;
+static int flg_ctr_inputboost = 0;
+static int flg_ctr_inputboost_punch = 0;
+static int flg_ctr_inputbooster_typingbooster = 0;
+static int ctr_inputboost_typingbooster_taps = 0;
+static struct timeval time_typingbooster_lasttapped;
+#ifdef ZZMOOVE_DEBUG
+static struct timeval time_touchbooster_lastrun;
+static unsigned int time_since_touchbooster_lastrun = 0;
+#endif /* ZZMOOVE_DEBUG */
+#endif /* ENABLE_INPUTBOOSTER */
+
+// ff: other variables
+static int scaling_up_block_cycles_count = 0;
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+#ifdef ENABLE_MUSIC_LIMITS
+static int music_max_freq_step = 0;
+#endif /* ENABLE_MUSIC_LIMITS */
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+
+#ifdef ENABLE_WORK_RESTARTLOOP
+struct work_struct work_restartloop;
+static bool work_restartloop_in_progress = false;		// ZZ: flag to avoid loop restart to frequently
+#endif /* ENABLE_WORK_RESTARTLOOP */
+
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+static void tmu_check_work(struct work_struct * work_tmu_check);
+static DECLARE_DELAYED_WORK(work_tmu_check, tmu_check_work);
+static int tmu_temp_cpu = 0;
+static int tmu_temp_cpu_last = 0;
+static int flg_ctr_tmu_overheating = 0;
+static int tmu_throttle_steps = 0;
+static int ctr_tmu_neutral = 0;
+static int ctr_tmu_falling = 0;
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+
+#ifdef ENABLE_HOTPLUGGING
+struct work_struct hotplug_offline_work;			// ZZ: hotplugging down work
+struct work_struct hotplug_online_work;				// ZZ: hotplugging up work
+#endif /* ENABLE_HOTPLUGGING */
+
+static void do_dbs_timer(struct work_struct *work);
+
+struct cpu_dbs_info_s {
+	u64 time_in_idle;					// ZZ: for exit time handling
+	u64 idle_exit_time;					// ZZ: for exit time handling
+	u64 prev_cpu_idle;
+	u64 prev_cpu_wall;
+	u64 prev_cpu_nice;
+	struct cpufreq_policy *cur_policy;
+	struct delayed_work work;
+	unsigned int down_skip;					// ZZ: Sampling Down reactivated
+	unsigned int requested_freq;
+	unsigned int rate_mult;					// ZZ: Sampling Down Momentum - sampling rate multiplier
+	unsigned int momentum_adder;				// ZZ: Sampling Down Momentum - adder
+	int cpu;
+	unsigned int enable:1;
+	unsigned int prev_load;					// ZZ: Early Demand - for previous load
+
+	/*
+	 * percpu mutex that serializes governor limit change with
+	 * do_dbs_timer invocation. We do not want do_dbs_timer to run
+	 * when user is changing the governor or limits.
+	 */
+	struct mutex timer_mutex;
+};
+
+static bool dbs_info_enabled = false;
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, cs_cpu_dbs_info);
+static unsigned int dbs_enable;					// number of CPUs using this policy
+static DEFINE_MUTEX(dbs_mutex);					// dbs_mutex protects dbs_enable in governor start/stop.
+static struct workqueue_struct *dbs_wq;
+#ifdef ENABLE_WORK_RESTARTLOOP
+static struct workqueue_struct *dbs_aux_wq;
+#endif /* ENABLE_WORK_RESTARTLOOP */
+static struct dbs_tuners {
+#ifdef ENABLE_PROFILES_SUPPORT
+	char profile[20];					// ZZ: profile tuneable
+	unsigned int profile_number;				// ZZ: profile number tuneable
+	unsigned int profile_sticky_mode;			// ff: sticky profile mode
+#endif /* ENABLE_PROFILES_SUPPORT */
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+	unsigned int auto_adjust_freq_thresholds;		// ZZ: auto adjust freq thresholds tuneable
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+	unsigned int sampling_rate;				// ZZ: normal sampling rate tuneable
+	unsigned int sampling_rate_current;			// ZZ: currently active sampling rate tuneable
+	unsigned int sampling_rate_idle;			// ZZ: sampling rate at idle tuneable
+	unsigned int sampling_rate_idle_threshold;		// ZZ: sampling rate switching threshold tuneable
+	unsigned int sampling_rate_idle_delay;			// ZZ: sampling rate switching delay tuneable
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	unsigned int disable_sleep_mode;			// ZZ: switch for sleep mode
+	unsigned int sampling_rate_sleep_multiplier;		// ZZ: sampling rate sleep multiplier tuneable for early suspend
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	unsigned int sampling_down_factor;			// ZZ: sampling down factor tuneable (reactivated)
+	unsigned int sampling_down_momentum;			// ZZ: sampling down momentum tuneable
+	unsigned int sampling_down_max_mom;			// ZZ: sampling down momentum max tuneable
+	unsigned int sampling_down_mom_sens;			// ZZ: sampling down momentum sensitivity tuneable
+	unsigned int up_threshold;				// ZZ: scaling up threshold tuneable
+#ifdef ENABLE_HOTPLUGGING
+	unsigned int up_threshold_hotplug1;			// ZZ: up threshold hotplug tuneable for core1
+	unsigned int up_threshold_hotplug_freq1;		// Yank: up threshold hotplug freq tuneable for core1
+	unsigned int block_up_multiplier_hotplug1;
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+	unsigned int up_threshold_hotplug2;			// ZZ: up threshold hotplug tuneable for core2
+	unsigned int up_threshold_hotplug_freq2;		// Yank: up threshold hotplug freq tuneable for core2
+	unsigned int block_up_multiplier_hotplug2;
+	unsigned int up_threshold_hotplug3;			// ZZ: up threshold hotplug tuneable for core3
+	unsigned int up_threshold_hotplug_freq3;		// Yank: up threshold hotplug freq tuneable for core3
+	unsigned int block_up_multiplier_hotplug3;
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+	unsigned int up_threshold_hotplug4;			// ZZ: up threshold hotplug tuneable for core4
+	unsigned int up_threshold_hotplug_freq4;		// Yank: up threshold hotplug freq tuneable for core4
+	unsigned int block_up_multiplier_hotplug4;
+	unsigned int up_threshold_hotplug5;			// ZZ: up threshold hotplug tuneable for core5
+	unsigned int up_threshold_hotplug_freq5;		// Yank: up threshold hotplug freq tuneable for core5
+	unsigned int block_up_multiplier_hotplug5;
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+	unsigned int up_threshold_hotplug6;			// ZZ: up threshold hotplug tuneable for core6
+	unsigned int up_threshold_hotplug_freq6;		// Yank: up threshold hotplug freq tuneable  for core6
+	unsigned int block_up_multiplier_hotplug6;
+	unsigned int up_threshold_hotplug7;			// ZZ: up threshold hotplug tuneable for core7
+	unsigned int up_threshold_hotplug_freq7;		// Yank: up threshold hotplug freq tuneable for core7
+	unsigned int block_up_multiplier_hotplug7;
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+	unsigned int up_threshold_sleep;			// ZZ: up threshold sleep tuneable for early suspend
+	unsigned int down_threshold;				// ZZ: down threshold tuneable
+#ifdef ENABLE_HOTPLUGGING
+	unsigned int down_threshold_hotplug1;			// ZZ: down threshold hotplug tuneable for core1
+	unsigned int down_threshold_hotplug_freq1;		// Yank: down threshold hotplug freq tuneable for core1
+	unsigned int block_down_multiplier_hotplug1;
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+	unsigned int down_threshold_hotplug2;			// ZZ: down threshold hotplug tuneable for core2
+	unsigned int down_threshold_hotplug_freq2;		// Yank: down threshold hotplug freq tuneable for core2
+	unsigned int block_down_multiplier_hotplug2;
+	unsigned int down_threshold_hotplug3;			// ZZ: down threshold hotplug tuneable for core3
+	unsigned int down_threshold_hotplug_freq3;		// Yank: down threshold hotplug freq tuneable for core3
+	unsigned int block_down_multiplier_hotplug3;
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+	unsigned int down_threshold_hotplug4;			// ZZ: down threshold hotplug tuneable for core4
+	unsigned int down_threshold_hotplug_freq4;		// Yank: down threshold hotplug freq tuneable for core4
+	unsigned int block_down_multiplier_hotplug4;
+	unsigned int down_threshold_hotplug5;			// ZZ: down threshold hotplug tuneable for core5
+	unsigned int down_threshold_hotplug_freq5;		// Yank: down threshold hotplug_freq tuneable for core5
+	unsigned int block_down_multiplier_hotplug5;
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+	unsigned int down_threshold_hotplug6;			// ZZ: down threshold hotplug tuneable for core6
+	unsigned int down_threshold_hotplug_freq6;		// Yank: down threshold hotplug freq tuneable for core6
+	unsigned int block_down_multiplier_hotplug6;
+	unsigned int down_threshold_hotplug7;			// ZZ: down threshold hotplug tuneable for core7
+	unsigned int down_threshold_hotplug_freq7;		// Yank: down threshold hotplug freq tuneable for core7
+	unsigned int block_down_multiplier_hotplug7;
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	unsigned int down_threshold_sleep;			// ZZ: down threshold sleep tuneable for early suspend
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	unsigned int ignore_nice;				// ZZ: ignore nice load tuneable
+	unsigned int smooth_up;					// ZZ: smooth up tuneable
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	unsigned int smooth_up_sleep;				// ZZ: smooth up sleep tuneable for early suspend
+#ifdef ENABLE_HOTPLUGGING
+	unsigned int hotplug_sleep;				// ZZ: hotplug sleep tuneable for early suspend
+#endif /* ENABLE_HOTPLUGGING */
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	unsigned int freq_limit;				// ZZ: freq limit tuneable
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	unsigned int freq_limit_sleep;				// ZZ: freq limit sleep tuneable for early suspend
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	unsigned int fast_scaling_up;				// Yank: fast scaling tuneable for upscaling
+	unsigned int fast_scaling_down;				// Yank: fast scaling tuneable for downscaling
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	unsigned int fast_scaling_sleep_up;			// Yank: fast scaling sleep tuneable for early suspend for upscaling
+	unsigned int fast_scaling_sleep_down;			// Yank: fast scaling sleep tuneable for early suspend for downscaling
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	unsigned int afs_threshold1;				// ZZ: auto fast scaling step one threshold
+	unsigned int afs_threshold2;				// ZZ: auto fast scaling step two threshold
+	unsigned int afs_threshold3;				// ZZ: auto fast scaling step three threshold
+	unsigned int afs_threshold4;				// ZZ: auto fast scaling step four threshold
+	unsigned int grad_up_threshold;				// ZZ: early demand grad up threshold tuneable
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	unsigned int grad_up_threshold_sleep;			// ZZ: early demand grad up threshold tuneable for early suspend
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	unsigned int early_demand;				// ZZ: early demand master switch tuneable
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	unsigned int early_demand_sleep;			// ZZ: early demand master switch tuneable for early suspend
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+	unsigned int disable_hotplug;				// ZZ: hotplug switch tuneable
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	unsigned int disable_hotplug_sleep;			// ZZ: hotplug switch for sleep tuneable for early suspend
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	unsigned int hotplug_block_up_cycles;			// ZZ: hotplug up block cycles tuneable
+	unsigned int hotplug_block_down_cycles;			// ZZ: hotplug down block cycles tuneable
+	unsigned int hotplug_stagger_up;			// ff: hotplug stagger up tuneable
+	unsigned int hotplug_stagger_down;			// ff: hotplug stagger down tuneable
+	unsigned int hotplug_idle_threshold;			// ZZ: hotplug idle threshold tuneable
+	unsigned int hotplug_idle_freq;				// ZZ: hotplug idle freq tuneable
+	unsigned int hotplug_engage_freq;			// ZZ: frequency below which we run on only one core (ffolkes)
+	unsigned int hotplug_max_limit;				// ff: the number of cores we allow to be online
+	unsigned int hotplug_min_limit;				// ff: the number of cores we require to be online
+	unsigned int hotplug_min_limit_saved;			// ff: the number of cores we require to be online
+	unsigned int hotplug_min_limit_touchbooster;		// ff: the number of cores we require to be online
+	unsigned int hotplug_lock;				// ff: the number of cores we allow to be online
+#endif /* ENABLE_HOTPLUGGING */
+	unsigned int scaling_block_threshold;			// ZZ: scaling block threshold tuneable
+	unsigned int scaling_block_cycles;			// ZZ: scaling block cycles tuneable
+	unsigned int scaling_up_block_cycles;			// ff: scaling-up block cycles tuneable
+	unsigned int scaling_up_block_freq;			// ff: scaling-up block freq threshold tuneable
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+	unsigned int scaling_block_temp;			// ZZ: scaling block temp tuneable
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+	unsigned int scaling_trip_temp;				// ff: snapdragon temperature tripping
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+	unsigned int scaling_block_freq;			// ZZ: scaling block freq tuneable
+	unsigned int scaling_block_force_down;			// ZZ: scaling block force down tuneable
+	unsigned int scaling_fastdown_freq;			// ZZ: frequency beyond which we apply a different up threshold (ffolkes)
+	unsigned int scaling_fastdown_up_threshold;		// ZZ: up threshold when scaling fastdown freq exceeded (ffolkes)
+	unsigned int scaling_fastdown_down_threshold;		// ZZ: down threshold when scaling fastdown freq exceeded (ffolkes)
+	unsigned int scaling_responsiveness_freq;		// ZZ: frequency below which we use a lower up threshold (ffolkes)
+	unsigned int scaling_responsiveness_up_threshold;	// ZZ: up threshold we use when below scaling responsiveness freq (ffolkes)
+	unsigned int scaling_proportional;			// ZZ: proportional to load scaling
+#ifdef ENABLE_INPUTBOOSTER
+	// ff: input booster
+	unsigned int inputboost_cycles;				// ff: default number of cycles to boost up/down thresholds
+	unsigned int inputboost_up_threshold;			// ff: default up threshold for inputbooster
+	unsigned int inputboost_punch_cycles;			// ff: default number of cycles to meet or exceed punch freq
+	unsigned int inputboost_punch_freq;			// ff: default frequency to keep cur_freq at or above
+	unsigned int inputboost_punch_on_fingerdown;
+	unsigned int inputboost_punch_on_fingermove;
+	unsigned int inputboost_punch_on_epenmove;
+	unsigned int inputboost_typingbooster_up_threshold;
+	unsigned int inputboost_typingbooster_cores;
+#endif /* ENABLE_INPUTBOOSTER */
+
+#ifdef ENABLE_MUSIC_LIMITS
+	// ff: Music Detection
+	unsigned int music_max_freq;				// ff: music max freq
+	unsigned int music_min_freq;				// ff: music min freq
+#ifdef ENABLE_HOTPLUGGING
+	unsigned int music_min_cores;				// ff: music min freq
+#endif /* ENABLE_HOTPLUGGING */
+	unsigned int music_state;				// ff: music state
+#endif /* ENABLE_MUSIC_LIMITS */
+
+// ZZ: set tuneable default values
+} dbs_tuners_ins = {
+#ifdef ENABLE_PROFILES_SUPPORT
+	.profile = "none",
+	.profile_number = DEF_PROFILE_NUMBER,
+	.profile_sticky_mode = DEF_PROFILE_STICKY_MODE,
+#endif /* ENABLE_PROFILES_SUPPORT */
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+	.auto_adjust_freq_thresholds = DEF_AUTO_ADJUST_FREQ,
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+	.sampling_rate_idle = DEF_SAMPLING_RATE_IDLE,
+	.sampling_rate_idle_threshold = DEF_SAMPLING_RATE_IDLE_THRESHOLD,
+	.sampling_rate_idle_delay = DEF_SAMPLING_RATE_IDLE_DELAY,
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	.disable_sleep_mode = DEF_DISABLE_SLEEP_MODE,
+	.sampling_rate_sleep_multiplier = DEF_SAMPLING_RATE_SLEEP_MULTIPLIER,
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.sampling_down_momentum = DEF_SAMPLING_DOWN_MOMENTUM,
+	.sampling_down_max_mom = DEF_SAMPLING_DOWN_MAX_MOMENTUM,
+	.sampling_down_mom_sens = DEF_SAMPLING_DOWN_MOMENTUM_SENSITIVITY,
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+#ifdef ENABLE_HOTPLUGGING
+	.up_threshold_hotplug1 = DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG,
+	.up_threshold_hotplug_freq1 = DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG_FREQ,
+	.block_up_multiplier_hotplug1 = DEF_BLOCK_UP_MULTIPLIER_HOTPLUG1,
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+	.up_threshold_hotplug2 = DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG,
+	.up_threshold_hotplug_freq2 = DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG_FREQ,
+	.block_up_multiplier_hotplug2 = DEF_BLOCK_UP_MULTIPLIER_HOTPLUG2,
+	.up_threshold_hotplug3 = DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG,
+	.up_threshold_hotplug_freq3 = DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG_FREQ,
+	.block_up_multiplier_hotplug3 = DEF_BLOCK_UP_MULTIPLIER_HOTPLUG3,
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+	.up_threshold_hotplug4 = DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG,
+	.up_threshold_hotplug_freq4 = DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG_FREQ,
+	.block_up_multiplier_hotplug4 = DEF_BLOCK_UP_MULTIPLIER_HOTPLUG4,
+	.up_threshold_hotplug5 = DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG,
+	.up_threshold_hotplug_freq5 = DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG_FREQ,
+	.block_up_multiplier_hotplug5 = DEF_BLOCK_UP_MULTIPLIER_HOTPLUG5,
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+	.up_threshold_hotplug6 = DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG,
+	.up_threshold_hotplug_freq6 = DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG_FREQ,
+	.block_up_multiplier_hotplug6 = DEF_BLOCK_UP_MULTIPLIER_HOTPLUG6,
+	.up_threshold_hotplug7 = DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG,
+	.up_threshold_hotplug_freq7 = DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG_FREQ,
+	.block_up_multiplier_hotplug7 = DEF_BLOCK_UP_MULTIPLIER_HOTPLUG7,
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	.up_threshold_sleep = DEF_UP_THRESHOLD_SLEEP,
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	.down_threshold = DEF_FREQUENCY_DOWN_THRESHOLD,
+#ifdef ENABLE_HOTPLUGGING
+	.down_threshold_hotplug1 = DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG,
+	.down_threshold_hotplug_freq1 = DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG_FREQ,
+	.block_down_multiplier_hotplug1 = DEF_BLOCK_DOWN_MULTIPLIER_HOTPLUG1,
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+	.down_threshold_hotplug2 = DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG,
+	.down_threshold_hotplug_freq2 = DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG_FREQ,
+	.block_down_multiplier_hotplug2 = DEF_BLOCK_DOWN_MULTIPLIER_HOTPLUG2,
+	.down_threshold_hotplug3 = DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG,
+	.down_threshold_hotplug_freq3 = DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG_FREQ,
+	.block_down_multiplier_hotplug3 = DEF_BLOCK_DOWN_MULTIPLIER_HOTPLUG3,
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+	.down_threshold_hotplug4 = DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG,
+	.down_threshold_hotplug_freq4 = DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG_FREQ,
+	.block_down_multiplier_hotplug4 = DEF_BLOCK_DOWN_MULTIPLIER_HOTPLUG4,
+	.down_threshold_hotplug5 = DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG,
+	.down_threshold_hotplug_freq5 = DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG_FREQ,
+	.block_down_multiplier_hotplug5 = DEF_BLOCK_DOWN_MULTIPLIER_HOTPLUG5,
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+	.down_threshold_hotplug6 = DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG,
+	.down_threshold_hotplug_freq6 = DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG_FREQ,
+	.block_down_multiplier_hotplug6 = DEF_BLOCK_DOWN_MULTIPLIER_HOTPLUG6,
+	.down_threshold_hotplug7 = DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG,
+	.down_threshold_hotplug_freq7 = DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG_FREQ,
+	.block_down_multiplier_hotplug7 = DEF_BLOCK_DOWN_MULTIPLIER_HOTPLUG7,
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	.down_threshold_sleep = DEF_DOWN_THRESHOLD_SLEEP,
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	.ignore_nice = DEF_IGNORE_NICE,
+	.smooth_up = DEF_SMOOTH_UP,
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	.smooth_up_sleep = DEF_SMOOTH_UP_SLEEP,
+#ifdef ENABLE_HOTPLUGGING
+	.hotplug_sleep = DEF_HOTPLUG_SLEEP,
+#endif /* ENABLE_HOTPLUGGING */
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	.freq_limit = DEF_FREQ_LIMIT,
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	.freq_limit_sleep = DEF_FREQ_LIMIT_SLEEP,
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	.fast_scaling_up = DEF_FAST_SCALING_UP,
+	.fast_scaling_down = DEF_FAST_SCALING_DOWN,
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	.fast_scaling_sleep_up = DEF_FAST_SCALING_SLEEP_UP,
+	.fast_scaling_sleep_down = DEF_FAST_SCALING_SLEEP_DOWN,
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	.afs_threshold1 = DEF_AFS_THRESHOLD1,
+	.afs_threshold2 = DEF_AFS_THRESHOLD2,
+	.afs_threshold3 = DEF_AFS_THRESHOLD3,
+	.afs_threshold4 = DEF_AFS_THRESHOLD4,
+	.grad_up_threshold = DEF_GRAD_UP_THRESHOLD,
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	.grad_up_threshold_sleep = DEF_GRAD_UP_THRESHOLD_SLEEP,
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	.early_demand = DEF_EARLY_DEMAND,
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	.early_demand_sleep = DEF_EARLY_DEMAND_SLEEP,
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+	.disable_hotplug = DEF_DISABLE_HOTPLUG,
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	.disable_hotplug_sleep = DEF_DISABLE_HOTPLUG_SLEEP,
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	.hotplug_block_up_cycles = DEF_HOTPLUG_BLOCK_UP_CYCLES,
+	.hotplug_block_down_cycles = DEF_HOTPLUG_BLOCK_DOWN_CYCLES,
+	.hotplug_stagger_up = DEF_HOTPLUG_STAGGER_UP,
+	.hotplug_stagger_down = DEF_HOTPLUG_STAGGER_DOWN,
+	.hotplug_idle_threshold = DEF_HOTPLUG_IDLE_THRESHOLD,
+	.hotplug_idle_freq = DEF_HOTPLUG_IDLE_FREQ,
+	.hotplug_engage_freq = DEF_HOTPLUG_ENGAGE_FREQ,
+	.hotplug_max_limit = DEF_HOTPLUG_MAX_LIMIT,
+	.hotplug_min_limit = DEF_HOTPLUG_MIN_LIMIT,
+	.hotplug_min_limit_saved = DEF_HOTPLUG_MIN_LIMIT,
+	.hotplug_min_limit_touchbooster = 0,
+	.hotplug_lock = DEF_HOTPLUG_LOCK,
+#endif /* ENABLE_HOTPLUGGING */
+	.scaling_block_threshold = DEF_SCALING_BLOCK_THRESHOLD,
+	.scaling_block_cycles = DEF_SCALING_BLOCK_CYCLES,
+	.scaling_up_block_cycles = DEF_SCALING_UP_BLOCK_CYCLES,
+	.scaling_up_block_freq = DEF_SCALING_UP_BLOCK_FREQ,
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+	.scaling_block_temp = DEF_SCALING_BLOCK_TEMP,
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+	.scaling_trip_temp = DEF_SCALING_TRIP_TEMP,
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+	.scaling_block_freq = DEF_SCALING_BLOCK_FREQ,
+	.scaling_block_force_down = DEF_SCALING_BLOCK_FORCE_DOWN,
+	.scaling_fastdown_freq = DEF_SCALING_FASTDOWN_FREQ,
+	.scaling_fastdown_up_threshold = DEF_SCALING_FASTDOWN_UP_THRESHOLD,
+	.scaling_fastdown_down_threshold = DEF_SCALING_FASTDOWN_DOWN_THRESHOLD,
+	.scaling_responsiveness_freq = DEF_SCALING_RESPONSIVENESS_FREQ,
+	.scaling_responsiveness_up_threshold = DEF_SCALING_RESPONSIVENESS_UP_THRESHOLD,
+	.scaling_proportional = DEF_SCALING_PROPORTIONAL,
+#ifdef ENABLE_INPUTBOOSTER
+	// ff: Input Booster
+	.inputboost_cycles = DEF_INPUTBOOST_CYCLES,
+	.inputboost_up_threshold = DEF_INPUTBOOST_UP_THRESHOLD,
+	.inputboost_punch_cycles = DEF_INPUTBOOST_PUNCH_CYCLES,
+	.inputboost_punch_freq = DEF_INPUTBOOST_PUNCH_FREQ,
+	.inputboost_punch_on_fingerdown = DEF_INPUTBOOST_PUNCH_ON_FINGERDOWN,
+	.inputboost_punch_on_fingermove = DEF_INPUTBOOST_PUNCH_ON_FINGERMOVE,
+	.inputboost_punch_on_epenmove = DEF_INPUTBOOST_PUNCH_ON_EPENMOVE,
+	.inputboost_typingbooster_up_threshold = DEF_INPUTBOOST_TYPINGBOOSTER_UP_THRESHOLD,
+	.inputboost_typingbooster_cores = DEF_INPUTBOOST_TYPINGBOOSTER_CORES,
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_MUSIC_LIMITS
+	.music_max_freq = DEF_MUSIC_MAX_FREQ,
+	.music_min_freq = DEF_MUSIC_MIN_FREQ,
+#ifdef ENABLE_HOTPLUGGING
+	.music_min_cores = DEF_MUSIC_MIN_CORES,
+#endif /* ENABLE_HOTPLUGGING */
+	.music_state = 0,
+#endif /* ENABLE_MUSIC_LIMITS */
+};
+
+#ifdef ENABLE_INPUTBOOSTER
+// ff: Input Booster
+static void interactive_input_event(struct input_handle *handle, unsigned int type, unsigned int code, int value)
+{
+	unsigned int time_since_typingbooster_lasttapped = 0;
+	unsigned int flg_do_punch_id = 0;
+	struct timeval time_now;
+	bool flg_inputboost_mt_touchmajor = false;
+	bool flg_inputboost_abs_xy = false;
+	bool flg_force_punch = false;
+	int inputboost_btn_toolfinger = -1;
+	int inputboost_btn_touch = -1;
+	int inputboost_mt_trackingid = 0;
+	int tmp_flg_ctr_inputboost_punch = 0;
+
+	// ff: don't do any boosting when overheating
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+	if (tmu_throttle_steps > 0)
+		return;
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+	/*
+	 * ff: ignore events if inputboost isn't enabled (we shouldn't ever be here in that case)
+	 *     or screen-off (but allow power press)
+	 */
+	if (!dbs_tuners_ins.inputboost_cycles
+		|| (suspend_flag && inputboost_last_code != 116))
+		return;
+
+	if (type == EV_SYN && code == SYN_REPORT) {
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove] syn input event. device: %s, saw_touchmajor: %d, saw_xy: %d, toolfinger: %d, touch: %d, trackingid: %d\n",
+				handle->dev->name, flg_inputboost_report_mt_touchmajor, flg_inputboost_report_abs_xy, inputboost_report_btn_toolfinger, inputboost_report_btn_touch, inputboost_report_mt_trackingid);
+#endif /* ZZMOOVE_DEBUG */
+		if (strstr(handle->dev->name, "touchscreen") || strstr(handle->dev->name, "synaptics")) {
+
+			// ff: don't boost if not enabled, or while sleeping
+			if (!boost_on_tsp || suspend_flag)
+				return;
+
+			// ff: save the event's data flags
+			flg_inputboost_mt_touchmajor = flg_inputboost_report_mt_touchmajor;
+			flg_inputboost_abs_xy = flg_inputboost_report_abs_xy;
+			inputboost_btn_toolfinger = inputboost_report_btn_toolfinger;
+			inputboost_btn_touch = inputboost_report_btn_touch;
+			inputboost_mt_trackingid = inputboost_report_mt_trackingid;
+
+			// ff: reset the event's data flags
+			flg_inputboost_report_mt_touchmajor = false;
+			flg_inputboost_report_abs_xy = false;
+			inputboost_report_btn_toolfinger = -1;
+			inputboost_report_btn_touch = -1;
+			inputboost_report_mt_trackingid = 0;
+#ifdef ZZMOOVE_DEBUG
+			pr_info("[zzmoove] syn input event. device: %s, saw_touchmajor: %d, saw_xy: %d, toolfinger: %d, touch: %d, trackingid: %d\n",
+					handle->dev->name, flg_inputboost_mt_touchmajor, flg_inputboost_abs_xy, inputboost_btn_toolfinger, inputboost_btn_touch, inputboost_mt_trackingid);
+#endif /* ZZMOOVE_DEBUG */
+			// ff: try to determine what kind of event we just saw
+			if (!flg_inputboost_mt_touchmajor
+				&& (flg_inputboost_abs_xy || inputboost_mt_trackingid < 0)
+				&& inputboost_btn_touch < 0) {
+				/*
+				 * ff: assume hovering since:
+				 *     no width was reported, and btn_touch isn't set, and (xy coords were included or trackingid is -1 meaning hover-up)
+				 */
+
+				 // ff: don't boost if not enabled
+				if (!boost_on_tsp_hover)
+					return;
+#ifdef ZZMOOVE_DEBUG
+				// ff: hover is hardcoded to only punch on first hover, otherwise it'd be punching constantly
+				if (inputboost_mt_trackingid > 0) {
+
+					pr_info("[zzmoove] touch - first hover btn_touch: %d\n", inputboost_btn_touch);
+					/*// ff: unused, but kept for future use
+					flg_do_punch_id = 4;
+					flg_force_punch = false;*/
+				} else if (inputboost_mt_trackingid < 0) {
+
+					pr_info("[zzmoove] touch - end hover btn_touch: %d\n", inputboost_btn_touch);
+					/*// ff: don't boost if not enabled, or while sleeping
+					flg_ctr_inputboost_punch = 0;
+					flg_ctr_inputboost = 0;*/
+				} else {
+					pr_info("[zzmoove] touch - update hover btn_touch: %d\n", inputboost_btn_touch);
+				}
+#endif /* ZZMOOVE_DEBUG */
+			} else if (inputboost_mt_trackingid > 0) {
+				// ff: new finger detected event
+#ifdef ZZMOOVE_DEBUG
+				pr_info("[zzmoove] touch - first touch\n");
+#endif /* ZZMOOVE_DEBUG */
+				flg_do_punch_id = 2;
+
+				// ff: should we boost on every finger down event?
+				if (dbs_tuners_ins.inputboost_punch_on_fingerdown)
+					flg_force_punch = true;
+
+				// ff: typing booster. detects rapid taps, and if found, boosts core count and up_threshold
+				if (dbs_tuners_ins.inputboost_typingbooster_up_threshold) {
+
+					// ff: save current time
+					do_gettimeofday(&time_now);
+
+					// ff: get time difference
+					time_since_typingbooster_lasttapped = (time_now.tv_sec - time_typingbooster_lasttapped.tv_sec) * MSEC_PER_SEC +
+										(time_now.tv_usec - time_typingbooster_lasttapped.tv_usec) / USEC_PER_MSEC;
+
+					// ff: was that typing or just a doubletap?
+					if (time_since_typingbooster_lasttapped < 250) {
+
+						// ff: tap is probably typing
+						ctr_inputboost_typingbooster_taps++;
+#ifdef ZZMOOVE_DEBUG
+						pr_info("[zzmoove] inputboost - typing booster - valid tap: %d\n", ctr_inputboost_typingbooster_taps);
+#endif /* ZZMOOVE_DEBUG */
+					} else {
+						// ff: tap too quick, probably a doubletap, ignore
+						ctr_inputboost_typingbooster_taps = 0;
+#ifdef ZZMOOVE_DEBUG
+						pr_info("[zzmoove] inputboost - typing booster - invalid tap: %d (age: %d)\n", ctr_inputboost_typingbooster_taps, time_since_typingbooster_lasttapped);
+#endif /* ZZMOOVE_DEBUG */
+					}
+
+					if ((flg_ctr_inputbooster_typingbooster < 1 && ctr_inputboost_typingbooster_taps > 1)			// ff: if booster wasn't on, require 3 taps
+						|| (flg_ctr_inputbooster_typingbooster > 0 && ctr_inputboost_typingbooster_taps > 0)) {		// ff: otherwise, refill with only 2
+#ifdef ZZMOOVE_DEBUG
+						// ff: probably typing, so start the typing booster
+						if (flg_ctr_inputbooster_typingbooster < 1)
+						    pr_info("[zzmoove] inputboost - typing booster on!\n");
+#endif /* ZZMOOVE_DEBUG */
+						// ff: set typing booster up_threshold counter
+						flg_ctr_inputbooster_typingbooster = 15;
+
+						// ff: request a punch
+						flg_do_punch_id = 12;
+
+						/*
+						 * ff: forcing this will effectively turn this into a touchbooster,
+						 *     as it will keep applying the punch freq until the typing (taps) stops
+						 */
+						flg_force_punch = true;
+
+#ifdef ZZMOOVE_DEBUG
+					} else {
+						pr_info("[zzmoove] inputboost - typing booster - tapctr: %d, flgctr: %d\n", ctr_inputboost_typingbooster_taps, flg_ctr_inputbooster_typingbooster);
+#endif /* ZZMOOVE_DEBUG */
+					}
+					// ff: and finally, set the time so we can compare to it on the next tap
+					do_gettimeofday(&time_typingbooster_lasttapped);
+				}
+
+#ifdef ZZMOOVE_DEBUG
+			} else if (inputboost_mt_trackingid < 0) {
+				// ff: finger-lifted event. do nothing
+				pr_info("[zzmoove] touch - end touch\n");
+#endif /* ZZMOOVE_DEBUG */
+			} else if (flg_inputboost_mt_touchmajor) {
+				// ff: width was reported, assume regular tap
+#ifdef ZZMOOVE_DEBUG
+				pr_info("[zzmoove] touch - update touch\n");
+#endif /* ZZMOOVE_DEBUG */
+				// ff: should we treat this like a touchbooster and always punch on movement?
+				if (dbs_tuners_ins.inputboost_punch_on_fingermove) {
+					flg_do_punch_id = 3;
+					flg_force_punch = true;
+				}
+
+			} else {
+				// ff: unknown event. do nothing
+#ifdef ZZMOOVE_DEBUG
+				pr_info("[zzmoove] touch - unknown\n");
+#endif /* ZZMOOVE_DEBUG */
+				return;
+			}
+
+		} else if (strstr(handle->dev->name, "gpio")) {
+
+			// ff: don't boost if not enabled, or while sleeping
+			if (!boost_on_gpio || suspend_flag)
+				return;
+
+			// ff: check for home button
+			if (inputboost_last_code == 172) {
+				if (suspend_flag) {
+					// ff: home press while suspended shouldn't boost as hard
+					flg_ctr_cpuboost = 2;
+#ifdef ZZMOOVE_DEBUG
+					pr_info("[zzmoove] inputboost - gpio punched freq immediately!\n");
+#endif /* ZZMOOVE_DEBUG */
+				} else {
+					// ff: home press while screen on should boost
+					flg_ctr_cpuboost = 10;
+#ifdef ENABLE_WORK_RESTARTLOOP
+					queue_work_on(0, dbs_aux_wq, &work_restartloop);
+#ifdef ZZMOOVE_DEBUG
+					pr_info("[zzmoove] inputboost - gpio punched freq immediately!\n");
+#endif /* ZZMOOVE_DEBUG */
+					// ff: don't punch, since we just did manually
+#endif /* ENABLE_WORK_RESTARTLOOP */
+				}
+			} else {
+				/*
+				 * ff: other press (aka vol up on note4)
+				 *     treat it as a normal button press
+				 */
+				flg_do_punch_id = 7;
+				flg_force_punch = true;
+			}
+
+		} else if (strstr(handle->dev->name, "touchkey")) {
+
+			// ff: don't boost if not enabled, or while sleeping
+			if (!boost_on_tk || suspend_flag)
+				return;
+
+			// ff: check for recents button
+			if (inputboost_last_code == 254) {
+				/*
+				 * ff: recents press. do more than punch,
+				 *     and set the max-boost and max-core counters, too
+				 */
+				flg_ctr_cpuboost = 20;
+			} else {
+				// ff: anything else (ie. back press)
+				flg_ctr_cpuboost = 3;
+			}
+
+			// ff: always manually punch for touchkeys
+#ifdef ENABLE_WORK_RESTARTLOOP
+#ifdef ZZMOOVE_DEBUG
+			pr_info("[zzmoove] inputboost - tk punched freq immediately!\n");
+#endif /* ZZMOOVE_DEBUG */
+			queue_work_on(0, dbs_aux_wq, &work_restartloop);
+#endif /* ENABLE_WORK_RESTARTLOOP */
+			// ff: don't punch, since we just did manually
+
+		} else if (strstr(handle->dev->name, "e-pen")) {
+			// ff: s-pen is hovering or writing
+
+			// ff: don't boost if not enabled, or while sleeping
+			if (!boost_on_epen || suspend_flag)
+				return;
+
+			// ff: request a punch
+			flg_do_punch_id = 11;
+
+			// ff: should we treat this like a touchbooster and always punch on movement?
+			if (dbs_tuners_ins.inputboost_punch_on_epenmove)
+				flg_force_punch = true;
+
+		} else if (strstr(handle->dev->name, "qpnp_pon")) {
+			// ff: on the note4/opo, this is the power key and volume down
+
+			/*
+			 * ff: only boost if power is press while screen-off
+			 *     let it still apply a normal boost on screen-on to speed up going into suspend
+			 */
+			if (inputboost_last_code == 116 && suspend_flag) {
+				// disabled since we're still boosting in the pon driver
+				/*flg_ctr_cpuboost = 25;
+				flg_ctr_inputboost = 100;
+				queue_work_on(0, dbs_aux_wq, &work_restartloop);
+				pr_info("[zzmoove] inputboost - gpio/powerkey punched freq immediately and skipped the rest\n");*/
+
+				// ff: not only don't punch, but don't even start the booster, since we just did both with zzmoove_boost() externally
+				return;
+			} else {
+				// ff: even though it's coming from a different device, treat this if it was a gpio event anyway
+				flg_do_punch_id = 7;
+				flg_force_punch = true;
+			}
+		}
+
+		if (flg_do_punch_id						// ff: punch is requested
+			&& dbs_tuners_ins.inputboost_punch_cycles		// ff: punch is enabled
+			&& dbs_tuners_ins.inputboost_punch_freq			// ff: punch is enabled
+			&& (flg_ctr_inputboost < 1 || flg_force_punch)) {	// ff: this is the first event since the inputbooster ran out, or it is forced
+										//     a punch length and frequency is set, so boost!
+			// ff: but only do so if it hasn't been punched yet, or if it hasn't been punched by a touch yet
+			// ff: save the punch counter state so we can avoid redundantly flushing the punch
+			tmp_flg_ctr_inputboost_punch = flg_ctr_inputboost_punch;
+
+			// ff: refill the punch counter. remember, flg_ctr_inputboost_punch is decremented before it is used, so add 1
+			flg_ctr_inputboost_punch = dbs_tuners_ins.inputboost_punch_cycles + 1;
+
+			// ff: don't immediately apply the punch if we're already boosted or punched
+#ifdef ENABLE_WORK_RESTARTLOOP
+			if (flg_ctr_cpuboost < 5 && tmp_flg_ctr_inputboost_punch < 1) {
+				queue_work_on(0, dbs_aux_wq, &work_restartloop);
+#endif /* ENABLE_WORK_RESTARTLOOP */
+#ifdef ZZMOOVE_DEBUG
+				do_gettimeofday(&time_touchbooster_lastrun);
+				pr_info("[zzmoove] inputboost - punched freq immediately for: %d\n", flg_do_punch_id);
+#endif /* ZZMOOVE_DEBUG */
+#ifdef ENABLE_WORK_RESTARTLOOP
+			}
+#endif /* ENABLE_WORK_RESTARTLOOP */
+#ifdef ZZMOOVE_DEBUG
+			pr_info("[zzmoove] inputboost - punch set to %d mhz for %d cycles (punched by: %d, forced: %d)\n",
+					dbs_tuners_ins.inputboost_punch_freq, dbs_tuners_ins.inputboost_punch_cycles, flg_do_punch_id, flg_force_punch);
+#endif /* ZZMOOVE_DEBUG */
+		}
+
+		// ff: refill the inputboost counter to apply the up_threshold
+		flg_ctr_inputboost = dbs_tuners_ins.inputboost_cycles;
+
+	} else {
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove] ev input event. name: %s, type: %d, code: %d, value: %d\n", handle->dev->name, type, code, value);
+#endif /* ZZMOOVE_DEBUG */
+		// ff: we need to keep track of the data sent in this report
+		if (strstr(handle->dev->name, "touchscreen") || strstr(handle->dev->name, "synaptics")) {
+			if (code == BTN_TOOL_FINGER) {
+				// ff: 0 = nothing at all, 1 - touch OR hover starting
+				inputboost_report_btn_toolfinger = value;
+
+			} else if (code == BTN_TOUCH) {
+				// ff: 0 = up/hovering, 1 - touch starting
+				inputboost_report_btn_touch = value;
+
+			} else if (code == ABS_MT_TRACKING_ID) {
+				// ff: -1 = finger-up, >1 = finger-down
+				inputboost_report_mt_trackingid = value;
+#ifdef ABS_MT_SUMSIZE
+			} else if (code == ABS_MT_TOUCH_MAJOR || code == ABS_MT_SUMSIZE) {
+#else
+			} else if (code == ABS_MT_TOUCH_MAJOR) {
+#endif /* ABS_MT_SUMSIZE */
+				// ff: this is a touch report
+				flg_inputboost_report_mt_touchmajor = true;
+
+			} else if (code == ABS_MT_POSITION_X || code == ABS_MT_POSITION_Y) {
+				// ff: this is a hover report, maybe
+				flg_inputboost_report_abs_xy = true;
+			}
+		} else {
+			// ff: a simple saving of the last event is sufficent for non-tsp events
+			inputboost_last_type = type;
+			inputboost_last_code = code;
+			inputboost_last_value = value;
+		}
+	}
+}
+
+static int input_dev_filter(const char *input_dev_name)
+{
+	if (strstr(input_dev_name, "sec_touchscreen") ||
+		strstr(input_dev_name, "sec_e-pen") ||
+		strstr(input_dev_name, "gpio-keys") ||
+		strstr(input_dev_name, "sec_touchkey") ||
+		strstr(input_dev_name, "s2s_pwrkey") ||						// ZZ: opo power key?
+		strstr(input_dev_name, "msm8974-taiko-mtp-snd-card Button Jack") ||		// ZZ: opo sound button?
+		strstr(input_dev_name, "msm8974-taiko-mtp-snd-card Headset Jack") ||		// ZZ: opo headset jack
+		strstr(input_dev_name, "synaptics-rmi-ts") ||					// ZZ: opo touchscreen
+		strstr(input_dev_name, "qpnp_pon")						// ff: note4/opo power and volume-down key
+		//strstr(input_dev_name, "es705")						// ff: note4 always-on audio monitoring, but no input events are sent, so it's pointless
+		) {
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove] inputboost - monitoring input device: %s\n", input_dev_name);
+#endif /* ZZMOOVE_DEBUG */
+		return 0;
+
+	} else {
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove] inputboost - ignored input device: %s\n", input_dev_name);
+#endif /* ZZMOOVE_DEBUG */
+		return 1;
+	}
+}
+
+static int interactive_input_connect(struct input_handler *handler, struct input_dev *dev, const struct input_device_id *id)
+{
+	struct input_handle *handle;
+	int error;
+
+	if (input_dev_filter(dev->name))
+		return -ENODEV;
+
+	handle = kzalloc(sizeof(struct input_handle), GFP_KERNEL);
+	if (!handle)
+		return -ENOMEM;
+
+	handle->dev = dev;
+	handle->handler = handler;
+	handle->name = "cpufreq";
+
+	error = input_register_handle(handle);
+	if (error)
+		goto err2;
+
+	error = input_open_device(handle);
+	if (error)
+		goto err1;
+
+	return 0;
+
+err1:
+	input_unregister_handle(handle);
+err2:
+	kfree(handle);
+	return error;
+}
+
+static void interactive_input_disconnect(struct input_handle *handle)
+{
+	// ff: reset counters
+	flg_ctr_inputboost = 0;
+	flg_ctr_inputboost_punch = 0;
+#ifdef ZZMOOVE_DEBUG
+	pr_info("[zzmoove/interactive_input_disconnect] inputbooster - unregistering\n");
+#endif /* ZZMOOVE_DEBUG */
+	input_close_device(handle);
+	input_unregister_handle(handle);
+	kfree(handle);
+}
+
+static const struct input_device_id interactive_ids[] = {
+	{ .driver_info = 1 },
+	{ },
+};
+
+static struct input_handler interactive_input_handler = {
+	.event = interactive_input_event,
+	.connect = interactive_input_connect,
+	.disconnect = interactive_input_disconnect,
+	.name = "zzmoove",
+	.id_table = interactive_ids,
+};
+#endif /* ENABLE_INPUTBOOSTER */
+
+// Yank: return a valid value between min and max
+static int validate_min_max(int val, int min, int max)
+{
+	return min(max(val, min), max);
+}
+
+// ZZ: system table scaling mode with freq search optimizations and proportional freq option
+static inline int zz_get_next_freq(unsigned int curfreq, unsigned int updown, unsigned int load)
+{
+	int i = 0;
+	unsigned int prop_target = 0, zz_target = 0, dead_band_freq = 0;	// ZZ: proportional freq, system table freq, dead band freq
+	int smooth_up_steps = 0;						// Yank: smooth up steps
+	static int tmp_limit_table_start = 0;
+	static int tmp_max_scaling_freq_soft = 0;
+	static int tmp_limit_table_end = 0;
+
+	prop_target = pol_min + load * (pol_max - pol_min) / 100;		// ZZ: prepare proportional target freq whitout deadband (directly mapped to min->max load)
+
+	if (dbs_tuners_ins.scaling_proportional == 2)				// ZZ: mode '2' use proportional target frequencies only
+	    return prop_target;
+
+	if (dbs_tuners_ins.scaling_proportional == 3) {				// ZZ: mode '3' use proportional target frequencies only and switch to pol_min in deadband range
+	    dead_band_freq = pol_max / 100 * load;				// ZZ: use old calculation to get deadband frequencies (=lower than pol_min)
+	    if (dead_band_freq > pol_min)					// ZZ: the load usually is too unsteady so we rarely would reach pol_min when load is low
+		return prop_target;						// ZZ: in fact it only will happen when load=0, so only return proportional frequencies if they
+	    else								//     are out of deadband range and if we are in deadband range return min freq
+		return pol_min;							//     (thats a similar behaving as with old propotional freq calculation)
+	}
+
+	if (load <= dbs_tuners_ins.smooth_up)					// Yank: consider smooth up
+	    smooth_up_steps = 0;						// Yank: load not reached, move by one step
+	else
+	    smooth_up_steps = 1;						// Yank: load reached, move by two steps
+
+	tmp_limit_table_start = limit_table_start;				// ZZ: first assign new limits...
+	tmp_limit_table_end = limit_table_end;
+	tmp_max_scaling_freq_soft = max_scaling_freq_soft;
+
+	// ZZ: asc: min freq limit changed
+	if (!freq_table_desc && curfreq
+	    < system_freq_table[min_scaling_freq].frequency)			// ZZ: asc: but reset starting index if current freq is lower than soft/hard min limit otherwise we are
+	    tmp_limit_table_start = 0;						//     shifting out of range and proportional freq is used instead because freq can't be found by loop
+
+	// ZZ: asc: max freq limit changed
+	if (!freq_table_desc && curfreq
+	    > system_freq_table[max_scaling_freq_soft].frequency)		// ZZ: asc: but reset ending index if current freq is higher than soft/hard max limit otherwise we are
+	    tmp_limit_table_end = system_freq_table[freq_table_size].frequency;	//     shifting out of range and proportional freq is used instead because freq can't be found by loop
+
+	// ZZ: desc: max freq limit changed
+	if (freq_table_desc && curfreq
+	    > system_freq_table[limit_table_start].frequency)			// ZZ: desc: but reset starting index if current freq is higher than soft/hard max limit otherwise we are
+	    tmp_limit_table_start = 0;						//     shifting out of range and proportional freq is used instead because freq can't be found by loop
+
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+#ifdef ENABLE_MUSIC_LIMITS
+	// ff: check to see if we need to override the screen-off limit with the music one
+	if (suspend_flag && dbs_tuners_ins.freq_limit_sleep
+	    && dbs_tuners_ins.freq_limit_sleep < dbs_tuners_ins.music_max_freq
+	    && dbs_tuners_ins.music_state) {
+
+	    tmp_max_scaling_freq_soft = music_max_freq_step;
+
+	    if (!freq_table_desc && curfreq					// ZZ: asc: assign only if current freq is lower or equal soft/hard limit otherwise we are shifting out
+		<= system_freq_table[music_max_freq_step].frequency)		//     of range and proportional freq is used instead because freq can't be found by loop
+		tmp_limit_table_end = system_freq_table[music_max_freq_step].frequency;
+
+	    if (freq_table_desc && curfreq
+		<= system_freq_table[music_max_freq_step].frequency) {		// ZZ: desc: assign only if current freq is lower or equal soft/hard limit otherwise we are shifting out
+		tmp_limit_table_start = music_max_freq_step;			//     of range and proportional freq is used instead because freq can't be found by loop
+	    } else if (freq_table_desc && curfreq
+		> system_freq_table[music_max_freq_step].frequency) {
+		tmp_limit_table_start = 0;
+	    }
+	}
+#endif /* ENABLE_MUSIC_LIMITS */
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+
+	// ZZ: feq search loop with optimization
+	if (freq_table_desc) {
+	    for (i = tmp_limit_table_start; (likely(system_freq_table[i].frequency != tmp_limit_table_end)); i++) {
+		if (unlikely(curfreq == system_freq_table[i].frequency)) {	// Yank: we found where we currently are (i)
+		    if (updown == 1) {						// Yank: scale up, but don't go above softlimit
+			zz_target = min(system_freq_table[tmp_max_scaling_freq_soft].frequency,
+		        system_freq_table[validate_min_max(i - 1 - smooth_up_steps - scaling_mode_up, 0, freq_table_size)].frequency);
+#ifdef ZZMOOVE_DEBUG
+			pr_info("[zzmoove/zz_get_next_freq] desc - UP - val1: %d - val2: %d [steps: %d, smooth_up: %d, scaling_mode_up: %d]\n",
+				system_freq_table[tmp_max_scaling_freq_soft].frequency, system_freq_table[validate_min_max(i - 1 - smooth_up_steps - scaling_mode_up,
+				0, freq_table_size)].frequency, (1 + smooth_up_steps + scaling_mode_up), smooth_up_steps, scaling_mode_up);
+#endif /* ZZMOOVE_DEBUG */
+			if (dbs_tuners_ins.scaling_proportional == 1)		// ZZ: if proportional scaling is enabled
+			    return min(zz_target, prop_target);			// ZZ: check which freq is lower and return it
+			else
+			    return zz_target;					// ZZ: or return the found system table freq as usual
+		    } else {							// Yank: scale down, but don't go below min. freq.
+			zz_target = max(system_freq_table[min_scaling_freq].frequency,
+		        system_freq_table[validate_min_max(i + 1 + scaling_mode_down, 0, freq_table_size)].frequency);
+#ifdef ZZMOOVE_DEBUG
+			pr_info("[zzmoove/zz_get_next_freq] desc - DOWN - val1: %d - val2: %d [steps: %d, scaling_mode_down: %d]\n",
+				system_freq_table[tmp_max_scaling_freq_soft].frequency, system_freq_table[validate_min_max(i + 1 + scaling_mode_down,
+				0, freq_table_size)].frequency, (1 + scaling_mode_down), scaling_mode_down);
+#endif /* ZZMOOVE_DEBUG */
+			if (dbs_tuners_ins.scaling_proportional == 1)		// ZZ: if proportional scaling is enabled
+			    return min(zz_target, prop_target);			// ZZ: check which freq is lower and return it
+			else
+			    return zz_target;					// ZZ: or return the found system table freq as usual
+		    }
+		}
+	    }									// ZZ: this shouldn't happen but if the freq is not found in system table
+	    return prop_target;							//     fall back to proportional freq target to avoid returning 0
+	} else {
+	    for (i = tmp_limit_table_start; (likely(system_freq_table[i].frequency <= tmp_limit_table_end)); i++) {
+		if (unlikely(curfreq == system_freq_table[i].frequency)) {	// Yank: we found where we currently are (i)
+		    if (updown == 1) {						// Yank: scale up, but don't go above softlimit
+			zz_target = min(system_freq_table[tmp_max_scaling_freq_soft].frequency,
+			system_freq_table[validate_min_max(i + 1 + smooth_up_steps + scaling_mode_up, 0, freq_table_size)].frequency);
+#ifdef ZZMOOVE_DEBUG
+			pr_info("[zzmoove/zz_get_next_freq] asc - UP - val1: %d - val2: %d [steps: %d, smooth_up: %d, scaling_mode_up: %d]\n",
+				system_freq_table[tmp_max_scaling_freq_soft].frequency, system_freq_table[validate_min_max(i + 1 + smooth_up_steps + scaling_mode_up,
+				0, freq_table_size)].frequency, (1 + smooth_up_steps + scaling_mode_up), smooth_up_steps, scaling_mode_up);
+#endif /* ZZMOOVE_DEBUG */
+			if (dbs_tuners_ins.scaling_proportional == 1)		// ZZ: if proportional scaling is enabled
+			    return min(zz_target, prop_target);			// ZZ: check which freq is lower and return it
+			else
+			    return zz_target;					// ZZ: or return the found system table freq as usual
+		    } else {							// Yank: scale down, but don't go below min. freq.
+			zz_target = max(system_freq_table[min_scaling_freq].frequency,
+			system_freq_table[validate_min_max(i - 1 - scaling_mode_down, 0, freq_table_size)].frequency);
+#ifdef ZZMOOVE_DEBUG
+			pr_info("[zzmoove/zz_get_next_freq] asc - DOWN - val1: %d - val2: %d [steps: %d, scaling_mode_down: %d]\n",
+				system_freq_table[min_scaling_freq].frequency, system_freq_table[validate_min_max(i - 1 - scaling_mode_down,
+				0, freq_table_size)].frequency, (1 + scaling_mode_down), scaling_mode_down);
+#endif /* ZZMOOVE_DEBUG */
+			if (dbs_tuners_ins.scaling_proportional == 1)		// ZZ: if proportional scaling is enabled
+			    return min(zz_target, prop_target);			// ZZ: check which freq is lower and return it
+			else
+			    return zz_target;					// ZZ: or return the found system table freq as usual
+		    }
+		}
+	    }									// ZZ: this shouldn't happen but if the freq is not found in system table
+	    return prop_target;							//     fall back to proportional freq target to avoid returning 0
+	}
+}
+
+#ifdef ENABLE_HOTPLUGGING
+// ZZ: function for enabling/disabling cores from offline/online state
+static inline void __cpuinit enable_disable_cores(void)
+{
+	int i = 0;
+
+	if (enable_cores == 1) {							// ZZ: no limit, all cores
+
+		for (i = 1; i < possible_cpus; i++) {					// ZZ: enable all offline cores
+		    if (!cpu_online(i))
+		    cpu_up(i);
+		}
+		enable_cores = 0;							// ZZ: reset enable flag again
+	}
+
+	if (enable_cores == 2) {							// ZZ: limit min cores
+		for (i = 1; i < num_possible_cpus(); i++) {
+#ifdef ENABLE_MUSIC_LIMITS
+		    if (!cpu_online(i) && (i < dbs_tuners_ins.hotplug_min_limit || (dbs_tuners_ins.music_state && i < dbs_tuners_ins.music_min_cores))
+#else
+		    if (!cpu_online(i) && (i < dbs_tuners_ins.hotplug_min_limit)
+#endif /* ENABLE_MUSIC_LIMITS */
+		    && (!dbs_tuners_ins.hotplug_max_limit || i < dbs_tuners_ins.hotplug_max_limit)) {
+			    // ff: this core is below the minimum, so bring it up
+			    cpu_up(i);
+#ifdef ZZMOOVE_DEBUG
+			    pr_info("[zzmoove] hotplug_min_limit: CPU%d forced up\n", i);
+#endif /* ZZMOOVE_DEBUG */
+		    }
+		}
+		enable_cores = 0;							// ZZ: reset disable flag again
+	}
+
+	if (disable_cores == 1) {							// ZZ: limit max cores
+		for (i = num_possible_cpus() - 1; i >= 1; i--) {
+		    if (cpu_online(i) && i >= dbs_tuners_ins.hotplug_max_limit && (!dbs_tuners_ins.hotplug_min_limit
+#ifdef ENABLE_MUSIC_LIMITS
+		    || i >= dbs_tuners_ins.hotplug_min_limit || (dbs_tuners_ins.music_state && (!dbs_tuners_ins.music_min_cores || i >= dbs_tuners_ins.music_min_cores)))) {
+#else
+		    || i >= dbs_tuners_ins.hotplug_min_limit)) {
+#endif /* ENABLE_MUSIC_LIMITS */
+			    // ff: this core is more than the limit, so turn it off, but don't go below hotplug_min_limit or music_min_cores
+			    cpu_down(i);
+#ifdef ZZMOOVE_DEBUG
+			    pr_info("[zzmoove] hotplug_max_limit: CPU%d forced down\n", i);
+#endif /* ZZMOOVE_DEBUG */
+		    }
+		}
+		disable_cores = 0;							// ZZ: reset disable flag again
+	}
+
+	if (disable_cores == 2) {							// ZZ: lock current cores
+		for (i = 1; i < num_possible_cpus(); i++) {
+			if (!cpu_online(i) && i < dbs_tuners_ins.hotplug_lock) {
+			    // ff: this core is less than the lock, so bring it up
+			    cpu_up(i);
+#ifdef ZZMOOVE_DEBUG
+			    pr_info("[zzmoove] hotplug_lock: CPU%d forced up\n", i);
+#endif /* ZZMOOVE_DEBUG */
+			} else if (cpu_online(i) && i >= dbs_tuners_ins.hotplug_lock) {
+			    // ff: this core is more than the lock, so turn it off
+			    cpu_down(i);
+#ifdef ZZMOOVE_DEBUG
+			    pr_info("[zzmoove] hotplug_lock: CPU%d forced down\n", i);
+#endif /* ZZMOOVE_DEBUG */
+			}
+		}
+		disable_cores = 0;							// ZZ: reset disable flag again
+	}
+}
+#endif /* ENABLE_HOTPLUGGING */
+
+#ifdef ENABLE_PROFILES_SUPPORT
+// ZZ: function for checking/adapting frequency values in tuneables
+static inline unsigned int check_frequency(unsigned int input_freq)
+{
+	int i, f;
+	unsigned int profile_relation = 0, system_min_freq = 0,
+	mid_freq = 0, found_freq = 0, adapted_freq = 0, calc_freq = 0,
+	system_max_freq = 0, next_freq = 0, max_freq_index = 0;
+
+	// ZZ: return if we have nothing to check
+	if (input_freq == 0)
+	    return 0;
+
+	// ZZ: normal checking if given frequency is already in system freq table
+	for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+		if (unlikely(system_freq_table[i].frequency == input_freq))
+		    found_freq = system_freq_table[i].frequency;
+	}
+
+	// ZZ: max freq index in table
+	max_freq_index = i - 1;
+
+	// ZZ: return if we already found a freq
+	if (found_freq != 0) {
+#ifdef ZZMOOVE_DEBUG
+	    pr_info("[zzmoove] check_freq: freq %d diretly found!\n", found_freq);
+#endif /* ZZMOOVE_DEBUG */
+	    return found_freq;
+	}
+
+	// ZZ: get highest/lowest freq in system table
+	if (!freq_table_desc) {
+	    system_max_freq = system_freq_table[max_freq_index].frequency;			// ZZ: ascending order so last freq is the max freq
+	    for (f = 0; (likely(system_freq_table[f].frequency != system_table_end)); f++) {	// ZZ: descending order so use the first available one as min freq
+		if (likely(system_freq_table[f].frequency != 0)) {
+		    system_min_freq = system_freq_table[f].frequency;
+#ifdef ZZMOOVE_DEBUG
+	    pr_info("[zzmoove] check_freq: minimal possible freq %d\n", system_min_freq);
+#endif /* ZZMOOVE_DEBUG */
+		    break;
+		}
+	    }
+	} else {
+	    system_min_freq = system_freq_table[max_freq_index].frequency;			// ZZ: ascending order so last freq is the min freq
+	    for (f = 0; (likely(system_freq_table[f].frequency != system_table_end)); f++) {	// ZZ: descending order so use the first available one as max freq
+		if (likely(system_freq_table[f].frequency != 0)) {
+		    system_max_freq = system_freq_table[f].frequency;
+#ifdef ZZMOOVE_DEBUG
+	    pr_info("[zzmoove] check_freq: maximal possible freq %d\n", system_max_freq);
+#endif /* ZZMOOVE_DEBUG */
+		    break;
+		}
+	    }
+	}
+
+	// ZZ: if we exceed limits return the max/min possible freq
+	if (input_freq > system_max_freq) {
+#ifdef ZZMOOVE_DEBUG
+	    pr_info("[zzmoove] check_freq: input freq too high switching to %d\n", system_max_freq);
+#endif /* ZZMOOVE_DEBUG */
+	    return system_max_freq;
+	} else if (input_freq < system_min_freq) {
+#ifdef ZZMOOVE_DEBUG
+	    pr_info("[zzmoove] check_freq: input freq too low switching to %d\n", system_min_freq);
+#endif /* ZZMOOVE_DEBUG */
+	    return system_min_freq;
+	}
+
+	// ZZ: if not found try to adapt frequency by using a per mille relation
+	// first reduce input value for calculation below
+	calc_freq = input_freq / 100;
+
+	/*
+	 * ZZ: calcualte relation in per mille to the max freq given in profile
+	 * and apply it on current system freq range, this gives some calculated freq
+	 */
+	profile_relation = ((calc_freq * 1000) / (PROFILE_MAX_FREQ / 100));
+	calc_freq = ((system_max_freq) / 1000) * profile_relation;
+
+	// ZZ: start searching in current system table
+	for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+
+		// ZZ: set where next freq is (table order)
+		if (!freq_table_desc) {
+		    next_freq = i + 1;
+		} else {
+		    next_freq = i - 1;
+		}
+
+		// ZZ: if the calculated freq is already a valid one, use it
+		if (system_freq_table[i].frequency == calc_freq || system_freq_table[next_freq].frequency == calc_freq)
+		    return calc_freq;
+
+		// ZZ: if not go on with adaptation
+		if (system_max_freq == PROFILE_MAX_FREQ) {				// ZZ: if we are in the same freq range
+											// ZZ: use input freq and if the given freq is between an existing freq pair
+			if (system_freq_table[i].frequency < input_freq && system_freq_table[next_freq].frequency > input_freq) {
+			    mid_freq = (system_freq_table[i].frequency + system_freq_table[next_freq].frequency) / 2;	// ZZ: calcualte the mid freq of that pair
+			    if (input_freq > mid_freq)					// ZZ: and decide if lower or higher value will be used
+				adapted_freq = system_freq_table[next_freq].frequency;	// ZZ: adapt freq to max in range
+			    else
+				adapted_freq = system_freq_table[i].frequency;		// ZZ: adapt freq to min in range
+			break;
+			}
+		} else {								// ZZ: we are on a different freq range so use calc freq
+			if (system_freq_table[i].frequency < calc_freq && system_freq_table[next_freq].frequency > calc_freq) {
+			    mid_freq = (system_freq_table[i].frequency + system_freq_table[next_freq].frequency) / 2;	// ZZ: calcualte the mid freq of that pair
+			    if (calc_freq > mid_freq)					// ZZ: and decide if lower or higher value will be used
+				adapted_freq = system_freq_table[next_freq].frequency;	// ZZ: adapt freq to max in range
+			    else
+				adapted_freq = system_freq_table[i].frequency;		// ZZ: adapt freq to min in range
+			break;
+			}
+		}
+	}
+#ifdef ZZMOOVE_DEBUG
+	pr_info("[zzmoove] check_freq: input freq was %d\n", input_freq);
+	pr_info("[zzmoove] check_freq: calculated freq is %d\n", calc_freq);
+	pr_info("[zzmoove] check_freq: relation to profile in per mille %d\n", profile_relation);
+	pr_info("[zzmoove] check_freq: lower freq %d on index %d", system_freq_table[i].frequency, i);
+	pr_info("[zzmoove] check_freq: higher freq %d on index %d", system_freq_table[next_freq].frequency, next_freq);
+	pr_info("[zzmoove] check_freq: found freq %d / mid freq %d / adapted freq %d\n", found_freq, mid_freq, adapted_freq);
+#endif /* ZZMOOVE_DEBUG */
+	if (adapted_freq != 0)								// ZZ: if freq was adapted
+	    return adapted_freq;							// ZZ: return adapted
+
+return 0;										// ZZ: freq not found = input out of range
+}
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+// ZZ: function for frequency table order detection and limit optimization
+static inline void evaluate_scaling_order_limit_range(bool start, bool limit, bool suspend, unsigned int min_freq, unsigned int max_freq)
+{
+	int i = 0;
+	int calc_index = 0;
+	system_freq_table = cpufreq_frequency_get_table(0);			// ZZ: update static system frequency table
+
+	/*
+	 * ZZ: execute at start and at limit case and in combination with limit case 3 times
+	 * to catch all scaling max/min changes at/after gov start
+	 */
+	if (start || (limit && freq_init_count <= 1)) {
+
+	    // ZZ: initialisation of freq search in scaling table
+	    for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+		if (unlikely(max_freq == system_freq_table[i].frequency))
+		    max_scaling_freq_hard = max_scaling_freq_soft = i;		// ZZ: init soft and hard max value
+		if (unlikely(min_freq == system_freq_table[i].frequency))
+		    min_scaling_freq_hard = i;					// ZZ: init hard min value
+		    // Yank: continue looping until table end is reached, we need this to set the table size limit below
+	    }
+
+	    freq_table_size = i - 1;						// Yank: upper index limit of freq. table
+
+	    /*
+	     * ZZ: we have to take care about where we are in the frequency table. when using kernel sources without OC capability
+	     * it might be that the first few indexes are containg no frequencies so a save index start point is needed.
+	     */
+	    calc_index = freq_table_size - max_scaling_freq_hard;		// ZZ: calculate the difference and use it as start point
+	    if (calc_index == freq_table_size)					// ZZ: if we are at the end of the table
+		calc_index = calc_index - 1;					// ZZ: shift in range for order calculation below
+
+	    // Yank: assert if CPU freq. table is in ascending or descending order
+	    if (system_freq_table[calc_index].frequency > system_freq_table[calc_index+1].frequency) {
+		freq_table_desc = true;						// Yank: table is in descending order as expected, lowest freq at the bottom of the table
+		min_scaling_freq = i - 1;					// Yank: last valid frequency step (lowest frequency)
+		limit_table_start = max_scaling_freq_soft;			// ZZ: we should use the actual max scaling soft limit value as search start point
+	    } else {
+		freq_table_desc = false;					// Yank: table is in ascending order, lowest freq at the top of the table
+		min_scaling_freq = 0;						// Yank: first valid frequency step (lowest frequency)
+		limit_table_start = min_scaling_freq_hard;			// ZZ: we should use the actual min scaling hard limit value as search start point
+		limit_table_end = max_freq;					// ZZ: end searching at highest frequency limit
+	    }
+	}
+
+	// ZZ: execute at limit case but not at suspend and in combination with start case 3 times at/after gov start
+	if ((limit && !suspend) || (limit && freq_init_count <= 1)) {
+
+	    /*
+	     * ZZ: obviously the 'limit case' will be executed multiple times at suspend for 'sanity' checks or boosts
+	     * but we have already a early suspend code to handle scaling search limits so we have to differentiate
+	     * to avoid double execution at suspend!
+	     */
+	    if (max_freq != system_freq_table[max_scaling_freq_hard].frequency) {	// Yank: if policy->max has changed...
+		for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+		    if (unlikely(max_freq == system_freq_table[i].frequency)) {
+			max_scaling_freq_hard = i;					// ZZ: ...set new freq scaling index
+			break;
+		    }
+		}
+	    }
+
+	    if (!freq_table_desc) {							// ZZ: if ascending ordered table is used
+		if (min_freq != system_freq_table[min_scaling_freq_hard].frequency) {	// ZZ: and policy->min has changed...
+		    for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+			if (unlikely(min_freq == system_freq_table[i].frequency)) {
+			    min_scaling_freq_hard = limit_table_start = i;		// ZZ: ...set new freq scaling index
+			    if (min_scaling_freq_soft && (max_scaling_freq_hard < min_scaling_freq_soft))	// ZZ: if max hard limit is lower than min soft limit
+				limit_table_start = 0;					// ZZ: reset min index
+			    if (min_scaling_freq_soft && (min_scaling_freq_hard > min_scaling_freq_soft)) {	// ZZ: if min hard limit is higher than min soft limit
+				limit_table_start = min_scaling_freq_hard;		// ZZ: use min hard index
+			    } else if (min_scaling_freq_soft && (min_scaling_freq_hard < min_scaling_freq_soft)) {
+				limit_table_start = min_scaling_freq_soft;		// ZZ: use min soft index
+			    }
+			break;
+			}
+		    }
+		}
+	    }
+
+	    if (dbs_tuners_ins.freq_limit == 0 ||					// Yank: if there is no awake freq. limit
+		dbs_tuners_ins.freq_limit > system_freq_table[max_scaling_freq_hard].frequency) {	// Yank: or it is higher than hard max frequency
+		max_scaling_freq_soft = max_scaling_freq_hard;				// Yank: use hard max frequency
+		if (freq_table_desc)							// ZZ: if descending ordered table is used
+		    limit_table_start = max_scaling_freq_soft;				// ZZ: we should use the actual scaling soft limit value as search start point
+		else
+		    limit_table_end = system_freq_table[max_scaling_freq_soft].frequency;	// ZZ: set search end point to max freq limit when using ascending table
+	    } else {
+		for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+		    if (unlikely(dbs_tuners_ins.freq_limit == system_freq_table[i].frequency)) {	// Yank: else lookup awake max. frequency index
+			max_scaling_freq_soft = i;
+			if (freq_table_desc)						// ZZ: if descending ordered table is used
+			    limit_table_start = max_scaling_freq_soft;			// ZZ: we should use the actual scaling soft limit value as search start point
+			else
+			    limit_table_end = system_freq_table[i].frequency;		// ZZ: set search end point to soft freq limit when using ascending table
+		    break;
+		    }
+		}
+	    }
+	    if (freq_init_count < 2)							// ZZ: execute start and limit part together 3 times to catch a possible setting of
+	    freq_init_count++;								// ZZ: hard freq limit after gov start - after that skip 'start' part during
+	}										// ZZ: normal operation and use only limit part to adjust limit optimizations
+
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	// ZZ: execute only at suspend but not at limit case
+	if (suspend && !limit) {							// ZZ: only if we are at suspend
+	    if (freq_limit_asleep == 0 ||						// Yank: if there is no sleep frequency limit
+		freq_limit_asleep > system_freq_table[max_scaling_freq_hard].frequency) {	// Yank: or it is higher than hard max frequency
+		max_scaling_freq_soft = max_scaling_freq_hard;				// Yank: use hard max frequency
+		if (freq_table_desc)							// ZZ: if descending ordered table is used
+		    limit_table_start = max_scaling_freq_soft;				// ZZ: we should use the actual scaling soft limit value as search start point
+		else
+		    limit_table_end = system_freq_table[max_scaling_freq_soft].frequency;	// ZZ: set search end point to max freq limit when using ascending table
+	    } else {
+		for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+		    if (unlikely(freq_limit_asleep == system_freq_table[i].frequency)) {	// Yank: else lookup sleep max. frequency index
+			max_scaling_freq_soft = i;
+			if (freq_table_desc)						// ZZ: if descending ordered table is used
+			    limit_table_start = max_scaling_freq_soft;			// ZZ: we should use the actual scaling soft limit value as search start point
+			else
+			    limit_table_end = system_freq_table[i].frequency;		// ZZ: set search end point to max freq limit when using ascending table
+		    break;
+		    }
+		}
+	    }
+	}
+
+	// ZZ: execute only at resume but not at limit or start case
+	if (!suspend && !limit && !start) {						// ZZ: only if we are not at suspend
+	    if (freq_limit_awake == 0 ||						// Yank: if there is no awake frequency limit
+		freq_limit_awake > system_freq_table[max_scaling_freq_hard].frequency) {	// Yank: or it is higher than hard max frequency
+		max_scaling_freq_soft = max_scaling_freq_hard;				// Yank: use hard max frequency
+		if (freq_table_desc)							// ZZ: if descending ordered table is used
+		    limit_table_start = max_scaling_freq_soft;				// ZZ: we should use the actual scaling soft limit value as search start point
+		else
+		    limit_table_end = system_freq_table[max_scaling_freq_soft].frequency;	// ZZ: set search end point to max freq limit when using ascending table
+	    } else {
+		for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+		    if (unlikely(freq_limit_awake == system_freq_table[i].frequency)) {		// Yank: else lookup awake max. frequency index
+			max_scaling_freq_soft = i;
+			if (freq_table_desc)						// ZZ: if descending ordered table is used
+			    limit_table_start = max_scaling_freq_soft;			// ZZ: we should use the actual scaling soft limit value as search start point
+			else
+			    limit_table_end = system_freq_table[i].frequency;		// ZZ: set search end point to soft freq limit when using ascending table
+		    break;
+		    }
+		}
+	    }
+	}
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+}
+
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+// ZZ: function for auto adjusting frequency thresholds if max policy has changed
+static inline void adjust_freq_thresholds(unsigned int step)
+{
+	if (dbs_tuners_ins.auto_adjust_freq_thresholds != 0 && step != 0 && freq_init_count > 0) {	// ZZ: start adjusting if enabled and after freq search is ready initialized
+#ifdef ENABLE_HOTPLUGGING
+		// ZZ: adjust hotplug engage freq
+		if (dbs_tuners_ins.hotplug_engage_freq != 0) {						// ZZ: adjust only if tuneable is set
+		    if ((dbs_tuners_ins.hotplug_engage_freq + step < pol_min
+			|| dbs_tuners_ins.hotplug_engage_freq + step > pol_max)
+			&& !temp_hotplug_engage_freq_flag) {						// ZZ: check if we would go under/over limits
+			temp_hotplug_engage_freq = dbs_tuners_ins.hotplug_engage_freq + step;		// ZZ: if so do it temporary but do not save tuneable yet
+			temp_hotplug_engage_freq_flag = true;						// ZZ: set temp saving flag
+		    } else if (temp_hotplug_engage_freq_flag) {						// ZZ: last time we were under/over limits
+			if (temp_hotplug_engage_freq + step < pol_min
+			    || temp_hotplug_engage_freq + step > pol_max) {				// ZZ: and if we are still there
+			    temp_hotplug_engage_freq = temp_hotplug_engage_freq + step;			// ZZ: add step to temp var instead of tuneable var
+			} else {
+			    dbs_tuners_ins.hotplug_engage_freq = temp_hotplug_engage_freq + step;	// ZZ: else use it as offset for next step and finally save it in tuneable
+			    temp_hotplug_engage_freq = 0;						// ZZ: reset temp var
+			    temp_hotplug_engage_freq_flag = false;					// ZZ: reset temp flag
+			}
+		    } else {
+			dbs_tuners_ins.hotplug_engage_freq += step;					// ZZ: or change it directly in the tuneable if we are in good range
+		    }
+		}
+
+		// ZZ: adjust hotplug idle freq
+		if (dbs_tuners_ins.hotplug_idle_freq != 0) {
+		    if ((dbs_tuners_ins.hotplug_idle_freq + step < pol_min
+			|| dbs_tuners_ins.hotplug_idle_freq + step > pol_max)
+			&& !temp_hotplug_idle_freq_flag) {
+			temp_hotplug_idle_freq = dbs_tuners_ins.hotplug_idle_freq + step;
+			temp_hotplug_idle_freq_flag = true;
+		    } else if (temp_hotplug_idle_freq_flag) {
+			if (temp_hotplug_idle_freq + step < pol_min
+			    || temp_hotplug_idle_freq + step > pol_max) {
+			    temp_hotplug_idle_freq = temp_hotplug_idle_freq + step;
+			} else {
+			    dbs_tuners_ins.hotplug_idle_freq = temp_hotplug_idle_freq + step;
+			    temp_hotplug_idle_freq = 0;
+			    temp_hotplug_idle_freq_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.hotplug_idle_freq += step;
+		    }
+		}
+#endif /* ENABLE_HOTPLUGGING */
+#ifdef ENABLE_INPUTBOOSTER
+		// ZZ: adjust inputboost punch freq
+		if (dbs_tuners_ins.inputboost_punch_freq != 0) {
+		    if ((dbs_tuners_ins.inputboost_punch_freq + step < pol_min
+			|| dbs_tuners_ins.inputboost_punch_freq + step > pol_max)
+			&& !temp_inputboost_punch_freq_flag) {
+			temp_inputboost_punch_freq = dbs_tuners_ins.inputboost_punch_freq + step;
+			temp_inputboost_punch_freq_flag = true;
+		    } else if (temp_inputboost_punch_freq_flag) {
+			if (temp_inputboost_punch_freq + step < pol_min
+			    || temp_inputboost_punch_freq + step > pol_max) {
+			    temp_inputboost_punch_freq = temp_inputboost_punch_freq + step;
+			} else {
+			    dbs_tuners_ins.inputboost_punch_freq = temp_inputboost_punch_freq + step;
+			    temp_inputboost_punch_freq = 0;
+			    temp_inputboost_punch_freq = false;
+			}
+		    } else {
+			dbs_tuners_ins.inputboost_punch_freq += step;
+		    }
+		}
+#endif /* ENABLE_INPUTBOOSTER */
+		// ZZ: adjust scaling block freq
+		if (dbs_tuners_ins.scaling_block_freq != 0) {
+		    if ((dbs_tuners_ins.scaling_block_freq + step < pol_min
+			|| dbs_tuners_ins.scaling_block_freq + step > pol_max)
+			&& !temp_scaling_block_freq_flag) {
+			temp_scaling_block_freq = dbs_tuners_ins.scaling_block_freq + step;
+			temp_scaling_block_freq_flag = true;
+		    } else if (temp_scaling_block_freq_flag) {
+			if (temp_scaling_block_freq + step < pol_min
+			    || temp_scaling_block_freq + step > pol_max) {
+			    temp_scaling_block_freq = temp_scaling_block_freq + step;
+			} else {
+			    dbs_tuners_ins.scaling_block_freq = temp_scaling_block_freq + step;
+			    temp_scaling_block_freq = 0;
+			    temp_scaling_block_freq_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.scaling_block_freq += step;
+		    }
+		}
+
+		// ZZ: adjust scaling fastdown freq
+		if (dbs_tuners_ins.scaling_fastdown_freq != 0) {
+		    if ((dbs_tuners_ins.scaling_fastdown_freq + step < pol_min
+			|| dbs_tuners_ins.scaling_fastdown_freq + step > pol_max)
+			&& !temp_scaling_fastdown_freq_flag) {
+			temp_scaling_fastdown_freq = dbs_tuners_ins.scaling_fastdown_freq + step;
+			temp_scaling_fastdown_freq_flag = true;
+		    } else if (temp_scaling_fastdown_freq_flag) {
+			if (temp_scaling_fastdown_freq + step < pol_min
+			    || temp_scaling_fastdown_freq + step > pol_max) {
+			    temp_scaling_fastdown_freq = temp_scaling_fastdown_freq + step;
+			} else {
+			    dbs_tuners_ins.scaling_fastdown_freq = temp_scaling_fastdown_freq + step;
+			    temp_scaling_fastdown_freq = 0;
+			    temp_scaling_fastdown_freq_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.scaling_fastdown_freq += step;
+		    }
+		}
+
+		// ZZ: adjust scaling responsiveness freq
+		if (dbs_tuners_ins.scaling_responsiveness_freq != 0) {
+		    if ((dbs_tuners_ins.scaling_responsiveness_freq + step < pol_min
+			|| dbs_tuners_ins.scaling_responsiveness_freq + step > pol_max)
+			&& !temp_scaling_responsiveness_freq_flag) {
+			temp_scaling_responsiveness_freq = dbs_tuners_ins.scaling_responsiveness_freq + step;
+			temp_scaling_responsiveness_freq_flag = true;
+		    } else if (temp_scaling_responsiveness_freq_flag) {
+			if (temp_scaling_responsiveness_freq + step < pol_min
+			    || temp_scaling_responsiveness_freq + step > pol_max) {
+			    temp_scaling_responsiveness_freq = temp_scaling_responsiveness_freq + step;
+			} else {
+			    dbs_tuners_ins.scaling_responsiveness_freq = temp_scaling_responsiveness_freq + step;
+			    temp_scaling_responsiveness_freq = 0;
+			    temp_scaling_responsiveness_freq_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.scaling_responsiveness_freq += step;
+		    }
+		}
+#ifdef ENABLE_MUSIC_LIMITS
+		// ZZ: adjust music min freq
+		if (dbs_tuners_ins.music_min_freq != 0) {
+		    if ((dbs_tuners_ins.music_min_freq + step < pol_min
+			|| dbs_tuners_ins.music_min_freq + step > pol_max)
+			&& !temp_music_min_freq_flag) {
+			temp_music_min_freq = dbs_tuners_ins.music_min_freq + step;
+			temp_music_min_freq_flag = true;
+		    } else if (temp_music_min_freq_flag) {
+			if (temp_music_min_freq + step < pol_min
+			    || temp_music_min_freq + step > pol_max) {
+			    temp_music_min_freq = temp_music_min_freq + step;
+			} else {
+			    dbs_tuners_ins.music_min_freq = temp_music_min_freq + step;
+			    temp_music_min_freq = 0;
+			    temp_music_min_freq = false;
+			}
+		    } else {
+			dbs_tuners_ins.music_min_freq += step;
+		    }
+		}
+
+		// ZZ: adjust music max freq
+		if (dbs_tuners_ins.music_max_freq != 0) {
+		    if ((dbs_tuners_ins.music_max_freq + step < pol_min
+			|| dbs_tuners_ins.music_max_freq + step > pol_max)
+			&& !temp_music_max_freq_flag) {
+			temp_music_max_freq = dbs_tuners_ins.music_max_freq + step;
+			temp_music_max_freq_flag = true;
+		    } else if (temp_music_max_freq_flag) {
+			if (temp_music_max_freq + step < pol_min
+			    || temp_music_max_freq + step > pol_max) {
+			    temp_music_max_freq = temp_music_max_freq + step;
+			} else {
+			    dbs_tuners_ins.music_max_freq = temp_music_max_freq + step;
+			    temp_music_max_freq = 0;
+			    temp_music_max_freq = false;
+			}
+		    } else {
+			dbs_tuners_ins.music_max_freq += step;
+		    }
+		}
+#endif /* ENABLE_MUSIC_LIMITS */
+#ifdef ENABLE_HOTPLUGGING
+		// ZZ: adjust up threshold hotplug freq1
+		if (dbs_tuners_ins.up_threshold_hotplug_freq1 != 0) {
+		    if ((dbs_tuners_ins.up_threshold_hotplug_freq1 + step < pol_min
+			|| dbs_tuners_ins.up_threshold_hotplug_freq1 + step > pol_max)
+			&& !temp_up_threshold_hotplug_freq1_flag) {
+			temp_up_threshold_hotplug_freq1 = dbs_tuners_ins.up_threshold_hotplug_freq1 + step;
+			temp_up_threshold_hotplug_freq1_flag = true;
+		    } else if (temp_up_threshold_hotplug_freq1_flag) {
+			if (temp_up_threshold_hotplug_freq1 + step < pol_min
+			    || temp_up_threshold_hotplug_freq1 + step > pol_max) {
+			    temp_up_threshold_hotplug_freq1 = temp_up_threshold_hotplug_freq1 + step;
+			} else {
+			    dbs_tuners_ins.up_threshold_hotplug_freq1 = temp_up_threshold_hotplug_freq1 + step;
+			    temp_up_threshold_hotplug_freq1 = 0;
+			    temp_up_threshold_hotplug_freq1_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.up_threshold_hotplug_freq1 += step;
+		    }
+		}
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		// ZZ: adjust up threshold hotplug freq2
+		if (dbs_tuners_ins.up_threshold_hotplug_freq2 != 0) {
+		    if ((dbs_tuners_ins.up_threshold_hotplug_freq2 + step < pol_min
+			|| dbs_tuners_ins.up_threshold_hotplug_freq2 + step > pol_max)
+			&& !temp_up_threshold_hotplug_freq2_flag) {
+			temp_up_threshold_hotplug_freq2 = dbs_tuners_ins.up_threshold_hotplug_freq2 + step;
+			temp_up_threshold_hotplug_freq2_flag = true;
+		    } else if (temp_up_threshold_hotplug_freq2_flag) {
+			if (temp_up_threshold_hotplug_freq2 + step < pol_min
+			    || temp_up_threshold_hotplug_freq2 + step > pol_max) {
+			    temp_up_threshold_hotplug_freq2 = temp_up_threshold_hotplug_freq2 + step;
+			} else {
+			    dbs_tuners_ins.up_threshold_hotplug_freq2 = temp_up_threshold_hotplug_freq2 + step;
+			    temp_up_threshold_hotplug_freq2 = 0;
+			    temp_up_threshold_hotplug_freq2_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.up_threshold_hotplug_freq2 += step;
+		    }
+		}
+
+		// ZZ: adjust up threshold hotplug freq3
+		if (dbs_tuners_ins.up_threshold_hotplug_freq3 != 0) {
+		    if ((dbs_tuners_ins.up_threshold_hotplug_freq3 + step < pol_min
+			|| dbs_tuners_ins.up_threshold_hotplug_freq3 + step > pol_max)
+			&& !temp_up_threshold_hotplug_freq3_flag) {
+			temp_up_threshold_hotplug_freq3 = dbs_tuners_ins.up_threshold_hotplug_freq3 + step;
+			temp_up_threshold_hotplug_freq3_flag = true;
+		    } else if (temp_up_threshold_hotplug_freq3_flag) {
+			if (temp_up_threshold_hotplug_freq3 + step < pol_min
+			    || temp_up_threshold_hotplug_freq3 + step > pol_max) {
+			    temp_up_threshold_hotplug_freq3 = temp_up_threshold_hotplug_freq3 + step;
+			} else {
+			    dbs_tuners_ins.up_threshold_hotplug_freq3 = temp_up_threshold_hotplug_freq3 + step;
+			    temp_up_threshold_hotplug_freq3 = 0;
+			    temp_up_threshold_hotplug_freq3_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.up_threshold_hotplug_freq3 += step;
+		    }
+		}
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		// ZZ: adjust up threshold hotplug freq4
+		if (dbs_tuners_ins.up_threshold_hotplug_freq4 != 0) {
+		    if ((dbs_tuners_ins.up_threshold_hotplug_freq4 + step < pol_min
+			|| dbs_tuners_ins.up_threshold_hotplug_freq4 + step > pol_max)
+			&& !temp_up_threshold_hotplug_freq4_flag) {
+			temp_up_threshold_hotplug_freq4 = dbs_tuners_ins.up_threshold_hotplug_freq4 + step;
+			temp_up_threshold_hotplug_freq4_flag = true;
+		    } else if (!temp_up_threshold_hotplug_freq4) {
+			if (temp_up_threshold_hotplug_freq4 + step < pol_min
+			    || temp_up_threshold_hotplug_freq4 + step > pol_max) {
+			    temp_up_threshold_hotplug_freq4 = temp_up_threshold_hotplug_freq4 + step;
+			} else {
+			    dbs_tuners_ins.up_threshold_hotplug_freq4 = temp_up_threshold_hotplug_freq4 + step;
+			    temp_up_threshold_hotplug_freq4 = 0;
+			    temp_up_threshold_hotplug_freq4_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.up_threshold_hotplug_freq4 += step;
+		    }
+		}
+
+		// ZZ: adjust up threshold hotplug freq5
+		if (dbs_tuners_ins.up_threshold_hotplug_freq5 != 0) {
+		    if ((dbs_tuners_ins.up_threshold_hotplug_freq5 + step < pol_min
+			|| dbs_tuners_ins.up_threshold_hotplug_freq5 + step > pol_max)
+			&& !temp_up_threshold_hotplug_freq5_flag) {
+			temp_up_threshold_hotplug_freq5 = dbs_tuners_ins.up_threshold_hotplug_freq5 + step;
+			temp_up_threshold_hotplug_freq5_flag = true;
+		    } else if (temp_up_threshold_hotplug_freq5_flag) {
+			if (temp_up_threshold_hotplug_freq5 + step < pol_min
+			    || temp_up_threshold_hotplug_freq5 + step > pol_max) {
+			    temp_up_threshold_hotplug_freq5 = temp_up_threshold_hotplug_freq5 + step;
+			} else {
+			    dbs_tuners_ins.up_threshold_hotplug_freq5 = temp_up_threshold_hotplug_freq5 + step;
+			    temp_up_threshold_hotplug_freq5 = 0;
+			    temp_up_threshold_hotplug_freq5_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.up_threshold_hotplug_freq5 += step;
+		    }
+		}
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		// ZZ: adjust up threshold hotplug freq6
+		if (dbs_tuners_ins.up_threshold_hotplug_freq6 != 0) {
+		    if ((dbs_tuners_ins.up_threshold_hotplug_freq6 + step < pol_min
+			|| dbs_tuners_ins.up_threshold_hotplug_freq6 + step > pol_max)
+			&& !temp_up_threshold_hotplug_freq6_flag) {
+			temp_up_threshold_hotplug_freq6 = dbs_tuners_ins.up_threshold_hotplug_freq6 + step;
+			temp_up_threshold_hotplug_freq6_flag = true;
+		    } else if (temp_up_threshold_hotplug_freq6_flag) {
+			if (temp_up_threshold_hotplug_freq6 + step < pol_min
+			    || temp_up_threshold_hotplug_freq6 + step > pol_max) {
+			    temp_up_threshold_hotplug_freq6 = temp_up_threshold_hotplug_freq6 + step;
+			} else {
+			    dbs_tuners_ins.up_threshold_hotplug_freq6 = temp_up_threshold_hotplug_freq6 + step;
+			    temp_up_threshold_hotplug_freq6 = 0;
+			    temp_up_threshold_hotplug_freq6_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.up_threshold_hotplug_freq6 += step;
+		    }
+		}
+
+		// ZZ: adjust up threshold hotplug freq7
+		if (dbs_tuners_ins.up_threshold_hotplug_freq7 != 0) {
+		    if ((dbs_tuners_ins.up_threshold_hotplug_freq7 + step < pol_min
+			|| dbs_tuners_ins.up_threshold_hotplug_freq7 + step > pol_max)
+			&& !temp_up_threshold_hotplug_freq7_flag) {
+			temp_up_threshold_hotplug_freq7 = dbs_tuners_ins.up_threshold_hotplug_freq7 + step;
+			temp_up_threshold_hotplug_freq7_flag = true;
+		    } else if (temp_up_threshold_hotplug_freq7_flag) {
+			if (temp_up_threshold_hotplug_freq7 + step < pol_min
+			    || temp_up_threshold_hotplug_freq7 + step > pol_max) {
+			    temp_up_threshold_hotplug_freq7 = temp_up_threshold_hotplug_freq7 + step;
+			} else {
+			    dbs_tuners_ins.up_threshold_hotplug_freq7 = temp_up_threshold_hotplug_freq7 + step;
+			    temp_up_threshold_hotplug_freq7 = 0;
+			    temp_up_threshold_hotplug_freq7_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.up_threshold_hotplug_freq7 += step;
+		    }
+		}
+#endif /* (MAX_CORES == 8) */
+		// ZZ: adjust down threshold hotplug freq1
+		if (dbs_tuners_ins.down_threshold_hotplug_freq1 != 0) {
+		    if ((dbs_tuners_ins.down_threshold_hotplug_freq1 + step < pol_min
+			|| dbs_tuners_ins.down_threshold_hotplug_freq1 + step > pol_max)
+			&& !temp_down_threshold_hotplug_freq1_flag) {
+			temp_down_threshold_hotplug_freq1 = dbs_tuners_ins.down_threshold_hotplug_freq1 + step;
+			temp_down_threshold_hotplug_freq1_flag = true;
+		    } else if (temp_down_threshold_hotplug_freq1_flag) {
+			if (temp_down_threshold_hotplug_freq1 + step < pol_min
+			    || temp_down_threshold_hotplug_freq1 + step > pol_max) {
+			    temp_down_threshold_hotplug_freq1 = temp_down_threshold_hotplug_freq1 + step;
+			} else {
+			    dbs_tuners_ins.down_threshold_hotplug_freq1 = temp_down_threshold_hotplug_freq1 + step;
+			    temp_down_threshold_hotplug_freq1 = 0;
+			    temp_down_threshold_hotplug_freq1_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.down_threshold_hotplug_freq1 += step;
+		    }
+		}
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		// ZZ: adjust down threshold hotplug freq2
+		if (dbs_tuners_ins.down_threshold_hotplug_freq2 != 0) {
+		    if ((dbs_tuners_ins.down_threshold_hotplug_freq2 + step < pol_min
+			|| dbs_tuners_ins.down_threshold_hotplug_freq2 + step > pol_max)
+			&& !temp_down_threshold_hotplug_freq2_flag) {
+			temp_down_threshold_hotplug_freq2 = dbs_tuners_ins.down_threshold_hotplug_freq2 + step;
+			temp_down_threshold_hotplug_freq2_flag = true;
+		    } else if (temp_down_threshold_hotplug_freq2_flag) {
+			if (temp_down_threshold_hotplug_freq2 + step < pol_min
+			    || temp_down_threshold_hotplug_freq2 + step > pol_max) {
+			    temp_down_threshold_hotplug_freq2 = temp_down_threshold_hotplug_freq2 + step;
+			} else {
+			    dbs_tuners_ins.down_threshold_hotplug_freq2 = temp_down_threshold_hotplug_freq2 + step;
+			    temp_down_threshold_hotplug_freq2 = 0;
+			    temp_down_threshold_hotplug_freq2_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.down_threshold_hotplug_freq2 += step;
+		    }
+		}
+
+		// ZZ: adjust down threshold hotplug freq3
+		if (dbs_tuners_ins.down_threshold_hotplug_freq3 != 0) {
+		    if ((dbs_tuners_ins.down_threshold_hotplug_freq3 + step < pol_min
+			|| dbs_tuners_ins.down_threshold_hotplug_freq3 + step > pol_max)
+			&& !temp_down_threshold_hotplug_freq3_flag) {
+			temp_down_threshold_hotplug_freq3 = dbs_tuners_ins.down_threshold_hotplug_freq3 + step;
+			temp_down_threshold_hotplug_freq3_flag = true;
+		    } else if (temp_down_threshold_hotplug_freq3_flag) {
+			if (temp_down_threshold_hotplug_freq3 + step < pol_min
+			    || temp_down_threshold_hotplug_freq3 + step > pol_max) {
+			    temp_down_threshold_hotplug_freq3 = temp_down_threshold_hotplug_freq3 + step;
+			} else {
+			    dbs_tuners_ins.down_threshold_hotplug_freq3 = temp_down_threshold_hotplug_freq3 + step;
+			    temp_down_threshold_hotplug_freq3 = 0;
+			    temp_down_threshold_hotplug_freq3_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.down_threshold_hotplug_freq3 += step;
+		    }
+		}
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		// ZZ: adjust down threshold hotplug freq4
+		if (dbs_tuners_ins.down_threshold_hotplug_freq4 != 0) {
+		    if ((dbs_tuners_ins.down_threshold_hotplug_freq4 + step < pol_min
+			|| dbs_tuners_ins.down_threshold_hotplug_freq4 + step > pol_max)
+			&& !temp_down_threshold_hotplug_freq4_flag) {
+			temp_down_threshold_hotplug_freq4 = dbs_tuners_ins.down_threshold_hotplug_freq4 + step;
+			temp_down_threshold_hotplug_freq4_flag = true;
+		    } else if (temp_down_threshold_hotplug_freq4_flag) {
+			if (temp_down_threshold_hotplug_freq4 + step < pol_min
+			    || temp_down_threshold_hotplug_freq4 + step > pol_max) {
+			    temp_down_threshold_hotplug_freq4 = temp_down_threshold_hotplug_freq4 + step;
+			} else {
+			    dbs_tuners_ins.down_threshold_hotplug_freq4 = temp_down_threshold_hotplug_freq4 + step;
+			    temp_down_threshold_hotplug_freq4 = 0;
+			    temp_down_threshold_hotplug_freq4_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.down_threshold_hotplug_freq4 += step;
+		    }
+		}
+
+		// ZZ: adjust down threshold hotplug freq5
+		if (dbs_tuners_ins.down_threshold_hotplug_freq5 != 0) {
+		    if ((dbs_tuners_ins.down_threshold_hotplug_freq5 + step < pol_min
+			|| dbs_tuners_ins.down_threshold_hotplug_freq5 + step > pol_max)
+			&& !temp_down_threshold_hotplug_freq5_flag) {
+			temp_down_threshold_hotplug_freq5 = dbs_tuners_ins.down_threshold_hotplug_freq5 + step;
+			temp_down_threshold_hotplug_freq5_flag = true;
+		    } else if (temp_down_threshold_hotplug_freq5_flag) {
+			if (temp_down_threshold_hotplug_freq5 + step < pol_min
+			    || temp_down_threshold_hotplug_freq5 + step > pol_max) {
+			    temp_down_threshold_hotplug_freq5 = temp_down_threshold_hotplug_freq5 + step;
+			} else {
+			    dbs_tuners_ins.down_threshold_hotplug_freq5 = temp_down_threshold_hotplug_freq5 + step;
+			    temp_down_threshold_hotplug_freq5 = 0;
+			    temp_down_threshold_hotplug_freq5_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.down_threshold_hotplug_freq5 += step;
+		    }
+		}
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		// ZZ: adjust down threshold hotplug freq6
+		if (dbs_tuners_ins.down_threshold_hotplug_freq6 != 0) {
+		    if ((dbs_tuners_ins.down_threshold_hotplug_freq6 + step < pol_min
+			|| dbs_tuners_ins.down_threshold_hotplug_freq6 + step > pol_max)
+			&& !temp_down_threshold_hotplug_freq6_flag) {
+			temp_down_threshold_hotplug_freq6 = dbs_tuners_ins.down_threshold_hotplug_freq6 + step;
+			temp_down_threshold_hotplug_freq6_flag = true;
+		    } else if (temp_down_threshold_hotplug_freq6_flag) {
+			if (temp_down_threshold_hotplug_freq6 + step < pol_min
+			    || temp_down_threshold_hotplug_freq6 + step > pol_max) {
+			    temp_down_threshold_hotplug_freq6 = temp_down_threshold_hotplug_freq6 + step;
+			} else {
+			    dbs_tuners_ins.down_threshold_hotplug_freq6 = temp_down_threshold_hotplug_freq6 + step;
+			    temp_down_threshold_hotplug_freq6 = 0;
+			    temp_down_threshold_hotplug_freq6_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.down_threshold_hotplug_freq6 += step;
+		    }
+		}
+
+		// ZZ: adjust down threshold hotplug freq7
+		if (dbs_tuners_ins.down_threshold_hotplug_freq7 != 0) {
+		    if ((dbs_tuners_ins.down_threshold_hotplug_freq7 + step < pol_min
+			|| dbs_tuners_ins.down_threshold_hotplug_freq7 + step > pol_max)
+			&& !temp_down_threshold_hotplug_freq7_flag) {
+			temp_down_threshold_hotplug_freq7 = dbs_tuners_ins.down_threshold_hotplug_freq7 + step;
+			temp_down_threshold_hotplug_freq7_flag = true;
+		    } else if (temp_down_threshold_hotplug_freq7_flag) {
+			if (temp_down_threshold_hotplug_freq7 + step < pol_min
+			    || temp_down_threshold_hotplug_freq7 + step > pol_max) {
+			    temp_down_threshold_hotplug_freq7 = temp_down_threshold_hotplug_freq7 + step;
+			} else {
+			    dbs_tuners_ins.down_threshold_hotplug_freq7 = temp_down_threshold_hotplug_freq7 + step;
+			    temp_down_threshold_hotplug_freq7 = 0;
+			    temp_down_threshold_hotplug_freq7_flag = false;
+			}
+		    } else {
+			dbs_tuners_ins.down_threshold_hotplug_freq7 += step;
+		    }
+		}
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+	}
+#ifdef ENABLE_HOTPLUGGING
+		/*
+		 * ZZ: check if maximal/min freq is lower/higher than any hotplug freq thresholds,
+		 * if so mark the affected freq threshold as out of range via a flag and fall back
+		 * to the corresponding load threshold - this keeps hotplugging working properly
+		 */
+		if (unlikely(pol_max < dbs_tuners_ins.up_threshold_hotplug_freq1
+		    || dbs_tuners_ins.freq_limit < dbs_tuners_ins.up_threshold_hotplug_freq1
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    || dbs_tuners_ins.freq_limit_sleep < dbs_tuners_ins.up_threshold_hotplug_freq1
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_MUSIC_LIMITS
+		    || dbs_tuners_ins.music_max_freq < dbs_tuners_ins.up_threshold_hotplug_freq1
+		    || dbs_tuners_ins.music_min_freq > dbs_tuners_ins.up_threshold_hotplug_freq1)) {
+#else
+		    )) {
+#endif /* ENABLE_MUSIC_LIMITS */
+		    hotplug_freq_threshold_out_of_range[0][0] = 1;
+		} else {
+		    hotplug_freq_threshold_out_of_range[0][0] = 0;
+		}
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		if  (unlikely(pol_max < dbs_tuners_ins.up_threshold_hotplug_freq2
+		    || dbs_tuners_ins.freq_limit < dbs_tuners_ins.up_threshold_hotplug_freq2
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    || dbs_tuners_ins.freq_limit_sleep < dbs_tuners_ins.up_threshold_hotplug_freq2
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND) */
+#ifdef ENABLE_MUSIC_LIMITS
+		    || dbs_tuners_ins.music_max_freq < dbs_tuners_ins.up_threshold_hotplug_freq2
+		    || dbs_tuners_ins.music_min_freq > dbs_tuners_ins.up_threshold_hotplug_freq2)) {
+#else
+		    )) {
+#endif /* ENABLE_MUSIC_LIMITS */
+		    hotplug_freq_threshold_out_of_range[0][1] = 1;
+		} else {
+		    hotplug_freq_threshold_out_of_range[0][1] = 0;
+		}
+
+		if  (unlikely(pol_max < dbs_tuners_ins.up_threshold_hotplug_freq3
+		    || dbs_tuners_ins.freq_limit < dbs_tuners_ins.up_threshold_hotplug_freq3
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    || dbs_tuners_ins.freq_limit_sleep < dbs_tuners_ins.up_threshold_hotplug_freq3
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_MUSIC_LIMITS
+		    || dbs_tuners_ins.music_max_freq < dbs_tuners_ins.up_threshold_hotplug_freq3
+		    || dbs_tuners_ins.music_min_freq > dbs_tuners_ins.up_threshold_hotplug_freq3)) {
+#else
+		    )) {
+#endif /* ENABLE_MUSIC_LIMITS */
+		    hotplug_freq_threshold_out_of_range[0][2] = 1;
+		} else {
+		    hotplug_freq_threshold_out_of_range[0][2] = 0;
+		}
+#endif /* (MAX_CORES == 4 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		if  (unlikely(pol_max < dbs_tuners_ins.up_threshold_hotplug_freq4
+		    || dbs_tuners_ins.freq_limit < dbs_tuners_ins.up_threshold_hotplug_freq4
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    || dbs_tuners_ins.freq_limit_sleep < dbs_tuners_ins.up_threshold_hotplug_freq4
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_MUSIC_LIMITS
+		    || dbs_tuners_ins.music_max_freq < dbs_tuners_ins.up_threshold_hotplug_freq4
+		    || dbs_tuners_ins.music_min_freq > dbs_tuners_ins.up_threshold_hotplug_freq4)) {
+#else
+		    )) {
+#endif /* ENABLE_MUSIC_LIMITS */
+		    hotplug_freq_threshold_out_of_range[0][3] = 1;
+		} else {
+		    hotplug_freq_threshold_out_of_range[0][3] = 0;
+		}
+
+		if  (unlikely(pol_max < dbs_tuners_ins.up_threshold_hotplug_freq5
+		    || dbs_tuners_ins.freq_limit < dbs_tuners_ins.up_threshold_hotplug_freq5
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    || dbs_tuners_ins.freq_limit_sleep < dbs_tuners_ins.up_threshold_hotplug_freq5
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_MUSIC_LIMITS
+		    || dbs_tuners_ins.music_max_freq < dbs_tuners_ins.up_threshold_hotplug_freq5
+		    || dbs_tuners_ins.music_min_freq > dbs_tuners_ins.up_threshold_hotplug_freq5)) {
+#else
+		    )) {
+#endif /* ENABLE_MUSIC_LIMITS */
+		    hotplug_freq_threshold_out_of_range[0][4] = 1;
+		} else {
+		    hotplug_freq_threshold_out_of_range[0][4] = 0;
+		}
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		if  (unlikely(pol_max < dbs_tuners_ins.up_threshold_hotplug_freq6
+		    || dbs_tuners_ins.freq_limit < dbs_tuners_ins.up_threshold_hotplug_freq6
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    || dbs_tuners_ins.freq_limit_sleep < dbs_tuners_ins.up_threshold_hotplug_freq6
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_MUSIC_LIMITS
+		    || dbs_tuners_ins.music_max_freq < dbs_tuners_ins.up_threshold_hotplug_freq6
+		    || dbs_tuners_ins.music_min_freq > dbs_tuners_ins.up_threshold_hotplug_freq6)) {
+#else
+		    )) {
+#endif /* ENABLE_MUSIC_LIMITS */
+		    hotplug_freq_threshold_out_of_range[0][5] = 1;
+		} else {
+		    hotplug_freq_threshold_out_of_range[0][5] = 0;
+		}
+
+		if  (unlikely(pol_max < dbs_tuners_ins.up_threshold_hotplug_freq7
+		    || dbs_tuners_ins.freq_limit < dbs_tuners_ins.up_threshold_hotplug_freq7
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    || dbs_tuners_ins.freq_limit_sleep < dbs_tuners_ins.up_threshold_hotplug_freq7
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_MUSIC_LIMITS
+		    || dbs_tuners_ins.music_max_freq < dbs_tuners_ins.up_threshold_hotplug_freq7
+		    || dbs_tuners_ins.music_min_freq > dbs_tuners_ins.up_threshold_hotplug_freq7)) {
+#else
+		    )) {
+#endif /* ENABLE_MUSIC_LIMITS */
+		    hotplug_freq_threshold_out_of_range[0][6] = 1;
+		} else {
+		    hotplug_freq_threshold_out_of_range[0][6] = 0;
+		}
+#endif /* (MAX_CORES == 8) */
+		if  (unlikely(pol_max < dbs_tuners_ins.down_threshold_hotplug_freq1
+		    || dbs_tuners_ins.freq_limit < dbs_tuners_ins.down_threshold_hotplug_freq1
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    || dbs_tuners_ins.freq_limit_sleep < dbs_tuners_ins.down_threshold_hotplug_freq1
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_MUSIC_LIMITS
+		    || dbs_tuners_ins.music_max_freq < dbs_tuners_ins.down_threshold_hotplug_freq1
+		    || dbs_tuners_ins.music_min_freq > dbs_tuners_ins.down_threshold_hotplug_freq1)) {
+#else
+		    )) {
+#endif /* ENABLE_MUSIC_LIMITS */
+		    hotplug_freq_threshold_out_of_range[1][0] = 1;
+		} else {
+		    hotplug_freq_threshold_out_of_range[1][0] = 0;
+		}
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		if  (unlikely(pol_max < dbs_tuners_ins.down_threshold_hotplug_freq2
+		    || dbs_tuners_ins.freq_limit < dbs_tuners_ins.down_threshold_hotplug_freq2
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    || dbs_tuners_ins.freq_limit_sleep < dbs_tuners_ins.down_threshold_hotplug_freq2
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_MUSIC_LIMITS
+		    || dbs_tuners_ins.music_max_freq < dbs_tuners_ins.down_threshold_hotplug_freq2
+		    || dbs_tuners_ins.music_min_freq > dbs_tuners_ins.down_threshold_hotplug_freq2 )) {
+#else
+		    )) {
+#endif /* ENABLE_MUSIC_LIMITS */
+		    hotplug_freq_threshold_out_of_range[1][1] = 1;
+		} else {
+		    hotplug_freq_threshold_out_of_range[1][1] = 0;
+		}
+
+		if  (unlikely(pol_max < dbs_tuners_ins.down_threshold_hotplug_freq3
+		    || dbs_tuners_ins.freq_limit < dbs_tuners_ins.down_threshold_hotplug_freq3
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    || dbs_tuners_ins.freq_limit_sleep < dbs_tuners_ins.down_threshold_hotplug_freq3
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_MUSIC_LIMITS
+		    || dbs_tuners_ins.music_max_freq < dbs_tuners_ins.down_threshold_hotplug_freq3
+		    || dbs_tuners_ins.music_min_freq > dbs_tuners_ins.down_threshold_hotplug_freq3)) {
+#else
+		    )) {
+#endif /* ENABLE_MUSIC_LIMITS */
+		    hotplug_freq_threshold_out_of_range[1][2] = 1;
+		} else {
+		    hotplug_freq_threshold_out_of_range[1][2] = 0;
+		}
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		if  (unlikely(pol_max < dbs_tuners_ins.down_threshold_hotplug_freq4
+		    || dbs_tuners_ins.freq_limit < dbs_tuners_ins.down_threshold_hotplug_freq4
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    || dbs_tuners_ins.freq_limit_sleep < dbs_tuners_ins.down_threshold_hotplug_freq4
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_MUSIC_LIMITS
+		    || dbs_tuners_ins.music_max_freq < dbs_tuners_ins.down_threshold_hotplug_freq4
+		    || dbs_tuners_ins.music_min_freq > dbs_tuners_ins.down_threshold_hotplug_freq4)) {
+#else
+		    )) {
+#endif /* ENABLE_MUSIC_LIMITS */
+		    hotplug_freq_threshold_out_of_range[1][3] = 1;
+		} else {
+		    hotplug_freq_threshold_out_of_range[1][3] = 0;
+		}
+
+		if  (unlikely(pol_max < dbs_tuners_ins.down_threshold_hotplug_freq5
+		    || dbs_tuners_ins.freq_limit < dbs_tuners_ins.down_threshold_hotplug_freq5
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    || dbs_tuners_ins.freq_limit_sleep < dbs_tuners_ins.down_threshold_hotplug_freq5
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_MUSIC_LIMITS
+		    || dbs_tuners_ins.music_max_freq < dbs_tuners_ins.down_threshold_hotplug_freq5
+		    || dbs_tuners_ins.music_min_freq > dbs_tuners_ins.down_threshold_hotplug_freq5)) {
+#else
+		    )) {
+#endif /* ENABLE_MUSIC_LIMITS */
+		    hotplug_freq_threshold_out_of_range[1][4] = 1;
+		} else {
+		    hotplug_freq_threshold_out_of_range[1][4] = 0;
+		}
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		if  (unlikely(pol_max < dbs_tuners_ins.down_threshold_hotplug_freq6
+		    || dbs_tuners_ins.freq_limit < dbs_tuners_ins.down_threshold_hotplug_freq6
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    || dbs_tuners_ins.freq_limit_sleep < dbs_tuners_ins.down_threshold_hotplug_freq6
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_MUSIC_LIMITS
+		    || dbs_tuners_ins.music_max_freq < dbs_tuners_ins.down_threshold_hotplug_freq6
+		    || dbs_tuners_ins.music_min_freq > dbs_tuners_ins.down_threshold_hotplug_freq6)) {
+#else
+		    )) {
+#endif /* ENABLE_MUSIC_LIMITS */
+		    hotplug_freq_threshold_out_of_range[1][5] = 1;
+		} else {
+		    hotplug_freq_threshold_out_of_range[1][5] = 0;
+		}
+
+		if  (unlikely(pol_max < dbs_tuners_ins.down_threshold_hotplug_freq7
+		    || dbs_tuners_ins.freq_limit < dbs_tuners_ins.down_threshold_hotplug_freq7
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    || dbs_tuners_ins.freq_limit_sleep < dbs_tuners_ins.down_threshold_hotplug_freq7
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_MUSIC_LIMITS
+		    || dbs_tuners_ins.music_max_freq < dbs_tuners_ins.down_threshold_hotplug_freq7
+		    || dbs_tuners_ins.music_min_freq > dbs_tuners_ins.down_threshold_hotplug_freq7)) {
+#else
+		    )) {
+#endif /* ENABLE_MUSIC_LIMITS */
+		    hotplug_freq_threshold_out_of_range[1][6] = 1;
+		} else {
+		    hotplug_freq_threshold_out_of_range[1][6] = 0;
+		}
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+}
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+
+// ZZ: compatibility with kernel version lower than 3.4
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+static inline u64 get_cpu_idle_time_jiffy(unsigned int cpu, u64 *wall)
+{
+	u64 idle_time;
+	u64 cur_wall_time;
+	u64 busy_time;
+
+	cur_wall_time = jiffies64_to_cputime64(get_jiffies_64());
+	busy_time = cputime64_add(kstat_cpu(cpu).cpustat.user,
+			kstat_cpu(cpu).cpustat.system);
+
+	busy_time = cputime64_add(busy_time, kstat_cpu(cpu).cpustat.irq);
+	busy_time = cputime64_add(busy_time, kstat_cpu(cpu).cpustat.softirq);
+	busy_time = cputime64_add(busy_time, kstat_cpu(cpu).cpustat.steal);
+	busy_time = cputime64_add(busy_time, kstat_cpu(cpu).cpustat.nice);
+
+	idle_time = cputime64_sub(cur_wall_time, busy_time);
+	if (wall)
+	    *wall = (u64)jiffies_to_usecs(cur_wall_time);
+
+	return (u64)jiffies_to_usecs(idle_time);
+}
+#endif /* LINUX_VERSION_CODE... */
+
+// ZZ: this function is placed here only from kernel version 3.4 to 3.8
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0) && LINUX_VERSION_CODE < KERNEL_VERSION(3,8,0)
+static inline u64 get_cpu_idle_time_jiffy(unsigned int cpu, u64 *wall)
+{
+	u64 idle_time;
+	u64 cur_wall_time;
+	u64 busy_time;
+	cur_wall_time = jiffies64_to_cputime64(get_jiffies_64());
+	busy_time  = kcpustat_cpu(cpu).cpustat[CPUTIME_USER];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SYSTEM];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_IRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SOFTIRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_STEAL];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_NICE];
+
+	idle_time = cur_wall_time - busy_time;
+	if (wall)
+	*wall = jiffies_to_usecs(cur_wall_time);
+	return jiffies_to_usecs(idle_time);
+}
+#endif /* LINUX_VERSION_CODE... */
+
+/*
+ * ZZ: function has been moved out of governor since kernel version 3.8 and finally moved to cpufreq.c in kernel version 3.11
+ *     overruling macro CPU_IDLE_TIME_IN_CPUFREQ included for sources with backported cpufreq implementation
+ */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,8,0) && !defined(CPU_IDLE_TIME_IN_CPUFREQ)
+static inline u64 get_cpu_idle_time(unsigned int cpu, u64 *wall)
+{
+	u64 idle_time = get_cpu_idle_time_us(cpu, NULL);
+
+	if (idle_time == -1ULL)
+		return get_cpu_idle_time_jiffy(cpu, wall);
+	else
+		idle_time += get_cpu_iowait_time_us(cpu, wall);
+
+	return idle_time;
+}
+#endif /* LINUX_VERSION_CODE... */
+
+// keep track of frequency transitions
+static int dbs_cpufreq_notifier(struct notifier_block *nb, unsigned long val, void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpu_dbs_info_s *this_dbs_info = &per_cpu(cs_cpu_dbs_info,
+							freq->cpu);
+	struct cpufreq_policy *policy;
+
+	if (!this_dbs_info->enable)
+	    return 0;
+
+	policy = this_dbs_info->cur_policy;
+
+	/*
+	 * we only care if our internally tracked freq moves outside
+	 * the 'valid' ranges of frequency available to us otherwise
+	 * we do not change it
+	 */
+	if (unlikely(this_dbs_info->requested_freq > policy->max
+	    || this_dbs_info->requested_freq < policy->min))
+		this_dbs_info->requested_freq = freq->new;
+	return 0;
+}
+
+static struct notifier_block dbs_cpufreq_notifier_block = {
+	.notifier_call = dbs_cpufreq_notifier
+};
+
+/************************** sysfs interface **************************/
+static ssize_t show_sampling_rate_min(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", min_sampling_rate);
+}
+
+define_one_global_ro(sampling_rate_min);
+
+// cpufreq_zzmoove governor tunables
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+#ifdef ENABLE_PROFILES_SUPPORT
+show_one(profile_number, profile_number);						// ZZ: profile number tuneable
+show_one(profile_sticky_mode, profile_sticky_mode);					// ff: sticky profile mode
+#endif /* ENABLE_PROFILES_SUPPORT */
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+show_one(auto_adjust_freq_thresholds, auto_adjust_freq_thresholds);			// ZZ: auto adjust freq thresholds tuneable
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+show_one(sampling_rate, sampling_rate);							// ZZ: normal sampling rate tuneable
+show_one(sampling_rate_current, sampling_rate_current);					// ZZ: tuneable for showing the actual sampling rate
+show_one(sampling_rate_idle_threshold, sampling_rate_idle_threshold);			// ZZ: sampling rate idle threshold tuneable
+show_one(sampling_rate_idle, sampling_rate_idle);					// ZZ: tuneable for sampling rate at idle
+show_one(sampling_rate_idle_delay, sampling_rate_idle_delay);				// ZZ: DSR switching delay tuneable
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+show_one(disable_sleep_mode, disable_sleep_mode);					// ZZ: disable sleep mode tuneable
+show_one(sampling_rate_sleep_multiplier, sampling_rate_sleep_multiplier);		// ZZ: sampling rate multiplier tuneable for early suspend
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+show_one(sampling_down_factor, sampling_down_factor);					// ZZ: sampling down factor tuneable
+show_one(sampling_down_max_momentum, sampling_down_max_mom);				// ZZ: sampling down momentum tuneable
+show_one(sampling_down_momentum_sensitivity, sampling_down_mom_sens);			// ZZ: sampling down momentum sensitivity tuneable
+show_one(up_threshold, up_threshold);
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+show_one(up_threshold_sleep, up_threshold_sleep);					// ZZ: up threshold sleep tuneable for early suspend
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+show_one(up_threshold_hotplug1, up_threshold_hotplug1);					// ZZ: up threshold hotplug tuneable for core1
+show_one(up_threshold_hotplug_freq1, up_threshold_hotplug_freq1);			// Yank: up threshold hotplug freq tuneable for core1
+show_one(block_up_multiplier_hotplug1, block_up_multiplier_hotplug1);			// ff: block up multiplier hotplug1 for core1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+show_one(up_threshold_hotplug2, up_threshold_hotplug2);					// ZZ: up threshold hotplug tuneable for core2
+show_one(up_threshold_hotplug_freq2, up_threshold_hotplug_freq2);			// Yank: up threshold hotplug freq tuneable for core2
+show_one(block_up_multiplier_hotplug2, block_up_multiplier_hotplug2);			// ff: block up multiplier hotplug2 for core2
+show_one(up_threshold_hotplug3, up_threshold_hotplug3);					// ZZ: up threshold hotplug tuneable for core3
+show_one(up_threshold_hotplug_freq3, up_threshold_hotplug_freq3);			// Yank: up threshold hotplug freq tuneable for core3
+show_one(block_up_multiplier_hotplug3, block_up_multiplier_hotplug3);			// ff: block up multiplier hotplug3 for core3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+show_one(up_threshold_hotplug4, up_threshold_hotplug4);					// ZZ: up threshold hotplug tuneable for core4
+show_one(up_threshold_hotplug_freq4, up_threshold_hotplug_freq4);			// Yank: up threshold hotplug freq tuneable for core4
+show_one(block_up_multiplier_hotplug4, block_up_multiplier_hotplug4);			// ff: block up multiplier hotplug3 for core4
+show_one(up_threshold_hotplug5, up_threshold_hotplug5);					// ZZ: up threshold hotplug tuneable for core5
+show_one(up_threshold_hotplug_freq5, up_threshold_hotplug_freq5);			// Yank: up threshold hotplug freq tuneable for core5
+show_one(block_up_multiplier_hotplug5, block_up_multiplier_hotplug5);			// ff: block up multiplier hotplug3 for core5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+show_one(up_threshold_hotplug6, up_threshold_hotplug6);					// ZZ: up threshold hotplug tuneable for core6
+show_one(up_threshold_hotplug_freq6, up_threshold_hotplug_freq6);			// Yank: up threshold hotplug freq tuneable for core6
+show_one(block_up_multiplier_hotplug6, block_up_multiplier_hotplug6);			// ff: block up multiplier hotplug3 for core6
+show_one(up_threshold_hotplug7, up_threshold_hotplug7);					// ZZ: up threshold hotplug tuneable for core7
+show_one(up_threshold_hotplug_freq7, up_threshold_hotplug_freq7);			// Yank: up threshold hotplug freq tuneable for core7
+show_one(block_up_multiplier_hotplug7, block_up_multiplier_hotplug7);			// ff: block up multiplier hotplug3 for core7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+show_one(down_threshold, down_threshold);
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+show_one(down_threshold_sleep, down_threshold_sleep);					// ZZ: down threshold sleep tuneable for early suspend
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+show_one(down_threshold_hotplug1, down_threshold_hotplug1);				// ZZ: down threshold hotplug tuneable for core1
+show_one(down_threshold_hotplug_freq1, down_threshold_hotplug_freq1);			// Yank: down threshold hotplug freq tuneable for core1
+show_one(block_down_multiplier_hotplug1, block_down_multiplier_hotplug1);		// ff: block down multiplier hotplug1 for core1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+show_one(down_threshold_hotplug2, down_threshold_hotplug2);				// ZZ: down threshold hotplug tuneable for core2
+show_one(down_threshold_hotplug_freq2, down_threshold_hotplug_freq2);			// Yank: down threshold hotplug freq tuneable for core2
+show_one(block_down_multiplier_hotplug2, block_down_multiplier_hotplug2);		// ff: block down multiplier hotplug2 for core 2
+show_one(down_threshold_hotplug3, down_threshold_hotplug3);				// ZZ: down threshold hotplug tuneable for core3
+show_one(down_threshold_hotplug_freq3, down_threshold_hotplug_freq3);			// Yank: down threshold hotplug freq tuneable for core3
+show_one(block_down_multiplier_hotplug3, block_down_multiplier_hotplug3);		// ff: block down multiplier hotplug3 for core3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+show_one(down_threshold_hotplug4, down_threshold_hotplug4);				// ZZ: down threshold hotplug tuneable for core4
+show_one(down_threshold_hotplug_freq4, down_threshold_hotplug_freq4);			// Yank: down threshold hotplug freq tuneable for core4
+show_one(block_down_multiplier_hotplug4, block_down_multiplier_hotplug4);		// ff: block down multiplier hotplug1 for core4
+show_one(down_threshold_hotplug5, down_threshold_hotplug5);				// ZZ: down threshold hotplug tuneable for core5
+show_one(down_threshold_hotplug_freq5, down_threshold_hotplug_freq5);			// Yank: down threshold hotplug freq tuneable for core5
+show_one(block_down_multiplier_hotplug5, block_down_multiplier_hotplug5);		// ff: block down multiplier hotplug1 for core5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+show_one(down_threshold_hotplug6, down_threshold_hotplug6);				// ZZ: down threshold hotplug tuneable for core6
+show_one(down_threshold_hotplug_freq6, down_threshold_hotplug_freq6);			// Yank: down threshold hotplug freq tuneable for core6
+show_one(block_down_multiplier_hotplug6, block_down_multiplier_hotplug6);		// ff: block down multiplier hotplug1 for core6
+show_one(down_threshold_hotplug7, down_threshold_hotplug7);				// ZZ: down threshold hotplug  tuneable for core7
+show_one(down_threshold_hotplug_freq7, down_threshold_hotplug_freq7);			// Yank: down threshold hotplug freq tuneable for core7
+show_one(block_down_multiplier_hotplug7, block_down_multiplier_hotplug7);		// ff: block down multiplier hotplug1 for core7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+show_one(ignore_nice_load, ignore_nice);						// ZZ: ignore nice load tuneable
+show_one(smooth_up, smooth_up);								// ZZ: smooth up tuneable
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+show_one(smooth_up_sleep, smooth_up_sleep);						// ZZ: smooth up sleep tuneable for early suspend
+#ifdef ENABLE_HOTPLUGGING
+show_one(hotplug_sleep, hotplug_sleep);							// ZZ: hotplug sleep tuneable for early suspend
+#endif /* ENABLE_HOTPLUGGING */
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+show_one(freq_limit, freq_limit);							// ZZ: freq limit tuneable
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+show_one(freq_limit_sleep, freq_limit_sleep);						// ZZ: freq limit sleep tuneable for early suspend
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+show_one(fast_scaling_up, fast_scaling_up);						// Yank: fast scaling tuneable for upscaling
+show_one(fast_scaling_down, fast_scaling_down);						// Yank: fast scaling tuneable for downscaling
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+show_one(fast_scaling_sleep_up, fast_scaling_sleep_up);					// Yank: fast scaling sleep tuneable for early suspend for upscaling
+show_one(fast_scaling_sleep_down, fast_scaling_sleep_down);				// Yank: fast scaling sleep tuneable for early suspend for downscaling
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+show_one(afs_threshold1, afs_threshold1);						// ZZ: auto fast scaling step one threshold
+show_one(afs_threshold2, afs_threshold2);						// ZZ: auto fast scaling step two threshold
+show_one(afs_threshold3, afs_threshold3);						// ZZ: auto fast scaling step three threshold
+show_one(afs_threshold4, afs_threshold4);						// ZZ: auto fast scaling step four threshold
+show_one(grad_up_threshold, grad_up_threshold);						// ZZ: early demand tuneable grad up threshold
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+show_one(grad_up_threshold_sleep, grad_up_threshold_sleep);				// ZZ: early demand sleep tuneable grad up threshold
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+show_one(early_demand, early_demand);							// ZZ: early demand tuneable master switch
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+show_one(early_demand_sleep, early_demand_sleep);					// ZZ: early demand sleep tuneable master switch
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+show_one(disable_hotplug, disable_hotplug);						// ZZ: hotplug switch tuneable
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+show_one(disable_hotplug_sleep, disable_hotplug_sleep);					// ZZ: hotplug switch tuneable for sleep
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+show_one(hotplug_block_up_cycles, hotplug_block_up_cycles);				// ZZ: hotplug up block cycles tuneable
+show_one(hotplug_block_down_cycles, hotplug_block_down_cycles);				// ZZ: hotplug down block cycles tuneable
+show_one(hotplug_stagger_up, hotplug_stagger_up);					// ff: hotplug stagger up tuneable
+show_one(hotplug_stagger_down, hotplug_stagger_down);					// ff: hotplug stagger down tuneable
+show_one(hotplug_idle_threshold, hotplug_idle_threshold);				// ZZ: hotplug idle threshold tuneable
+show_one(hotplug_idle_freq, hotplug_idle_freq);						// ZZ: hotplug idle freq tuneable
+show_one(hotplug_engage_freq, hotplug_engage_freq);					// ZZ: hotplug engage freq tuneable (ffolkes)
+show_one(hotplug_max_limit, hotplug_max_limit);						// ZZ: hotplug max limit tunable (ffolkes)
+show_one(hotplug_min_limit, hotplug_min_limit);						// ff: the number of cores we require to be online
+show_one(hotplug_lock, hotplug_lock);							// ff: the number of cores we require to be online
+#endif /* ENABLE_HOTPLUGGING */
+show_one(scaling_block_threshold, scaling_block_threshold);				// ZZ: scaling block threshold tuneable
+show_one(scaling_block_cycles, scaling_block_cycles);					// ZZ: scaling block cycles tuneable
+show_one(scaling_up_block_cycles, scaling_up_block_cycles);				// ff: scaling-up block cycles tuneable
+show_one(scaling_up_block_freq, scaling_up_block_freq);					// ff: scaling-up block freq threshold tuneable
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+show_one(scaling_block_temp, scaling_block_temp);					// ZZ: scaling block temp tuneable
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+show_one(scaling_trip_temp, scaling_trip_temp);						// ff: snapdragon thermal tripping temperature
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+show_one(scaling_block_freq, scaling_block_freq);					// ZZ: scaling block freq tuneable
+show_one(scaling_block_force_down, scaling_block_force_down);				// ZZ: scaling block force down tuneable
+show_one(scaling_fastdown_freq, scaling_fastdown_freq);					// ZZ: scaling fastdown freq tuneable (ffolkes)
+show_one(scaling_fastdown_up_threshold, scaling_fastdown_up_threshold);			// ZZ: scaling fastdown up threshold tuneable (ffolkes)
+show_one(scaling_fastdown_down_threshold, scaling_fastdown_down_threshold);		// ZZ: scaling fastdown down threshold tuneable (ffolkes-ZaneZam)
+show_one(scaling_responsiveness_freq, scaling_responsiveness_freq);			// ZZ: scaling responsiveness freq tuneable (ffolkes)
+show_one(scaling_responsiveness_up_threshold, scaling_responsiveness_up_threshold);	// ZZ: scaling responsiveness up threshold tuneable (ffolkes)
+show_one(scaling_proportional, scaling_proportional);					// ZZ: scaling proportional tuneable
+#ifdef ENABLE_INPUTBOOSTER
+show_one(inputboost_cycles, inputboost_cycles);						// ff: inputbooster duration
+show_one(inputboost_up_threshold, inputboost_up_threshold);				// ff: inputbooster up threshold
+show_one(inputboost_punch_cycles, inputboost_punch_cycles);				// ff: inputbooster punch cycles
+show_one(inputboost_punch_freq, inputboost_punch_freq);					// ff: inputbooster punch freq
+show_one(inputboost_punch_on_fingerdown, inputboost_punch_on_fingerdown);		// ff: inputbooster punch on finger down
+show_one(inputboost_punch_on_fingermove, inputboost_punch_on_fingermove);		// ff: inputbooster punch on finger move
+show_one(inputboost_punch_on_epenmove, inputboost_punch_on_epenmove);			// ff: inputbooster punch on epen move
+show_one(inputboost_typingbooster_up_threshold, inputboost_typingbooster_up_threshold); // ff: typingbooster up threshold
+show_one(inputboost_typingbooster_cores, inputboost_typingbooster_cores);		// ff: typingbooster boost cores
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_MUSIC_LIMITS
+show_one(music_max_freq, music_max_freq);						// ff: music max frequency
+show_one(music_min_freq, music_min_freq);						// ff: music min frequency
+#ifdef ENABLE_HOTPLUGGING
+show_one(music_min_cores, music_min_cores);						// ZZ: music min online cores
+#endif /* ENABLE_HOTPLUGGING */
+show_one(music_state, music_state);							// ff: music state
+#endif /* ENABLE_MUSIC_LIMITS */
+
+#ifdef ENABLE_PROFILES_SUPPORT
+// ZZ: tuneable for showing the currently active governor settings profile
+static ssize_t show_profile(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+    return sprintf(buf, "%s\n", dbs_tuners_ins.profile);
+}
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+// ZZ: tuneable -> possible values: 0 (disable) to MAX_SAMPLING_DOWN_FACTOR, if not set default is 0
+static ssize_t store_sampling_down_max_momentum(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input, j;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR -
+	    dbs_tuners_ins.sampling_down_factor || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.sampling_down_max_mom != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.sampling_down_max_mom = zz_sampling_down_max_mom = input;
+
+	orig_sampling_down_max_mom = dbs_tuners_ins.sampling_down_max_mom;
+
+	// ZZ: reset sampling down factor to default if momentum was disabled
+	if (dbs_tuners_ins.sampling_down_max_mom == 0)
+	    zz_sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR;
+
+	// ZZ: reset momentum_adder and reset down sampling multiplier in case momentum was disabled
+	for_each_online_cpu(j) {
+	    struct cpu_dbs_info_s *dbs_info;
+	    dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+	    dbs_info->momentum_adder = 0;
+	    if (dbs_tuners_ins.sampling_down_max_mom == 0)
+		dbs_info->rate_mult = 1;
+	}
+	return count;
+}
+
+// ZZ: tuneable -> possible values: 1 to MAX_SAMPLING_DOWN_MOMENTUM_SENSITIVITY, if not set default is 50
+static ssize_t store_sampling_down_momentum_sensitivity(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input, j;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_MOMENTUM_SENSITIVITY
+	    || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	// ff: do this so synapse can set 0.
+	if (!input)
+	    input = 1;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.sampling_down_mom_sens != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.sampling_down_mom_sens = input;
+
+	// ZZ: reset momentum_adder
+	for_each_online_cpu(j) {
+	    struct cpu_dbs_info_s *dbs_info;
+	    dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+	    dbs_info->momentum_adder = 0;
+	}
+	return count;
+}
+/*
+ * ZZ: tunable for sampling down factor (reactivated function) added reset loop for momentum functionality
+ * -> possible values: 1 (disabled) to MAX_SAMPLING_DOWN_FACTOR, if not set default is 1
+ */
+static ssize_t store_sampling_down_factor(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input, j;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR
+	    || input < 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.sampling_down_factor != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.sampling_down_factor = zz_sampling_down_factor = input;
+
+	// ZZ: reset down sampling multiplier in case it was active
+	for_each_online_cpu(j) {
+	    struct cpu_dbs_info_s *dbs_info;
+	    dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+	    dbs_info->rate_mult = 1;
+	}
+	return count;
+}
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret = 0;
+	int mpd = strcmp(current->comm, "mpdecision");
+
+	if (mpd == 0)
+		return ret;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.sampling_rate != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.sampling_rate = dbs_tuners_ins.sampling_rate_current
+	= max(input, min_sampling_rate); // ZZ: set it to new value
+
+	return count;
+}
+
+/*
+ * ZZ: tuneable -> possible values: 0 disable whole functionality and same as 'sampling_rate' any value
+ * above min_sampling_rate, if not set default is 180000
+ */
+static ssize_t store_sampling_rate_idle(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.sampling_rate_idle != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	if (input == 0)
+	    dbs_tuners_ins.sampling_rate_current = dbs_tuners_ins.sampling_rate_idle
+	    = dbs_tuners_ins.sampling_rate;	// ZZ: set current and idle rate to normal = disable feature
+	else
+	    dbs_tuners_ins.sampling_rate_idle = max(input, min_sampling_rate);	// ZZ: set idle rate to new value
+
+	return count;
+}
+
+// ZZ: tuneable -> possible values: 0 disable threshold, any value under 100, if not set default is 0
+static ssize_t store_sampling_rate_idle_threshold(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.sampling_rate_idle_threshold != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.sampling_rate_idle_threshold = input;
+
+	return count;
+}
+
+// ZZ: tuneable -> possible values: 0 to disable, any value above 0 to enable, if not set default is 0
+static ssize_t store_sampling_rate_idle_delay(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input == 0)
+	    sampling_rate_step_up_delay = 0;
+	    sampling_rate_step_down_delay = 0;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.sampling_rate_idle_delay != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.sampling_rate_idle_delay = input;
+
+	return count;
+}
+
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+// ZZ: added tuneable disable_sleep_mode -> possible values: 0 disabled, anything else enabled
+static ssize_t store_disable_sleep_mode(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0 || suspend_flag) // ZZ: dont set this tuneable during suspend
+	return -EINVAL;
+
+	dbs_tuners_ins.disable_sleep_mode = !!input;
+	return count;
+}
+
+// ZZ: tuneable -> possible values: 1 to 8, if not set default is 2
+static ssize_t store_sampling_rate_sleep_multiplier(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_RATE_SLEEP_MULTIPLIER || input < 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.sampling_rate_sleep_multiplier != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.sampling_rate_sleep_multiplier = input;
+
+	return count;
+}
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100
+	    || input <= dbs_tuners_ins.down_threshold
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.up_threshold != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.up_threshold = input;
+
+	return count;
+}
+
+// ZZ: tuneable -> possible values: range from above down_threshold_sleep value up to 100, if not set default is 90
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+static ssize_t store_up_threshold_sleep(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100
+	    || input <= dbs_tuners_ins.down_threshold_sleep
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.up_threshold_sleep != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+	dbs_tuners_ins.up_threshold_sleep = input;
+
+	return count;
+}
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+
+#ifdef ENABLE_HOTPLUGGING
+#ifdef ENABLE_PROFILES_SUPPORT
+// ff: tuneable -> possible values: range from 0 up to 25, if not set default is 1 (disabled)
+#define store_block_up_multiplier_hotplug(name)							\
+static ssize_t store_block_up_multiplier_hotplug##name						\
+(struct kobject *a, struct attribute *b, const char *buf, size_t count)				\
+{												\
+	unsigned int input;									\
+	int ret;										\
+	ret = sscanf(buf, "%u", &input);							\
+												\
+	if (ret != 1 || input < 0 || input > 25 || set_profile_active == true)			\
+		return -EINVAL;									\
+												\
+	if (!dbs_tuners_ins.profile_sticky_mode							\
+	    && dbs_tuners_ins.profile_number != 0						\
+	    && dbs_tuners_ins.block_up_multiplier_hotplug##name != input) {			\
+	    dbs_tuners_ins.profile_number = 0;							\
+	    strncpy(dbs_tuners_ins.profile, custom_profile,					\
+	    sizeof(dbs_tuners_ins.profile));							\
+	}											\
+	dbs_tuners_ins.block_up_multiplier_hotplug##name = input;				\
+												\
+	return count;										\
+}												\
+
+#define store_block_down_multiplier_hotplug(name)						\
+static ssize_t store_block_down_multiplier_hotplug##name					\
+(struct kobject *a, struct attribute *b, const char *buf, size_t count)				\
+{												\
+	unsigned int input;									\
+	int ret;										\
+	ret = sscanf(buf, "%u", &input);							\
+												\
+	if (ret != 1 || input < 0 || input > 25 || set_profile_active == true)			\
+		return -EINVAL;									\
+												\
+	if (!dbs_tuners_ins.profile_sticky_mode							\
+	    && dbs_tuners_ins.profile_number != 0						\
+	    && dbs_tuners_ins.block_down_multiplier_hotplug##name != input) {			\
+	    dbs_tuners_ins.profile_number = 0;							\
+	    strncpy(dbs_tuners_ins.profile, custom_profile,					\
+	    sizeof(dbs_tuners_ins.profile));							\
+	}											\
+	dbs_tuners_ins.block_down_multiplier_hotplug##name = input;				\
+												\
+return count;											\
+}												\
+
+// Yank: also use definitions for other hotplug tunables
+#define store_up_threshold_hotplug(name,core)							\
+static ssize_t store_up_threshold_hotplug##name							\
+(struct kobject *a, struct attribute *b, const char *buf, size_t count)				\
+{												\
+	unsigned int input;									\
+	int ret;										\
+	ret = sscanf(buf, "%u", &input);							\
+												\
+	    if (ret != 1 || input < 0 || input > 100 || set_profile_active == true)		\
+		return -EINVAL;									\
+												\
+	    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0	\
+		&& dbs_tuners_ins.up_threshold_hotplug##name != input) {			\
+		dbs_tuners_ins.profile_number = 0;						\
+		strncpy(dbs_tuners_ins.profile, custom_profile,					\
+		sizeof(dbs_tuners_ins.profile));						\
+	    }											\
+												\
+	    dbs_tuners_ins.up_threshold_hotplug##name = input;					\
+	    hotplug_thresholds[0][core] = input;						\
+												\
+	return count;										\
+}												\
+
+#define store_down_threshold_hotplug(name,core)							\
+static ssize_t store_down_threshold_hotplug##name						\
+(struct kobject *a, struct attribute *b, const char *buf, size_t count)				\
+{												\
+	unsigned int input;									\
+	int ret;										\
+	ret = sscanf(buf, "%u", &input);							\
+												\
+	    if (ret != 1 || input < 1 || input > 100 || set_profile_active == true)		\
+		return -EINVAL;									\
+												\
+	    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0	\
+		&& dbs_tuners_ins.down_threshold_hotplug##name != input) {			\
+		dbs_tuners_ins.profile_number = 0;						\
+		strncpy(dbs_tuners_ins.profile, custom_profile,					\
+		sizeof(dbs_tuners_ins.profile));						\
+	    }											\
+												\
+	    dbs_tuners_ins.down_threshold_hotplug##name = input;				\
+	    hotplug_thresholds[1][core] = input;						\
+												\
+	return count;										\
+}												\
+
+/*
+ * ZZ: tuneables -> possible values: 0 to disable core (only in up thresholds), range from appropriate
+ * down threshold value up to 100, if not set default for up threshold is 68 and for down threshold is 55
+ */
+store_up_threshold_hotplug(1,0);
+store_down_threshold_hotplug(1,0);
+store_block_up_multiplier_hotplug(1);
+store_block_down_multiplier_hotplug(1);
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+store_up_threshold_hotplug(2,1);
+store_down_threshold_hotplug(2,1);
+store_block_up_multiplier_hotplug(2);
+store_block_down_multiplier_hotplug(2);
+store_up_threshold_hotplug(3,2);
+store_down_threshold_hotplug(3,2);
+store_block_up_multiplier_hotplug(3);
+store_block_down_multiplier_hotplug(3);
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+store_up_threshold_hotplug(4,3);
+store_down_threshold_hotplug(4,3);
+store_block_up_multiplier_hotplug(4);
+store_block_down_multiplier_hotplug(4);
+store_up_threshold_hotplug(5,4);
+store_down_threshold_hotplug(5,4);
+store_block_up_multiplier_hotplug(5);
+store_block_down_multiplier_hotplug(5);
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+store_up_threshold_hotplug(6,5);
+store_down_threshold_hotplug(6,5);
+store_block_up_multiplier_hotplug(6);
+store_block_down_multiplier_hotplug(6);
+store_up_threshold_hotplug(7,6);
+store_down_threshold_hotplug(7,6);
+store_block_up_multiplier_hotplug(7);
+store_block_down_multiplier_hotplug(7);
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+#ifndef ENABLE_PROFILES_SUPPORT
+// ff: tuneable -> possible values: range from 0 up to 25, if not set default is 1 (disabled)
+#define store_block_up_multiplier_hotplug(name)							\
+static ssize_t store_block_up_multiplier_hotplug##name						\
+(struct kobject *a, struct attribute *b, const char *buf, size_t count)				\
+{												\
+	unsigned int input;									\
+	int ret;										\
+	ret = sscanf(buf, "%u", &input);							\
+												\
+	if (ret != 1 || input < 0 || input > 25)						\
+		return -EINVAL;									\
+												\
+	dbs_tuners_ins.block_up_multiplier_hotplug##name = input;				\
+												\
+	return count;										\
+}												\
+
+#define store_block_down_multiplier_hotplug(name)						\
+static ssize_t store_block_down_multiplier_hotplug##name					\
+(struct kobject *a, struct attribute *b, const char *buf, size_t count)				\
+{												\
+	unsigned int input;									\
+	int ret;										\
+	ret = sscanf(buf, "%u", &input);							\
+												\
+	if (ret != 1 || input < 0 || input > 25)						\
+		return -EINVAL;									\
+												\
+	dbs_tuners_ins.block_down_multiplier_hotplug##name = input;				\
+												\
+return count;											\
+}												\
+
+// Yank: also use definitions for other hotplug tunables
+#define store_up_threshold_hotplug(name,core)							\
+static ssize_t store_up_threshold_hotplug##name							\
+(struct kobject *a, struct attribute *b, const char *buf, size_t count)				\
+{												\
+	unsigned int input;									\
+	int ret;										\
+	ret = sscanf(buf, "%u", &input);							\
+												\
+	    if (ret != 1 || input < 0 || input > 100)						\
+		return -EINVAL;									\
+												\
+	    dbs_tuners_ins.up_threshold_hotplug##name = input;					\
+	    hotplug_thresholds[0][core] = input;						\
+												\
+	return count;										\
+}												\
+
+#define store_down_threshold_hotplug(name,core)							\
+static ssize_t store_down_threshold_hotplug##name						\
+(struct kobject *a, struct attribute *b, const char *buf, size_t count)				\
+{												\
+	unsigned int input;									\
+	int ret;										\
+	ret = sscanf(buf, "%u", &input);							\
+												\
+	    if (ret != 1 || input < 1 || input > 100)						\
+		return -EINVAL;									\
+												\
+	    dbs_tuners_ins.down_threshold_hotplug##name = input;				\
+	    hotplug_thresholds[1][core] = input;						\
+												\
+	return count;										\
+}												\
+
+/*
+ * ZZ: tuneables -> possible values: 0 to disable core (only in up thresholds), range from appropriate
+ * down threshold value up to 100, if not set default for up threshold is 68 and for down threshold is 55
+ */
+store_up_threshold_hotplug(1,0);
+store_down_threshold_hotplug(1,0);
+store_block_up_multiplier_hotplug(1);
+store_block_down_multiplier_hotplug(1);
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+store_up_threshold_hotplug(2,1);
+store_down_threshold_hotplug(2,1);
+store_block_up_multiplier_hotplug(2);
+store_block_down_multiplier_hotplug(2);
+store_up_threshold_hotplug(3,2);
+store_down_threshold_hotplug(3,2);
+store_block_up_multiplier_hotplug(3);
+store_block_down_multiplier_hotplug(3);
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+store_up_threshold_hotplug(4,3);
+store_down_threshold_hotplug(4,3);
+store_block_up_multiplier_hotplug(4);
+store_block_down_multiplier_hotplug(4);
+store_up_threshold_hotplug(5,4);
+store_down_threshold_hotplug(5,4);
+store_block_up_multiplier_hotplug(5);
+store_block_down_multiplier_hotplug(5);
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+store_up_threshold_hotplug(6,5);
+store_down_threshold_hotplug(6,5);
+store_block_up_multiplier_hotplug(6);
+store_block_down_multiplier_hotplug(6);
+store_up_threshold_hotplug(7,6);
+store_down_threshold_hotplug(7,6);
+store_block_up_multiplier_hotplug(7);
+store_block_down_multiplier_hotplug(7);
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_PROFILES_SUPPORT */
+#endif /* ENABLE_HOTPLUGGING */
+
+static ssize_t store_down_threshold(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	// ZZ: cannot be lower than 11 otherwise freq will not fall (conservative governor)
+	if (ret != 1 || input < 11 || input > 100
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	// ZZ: instead of failing when set too high set it to the highest it can safely go (ffolkes)
+	if (dbs_tuners_ins.up_threshold != 0 && input >= dbs_tuners_ins.up_threshold) {
+	    input = dbs_tuners_ins.up_threshold - 1;
+	}
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.down_threshold != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+	dbs_tuners_ins.down_threshold = input;
+
+	return count;
+}
+
+// ZZ: tuneable -> possible values: range from 11 to up_threshold_sleep but not up_threshold_sleep, if not set default is 44
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+static ssize_t store_down_threshold_sleep(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	// ZZ: cannot be lower than 11 otherwise freq will not fall (conservative governor)
+	if (ret != 1 || input < 11 || input > 100
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	// ZZ: instead of failing when set too high set it to the highest it can safely go (ffolkes)
+	if (dbs_tuners_ins.up_threshold != 0 && input >= dbs_tuners_ins.up_threshold) {
+	    input = dbs_tuners_ins.up_threshold - 1;
+	}
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.down_threshold_sleep != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+	dbs_tuners_ins.down_threshold_sleep = input;
+
+	return count;
+}
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+
+static ssize_t store_ignore_nice_load(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	unsigned int j;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input > 1)
+	    input = 1;
+
+	if (input == dbs_tuners_ins.ignore_nice) {		// ZZ: nothing to do
+	    return count;
+	}
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.ignore_nice != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.ignore_nice = input;
+
+	// ZZ: we need to re-evaluate prev_cpu_idle
+	for_each_online_cpu(j) {
+		 struct cpu_dbs_info_s *dbs_info;
+		 dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+		 dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,10,0) || defined(CPU_IDLE_TIME_IN_CPUFREQ) /* overrule for sources with backported cpufreq implementation */
+		 &dbs_info->prev_cpu_wall, 0);
+#else
+		 &dbs_info->prev_cpu_wall);
+#endif /* LINUX_VERSION_CODE... */
+		 if (dbs_tuners_ins.ignore_nice)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+		     dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+#else
+		     dbs_info->prev_cpu_nice = kstat_cpu(j).cpustat.nice;
+#endif /* LINUX_VERSION_CODE... */
+	}
+	return count;
+}
+
+static ssize_t store_smooth_up(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 || input < 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.smooth_up != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+	dbs_tuners_ins.smooth_up = input;
+
+	return count;
+}
+
+// ZZ: tuneable -> possible values: range from 1 to 100, if not set default is 100
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+static ssize_t store_smooth_up_sleep(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 || input < 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.smooth_up_sleep != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.smooth_up_sleep = input;
+
+	return count;
+}
+
+/*
+ * ZZ: tuneable -> possible values: 0 do not touch the hotplug values on early suspend,
+ * input value 1 to MAX_CORES -> value equals cores to run at early suspend, if not set default is 0 (= all cores enabled)
+ */
+#ifdef ENABLE_HOTPLUGGING
+static ssize_t store_hotplug_sleep(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input >= possible_cpus || (input < 0 && input != 0)
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.hotplug_sleep != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.hotplug_sleep = input;
+
+	return count;
+}
+#endif /* ENABLE_HOTPLUGGING */
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+
+/*
+ * ZZ: tuneable -> possible values: 0 disable, system table freq->min to freq->max in khz -> freqency soft-limit, if not set default is 0
+ * Yank: updated : possible values now depend on the system frequency table only
+ */
+static ssize_t store_freq_limit(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	int i = 0;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input == 0) {
+	    max_scaling_freq_soft = max_scaling_freq_hard;
+	    if (freq_table_desc)							// ZZ: if descending ordered table is used
+		limit_table_start = max_scaling_freq_soft;				// ZZ: we should use the actual scaling soft limit value as search start point
+	    else
+		limit_table_end = system_freq_table[freq_table_size].frequency;		// ZZ: set search end point to max freq when using ascending table
+
+#ifdef ENABLE_PROFILES_SUPPORT
+		// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+		if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.freq_limit != input) {
+		    dbs_tuners_ins.profile_number = 0;
+		    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+		}
+#endif /* ENABLE_PROFILES_SUPPORT */
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		freq_limit_awake = dbs_tuners_ins.freq_limit = input;
+#else
+		dbs_tuners_ins.freq_limit = input;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+		return count;
+	}
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	input = check_frequency(input);							// ZZ: check and adapt given freq if necessary
+#endif /* ENABLE_PROFILES_SUPPORT */
+	if (input > system_freq_table[max_scaling_freq_hard].frequency) {		// Yank: allow only frequencies below or equal to hard max limit
+	    return -EINVAL;
+	} else {
+		for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+		    if (unlikely(system_freq_table[i].frequency == input)) {
+			max_scaling_freq_soft = i;
+			if (freq_table_desc)						// ZZ: if descending ordered table is used
+			    limit_table_start = max_scaling_freq_soft;			// ZZ: we should use the actual scaling soft limit value as search start point
+			else
+			    limit_table_end = system_freq_table[i].frequency;		// ZZ: set search end point to max soft freq limit when using ascenting table
+#ifdef ENABLE_PROFILES_SUPPORT
+			// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+			if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.freq_limit != input) {
+			    dbs_tuners_ins.profile_number = 0;
+			    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+			}
+#endif /* ENABLE_PROFILES_SUPPORT */
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+			freq_limit_awake = dbs_tuners_ins.freq_limit = input;
+#else
+			dbs_tuners_ins.freq_limit = input;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+			return count;
+		    }
+		}
+	}
+	return -EINVAL;
+}
+
+/*
+ * ZZ: tuneable -> possible values: 0 disable, system table freq->min to freq->max in khz -> freqency soft-limit,
+ * if not set default is 0
+ * Yank: updated : possible values now depend on the system frequency table only
+ */
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+static ssize_t store_freq_limit_sleep(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	int i = 0;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input == 0) {
+#ifdef ENABLE_PROFILES_SUPPORT
+		// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+		if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.freq_limit_sleep != input) {
+		    dbs_tuners_ins.profile_number = 0;
+		    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+		}
+#endif /* ENABLE_PROFILES_SUPPORT */
+		freq_limit_asleep = dbs_tuners_ins.freq_limit_sleep = input;
+	return count;
+	}
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	input = check_frequency(input);						// ZZ: check and adapt given freq if necessary
+#endif /* ENABLE_PROFILES_SUPPORT */
+	if (input > system_freq_table[max_scaling_freq_hard].frequency) {	// Yank: allow only frequencies below or equal to hard max
+	    return -EINVAL;
+	} else {
+	    for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+		if (unlikely(system_freq_table[i].frequency == input)) {
+#ifdef ENABLE_PROFILES_SUPPORT
+		    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+		    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.freq_limit_sleep != input) {
+			dbs_tuners_ins.profile_number = 0;
+		        strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+		    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+		    freq_limit_asleep = dbs_tuners_ins.freq_limit_sleep = input;
+		return count;
+		}
+	    }
+	}
+	return -EINVAL;
+}
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+
+// Yank: tuneable -> possible values 1-4 to enable fast scaling and 5 for auto fast scaling (insane scaling)
+static ssize_t store_fast_scaling_up(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 5 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.fast_scaling_up != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.fast_scaling_up = input;
+
+	if (input > 4)				// ZZ: auto fast scaling mode
+	    return count;
+
+	scaling_mode_up = input;		// Yank: fast scaling up only
+
+	return count;
+}
+
+// Yank: tuneable -> possible values 1-4 to enable fast scaling and 5 for auto fast scaling (insane scaling)
+static ssize_t store_fast_scaling_down(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 5 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.fast_scaling_down != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.fast_scaling_down = input;
+
+	if (input > 4)				// ZZ: auto fast scaling mode
+	    return count;
+
+	scaling_mode_down = input;		// Yank: fast scaling up only
+
+	return count;
+}
+
+// Yank: tuneable -> possible values 1-4 to enable fast scaling and 5 for auto fast scaling (insane scaling) in early suspend
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+static ssize_t store_fast_scaling_sleep_up(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 5 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.fast_scaling_sleep_up != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.fast_scaling_sleep_up = input;
+
+	return count;
+}
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+
+// Yank: tuneable -> possible values 1-4 to enable fast scaling and 5 for auto fast scaling (insane scaling) in early suspend
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+static ssize_t store_fast_scaling_sleep_down(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 5 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.fast_scaling_sleep_down != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.fast_scaling_sleep_down = input;
+
+	return count;
+}
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+
+#ifdef ENABLE_PROFILES_SUPPORT
+// ZZ: tuneable -> possible values from 0 to 100
+#define store_afs_threshold(name)								\
+static ssize_t store_afs_threshold##name(struct kobject *a, struct attribute *b,		\
+				  const char *buf, size_t count)				\
+{												\
+	unsigned int input;									\
+	int ret;										\
+	ret = sscanf(buf, "%u", &input);							\
+												\
+	if (ret != 1 || input > 100 || input < 0 || set_profile_active == true)			\
+		return -EINVAL;									\
+												\
+												\
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0		\
+	&& dbs_tuners_ins.afs_threshold##name != input) {					\
+	    dbs_tuners_ins.profile_number = 0;							\
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));	\
+	}											\
+												\
+	dbs_tuners_ins.afs_threshold##name = input;						\
+												\
+	return count;										\
+}												\
+
+store_afs_threshold(1);
+store_afs_threshold(2);
+store_afs_threshold(3);
+store_afs_threshold(4);
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+#ifndef ENABLE_PROFILES_SUPPORT
+// ZZ: tuneable -> possible values from 0 to 100
+#define store_afs_threshold(name)								\
+static ssize_t store_afs_threshold##name(struct kobject *a, struct attribute *b,		\
+				  const char *buf, size_t count)				\
+{												\
+	unsigned int input;									\
+	int ret;										\
+	ret = sscanf(buf, "%u", &input);							\
+												\
+	if (ret != 1 || input > 100 || input < 0)						\
+		return -EINVAL;									\
+												\
+	dbs_tuners_ins.afs_threshold##name = input;						\
+												\
+	return count;										\
+}												\
+
+store_afs_threshold(1);
+store_afs_threshold(2);
+store_afs_threshold(3);
+store_afs_threshold(4);
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+// ZZ: Early demand - tuneable grad up threshold -> possible values: from 1 to 100, if not set default is 50
+static ssize_t store_grad_up_threshold(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 || input < 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.grad_up_threshold != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.grad_up_threshold = input;
+
+	return count;
+}
+
+// ZZ: Early demand - tuneable grad up threshold sleep -> possible values: from 1 to 100, if not set default is 50
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+static ssize_t store_grad_up_threshold_sleep(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 || input < 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.grad_up_threshold_sleep != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.grad_up_threshold_sleep = input;
+
+	return count;
+}
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+
+// ZZ: Early demand - tuneable master switch -> possible values: 0 to disable, any value above 0 to enable, if not set default is 0
+static ssize_t store_early_demand(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	input = !!input;
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.early_demand != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.early_demand = !!input;
+
+	return count;
+}
+
+// ZZ: Early demand sleep - tuneable master switch -> possible values: 0 to disable, any value above 0 to enable, if not set default is 0
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+static ssize_t store_early_demand_sleep(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	input = !!input;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.early_demand_sleep != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.early_demand_sleep = !!input;
+
+	return count;
+}
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+
+#ifdef ENABLE_HOTPLUGGING
+// ZZ: tuneable hotplug switch -> possible values: 0 to disable, any value above 0 to enable, if not set default is 0
+static ssize_t store_disable_hotplug(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 2 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.disable_hotplug != input) {
+		dbs_tuners_ins.profile_number = 0;
+		strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    dbs_tuners_ins.disable_hotplug = input;
+
+	if (input == 1) {
+	    enable_cores = 1;
+	    queue_work_on(0, dbs_wq, &hotplug_online_work);
+	}
+
+	return count;
+}
+
+// ZZ: tuneable hotplug switch for early supend -> possible values: 0 to disable, any value above 0 to enable, if not set default is 0
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+static ssize_t store_disable_hotplug_sleep(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 2 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.disable_hotplug_sleep != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.disable_hotplug_sleep = input;
+
+	return count;
+}
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+
+// ZZ: tuneable hotplug up block cycles -> possible values: 0 to disable, any value above 0 to enable, if not set default is 0
+static ssize_t store_hotplug_block_up_cycles(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input == 0)
+	    hplg_up_block_cycles = 0;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.hotplug_block_up_cycles != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.hotplug_block_up_cycles = input;
+
+	return count;
+}
+
+// ZZ: tuneable hotplug down block cycles -> possible values: 0 to disable, any value above 0 to enable, if not set default is 0
+static ssize_t store_hotplug_block_down_cycles(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input == 0)
+	    hplg_down_block_cycles = 0;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.hotplug_block_down_cycles != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.hotplug_block_down_cycles = input;
+
+	return count;
+}
+
+// ff: tuneable hotplug stagger up -> possible values: 0 to disable, any value above 0 to enable, if not set default is 0
+static ssize_t store_hotplug_stagger_up(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	input = !!input;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.hotplug_stagger_up != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+	dbs_tuners_ins.hotplug_stagger_up = input;
+
+	return count;
+}
+
+// ff: tuneable hotplug stagger down -> possible values: 0 to disable, any value above 0 to enable, if not set default is 0
+static ssize_t store_hotplug_stagger_down(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	input = !!input;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.hotplug_stagger_down != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+	dbs_tuners_ins.hotplug_stagger_down = input;
+
+	return count;
+}
+
+// ZZ: tuneable hotplug idle threshold -> possible values: range from 0 disabled to 100, if not set default is 0
+static ssize_t store_hotplug_idle_threshold(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (((ret != 1 || input < 0 || input > 100) && input != 0)
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.hotplug_idle_threshold != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+	dbs_tuners_ins.hotplug_idle_threshold = input;
+
+	return count;
+}
+
+// ZZ: tuneable hotplug idle frequency -> frequency from where the hotplug idle should begin. possible values: all valid system frequencies
+static ssize_t store_hotplug_idle_freq(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	int i = 0;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input == 0) {
+#ifdef ENABLE_PROFILES_SUPPORT
+	    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.hotplug_idle_freq != input) {
+	        dbs_tuners_ins.profile_number = 0;
+	        strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    dbs_tuners_ins.hotplug_idle_freq = input;
+	return count;
+	}
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	input = check_frequency(input);							// ZZ: check and adapt given freq if necessary
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+	if (input > system_freq_table[max_scaling_freq_hard].frequency) {		// Yank: allow only frequencies below or equal to hard max
+		   return -EINVAL;
+	} else {
+	    for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+		if (unlikely(system_freq_table[i].frequency == input)) {
+#ifdef ENABLE_PROFILES_SUPPORT
+		    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+		    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.hotplug_idle_freq != input) {
+			dbs_tuners_ins.profile_number = 0;
+			strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+		    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+		    dbs_tuners_ins.hotplug_idle_freq = input;
+		return count;
+		}
+	    }
+	}
+	return -EINVAL;
+}
+
+// ZZ: tuneable -> possible values: range from 0 (disabled) to policy->max, if not set default is 0 (ffolkes)
+static ssize_t store_hotplug_engage_freq(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	int i = 0;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input == 0) {
+#ifdef ENABLE_PROFILES_SUPPORT
+	    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.hotplug_engage_freq != input) {
+	        dbs_tuners_ins.profile_number = 0;
+	        strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    dbs_tuners_ins.hotplug_engage_freq = input;
+	return count;
+	}
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	input = check_frequency(input);							// ZZ: check and adapt given freq if necessary
+#endif /* ENABLE_PROFILES_SUPPORT */
+	if (input > system_freq_table[max_scaling_freq_hard].frequency) {		// Yank: allow only frequencies below or equal to hard max
+		   return -EINVAL;
+	} else {
+	    for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+		if (unlikely(system_freq_table[i].frequency == input)) {
+#ifdef ENABLE_PROFILES_SUPPORT
+		    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+		    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.hotplug_engage_freq != input) {
+			dbs_tuners_ins.profile_number = 0;
+			strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+		    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+		    dbs_tuners_ins.hotplug_engage_freq = input;
+		    return count;
+		}
+	    }
+	}
+	return -EINVAL;
+}
+
+// ff: added tuneable hotplug_max_limit -> possible values: range from 0 disabled to 8, if not set default is 0
+static ssize_t store_hotplug_max_limit(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (((ret != 1 || input < 0 || input > possible_cpus) && input != 0)
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.hotplug_max_limit != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+	dbs_tuners_ins.hotplug_max_limit = input;
+
+	if (input > 0) {
+	    disable_cores = 1;
+	    queue_work_on(0, dbs_wq, &hotplug_offline_work);
+	}
+
+	return count;
+}
+
+// ff: added tuneable hotplug_lock -> possible values: range from 0 disabled to 8, if not set default is 0
+static ssize_t store_hotplug_lock(struct kobject *a, struct attribute *b,
+ const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (((ret != 1 || input < 0 || input > possible_cpus) && input != 0)
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.hotplug_lock != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.hotplug_lock = input;
+
+	if (input > 0) {
+	    disable_cores = 2;
+	    queue_work_on(0, dbs_wq, &hotplug_offline_work);
+	}
+
+	return count;
+}
+
+// ff: added tuneable hotplug_min_limit -> possible values: range from 0 disabled to 8, if not set default is 0
+static ssize_t store_hotplug_min_limit(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (((ret != 1 || input < 1 || input > possible_cpus) && input != 0)
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.hotplug_min_limit != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.hotplug_min_limit = input;
+	dbs_tuners_ins.hotplug_min_limit_saved = input;
+
+	if (input > 1) {
+	    enable_cores = 1;
+	    queue_work_on(0, dbs_wq, &hotplug_online_work);
+	}
+
+	return count;
+}
+#endif /* ENABLE_HOTPLUGGING */
+
+// ZZ: tuneable -> possible values: range from 0 (disabled) to policy->max, if not set default is 0 (ffolkes)
+static ssize_t store_scaling_responsiveness_freq(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	int i = 0;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input == 0) {
+#ifdef ENABLE_PROFILES_SUPPORT
+	    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.scaling_responsiveness_freq != input) {
+	        dbs_tuners_ins.profile_number = 0;
+	        strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    dbs_tuners_ins.scaling_responsiveness_freq = input;
+	return count;
+	}
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	input = check_frequency(input);							// ZZ: check and adapt given freq if necessary
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+	if (input > system_freq_table[max_scaling_freq_hard].frequency) {		// Yank: allow only frequencies below or equal to hard max
+		   return -EINVAL;
+	} else {
+	    for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+		if (unlikely(system_freq_table[i].frequency == input)) {
+#ifdef ENABLE_PROFILES_SUPPORT
+		    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+		    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.scaling_responsiveness_freq != input) {
+			dbs_tuners_ins.profile_number = 0;
+			strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+		    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+		    dbs_tuners_ins.scaling_responsiveness_freq = input;
+		return count;
+		}
+	    }
+	}
+	return -EINVAL;
+}
+
+// ZZ: tuneable -> possible values: range from 11 to 100, if not set default is 30 (ffolkes)
+static ssize_t store_scaling_responsiveness_up_threshold(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 || input < 11
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.scaling_responsiveness_up_threshold != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.scaling_responsiveness_up_threshold = input;
+
+	return count;
+}
+
+// ZZ: tuneable scaling idle threshold -> possible values: range from 0 disabled to 100, if not set default is 0
+static ssize_t store_scaling_block_threshold(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (((ret != 1 || input < 0 || input > 100) && input != 0)
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.scaling_block_threshold != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.scaling_block_threshold = input;
+
+	return count;
+}
+
+// ZZ: tuneable scaling block cycles -> possible values: 0 to disable, any value above 0 to enable, if not set default is 0
+static ssize_t store_scaling_block_cycles(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input == 0)
+	    scaling_block_cycles_count = 0;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.scaling_block_cycles != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.scaling_block_cycles = input;
+
+	return count;
+}
+
+// ff: tuneable scaling-up block cycles -> possible values: 0 to disable, any value above 0 to enable, if not set default is 0
+static ssize_t store_scaling_up_block_cycles(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input == 0)
+		scaling_up_block_cycles_count = 0;
+
+	dbs_tuners_ins.scaling_up_block_cycles = input;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode
+	if (dbs_tuners_ins.profile_number != 0) {
+		dbs_tuners_ins.profile_number = 0;
+		strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	return count;
+}
+
+// ff: tuneable -> possible values: range from 0 (disabled) to policy->max, if not set default is 0
+static ssize_t store_scaling_up_block_freq(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	int i = 0;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input == 0) {
+		dbs_tuners_ins.scaling_up_block_freq = input;
+#ifdef ENABLE_PROFILES_SUPPORT
+		// ZZ: set profile number to 0 and profile name to custom mode
+		if (dbs_tuners_ins.profile_number != 0) {
+			dbs_tuners_ins.profile_number = 0;
+			strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+		}
+#endif /* ENABLE_PROFILES_SUPPORT */
+		return count;
+	}
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	input = check_frequency(input);							// ZZ: check and adapt given freq if necessary
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+	if (input > system_freq_table[max_scaling_freq_hard].frequency) {		// Yank: allow only frequencies below or equal to hard max
+		return -EINVAL;
+	} else {
+		for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+			if (unlikely(system_freq_table[i].frequency == input)) {
+				dbs_tuners_ins.scaling_up_block_freq = input;
+#ifdef ENABLE_PROFILES_SUPPORT
+				// ZZ: set profile number to 0 and profile name to custom mode
+				if (dbs_tuners_ins.profile_number != 0) {
+					dbs_tuners_ins.profile_number = 0;
+					strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+				}
+#endif /* ENABLE_PROFILES_SUPPORT */
+				return count;
+			}
+		}
+	}
+	return -EINVAL;
+}
+
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+// ZZ: tuneable scaling block temp -> possible values: 0 to disable, values from 30C to 80C, if not set default is 0
+static ssize_t store_scaling_block_temp(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || (input < 30 && input != 0) || input > 80
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.scaling_block_temp != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.scaling_block_temp = input;
+
+	return count;
+}
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+
+// ff: added tuneable scaling_trip_temp -> possible values: 0 to disable, range from 40C to 69C if not set default is 0
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+static ssize_t store_scaling_trip_temp(struct kobject *a, struct attribute *b,
+													const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || (input < 40 && input != 0) || input > 69
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.scaling_trip_temp != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.scaling_trip_temp = input;
+	return count;
+}
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+
+// ZZ: tuneable scaling up idle frequency -> frequency from where the scaling up idle should begin. possible values all valid system frequenies
+static ssize_t store_scaling_block_freq(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	int i = 0;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input == 0) {
+#ifdef ENABLE_PROFILES_SUPPORT
+	    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.scaling_block_freq != input) {
+	        dbs_tuners_ins.profile_number = 0;
+	        strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    dbs_tuners_ins.scaling_block_freq = input;
+	return count;
+	}
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	input = check_frequency(input);							// ZZ: check and adapt given freq if necessary
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+	if (input > system_freq_table[max_scaling_freq_hard].frequency) {		// Yank: allow only frequencies below or equal to hard max
+		   return -EINVAL;
+	} else {
+	    for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+		if (unlikely(system_freq_table[i].frequency == input)) {
+#ifdef ENABLE_PROFILES_SUPPORT
+		    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+		    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.scaling_block_freq != input) {
+			dbs_tuners_ins.profile_number = 0;
+			strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+		    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+		    dbs_tuners_ins.scaling_block_freq = input;
+		return count;
+		}
+	    }
+	}
+	return -EINVAL;
+}
+
+// ZZ: tuneable scaling block force down -> possible values: 0 to disable, 2 or any value above 2 to enable, if not set default is 2
+static ssize_t store_scaling_block_force_down(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0 || input == 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.scaling_block_force_down != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.scaling_block_force_down = input;
+
+	return count;
+}
+
+// ZZ: tuneable scaling_fastdown_freq -> possible values: range from 0 (disabled) to policy->max, if not set default is 0 (ffolkes)
+static ssize_t store_scaling_fastdown_freq(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	int i = 0;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input == 0) {
+#ifdef ENABLE_PROFILES_SUPPORT
+	    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.scaling_fastdown_freq != input) {
+	        dbs_tuners_ins.profile_number = 0;
+	        strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    dbs_tuners_ins.scaling_fastdown_freq = input;
+	return count;
+	}
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	input = check_frequency(input);							// ZZ: check and adapt given freq if necessary
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+	if (input > system_freq_table[max_scaling_freq_hard].frequency) {		// Yank: allow only frequencies below or equal to hard max
+		   return -EINVAL;
+	} else {
+	    for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+		if (unlikely(system_freq_table[i].frequency == input)) {
+#ifdef ENABLE_PROFILES_SUPPORT
+		    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+		    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.scaling_fastdown_freq != input) {
+			dbs_tuners_ins.profile_number = 0;
+			strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+		    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+		    dbs_tuners_ins.scaling_fastdown_freq = input;
+		return count;
+		}
+	    }
+	}
+	return -EINVAL;
+}
+
+// ZZ: tuneable scaling_fastdown_up_threshold -> possible values: range from above fastdown up threshold to 100, if not set default is 95 (ffolkes)
+static ssize_t store_scaling_fastdown_up_threshold(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 || input <= dbs_tuners_ins.scaling_fastdown_down_threshold
+#ifdef ENABLE_PROFILES_SUPPORT
+	|| set_profile_active == true)
+#else
+	)
+#endif /* ENABLE_PROFILES_SUPPORT */
+		return -EINVAL;
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.scaling_fastdown_up_threshold != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.scaling_fastdown_up_threshold = input;
+
+	return count;
+}
+
+// ZZ: tuneable scaling_fastdown_down_threshold -> possible values: range from 11 to 100, if not set default is 90 (ffolkes)
+static ssize_t store_scaling_fastdown_down_threshold(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if ((ret != 1 || input > 100 || (input < 11 && input >= dbs_tuners_ins.scaling_fastdown_up_threshold))
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.scaling_fastdown_down_threshold != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.scaling_fastdown_down_threshold = input;
+
+	return count;
+}
+
+// ZZ: tuneable scaling proportinal -> possible values: 0 to disable, 1 to enable comparision between proportional and optimized freq, 2 to enable propotional freq usage only
+//     3 to enable propotional freq usage only but with dead brand range to avoid not reaching of pol min freq, if not set default is 0
+static ssize_t store_scaling_proportional(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0 || input > 3
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.scaling_proportional != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.scaling_proportional = input;
+
+	return count;
+}
+
+#ifdef ENABLE_INPUTBOOSTER
+// ff: added tuneable inputboost_cycles -> possible values: range from 0 disabled to 1000, if not set default is 0
+static ssize_t store_inputboost_cycles(struct kobject *a, struct attribute *b,
+									   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	int rc;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0 || input > 1000
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.inputboost_cycles != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	if (!input && dbs_tuners_ins.inputboost_cycles != input) {
+		// input is 0, and it wasn't before.
+		// so remove booster and unregister.
+
+		    input_unregister_handler(&interactive_input_handler);
+
+	} else if (input && dbs_tuners_ins.inputboost_cycles == 0) {
+		// input is something other than 0, and it wasn't before,
+		// so add booster and register.
+
+		    rc = input_register_handler(&interactive_input_handler);
+		    if (!rc)
+			    pr_info("[zzmoove/store_inputboost_cycles] inputbooster - registered\n");
+		    else
+			    pr_info("[zzmoove/store_inputboost_cycles] inputbooster - register FAILED\n");
+	}
+
+	dbs_tuners_ins.inputboost_cycles = input;
+	return count;
+}
+
+// ff: added tuneable inputboost_up_threshold -> possible values: range from 0 disabled (future use) to 100, if not set default is 50
+static ssize_t store_inputboost_up_threshold(struct kobject *a, struct attribute *b,
+											 const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0 || input > 100
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.inputboost_up_threshold != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.inputboost_up_threshold = input;
+	return count;
+}
+
+// ff: added tuneable inputboost_punch_cycles -> possible values: range from 0 disabled to 500, if not set default is 0
+static ssize_t store_inputboost_punch_cycles(struct kobject *a, struct attribute *b,
+											 const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0 || input > 500
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.inputboost_punch_cycles != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	if (!input) {
+		// reset some stuff.
+		flg_ctr_inputboost = 0;
+		flg_ctr_inputboost_punch = 0;
+	}
+
+	dbs_tuners_ins.inputboost_punch_cycles = input;
+	return count;
+}
+
+// ff: added tuneable inputboost_punch_freq -> possible values: range from 0 disabled to policy->max, if not set default is 0
+static ssize_t store_inputboost_punch_freq(struct kobject *a, struct attribute *b,
+										   const char *buf, size_t count)
+{
+	unsigned int input;
+	unsigned int i;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input == 0) {
+#ifdef ENABLE_PROFILES_SUPPORT
+	    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.inputboost_punch_freq != input) {
+	        dbs_tuners_ins.profile_number = 0;
+	        strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    dbs_tuners_ins.inputboost_punch_freq = input;
+	return count;
+	}
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	input = check_frequency(input);							// ZZ: check and adapt given freq if necessary
+#endif /* ENABLE_PROFILES_SUPPORT */
+	if (input > system_freq_table[max_scaling_freq_hard].frequency) {		// Yank: allow only frequencies below or equal to hard max
+		   return -EINVAL;
+	} else {
+	    for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+		if (unlikely(system_freq_table[i].frequency == input)) {
+#ifdef ENABLE_PROFILES_SUPPORT
+		    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+		    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.inputboost_punch_freq != input) {
+			dbs_tuners_ins.profile_number = 0;
+			strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+		    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+		    dbs_tuners_ins.inputboost_punch_freq = input;
+		return count;
+		}
+	    }
+	}
+	return -EINVAL;
+}
+
+// ff: added tuneable inputboost_punch_on_fingerdown -> possible values: range from 0 disabled to >0 enabled, if not set default is 0
+static ssize_t store_inputboost_punch_on_fingerdown(struct kobject *a, struct attribute *b,
+											 const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.inputboost_punch_on_fingerdown != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.inputboost_punch_on_fingerdown = input;
+	return count;
+}
+
+// ff: added tuneable inputboost_punch_on_fingermove -> possible values: range from 0 disabled to >0 enabled, if not set default is 0
+static ssize_t store_inputboost_punch_on_fingermove(struct kobject *a, struct attribute *b,
+											  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.inputboost_punch_on_fingermove != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.inputboost_punch_on_fingermove = input;
+	return count;
+}
+
+// ff: added tuneable inputboost_punch_on_epenmove -> possible values: range from 0 disabled to >0 enabled, if not set default is 0
+static ssize_t store_inputboost_punch_on_epenmove(struct kobject *a, struct attribute *b,
+												  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.inputboost_punch_on_epenmove != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.inputboost_punch_on_epenmove = input;
+	return count;
+}
+
+// ff: added tuneable inputboost_typingbooster_up_threshold -> possible values: range from 0 disabled (future use) to 100, if not set default is 50
+static ssize_t store_inputboost_typingbooster_up_threshold(struct kobject *a, struct attribute *b,
+											 const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0 || input > 100
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.inputboost_typingbooster_up_threshold != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.inputboost_typingbooster_up_threshold = input;
+	return count;
+}
+
+#ifdef ENABLE_HOTPLUGGING
+// ff: added tuneable inputboost_typingbooster_cores -> possible values: range from 0 disabled to 4, if not set default is 0
+static ssize_t store_inputboost_typingbooster_cores(struct kobject *a, struct attribute *b,
+														   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0 || input > 4
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.inputboost_typingbooster_cores != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.inputboost_typingbooster_cores = input;
+	return count;
+}
+#endif /* ENABLE_HOTPLUGGING */
+#endif /* ENABLE_INPUTBOOSTER */
+
+#ifdef ENABLE_MUSIC_LIMITS
+// ff: added tuneable music_max_freq -> possible values: range from 0 disabled to policy->max, if not set default is 0
+static ssize_t store_music_max_freq(struct kobject *a, struct attribute *b,
+									const char *buf, size_t count)
+{
+	unsigned int input;
+	unsigned int i;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input == 0) {
+#ifdef ENABLE_PROFILES_SUPPORT
+	    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.music_max_freq != input) {
+	        dbs_tuners_ins.profile_number = 0;
+	        strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	    music_max_freq_step = 0;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	    dbs_tuners_ins.music_max_freq = input;
+	return count;
+	}
+#ifdef ENABLE_PROFILES_SUPPORT
+	input = check_frequency(input);							// ZZ: check and adapt given freq if necessary
+#endif /* ENABLE_PROFILES_SUPPORT */
+	if (input > system_freq_table[max_scaling_freq_hard].frequency) {		// Yank: allow only frequencies below or equal to hard max
+		return -EINVAL;
+	} else {
+	    for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+		if (unlikely(system_freq_table[i].frequency == input)) {
+#ifdef ENABLE_PROFILES_SUPPORT
+		    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+		    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.music_max_freq != input) {
+			dbs_tuners_ins.profile_number = 0;
+			strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+		    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+		    dbs_tuners_ins.music_max_freq = input;
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    music_max_freq_step = i;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+		    return count;
+		}
+	    }
+	}
+	return -EINVAL;
+}
+
+// ff: added tuneable music_min_freq -> possible values: range from 0 disabled to policy->max, if not set default is 0
+static ssize_t store_music_min_freq(struct kobject *a, struct attribute *b,
+									const char *buf, size_t count)
+{
+	unsigned int input;
+	unsigned int i;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	if (input == 0) {
+#ifdef ENABLE_PROFILES_SUPPORT
+	    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.music_min_freq != input) {
+	        dbs_tuners_ins.profile_number = 0;
+	        strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    dbs_tuners_ins.music_min_freq = input;
+	return count;
+	}
+#ifdef ENABLE_PROFILES_SUPPORT
+	input = check_frequency(input);							// ZZ: check and adapt given freq if necessary
+#endif /* ENABLE_PROFILES_SUPPORT */
+	if (input > system_freq_table[max_scaling_freq_hard].frequency) {		// Yank: allow only frequencies below or equal to hard max
+		   return -EINVAL;
+	} else {
+	    for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+		if (unlikely(system_freq_table[i].frequency == input)) {
+#ifdef ENABLE_PROFILES_SUPPORT
+		    // ZZ: set profile number to 0 and profile name to custom mode if value has changed
+		    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.music_min_freq != input) {
+			dbs_tuners_ins.profile_number = 0;
+			strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+		    }
+#endif /* ENABLE_PROFILES_SUPPORT */
+		    dbs_tuners_ins.music_min_freq = input;
+		return count;
+		}
+	    }
+	}
+	return -EINVAL;
+}
+
+#ifdef ENABLE_HOTPLUGGING
+// ZZ: added tuneable music_min_cores -> possible values: range from 0 disabled to 8, if not set default is 0
+static ssize_t store_music_min_cores(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (((ret != 1 || input < 1 || input > possible_cpus) && input != 0)
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.music_min_cores != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.music_min_cores = input;
+
+	return count;
+}
+#endif /* ENABLE_HOTPLUGGING */
+
+// ff: added tuneable music_state -> possible values: 0 or 1
+static ssize_t store_music_state(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret, i;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0)
+	    return -EINVAL;
+
+	if (input > 0) {
+
+#ifdef ENABLE_HOTPLUGGING
+		// ZZ: if music min cores are set apply core setting
+		if (dbs_tuners_ins.music_min_cores > 0) {
+			enable_cores = 1;
+			queue_work_on(0, dbs_wq, &hotplug_online_work);
+		}
+#endif /* ENABLE_HOTPLUGGING */
+		// ZZ: if music min limit is set change scaling min limit on ascending ordered table
+		if (dbs_tuners_ins.music_min_freq && !freq_table_desc) {
+		    for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {
+			if (unlikely(system_freq_table[i].frequency == dbs_tuners_ins.music_min_freq)) {
+			    if (i >= min_scaling_freq_hard)			// ZZ: only if it is higher than current hard limit
+				min_scaling_freq_soft = limit_table_start = i;
+			}
+		    }
+		}
+
+		dbs_tuners_ins.music_state = 1;
+		return count;
+
+	} else {
+		if (!freq_table_desc) {			// ZZ: only on ascending ordered table
+		    min_scaling_freq_soft = 0;		// ZZ: reset saved freq soft limit
+		    limit_table_start = 0;		// ZZ: reset freq limit start point
+		}
+		dbs_tuners_ins.music_state = 0;		// ZZ: disable music state
+	}
+	return count;
+}
+#endif /* ENABLE_MUSIC_LIMITS */
+
+#ifdef ENABLE_PROFILES_SUPPORT
+// ZZ: function for switching a settings profile either at governor start by macro 'DEF_PROFILE_NUMBER' or later by tuneable 'profile_number'
+static inline int set_profile(int profile_num)
+{
+	int i = 0;					// ZZ: for main profile loop
+	int t = 0;					// ZZ: for sub-loop
+#ifdef ENABLE_INPUTBOOSTER
+	int rc = 0;					// ZZ: for impubooster registering
+#endif /* ENABLE_INPUTBOOSTER */
+	unsigned int j;					// ZZ: for update routines
+
+	set_profile_active = true;			// ZZ: avoid additional setting of tuneables during following loop
+
+	for (i = 0; (unlikely(zzmoove_profiles[i].profile_number != PROFILE_TABLE_END)); i++) {
+	    if (unlikely(zzmoove_profiles[i].profile_number == profile_num)) {
+
+#ifdef ENABLE_HOTPLUGGING
+		// ZZ: set disable_hotplug value
+		if (zzmoove_profiles[i].disable_hotplug > 0 && zzmoove_profiles[i].disable_hotplug < 2) {
+		    dbs_tuners_ins.disable_hotplug = zzmoove_profiles[i].disable_hotplug;
+
+		    if (zzmoove_profiles[i].disable_hotplug == 1) {
+			enable_cores = 1;
+			queue_work_on(0, dbs_wq, &hotplug_online_work);
+		    }
+		}
+		// ZZ: set disable_hotplug_sleep value
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		if (zzmoove_profiles[i].disable_hotplug_sleep > 0 && zzmoove_profiles[i].disable_hotplug_sleep < 2)
+		    dbs_tuners_ins.disable_hotplug_sleep = zzmoove_profiles[i].disable_hotplug_sleep;
+
+		// ZZ: set hotplug_sleep value
+		if (zzmoove_profiles[i].hotplug_sleep <= possible_cpus && zzmoove_profiles[i].hotplug_sleep >= 0)
+		    dbs_tuners_ins.hotplug_sleep = zzmoove_profiles[i].hotplug_sleep;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#endif /* ENABLE_HOTPLUGGING */
+		// ZZ: set down_threshold value
+		if (zzmoove_profiles[i].down_threshold > 11 && zzmoove_profiles[i].down_threshold <= 100
+		    && zzmoove_profiles[i].down_threshold < zzmoove_profiles[i].up_threshold)
+		    dbs_tuners_ins.down_threshold = zzmoove_profiles[i].down_threshold;
+
+#ifdef ENABLE_HOTPLUGGING
+		// ZZ: set down_threshold_hotplug1 value
+		if ((zzmoove_profiles[i].down_threshold_hotplug1 <= 100
+		    && zzmoove_profiles[i].down_threshold_hotplug1 >= 1)
+		    || zzmoove_profiles[i].down_threshold_hotplug1 == 0) {
+		    dbs_tuners_ins.down_threshold_hotplug1 = zzmoove_profiles[i].down_threshold_hotplug1;
+		    hotplug_thresholds[1][0] = zzmoove_profiles[i].down_threshold_hotplug1;
+		}
+
+		// ff: set block up multiplier for hotplug1
+		if (zzmoove_profiles[i].block_up_multiplier_hotplug1 >= 0 && zzmoove_profiles[i].block_up_multiplier_hotplug1 <= 25)
+		    dbs_tuners_ins.block_up_multiplier_hotplug1 = zzmoove_profiles[i].block_up_multiplier_hotplug1;
+
+		// ff: set block down multiplier for hotplug1
+		if (zzmoove_profiles[i].block_down_multiplier_hotplug1 >= 0 && zzmoove_profiles[i].block_down_multiplier_hotplug1 <= 25)
+		    dbs_tuners_ins.block_down_multiplier_hotplug1 = zzmoove_profiles[i].block_down_multiplier_hotplug1;
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		// ZZ: set down_threshold_hotplug2 value
+		if ((zzmoove_profiles[i].down_threshold_hotplug2 <= 100
+		    && zzmoove_profiles[i].down_threshold_hotplug2 >= 1)
+		    || zzmoove_profiles[i].down_threshold_hotplug2 == 0) {
+		    dbs_tuners_ins.down_threshold_hotplug2 = zzmoove_profiles[i].down_threshold_hotplug2;
+		    hotplug_thresholds[1][1] = zzmoove_profiles[i].down_threshold_hotplug2;
+		}
+
+		// ff: set block up multiplier for hotplug2
+		if (zzmoove_profiles[i].block_up_multiplier_hotplug2 >= 0 && zzmoove_profiles[i].block_up_multiplier_hotplug2 <= 25)
+		    dbs_tuners_ins.block_up_multiplier_hotplug2 = zzmoove_profiles[i].block_up_multiplier_hotplug2;
+
+		// ff: set block down multiplier for hotplug2
+		if (zzmoove_profiles[i].block_down_multiplier_hotplug2 >= 0 && zzmoove_profiles[i].block_down_multiplier_hotplug2 <= 25)
+		    dbs_tuners_ins.block_down_multiplier_hotplug2 = zzmoove_profiles[i].block_down_multiplier_hotplug2;
+
+		// ZZ: set down_threshold_hotplug3 value
+		if ((zzmoove_profiles[i].down_threshold_hotplug3 <= 100
+		    && zzmoove_profiles[i].down_threshold_hotplug3 >= 1)
+		    || zzmoove_profiles[i].down_threshold_hotplug3 == 0) {
+		    dbs_tuners_ins.down_threshold_hotplug3 = zzmoove_profiles[i].down_threshold_hotplug3;
+		    hotplug_thresholds[1][2] = zzmoove_profiles[i].down_threshold_hotplug3;
+		}
+
+		// ff: set block up multiplier for hotplug3
+		if (zzmoove_profiles[i].block_up_multiplier_hotplug3 >= 0 && zzmoove_profiles[i].block_up_multiplier_hotplug3 <= 25)
+		    dbs_tuners_ins.block_up_multiplier_hotplug3 = zzmoove_profiles[i].block_up_multiplier_hotplug3;
+
+		// ff: set block down multiplier for hotplug3
+		if (zzmoove_profiles[i].block_down_multiplier_hotplug3 >= 0 && zzmoove_profiles[i].block_down_multiplier_hotplug3 <= 25)
+		    dbs_tuners_ins.block_down_multiplier_hotplug3 = zzmoove_profiles[i].block_down_multiplier_hotplug3;
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		// ZZ: set down_threshold_hotplug4 value
+		if ((zzmoove_profiles[i].down_threshold_hotplug4 <= 100
+		    && zzmoove_profiles[i].down_threshold_hotplug4 >= 1)
+		    || zzmoove_profiles[i].down_threshold_hotplug4 == 0) {
+		    dbs_tuners_ins.down_threshold_hotplug4 = zzmoove_profiles[i].down_threshold_hotplug4;
+		    hotplug_thresholds[1][3] = zzmoove_profiles[i].down_threshold_hotplug4;
+		}
+
+		// ff: set block up multiplier for hotplug4
+		if (zzmoove_profiles[i].block_up_multiplier_hotplug4 >= 0 && zzmoove_profiles[i].block_up_multiplier_hotplug4 <= 25)
+		    dbs_tuners_ins.block_up_multiplier_hotplug4 = zzmoove_profiles[i].block_up_multiplier_hotplug4;
+
+		// ff: set block down multiplier for hotplug4
+		if (zzmoove_profiles[i].block_down_multiplier_hotplug4 >= 0 && zzmoove_profiles[i].block_down_multiplier_hotplug4 <= 25)
+		    dbs_tuners_ins.block_down_multiplier_hotplug4 = zzmoove_profiles[i].block_down_multiplier_hotplug4;
+
+		// ZZ: set down_threshold_hotplug5 value
+		if ((zzmoove_profiles[i].down_threshold_hotplug5 <= 100
+		    && zzmoove_profiles[i].down_threshold_hotplug5 >= 1)
+		    || zzmoove_profiles[i].down_threshold_hotplug5 == 0) {
+		    dbs_tuners_ins.down_threshold_hotplug5 = zzmoove_profiles[i].down_threshold_hotplug5;
+		    hotplug_thresholds[1][4] = zzmoove_profiles[i].down_threshold_hotplug5;
+		}
+
+		// ff: set block up multiplier for hotplug5
+		if (zzmoove_profiles[i].block_up_multiplier_hotplug5 >= 0 && zzmoove_profiles[i].block_up_multiplier_hotplug5 <= 25)
+		    dbs_tuners_ins.block_up_multiplier_hotplug5 = zzmoove_profiles[i].block_up_multiplier_hotplug5;
+
+		// ff: set block down multiplier for hotplug5
+		if (zzmoove_profiles[i].block_down_multiplier_hotplug5 >= 0 && zzmoove_profiles[i].block_down_multiplier_hotplug5 <= 25)
+		    dbs_tuners_ins.block_down_multiplier_hotplug5 = zzmoove_profiles[i].block_down_multiplier_hotplug5;
+
+		// ZZ: set down_threshold_hotplug6 value
+		if ((zzmoove_profiles[i].down_threshold_hotplug6 <= 100
+		    && zzmoove_profiles[i].down_threshold_hotplug6 >= 1)
+		    || zzmoove_profiles[i].down_threshold_hotplug6 == 0) {
+		    dbs_tuners_ins.down_threshold_hotplug6 = zzmoove_profiles[i].down_threshold_hotplug6;
+		    hotplug_thresholds[1][5] = zzmoove_profiles[i].down_threshold_hotplug6;
+		}
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		// ff: set block up multiplier for hotplug6
+		if (zzmoove_profiles[i].block_up_multiplier_hotplug6 >= 0 && zzmoove_profiles[i].block_up_multiplier_hotplug6 <= 25)
+		    dbs_tuners_ins.block_up_multiplier_hotplug6 = zzmoove_profiles[i].block_up_multiplier_hotplug6;
+
+		// ff: set block down multiplier for hotplug6
+		if (zzmoove_profiles[i].block_down_multiplier_hotplug6 >= 0 && zzmoove_profiles[i].block_down_multiplier_hotplug6 <= 25)
+		    dbs_tuners_ins.block_down_multiplier_hotplug6 = zzmoove_profiles[i].block_down_multiplier_hotplug6;
+
+		// ZZ: set down_threshold_hotplug7 value
+		if ((zzmoove_profiles[i].down_threshold_hotplug7 <= 100
+		    && zzmoove_profiles[i].down_threshold_hotplug7 >= 1)
+		    || zzmoove_profiles[i].down_threshold_hotplug7 == 0) {
+		    dbs_tuners_ins.down_threshold_hotplug7 = zzmoove_profiles[i].down_threshold_hotplug7;
+		    hotplug_thresholds[1][6] = zzmoove_profiles[i].down_threshold_hotplug7;
+		}
+
+		// ff: set block up multiplier for hotplug7
+		if (zzmoove_profiles[i].block_up_multiplier_hotplug7 >= 0 && zzmoove_profiles[i].block_up_multiplier_hotplug7 <= 25)
+		    dbs_tuners_ins.block_up_multiplier_hotplug7 = zzmoove_profiles[i].block_up_multiplier_hotplug7;
+
+		// ff: set block down multiplier for hotplug7
+		if (zzmoove_profiles[i].block_down_multiplier_hotplug7 >= 0 && zzmoove_profiles[i].block_down_multiplier_hotplug7 <= 25)
+		    dbs_tuners_ins.block_down_multiplier_hotplug7 = zzmoove_profiles[i].block_down_multiplier_hotplug7;
+
+#endif /* (MAX_CORES == 8) */
+		// ZZ: set down_threshold_hotplug_freq1 value
+		if (zzmoove_profiles[i].down_threshold_hotplug_freq1 == 0) {
+		    dbs_tuners_ins.down_threshold_hotplug_freq1 = zzmoove_profiles[i].down_threshold_hotplug_freq1;
+		    hotplug_thresholds_freq[1][0] = zzmoove_profiles[i].down_threshold_hotplug_freq1;
+		}
+
+		zzmoove_profiles[i].down_threshold_hotplug_freq1 = check_frequency(zzmoove_profiles[i].down_threshold_hotplug_freq1);
+		if (system_freq_table && zzmoove_profiles[i].down_threshold_hotplug_freq1 <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].down_threshold_hotplug_freq1) {
+			    dbs_tuners_ins.down_threshold_hotplug_freq1 = zzmoove_profiles[i].down_threshold_hotplug_freq1;
+			    hotplug_thresholds_freq[1][0] = zzmoove_profiles[i].down_threshold_hotplug_freq1;
+			}
+		    }
+		}
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		// ZZ: set down_threshold_hotplug_freq2 value
+		if (zzmoove_profiles[i].down_threshold_hotplug_freq2 == 0) {
+		    dbs_tuners_ins.down_threshold_hotplug_freq2 = zzmoove_profiles[i].down_threshold_hotplug_freq2;
+		    hotplug_thresholds_freq[1][1] = zzmoove_profiles[i].down_threshold_hotplug_freq2;
+		}
+
+		zzmoove_profiles[i].down_threshold_hotplug_freq2 = check_frequency(zzmoove_profiles[i].down_threshold_hotplug_freq2);
+		if (system_freq_table && zzmoove_profiles[i].down_threshold_hotplug_freq2 <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].down_threshold_hotplug_freq2) {
+			    dbs_tuners_ins.down_threshold_hotplug_freq2 = zzmoove_profiles[i].down_threshold_hotplug_freq2;
+			    hotplug_thresholds_freq[1][1] = zzmoove_profiles[i].down_threshold_hotplug_freq2;
+			}
+		    }
+		}
+
+		// ZZ: set down_threshold_hotplug_freq3 value
+		if (zzmoove_profiles[i].down_threshold_hotplug_freq3 == 0) {
+		    dbs_tuners_ins.down_threshold_hotplug_freq3 = zzmoove_profiles[i].down_threshold_hotplug_freq3;
+		    hotplug_thresholds_freq[1][2] = zzmoove_profiles[i].down_threshold_hotplug_freq3;
+		}
+
+		zzmoove_profiles[i].down_threshold_hotplug_freq3 = check_frequency(zzmoove_profiles[i].down_threshold_hotplug_freq3);
+		if (system_freq_table && zzmoove_profiles[i].down_threshold_hotplug_freq3 <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].down_threshold_hotplug_freq3) {
+			    dbs_tuners_ins.down_threshold_hotplug_freq3 = zzmoove_profiles[i].down_threshold_hotplug_freq3;
+			    hotplug_thresholds_freq[1][2] = zzmoove_profiles[i].down_threshold_hotplug_freq3;
+			}
+		    }
+		}
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		// ZZ: set down_threshold_hotplug_freq4 value
+		if (zzmoove_profiles[i].down_threshold_hotplug_freq4 == 0) {
+		    dbs_tuners_ins.down_threshold_hotplug_freq4 = zzmoove_profiles[i].down_threshold_hotplug_freq4;
+		    hotplug_thresholds_freq[1][3] = zzmoove_profiles[i].down_threshold_hotplug_freq4;
+		}
+
+		zzmoove_profiles[i].down_threshold_hotplug_freq4 = check_frequency(zzmoove_profiles[i].down_threshold_hotplug_freq4);
+		if (system_freq_table && zzmoove_profiles[i].down_threshold_hotplug_freq4 <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].down_threshold_hotplug_freq4) {
+			    dbs_tuners_ins.down_threshold_hotplug_freq4 = zzmoove_profiles[i].down_threshold_hotplug_freq4;
+			    hotplug_thresholds_freq[1][3] = zzmoove_profiles[i].down_threshold_hotplug_freq4;
+			}
+		    }
+		}
+
+		// ZZ: set down_threshold_hotplug_freq5 value
+		if (zzmoove_profiles[i].down_threshold_hotplug_freq5 == 0) {
+		    dbs_tuners_ins.down_threshold_hotplug_freq5 = zzmoove_profiles[i].down_threshold_hotplug_freq5;
+		    hotplug_thresholds_freq[1][4] = zzmoove_profiles[i].down_threshold_hotplug_freq5;
+		}
+
+		zzmoove_profiles[i].down_threshold_hotplug_freq5 = check_frequency(zzmoove_profiles[i].down_threshold_hotplug_freq5);
+		if (system_freq_table && zzmoove_profiles[i].down_threshold_hotplug_freq5 <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].down_threshold_hotplug_freq5) {
+			    dbs_tuners_ins.down_threshold_hotplug_freq5 = zzmoove_profiles[i].down_threshold_hotplug_freq5;
+			    hotplug_thresholds_freq[1][4] = zzmoove_profiles[i].down_threshold_hotplug_freq5;
+			}
+		    }
+		}
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		// ZZ: set down_threshold_hotplug_freq6 value
+		if (zzmoove_profiles[i].down_threshold_hotplug_freq6 == 0) {
+		    dbs_tuners_ins.down_threshold_hotplug_freq6 = zzmoove_profiles[i].down_threshold_hotplug_freq6;
+		    hotplug_thresholds_freq[1][5] = zzmoove_profiles[i].down_threshold_hotplug_freq6;
+		}
+
+		zzmoove_profiles[i].down_threshold_hotplug_freq6 = check_frequency(zzmoove_profiles[i].down_threshold_hotplug_freq6);
+		if (system_freq_table && zzmoove_profiles[i].down_threshold_hotplug_freq6 <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].down_threshold_hotplug_freq6) {
+			    dbs_tuners_ins.down_threshold_hotplug_freq6 = zzmoove_profiles[i].down_threshold_hotplug_freq6;
+			    hotplug_thresholds_freq[1][5] = zzmoove_profiles[i].down_threshold_hotplug_freq6;
+			}
+		    }
+		}
+
+		// ZZ: set down_threshold_hotplug_freq7 value
+		if (zzmoove_profiles[i].down_threshold_hotplug_freq7 == 0) {
+		    dbs_tuners_ins.down_threshold_hotplug_freq7 = zzmoove_profiles[i].down_threshold_hotplug_freq7;
+		    hotplug_thresholds_freq[1][6] = zzmoove_profiles[i].down_threshold_hotplug_freq7;
+		}
+
+		zzmoove_profiles[i].down_threshold_hotplug_freq7 = check_frequency(zzmoove_profiles[i].down_threshold_hotplug_freq7);
+		if (system_freq_table && zzmoove_profiles[i].down_threshold_hotplug_freq7 <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].down_threshold_hotplug_freq7) {
+			    dbs_tuners_ins.down_threshold_hotplug_freq7 = zzmoove_profiles[i].down_threshold_hotplug_freq7;
+			    hotplug_thresholds_freq[1][6] = zzmoove_profiles[i].down_threshold_hotplug_freq7;
+			}
+		    }
+		}
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		// ZZ: set down_threshold_sleep value
+		if (zzmoove_profiles[i].down_threshold_sleep > 11 && zzmoove_profiles[i].down_threshold_sleep <= 100
+		    && zzmoove_profiles[i].down_threshold_sleep < dbs_tuners_ins.up_threshold_sleep)
+		    dbs_tuners_ins.down_threshold_sleep = zzmoove_profiles[i].down_threshold_sleep;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+		// ZZ: set early_demand value
+		dbs_tuners_ins.early_demand = !!zzmoove_profiles[i].early_demand;
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		dbs_tuners_ins.early_demand_sleep = !!zzmoove_profiles[i].early_demand_sleep;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+		// Yank: set fast_scaling value
+		if (zzmoove_profiles[i].fast_scaling_up <= 5 && zzmoove_profiles[i].fast_scaling_up >= 0) {
+			dbs_tuners_ins.fast_scaling_up = zzmoove_profiles[i].fast_scaling_up;
+			if (zzmoove_profiles[i].fast_scaling_up > 4)
+				scaling_mode_up = 0;
+			else
+				scaling_mode_up = zzmoove_profiles[i].fast_scaling_up;
+		}
+
+		if (zzmoove_profiles[i].fast_scaling_down <= 5 && zzmoove_profiles[i].fast_scaling_down >= 0) {
+			dbs_tuners_ins.fast_scaling_down = zzmoove_profiles[i].fast_scaling_down;
+			if (zzmoove_profiles[i].fast_scaling_down > 4)
+				scaling_mode_down = 0;
+			else
+				scaling_mode_down = zzmoove_profiles[i].fast_scaling_down;
+		}
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		// ZZ: set fast_scaling_sleep value
+		if (zzmoove_profiles[i].fast_scaling_sleep_up <= 5 && zzmoove_profiles[i].fast_scaling_sleep_up >= 0)
+			dbs_tuners_ins.fast_scaling_sleep_up = zzmoove_profiles[i].fast_scaling_sleep_up;
+
+		if (zzmoove_profiles[i].fast_scaling_sleep_down <= 5 && zzmoove_profiles[i].fast_scaling_sleep_down >= 0)
+			dbs_tuners_ins.fast_scaling_sleep_down = zzmoove_profiles[i].fast_scaling_sleep_down;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+		// ZZ: set afs_threshold1 value
+		if (zzmoove_profiles[i].afs_threshold1 <= 100)
+		    dbs_tuners_ins.afs_threshold1 = zzmoove_profiles[i].afs_threshold1;
+
+		// ZZ: set afs_threshold2 value
+		if (zzmoove_profiles[i].afs_threshold2 <= 100)
+		    dbs_tuners_ins.afs_threshold2 = zzmoove_profiles[i].afs_threshold2;
+
+		// ZZ: set afs_threshold3 value
+		if (zzmoove_profiles[i].afs_threshold3 <= 100)
+		    dbs_tuners_ins.afs_threshold3 = zzmoove_profiles[i].afs_threshold3;
+
+		// ZZ: set afs_threshold4 value
+		if (zzmoove_profiles[i].afs_threshold4 <= 100)
+		    dbs_tuners_ins.afs_threshold4 = zzmoove_profiles[i].afs_threshold4;
+
+		// ZZ: set freq_limit value
+		if (system_freq_table && zzmoove_profiles[i].freq_limit == 0) {
+		    max_scaling_freq_soft = max_scaling_freq_hard;
+
+		    if (freq_table_desc)
+			limit_table_start = max_scaling_freq_soft;
+		    else
+			limit_table_end = system_freq_table[freq_table_size].frequency;
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    freq_limit_awake = dbs_tuners_ins.freq_limit = zzmoove_profiles[i].freq_limit;
+#else
+		    dbs_tuners_ins.freq_limit = zzmoove_profiles[i].freq_limit;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+		} else if (system_freq_table && zzmoove_profiles[i].freq_limit <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    zzmoove_profiles[i].freq_limit = check_frequency(zzmoove_profiles[i].freq_limit);
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].freq_limit) {
+			    max_scaling_freq_soft = t;
+			    if (freq_table_desc)
+				limit_table_start = max_scaling_freq_soft;
+			    else
+				limit_table_end = system_freq_table[t].frequency;
+			}
+		    }
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    freq_limit_awake = dbs_tuners_ins.freq_limit = zzmoove_profiles[i].freq_limit;
+#else
+		    dbs_tuners_ins.freq_limit = zzmoove_profiles[i].freq_limit;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+		}
+
+		// ZZ: set freq_limit_sleep value
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		if (system_freq_table && zzmoove_profiles[i].freq_limit_sleep == 0) {
+		    freq_limit_asleep = dbs_tuners_ins.freq_limit_sleep = zzmoove_profiles[i].freq_limit_sleep;
+
+		} else if (system_freq_table && zzmoove_profiles[i].freq_limit_sleep <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    zzmoove_profiles[i].freq_limit_sleep = check_frequency(zzmoove_profiles[i].freq_limit_sleep);
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].freq_limit_sleep)
+			    freq_limit_asleep = dbs_tuners_ins.freq_limit_sleep = zzmoove_profiles[i].freq_limit_sleep;
+		    }
+		}
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+		// ZZ: set grad_up_threshold value
+		if (zzmoove_profiles[i].grad_up_threshold < 100 && zzmoove_profiles[i].grad_up_threshold > 1)
+		    dbs_tuners_ins.grad_up_threshold = zzmoove_profiles[i].grad_up_threshold;
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		// ZZ: set grad_up_threshold sleep value
+		if (zzmoove_profiles[i].grad_up_threshold_sleep < 100 && zzmoove_profiles[i].grad_up_threshold_sleep > 1)
+		    dbs_tuners_ins.grad_up_threshold_sleep = zzmoove_profiles[i].grad_up_threshold_sleep;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+		// ZZ: set hotplug_block_up_cycles value
+		if (zzmoove_profiles[i].hotplug_block_up_cycles >= 0)
+		    dbs_tuners_ins.hotplug_block_up_cycles = zzmoove_profiles[i].hotplug_block_up_cycles;
+
+		// ZZ: set hotplug_block_down_cycles value
+		if (zzmoove_profiles[i].hotplug_block_down_cycles >= 0)
+		    dbs_tuners_ins.hotplug_block_down_cycles = zzmoove_profiles[i].hotplug_block_down_cycles;
+
+		// ff: set hotplug_stagger_up value
+		if (zzmoove_profiles[i].hotplug_stagger_up >= 0)
+		    dbs_tuners_ins.hotplug_stagger_up = zzmoove_profiles[i].hotplug_stagger_up;
+
+		// ff: set hotplug_stagger_down value
+		if (zzmoove_profiles[i].hotplug_stagger_down >= 0)
+		    dbs_tuners_ins.hotplug_stagger_down = zzmoove_profiles[i].hotplug_stagger_down;
+
+		// ZZ: set hotplug_idle_threshold value
+		if (zzmoove_profiles[i].hotplug_idle_threshold >= 0 && zzmoove_profiles[i].hotplug_idle_threshold < 100)
+		    dbs_tuners_ins.hotplug_idle_threshold = zzmoove_profiles[i].hotplug_idle_threshold;
+
+		// ZZ: set hotplug_idle_freq value
+		if (zzmoove_profiles[i].hotplug_idle_freq == 0) {
+		    dbs_tuners_ins.hotplug_idle_freq = zzmoove_profiles[i].hotplug_idle_freq;
+
+		} else if (system_freq_table && zzmoove_profiles[i].hotplug_idle_freq <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    zzmoove_profiles[i].hotplug_idle_freq = check_frequency(zzmoove_profiles[i].hotplug_idle_freq);
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].hotplug_idle_freq) {
+			    dbs_tuners_ins.hotplug_idle_freq = zzmoove_profiles[i].hotplug_idle_freq;
+			}
+		    }
+		}
+
+		// ZZ: set hotplug_engage_freq value
+		if (zzmoove_profiles[i].hotplug_engage_freq == 0) {
+		    dbs_tuners_ins.hotplug_engage_freq = zzmoove_profiles[i].hotplug_engage_freq;
+
+		} else if (system_freq_table && zzmoove_profiles[i].hotplug_engage_freq <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    zzmoove_profiles[i].hotplug_engage_freq = check_frequency(zzmoove_profiles[i].hotplug_engage_freq);
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].hotplug_engage_freq) {
+			    dbs_tuners_ins.hotplug_engage_freq = zzmoove_profiles[i].hotplug_engage_freq;
+			}
+		    }
+		}
+
+		// ff: set hotplug_max_limit value
+		if (zzmoove_profiles[i].hotplug_max_limit >= 0 && zzmoove_profiles[i].hotplug_max_limit < MAX_CORES)
+		    dbs_tuners_ins.hotplug_max_limit = zzmoove_profiles[i].hotplug_max_limit;
+
+		// ff: set hotplug_min_limit value
+		if (zzmoove_profiles[i].hotplug_min_limit >= 0 && zzmoove_profiles[i].hotplug_min_limit < MAX_CORES)
+		    dbs_tuners_ins.hotplug_min_limit = zzmoove_profiles[i].hotplug_min_limit;
+
+		// ff: set hotplug_lock value
+		if (zzmoove_profiles[i].hotplug_lock >= 0 && zzmoove_profiles[i].hotplug_lock < MAX_CORES)
+		    dbs_tuners_ins.hotplug_lock = zzmoove_profiles[i].hotplug_lock;
+#endif /* ENABLE_HOTPLUGGING */
+#ifdef ENABLE_MUSIC_LIMITS
+#ifdef ENABLE_HOTPLUGGING
+		// ZZ: set music min cores value
+		if (zzmoove_profiles[i].music_min_cores >= 0 && zzmoove_profiles[i].music_min_cores < MAX_CORES)
+		    dbs_tuners_ins.music_min_cores = zzmoove_profiles[i].music_min_cores;
+#endif /* ENABLE_HOTPLUGGING */
+
+		// ZZ: set music min freq value
+		if (zzmoove_profiles[i].music_min_freq == 0) {
+		    dbs_tuners_ins.music_min_freq = zzmoove_profiles[i].music_min_freq;
+
+		} else if (system_freq_table && zzmoove_profiles[i].music_min_freq <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    zzmoove_profiles[i].music_min_freq = check_frequency(zzmoove_profiles[i].music_min_freq);
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].music_min_freq) {
+			    dbs_tuners_ins.music_min_freq = zzmoove_profiles[i].music_min_freq;
+			}
+		    }
+		}
+
+		// ZZ: set music max freq value
+		if (zzmoove_profiles[i].music_max_freq == 0) {
+		    dbs_tuners_ins.music_max_freq = zzmoove_profiles[i].music_max_freq;
+
+		} else if (system_freq_table && zzmoove_profiles[i].music_max_freq <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    zzmoove_profiles[i].music_max_freq = check_frequency(zzmoove_profiles[i].music_max_freq);
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].music_max_freq) {
+			    dbs_tuners_ins.music_max_freq = zzmoove_profiles[i].music_max_freq;
+			}
+		    }
+		}
+#endif /* ENABLE_MUSIC_LIMITS */
+		// ZZ: set scaling_responsiveness_freq value
+		if (zzmoove_profiles[i].scaling_responsiveness_freq == 0) {
+		    dbs_tuners_ins.scaling_responsiveness_freq = zzmoove_profiles[i].scaling_responsiveness_freq;
+
+		} else if (system_freq_table && zzmoove_profiles[i].scaling_responsiveness_freq <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    zzmoove_profiles[i].scaling_responsiveness_freq = check_frequency(zzmoove_profiles[i].scaling_responsiveness_freq);
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].scaling_responsiveness_freq) {
+			    dbs_tuners_ins.scaling_responsiveness_freq = zzmoove_profiles[i].scaling_responsiveness_freq;
+			}
+		    }
+		}
+
+		// ZZ: set scaling_proportional value
+		if (zzmoove_profiles[i].scaling_proportional > 3) {
+		    dbs_tuners_ins.scaling_proportional = 3;
+		} else if (zzmoove_profiles[i].scaling_proportional < 0) {
+			dbs_tuners_ins.scaling_proportional = 1;
+		} else {
+			dbs_tuners_ins.scaling_proportional = zzmoove_profiles[i].scaling_proportional;
+		}
+#ifdef ENABLE_INPUTBOOSTER
+		// ZZ: set inputboost cycles value
+		if (zzmoove_profiles[i].inputboost_cycles >= 0 && zzmoove_profiles[i].inputboost_cycles <= 1000) {
+			if (!zzmoove_profiles[i].inputboost_cycles && dbs_tuners_ins.inputboost_cycles != zzmoove_profiles[i].inputboost_cycles) {
+			    // ff: input is 0, and it wasn't before.
+			    // ff: so remove booster and unregister.
+			    input_unregister_handler(&interactive_input_handler);
+			} else if (zzmoove_profiles[i].inputboost_cycles && dbs_tuners_ins.inputboost_cycles == 0) {
+			    // ff: input is something other than 0, and it wasn't before,
+			    // ff: so add booster and register.
+			    rc = input_register_handler(&interactive_input_handler);
+			    if (!rc)
+				pr_info("[zzmoove/store_inputboost_cycles] inputbooster - registered\n");
+			    else
+				pr_info("[zzmoove/store_inputboost_cycles] inputbooster - register FAILED\n");
+			}
+		dbs_tuners_ins.inputboost_cycles = zzmoove_profiles[i].inputboost_cycles;
+		}
+
+		// ZZ: set inputboost up threshold value
+		if (zzmoove_profiles[i].inputboost_up_threshold <= 100)
+		    dbs_tuners_ins.inputboost_up_threshold = zzmoove_profiles[i].inputboost_up_threshold;
+
+		// ZZ: set inputboost punch cycles value
+		if (zzmoove_profiles[i].inputboost_punch_cycles >= 0)
+		    dbs_tuners_ins.inputboost_punch_cycles = zzmoove_profiles[i].inputboost_punch_cycles;
+
+		// ZZ: set inputboost freq value
+		if (zzmoove_profiles[i].inputboost_punch_freq == 0) {
+		    dbs_tuners_ins.inputboost_punch_freq = zzmoove_profiles[i].inputboost_punch_freq;
+
+		} else if (system_freq_table && zzmoove_profiles[i].inputboost_punch_freq <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    zzmoove_profiles[i].inputboost_punch_freq = check_frequency(zzmoove_profiles[i].inputboost_punch_freq);
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].inputboost_punch_freq) {
+			    dbs_tuners_ins.inputboost_punch_freq = zzmoove_profiles[i].inputboost_punch_freq;
+			}
+		    }
+		}
+
+		// ZZ: set inputboost punch punch on finger down value
+		if (zzmoove_profiles[i].inputboost_punch_on_fingerdown >= 0)
+		    dbs_tuners_ins.inputboost_punch_on_fingerdown = zzmoove_profiles[i].inputboost_punch_on_fingerdown;
+
+		// ZZ: set inputboost punch punch on finger move value
+		if (zzmoove_profiles[i].inputboost_punch_on_fingermove >= 0)
+		    dbs_tuners_ins.inputboost_punch_on_fingermove = zzmoove_profiles[i].inputboost_punch_on_fingermove;
+
+		// ZZ: set inputboost punch punch on epen move value
+		if (zzmoove_profiles[i].inputboost_punch_on_epenmove >= 0)
+		    dbs_tuners_ins.inputboost_punch_on_epenmove = zzmoove_profiles[i].inputboost_punch_on_epenmove;
+
+		// ZZ: set inputboost up threshold value
+		if (zzmoove_profiles[i].inputboost_typingbooster_up_threshold <= 100)
+		    dbs_tuners_ins.inputboost_typingbooster_up_threshold = zzmoove_profiles[i].inputboost_typingbooster_up_threshold;
+#ifdef ENABLE_HOTPLUGGING
+		// ZZ: set inputboost cores value
+		if (zzmoove_profiles[i].inputboost_typingbooster_cores >= 0 && zzmoove_profiles[i].inputboost_typingbooster_cores < MAX_CORES)
+		    dbs_tuners_ins.inputboost_typingbooster_cores = zzmoove_profiles[i].inputboost_typingbooster_cores;
+#endif /* ENABLE_HOTPLUGGING */
+#endif /* ENABLE_INPUTBOOSTER */
+
+		// ZZ: set scaling_responsiveness_up_threshold value
+		if (zzmoove_profiles[i].scaling_responsiveness_up_threshold <= 100 && zzmoove_profiles[i].scaling_responsiveness_up_threshold >= 11)
+		    dbs_tuners_ins.scaling_responsiveness_up_threshold = zzmoove_profiles[i].scaling_responsiveness_up_threshold;
+
+		// ZZ: set ignore_nice_load value
+		if (zzmoove_profiles[i].ignore_nice_load > 1)
+		    zzmoove_profiles[i].ignore_nice_load = 1;
+
+		dbs_tuners_ins.ignore_nice = zzmoove_profiles[i].ignore_nice_load;
+
+		// we need to re-evaluate prev_cpu_idle
+		for_each_online_cpu(j) {
+		     struct cpu_dbs_info_s *dbs_info;
+		     dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+		     dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,10,0) || defined(CPU_IDLE_TIME_IN_CPUFREQ) /* overrule for sources with backported cpufreq implementation */
+		 &dbs_info->prev_cpu_wall, 0);
+#else
+		 &dbs_info->prev_cpu_wall);
+#endif /* LINUX_VERSION_CODE... */
+		 if (dbs_tuners_ins.ignore_nice)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+		     dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+#else
+		     dbs_info->prev_cpu_nice = kstat_cpu(j).cpustat.nice;
+#endif /* LINUX_VERSION_CODE... */
+		}
+
+		// ZZ: set sampling_down_factor value
+		if (zzmoove_profiles[i].sampling_down_factor <= MAX_SAMPLING_DOWN_FACTOR
+		    && zzmoove_profiles[i].sampling_down_factor >= 1)
+		    dbs_tuners_ins.sampling_down_factor = zz_sampling_down_factor = zzmoove_profiles[i].sampling_down_factor;
+
+		    // ZZ: Reset down sampling multiplier in case it was active
+		    for_each_online_cpu(j) {
+			struct cpu_dbs_info_s *dbs_info;
+			dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+			dbs_info->rate_mult = 1;
+		    }
+
+		// ZZ: set sampling_down_max_momentum value
+		if (zzmoove_profiles[i].sampling_down_max_momentum <= MAX_SAMPLING_DOWN_FACTOR - dbs_tuners_ins.sampling_down_factor
+		    && zzmoove_profiles[i].sampling_down_max_momentum >= 0) {
+		    dbs_tuners_ins.sampling_down_max_mom = zz_sampling_down_max_mom = zzmoove_profiles[i].sampling_down_max_momentum;
+		    orig_sampling_down_max_mom = dbs_tuners_ins.sampling_down_max_mom;
+		}
+
+		// ZZ: Reset sampling down factor to default if momentum was disabled
+		if (dbs_tuners_ins.sampling_down_max_mom == 0)
+		    zz_sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR;
+
+		    // ZZ: Reset momentum_adder and reset down sampling multiplier in case momentum was disabled
+		    for_each_online_cpu(j) {
+			struct cpu_dbs_info_s *dbs_info;
+			dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+			dbs_info->momentum_adder = 0;
+			if (dbs_tuners_ins.sampling_down_max_mom == 0)
+			dbs_info->rate_mult = 1;
+		    }
+
+		// ZZ: set sampling_down_momentum_sensitivity value
+		if (zzmoove_profiles[i].sampling_down_momentum_sensitivity <= MAX_SAMPLING_DOWN_MOMENTUM_SENSITIVITY
+		    && zzmoove_profiles[i].sampling_down_momentum_sensitivity >= 1) {
+		    dbs_tuners_ins.sampling_down_mom_sens = zzmoove_profiles[i].sampling_down_momentum_sensitivity;
+
+		    // ZZ: Reset momentum_adder
+		    for_each_online_cpu(j) {
+			struct cpu_dbs_info_s *dbs_info;
+			dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+			dbs_info->momentum_adder = 0;
+		    }
+
+		// ZZ: set sampling_rate value
+		dbs_tuners_ins.sampling_rate = dbs_tuners_ins.sampling_rate_current
+		= max(zzmoove_profiles[i].sampling_rate, min_sampling_rate);
+
+		// ZZ: set sampling_rate_idle value
+		if (zzmoove_profiles[i].sampling_rate_idle == 0) {
+		    dbs_tuners_ins.sampling_rate_current = dbs_tuners_ins.sampling_rate
+		    = dbs_tuners_ins.sampling_rate_idle;
+		} else {
+		    dbs_tuners_ins.sampling_rate_idle = max(zzmoove_profiles[i].sampling_rate_idle, min_sampling_rate);
+		}
+
+		// ZZ: set sampling_rate_idle_delay value
+		if (zzmoove_profiles[i].sampling_rate_idle_delay >= 0) {
+		    sampling_rate_step_up_delay = 0;
+		    sampling_rate_step_down_delay = 0;
+		    dbs_tuners_ins.sampling_rate_idle_delay = zzmoove_profiles[i].sampling_rate_idle_delay;
+		}
+
+		// ZZ: set sampling_rate_idle_threshold value
+		if (zzmoove_profiles[i].sampling_rate_idle_threshold <= 100)
+		    dbs_tuners_ins.sampling_rate_idle_threshold = zzmoove_profiles[i].sampling_rate_idle_threshold;
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		// ZZ: set sampling_rate_sleep_multiplier value
+		if (zzmoove_profiles[i].sampling_rate_sleep_multiplier <= MAX_SAMPLING_RATE_SLEEP_MULTIPLIER
+		    && zzmoove_profiles[i].sampling_rate_sleep_multiplier >= 1)
+		    dbs_tuners_ins.sampling_rate_sleep_multiplier = zzmoove_profiles[i].sampling_rate_sleep_multiplier;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+		// ZZ: set scaling_block_cycles value
+		if (zzmoove_profiles[i].scaling_block_cycles >= 0) {
+		    dbs_tuners_ins.scaling_block_cycles = zzmoove_profiles[i].scaling_block_cycles;
+		    if (zzmoove_profiles[i].scaling_block_cycles == 0)
+			scaling_block_cycles_count = 0;
+		}
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+		// ZZ: set scaling_block_temp value
+		if ((zzmoove_profiles[i].scaling_block_temp >= 30 && zzmoove_profiles[i].scaling_block_temp <= 80)
+		    || zzmoove_profiles[i].scaling_block_temp == 0) {
+		    dbs_tuners_ins.scaling_block_temp = zzmoove_profiles[i].scaling_block_temp;
+		}
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+		// ZZ: set scaling_block_freq value
+		if (zzmoove_profiles[i].scaling_block_freq == 0) {
+		    dbs_tuners_ins.scaling_block_freq = zzmoove_profiles[i].scaling_block_freq;
+
+		} else if (system_freq_table && zzmoove_profiles[i].scaling_block_freq <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    zzmoove_profiles[i].scaling_block_freq = check_frequency(zzmoove_profiles[i].scaling_block_freq);
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].scaling_block_freq) {
+			    dbs_tuners_ins.scaling_block_freq = zzmoove_profiles[i].scaling_block_freq;
+			}
+		    }
+		}
+
+		// ZZ: set scaling_block_threshold value
+		if (zzmoove_profiles[i].scaling_block_threshold >= 0
+		    && zzmoove_profiles[i].scaling_block_threshold <= 100)
+		    dbs_tuners_ins.scaling_block_threshold = zzmoove_profiles[i].scaling_block_threshold;
+
+		// ZZ: set scaling_block_force_down value
+		if (zzmoove_profiles[i].scaling_block_force_down >= 0
+		    && zzmoove_profiles[i].scaling_block_force_down != 1)
+		    dbs_tuners_ins.scaling_block_force_down = zzmoove_profiles[i].scaling_block_force_down;
+
+		// ZZ: set scaling_fastdown_freq value
+		if (zzmoove_profiles[i].scaling_fastdown_freq == 0) {
+		    dbs_tuners_ins.scaling_fastdown_freq = zzmoove_profiles[i].scaling_fastdown_freq;
+
+		} else if (system_freq_table && zzmoove_profiles[i].scaling_fastdown_freq <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    zzmoove_profiles[i].scaling_fastdown_freq = check_frequency(zzmoove_profiles[i].scaling_fastdown_freq);
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].scaling_fastdown_freq) {
+			    dbs_tuners_ins.scaling_fastdown_freq = zzmoove_profiles[i].scaling_fastdown_freq;
+			}
+		    }
+		}
+
+		// ZZ: set scaling_fastdown_up_threshold value
+		if (zzmoove_profiles[i].scaling_fastdown_up_threshold <= 100 && zzmoove_profiles[i].scaling_fastdown_up_threshold
+		    > zzmoove_profiles[i].scaling_fastdown_down_threshold)
+		    dbs_tuners_ins.scaling_fastdown_up_threshold = zzmoove_profiles[i].scaling_fastdown_up_threshold;
+
+		// ZZ: set scaling_fastdown_down_threshold value
+		if (zzmoove_profiles[i].scaling_fastdown_down_threshold < zzmoove_profiles[i].scaling_fastdown_up_threshold
+		    && zzmoove_profiles[i].scaling_fastdown_down_threshold > 11)
+		    dbs_tuners_ins.scaling_fastdown_down_threshold = zzmoove_profiles[i].scaling_fastdown_down_threshold;
+
+		// ZZ: set smooth_up value
+		if (zzmoove_profiles[i].smooth_up <= 100 && zzmoove_profiles[i].smooth_up >= 1)
+		    dbs_tuners_ins.smooth_up = zzmoove_profiles[i].smooth_up;
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		// ZZ: set smooth_up_sleep value
+		if (zzmoove_profiles[i].smooth_up_sleep <= 100 && zzmoove_profiles[i].smooth_up_sleep >= 1)
+		    dbs_tuners_ins.smooth_up_sleep = zzmoove_profiles[i].smooth_up_sleep;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+		// ZZ: set up_threshold value
+		if (zzmoove_profiles[i].up_threshold <= 100 && zzmoove_profiles[i].up_threshold
+		    >= zzmoove_profiles[i].down_threshold)
+		    dbs_tuners_ins.up_threshold = zzmoove_profiles[i].up_threshold;
+#ifdef ENABLE_HOTPLUGGING
+		// ZZ: set up_threshold_hotplug1 value
+		if (zzmoove_profiles[i].up_threshold_hotplug1 >= 0 && zzmoove_profiles[i].up_threshold_hotplug1 <= 100) {
+		    dbs_tuners_ins.up_threshold_hotplug1 = zzmoove_profiles[i].up_threshold_hotplug1;
+		    hotplug_thresholds[0][0] = zzmoove_profiles[i].up_threshold_hotplug1;
+		}
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		// ZZ: set up_threshold_hotplug2 value
+		if (zzmoove_profiles[i].up_threshold_hotplug2 >= 0 && zzmoove_profiles[i].up_threshold_hotplug2 <= 100) {
+		    dbs_tuners_ins.up_threshold_hotplug2 = zzmoove_profiles[i].up_threshold_hotplug2;
+		    hotplug_thresholds[0][1] = zzmoove_profiles[i].up_threshold_hotplug2;
+		}
+
+		// ZZ: set up_threshold_hotplug3 value
+		if (zzmoove_profiles[i].up_threshold_hotplug3 >= 0 && zzmoove_profiles[i].up_threshold_hotplug3 <= 100) {
+		    dbs_tuners_ins.up_threshold_hotplug3 = zzmoove_profiles[i].up_threshold_hotplug3;
+		    hotplug_thresholds[0][2] = zzmoove_profiles[i].up_threshold_hotplug3;
+		}
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		// ZZ: set up_threshold_hotplug4 value
+		if (zzmoove_profiles[i].up_threshold_hotplug4 >= 0 && zzmoove_profiles[i].up_threshold_hotplug4 <= 100) {
+		    dbs_tuners_ins.up_threshold_hotplug4 = zzmoove_profiles[i].up_threshold_hotplug4;
+		    hotplug_thresholds[0][3] = zzmoove_profiles[i].up_threshold_hotplug4;
+		}
+
+		// ZZ: set up_threshold_hotplug5 value
+		if (zzmoove_profiles[i].up_threshold_hotplug5 >= 0 && zzmoove_profiles[i].up_threshold_hotplug5 <= 100) {
+		    dbs_tuners_ins.up_threshold_hotplug5 = zzmoove_profiles[i].up_threshold_hotplug5;
+		    hotplug_thresholds[0][4] = zzmoove_profiles[i].up_threshold_hotplug5;
+		}
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		// ZZ: set up_threshold_hotplug6 value
+		if (zzmoove_profiles[i].up_threshold_hotplug6 >= 0 && zzmoove_profiles[i].up_threshold_hotplug6 <= 100) {
+		    dbs_tuners_ins.up_threshold_hotplug6 = zzmoove_profiles[i].up_threshold_hotplug6;
+		    hotplug_thresholds[0][5] = zzmoove_profiles[i].up_threshold_hotplug6;
+		}
+
+		// ZZ: set up_threshold_hotplug7 value
+		if (zzmoove_profiles[i].up_threshold_hotplug7 >= 0 && zzmoove_profiles[i].up_threshold_hotplug7 <= 100) {
+		    dbs_tuners_ins.up_threshold_hotplug7 = zzmoove_profiles[i].up_threshold_hotplug7;
+		    hotplug_thresholds[0][6] = zzmoove_profiles[i].up_threshold_hotplug7;
+		}
+#endif /* (MAX_CORES == 8) */
+		// ZZ: set up_threshold_hotplug_freq1 value
+		if (zzmoove_profiles[i].up_threshold_hotplug_freq1 == 0) {
+		    dbs_tuners_ins.up_threshold_hotplug_freq1 = zzmoove_profiles[i].up_threshold_hotplug_freq1;
+		    hotplug_thresholds_freq[0][0] = zzmoove_profiles[i].up_threshold_hotplug_freq1;
+		}
+
+		if (system_freq_table && zzmoove_profiles[i].up_threshold_hotplug_freq1 <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    zzmoove_profiles[i].up_threshold_hotplug_freq1 = check_frequency(zzmoove_profiles[i].up_threshold_hotplug_freq1);
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+		        if (system_freq_table[t].frequency == zzmoove_profiles[i].up_threshold_hotplug_freq1) {
+			    dbs_tuners_ins.up_threshold_hotplug_freq1 = zzmoove_profiles[i].up_threshold_hotplug_freq1;
+			    hotplug_thresholds_freq[0][0] = zzmoove_profiles[i].up_threshold_hotplug_freq1;
+		        }
+		    }
+		}
+
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		// ZZ: set up_threshold_hotplug_freq2 value
+		if (zzmoove_profiles[i].up_threshold_hotplug_freq2 == 0) {
+		    dbs_tuners_ins.up_threshold_hotplug_freq2 = zzmoove_profiles[i].up_threshold_hotplug_freq2;
+		    hotplug_thresholds_freq[0][1] = zzmoove_profiles[i].up_threshold_hotplug_freq2;
+		}
+
+		if (system_freq_table && zzmoove_profiles[i].up_threshold_hotplug_freq2 <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    zzmoove_profiles[i].up_threshold_hotplug_freq2 = check_frequency(zzmoove_profiles[i].up_threshold_hotplug_freq2);
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].up_threshold_hotplug_freq2) {
+			    dbs_tuners_ins.up_threshold_hotplug_freq2 = zzmoove_profiles[i].up_threshold_hotplug_freq2;
+			    hotplug_thresholds_freq[0][1] = zzmoove_profiles[i].up_threshold_hotplug_freq2;
+			}
+		    }
+		}
+
+		// ZZ: set up_threshold_hotplug_freq3 value
+		if (zzmoove_profiles[i].up_threshold_hotplug_freq3 == 0) {
+		    dbs_tuners_ins.up_threshold_hotplug_freq3 = zzmoove_profiles[i].up_threshold_hotplug_freq3;
+		    hotplug_thresholds_freq[0][2] = zzmoove_profiles[i].up_threshold_hotplug_freq3;
+		}
+
+		if (system_freq_table && zzmoove_profiles[i].up_threshold_hotplug_freq3 <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    zzmoove_profiles[i].up_threshold_hotplug_freq3 = check_frequency(zzmoove_profiles[i].up_threshold_hotplug_freq3);
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].up_threshold_hotplug_freq3) {
+			    dbs_tuners_ins.up_threshold_hotplug_freq3 = zzmoove_profiles[i].up_threshold_hotplug_freq3;
+			    hotplug_thresholds_freq[0][2] = zzmoove_profiles[i].up_threshold_hotplug_freq3;
+			}
+		    }
+		}
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		// ZZ: set up_threshold_hotplug_freq4 value
+		if (zzmoove_profiles[i].up_threshold_hotplug_freq4 == 0) {
+		    dbs_tuners_ins.up_threshold_hotplug_freq4 = zzmoove_profiles[i].up_threshold_hotplug_freq4;
+		    hotplug_thresholds_freq[0][3] = zzmoove_profiles[i].up_threshold_hotplug_freq4;
+		}
+
+		if (system_freq_table && zzmoove_profiles[i].up_threshold_hotplug_freq4 <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    zzmoove_profiles[i].up_threshold_hotplug_freq4 = check_frequency(zzmoove_profiles[i].up_threshold_hotplug_freq4);
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].up_threshold_hotplug_freq4) {
+			    dbs_tuners_ins.up_threshold_hotplug_freq4 = zzmoove_profiles[i].up_threshold_hotplug_freq4;
+			    hotplug_thresholds_freq[0][3] = zzmoove_profiles[i].up_threshold_hotplug_freq4;
+			}
+		    }
+		}
+
+		// ZZ: set up_threshold_hotplug_freq5 value
+		if (zzmoove_profiles[i].up_threshold_hotplug_freq5 == 0) {
+		    dbs_tuners_ins.up_threshold_hotplug_freq5 = zzmoove_profiles[i].up_threshold_hotplug_freq5;
+		    hotplug_thresholds_freq[0][4] = zzmoove_profiles[i].up_threshold_hotplug_freq5;
+		}
+
+		if (system_freq_table && zzmoove_profiles[i].up_threshold_hotplug_freq5 <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    zzmoove_profiles[i].up_threshold_hotplug_freq5 = check_frequency(zzmoove_profiles[i].up_threshold_hotplug_freq5);
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].up_threshold_hotplug_freq5) {
+			    dbs_tuners_ins.up_threshold_hotplug_freq5 = zzmoove_profiles[i].up_threshold_hotplug_freq5;
+			    hotplug_thresholds_freq[0][4] = zzmoove_profiles[i].up_threshold_hotplug_freq5;
+			}
+		    }
+		}
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		// ZZ: set up_threshold_hotplug_freq6 value
+		if (zzmoove_profiles[i].up_threshold_hotplug_freq6 == 0) {
+		    dbs_tuners_ins.up_threshold_hotplug_freq6 = zzmoove_profiles[i].up_threshold_hotplug_freq6;
+		    hotplug_thresholds_freq[0][5] = zzmoove_profiles[i].up_threshold_hotplug_freq6;
+		}
+
+		if (system_freq_table && zzmoove_profiles[i].up_threshold_hotplug_freq6 <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    zzmoove_profiles[i].up_threshold_hotplug_freq6 = check_frequency(zzmoove_profiles[i].up_threshold_hotplug_freq6);
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].up_threshold_hotplug_freq6) {
+			    dbs_tuners_ins.up_threshold_hotplug_freq6 = zzmoove_profiles[i].up_threshold_hotplug_freq6;
+			    hotplug_thresholds_freq[0][5] = zzmoove_profiles[i].up_threshold_hotplug_freq6;
+			}
+		    }
+		}
+
+		// ZZ: set up_threshold_hotplug_freq7 value
+		if (zzmoove_profiles[i].up_threshold_hotplug_freq7 == 0) {
+		    dbs_tuners_ins.up_threshold_hotplug_freq7 = zzmoove_profiles[i].up_threshold_hotplug_freq7;
+		    hotplug_thresholds_freq[0][6] = zzmoove_profiles[i].up_threshold_hotplug_freq7;
+		}
+
+		if (system_freq_table && zzmoove_profiles[i].up_threshold_hotplug_freq7 <= system_freq_table[max_scaling_freq_hard].frequency) {
+		    zzmoove_profiles[i].up_threshold_hotplug_freq7 = check_frequency(zzmoove_profiles[i].up_threshold_hotplug_freq7);
+		    for (t = 0; (system_freq_table[t].frequency != system_table_end); t++) {
+			if (system_freq_table[t].frequency == zzmoove_profiles[i].up_threshold_hotplug_freq7) {
+			    dbs_tuners_ins.up_threshold_hotplug_freq7 = zzmoove_profiles[i].up_threshold_hotplug_freq7;
+			    hotplug_thresholds_freq[0][6] = zzmoove_profiles[i].up_threshold_hotplug_freq7;
+			}
+		    }
+		}
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		// ZZ: set up_threshold_sleep value
+		if (zzmoove_profiles[i].up_threshold_sleep <= 100 && zzmoove_profiles[i].up_threshold_sleep
+		    > dbs_tuners_ins.down_threshold_sleep)
+		    dbs_tuners_ins.up_threshold_sleep = zzmoove_profiles[i].up_threshold_sleep;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND) */
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+		// ZZ: set auto_adjust_freq_thresholds value
+		if (zzmoove_profiles[i].auto_adjust_freq_thresholds > 1) {
+		    zzmoove_profiles[i].auto_adjust_freq_thresholds = 1;
+		    dbs_tuners_ins.auto_adjust_freq_thresholds = zzmoove_profiles[i].auto_adjust_freq_thresholds;
+		} else {
+		    dbs_tuners_ins.auto_adjust_freq_thresholds = zzmoove_profiles[i].auto_adjust_freq_thresholds;
+		    pol_step = 0;
+		}
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+		// ZZ: set current profile number
+		dbs_tuners_ins.profile_number = profile_num;
+
+		// ZZ: set current profile name
+		strncpy(dbs_tuners_ins.profile, zzmoove_profiles[i].profile_name, sizeof(dbs_tuners_ins.profile));
+		set_profile_active = false; // ZZ: profile found - allow setting of tuneables again
+		return 1;
+	    }
+	}
+    }
+// ZZ: profile not found - allow setting of tuneables again
+set_profile_active = false;
+return 0;
+}
+
+// ff: added tuneable profile_sticky_mode -> possible values: 0 disabled, anything else enabled
+static ssize_t store_profile_sticky_mode(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 0)
+	return -EINVAL;
+
+	dbs_tuners_ins.profile_sticky_mode = !!input;
+	return count;
+}
+
+// ZZ: tunable profile number -> for switching settings profiles, check zzmoove_profiles.h file for possible values
+static ssize_t store_profile_number(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;				// ZZ: regular input handling of this tuneable
+	int ret_profile;				// ZZ: return value for set_profile function
+	int ret;					// ZZ: regular input handling of this tuneable
+
+	ret = sscanf(buf, "%u", &input);		// ZZ: regular input handling of this tuneable
+
+	if (ret != 1)
+	    return -EINVAL;
+
+	// ZZ: if input is 0 set profile to custom mode
+	if (input == 0) {
+	    dbs_tuners_ins.profile_number = input;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	return count;
+	}
+
+	// ZZ: set profile and check result
+	ret_profile = set_profile(input);
+
+	if (ret_profile != 1)
+	    return -EINVAL; // ZZ: given profile not available
+	else
+	    return count; // ZZ: profile found return as normal
+}
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+// ZZ: tunable auto adjust freq thresholds -> for a automatic adjustment of all freq thresholds.
+static ssize_t store_auto_adjust_freq_thresholds(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1
+#ifdef ENABLE_PROFILES_SUPPORT
+	    || set_profile_active == true)
+#else
+	    )
+#endif /* ENABLE_PROFILES_SUPPORT */
+	    return -EINVAL;
+
+	input = !!input;
+
+	if (input == 0)
+	    pol_step = 0;
+#ifdef ENABLE_PROFILES_SUPPORT
+	// ZZ: set profile number to 0 and profile name to custom mode if value has changed
+	if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0 && dbs_tuners_ins.auto_adjust_freq_thresholds != input) {
+	    dbs_tuners_ins.profile_number = 0;
+	    strncpy(dbs_tuners_ins.profile, custom_profile, sizeof(dbs_tuners_ins.profile));
+	}
+#endif /* ENABLE_PROFILES_SUPPORT */
+	dbs_tuners_ins.auto_adjust_freq_thresholds = input;
+
+	return count;
+}
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+
+// Yank: add hotplug up/down threshold sysfs store interface
+#ifdef ENABLE_HOTPLUGGING
+#ifdef ENABLE_PROFILES_SUPPORT
+#define store_up_threshold_hotplug_freq(name,core)						\
+static ssize_t store_up_threshold_hotplug_freq##name						\
+(struct kobject *a, struct attribute *b, const char *buf, size_t count)				\
+{												\
+	unsigned int input;									\
+	int ret;										\
+	int i = 0;										\
+												\
+	ret = sscanf(buf, "%u", &input);							\
+	if (ret != 1 || set_profile_active == true)						\
+	    return -EINVAL;									\
+												\
+	if (input == 0) {									\
+	    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0	\
+	    && (dbs_tuners_ins.up_threshold_hotplug_freq##name != input				\
+	    || hotplug_thresholds_freq[0][core] == input)) {					\
+		dbs_tuners_ins.profile_number = 0;						\
+		strncpy(dbs_tuners_ins.profile, custom_profile,					\
+		sizeof(dbs_tuners_ins.profile));						\
+	    }											\
+	    dbs_tuners_ins.up_threshold_hotplug_freq##name = input;				\
+	    hotplug_thresholds_freq[0][core] = input;						\
+	return count;										\
+	}											\
+												\
+	input = check_frequency(input);								\
+												\
+	if (input > system_freq_table[max_scaling_freq_hard].frequency) {			\
+	    return -EINVAL;									\
+	} else {										\
+	    for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {	\
+		    if (unlikely(system_freq_table[i].frequency == input)) {			\
+			if (dbs_tuners_ins.profile_number != 0					\
+			&& !dbs_tuners_ins.profile_sticky_mode					\
+			&& (dbs_tuners_ins.up_threshold_hotplug_freq##name != input		\
+			|| hotplug_thresholds_freq[0][core] == input)) {			\
+			    dbs_tuners_ins.profile_number = 0;					\
+			    strncpy(dbs_tuners_ins.profile, custom_profile,			\
+			    sizeof(dbs_tuners_ins.profile));					\
+			}									\
+			dbs_tuners_ins.up_threshold_hotplug_freq##name = input;			\
+			hotplug_thresholds_freq[0][core] = input;				\
+		    return count;								\
+		    }										\
+	    }											\
+	}											\
+	return -EINVAL;										\
+}												\
+
+#define store_down_threshold_hotplug_freq(name,core)						\
+static ssize_t store_down_threshold_hotplug_freq##name						\
+(struct kobject *a, struct attribute *b, const char *buf, size_t count)				\
+{												\
+	unsigned int input;									\
+	int ret;										\
+	int i = 0;										\
+												\
+	ret = sscanf(buf, "%u", &input);							\
+	if (ret != 1 || set_profile_active == true)						\
+	    return -EINVAL;									\
+												\
+	if (input == 0) {									\
+	    if (!dbs_tuners_ins.profile_sticky_mode && dbs_tuners_ins.profile_number != 0	\
+	    && (dbs_tuners_ins.down_threshold_hotplug_freq##name != input			\
+	    || hotplug_thresholds_freq[1][core] == input)) {					\
+		dbs_tuners_ins.profile_number = 0;						\
+		strncpy(dbs_tuners_ins.profile, custom_profile,					\
+		sizeof(dbs_tuners_ins.profile));						\
+	    }											\
+	    dbs_tuners_ins.down_threshold_hotplug_freq##name = input;				\
+	    hotplug_thresholds_freq[1][core] = input;						\
+	return count;										\
+	}											\
+												\
+	input = check_frequency(input);								\
+												\
+	if (input > system_freq_table[max_scaling_freq_hard].frequency) {			\
+	    return -EINVAL;									\
+	} else {										\
+	    for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {	\
+		    if (unlikely(system_freq_table[i].frequency == input)) {			\
+			if (dbs_tuners_ins.profile_number != 0					\
+			&& !dbs_tuners_ins.profile_sticky_mode					\
+			&& (dbs_tuners_ins.down_threshold_hotplug_freq##name != input		\
+			|| hotplug_thresholds_freq[1][core] == input)) {			\
+			    dbs_tuners_ins.profile_number = 0;					\
+			    strncpy(dbs_tuners_ins.profile, custom_profile,			\
+			    sizeof(dbs_tuners_ins.profile));					\
+			}									\
+			dbs_tuners_ins.down_threshold_hotplug_freq##name = input;		\
+			hotplug_thresholds_freq[1][core] = input;				\
+		    return count;								\
+		    }										\
+	    }											\
+	}											\
+	return -EINVAL;										\
+}												\
+
+/*
+ * ZZ: tuneables -> possible values: 0 to disable core (only in up thresholds), range from above
+ * appropriate down thresholds up to scaling max frequency, if not set default for up and down
+ * thresholds is 0
+ */
+store_up_threshold_hotplug_freq(1,0);
+store_down_threshold_hotplug_freq(1,0);
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+store_up_threshold_hotplug_freq(2,1);
+store_down_threshold_hotplug_freq(2,1);
+store_up_threshold_hotplug_freq(3,2);
+store_down_threshold_hotplug_freq(3,2);
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+store_up_threshold_hotplug_freq(4,3);
+store_down_threshold_hotplug_freq(4,3);
+store_up_threshold_hotplug_freq(5,4);
+store_down_threshold_hotplug_freq(5,4);
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+store_up_threshold_hotplug_freq(6,5);
+store_down_threshold_hotplug_freq(6,5);
+store_up_threshold_hotplug_freq(7,6);
+store_down_threshold_hotplug_freq(7,6);
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+#ifndef ENABLE_PROFILES_SUPPORT
+#define store_up_threshold_hotplug_freq(name,core)						\
+static ssize_t store_up_threshold_hotplug_freq##name						\
+(struct kobject *a, struct attribute *b, const char *buf, size_t count)				\
+{												\
+	unsigned int input;									\
+	int ret;										\
+	int i = 0;										\
+												\
+	ret = sscanf(buf, "%u", &input);							\
+	if (ret != 1)										\
+	    return -EINVAL;									\
+												\
+	if (input == 0) {									\
+	    dbs_tuners_ins.up_threshold_hotplug_freq##name = input;				\
+	    hotplug_thresholds_freq[0][core] = input;						\
+	return count;										\
+	}											\
+												\
+	if (input > system_freq_table[max_scaling_freq_hard].frequency) {			\
+	    return -EINVAL;									\
+	} else {										\
+	    for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {	\
+		    if (unlikely(system_freq_table[i].frequency == input)) {			\
+			dbs_tuners_ins.up_threshold_hotplug_freq##name = input;			\
+			hotplug_thresholds_freq[0][core] = input;				\
+		    return count;								\
+		    }										\
+	    }											\
+	}											\
+	return -EINVAL;										\
+}												\
+
+#define store_down_threshold_hotplug_freq(name,core)						\
+static ssize_t store_down_threshold_hotplug_freq##name						\
+(struct kobject *a, struct attribute *b, const char *buf, size_t count)				\
+{												\
+	unsigned int input;									\
+	int ret;										\
+	int i = 0;										\
+												\
+	ret = sscanf(buf, "%u", &input);							\
+	if (ret != 1)										\
+	    return -EINVAL;									\
+												\
+	if (input == 0) {									\
+	    dbs_tuners_ins.down_threshold_hotplug_freq##name = input;				\
+	    hotplug_thresholds_freq[1][core] = input;						\
+	return count;										\
+	}											\
+												\
+	if (input > system_freq_table[max_scaling_freq_hard].frequency) {			\
+	    return -EINVAL;									\
+	} else {										\
+	    for (i = 0; (likely(system_freq_table[i].frequency != system_table_end)); i++) {	\
+		    if (unlikely(system_freq_table[i].frequency == input)) {			\
+			dbs_tuners_ins.down_threshold_hotplug_freq##name = input;		\
+			hotplug_thresholds_freq[1][core] = input;				\
+		    return count;								\
+		    }										\
+	    }											\
+	}											\
+	return -EINVAL;										\
+}												\
+
+/*
+ * ZZ: tuneables -> possible values: 0 to disable core (only in up thresholds), range from above
+ * appropriate down thresholds up to scaling max frequency, if not set default for up and down
+ * thresholds is 0
+ */
+store_up_threshold_hotplug_freq(1,0);
+store_down_threshold_hotplug_freq(1,0);
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+store_up_threshold_hotplug_freq(2,1);
+store_down_threshold_hotplug_freq(2,1);
+store_up_threshold_hotplug_freq(3,2);
+store_down_threshold_hotplug_freq(3,2);
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+store_up_threshold_hotplug_freq(4,3);
+store_down_threshold_hotplug_freq(4,3);
+store_up_threshold_hotplug_freq(5,4);
+store_down_threshold_hotplug_freq(5,4);
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+store_up_threshold_hotplug_freq(6,5);
+store_down_threshold_hotplug_freq(6,5);
+store_up_threshold_hotplug_freq(7,6);
+store_down_threshold_hotplug_freq(7,6);
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_PROFILES_SUPPORT */
+#endif /* ENABLE_HOTPLUGGING */
+
+#ifdef ENABLE_PROFILES_SUPPORT
+define_one_global_rw(profile_number);
+define_one_global_rw(profile_sticky_mode);
+define_one_global_ro(profile);
+#endif /* ENABLE_PROFILES_SUPPORT */
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+define_one_global_rw(auto_adjust_freq_thresholds);
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+define_one_global_ro(sampling_rate_current);
+define_one_global_rw(sampling_rate);
+define_one_global_rw(sampling_rate_idle_threshold);
+define_one_global_rw(sampling_rate_idle);
+define_one_global_rw(sampling_rate_idle_delay);
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+define_one_global_rw(disable_sleep_mode);
+define_one_global_rw(sampling_rate_sleep_multiplier);
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+define_one_global_rw(sampling_down_factor);
+define_one_global_rw(sampling_down_max_momentum);
+define_one_global_rw(sampling_down_momentum_sensitivity);
+define_one_global_rw(up_threshold);
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+define_one_global_rw(up_threshold_sleep);
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+define_one_global_rw(up_threshold_hotplug1);
+define_one_global_rw(up_threshold_hotplug_freq1);
+define_one_global_rw(block_up_multiplier_hotplug1);
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+define_one_global_rw(up_threshold_hotplug2);
+define_one_global_rw(up_threshold_hotplug_freq2);
+define_one_global_rw(block_up_multiplier_hotplug2);
+define_one_global_rw(up_threshold_hotplug3);
+define_one_global_rw(up_threshold_hotplug_freq3);
+define_one_global_rw(block_up_multiplier_hotplug3);
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+define_one_global_rw(up_threshold_hotplug4);
+define_one_global_rw(up_threshold_hotplug_freq4);
+define_one_global_rw(block_up_multiplier_hotplug4);
+define_one_global_rw(up_threshold_hotplug5);
+define_one_global_rw(up_threshold_hotplug_freq5);
+define_one_global_rw(block_up_multiplier_hotplug5);
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+define_one_global_rw(up_threshold_hotplug6);
+define_one_global_rw(up_threshold_hotplug_freq6);
+define_one_global_rw(block_up_multiplier_hotplug6);
+define_one_global_rw(up_threshold_hotplug7);
+define_one_global_rw(up_threshold_hotplug_freq7);
+define_one_global_rw(block_up_multiplier_hotplug7);
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+define_one_global_rw(down_threshold);
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+define_one_global_rw(down_threshold_sleep);
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+define_one_global_rw(down_threshold_hotplug1);
+define_one_global_rw(down_threshold_hotplug_freq1);
+define_one_global_rw(block_down_multiplier_hotplug1);
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+define_one_global_rw(down_threshold_hotplug2);
+define_one_global_rw(down_threshold_hotplug_freq2);
+define_one_global_rw(block_down_multiplier_hotplug2);
+define_one_global_rw(down_threshold_hotplug3);
+define_one_global_rw(down_threshold_hotplug_freq3);
+define_one_global_rw(block_down_multiplier_hotplug3);
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+define_one_global_rw(down_threshold_hotplug4);
+define_one_global_rw(down_threshold_hotplug_freq4);
+define_one_global_rw(block_down_multiplier_hotplug4);
+define_one_global_rw(down_threshold_hotplug5);
+define_one_global_rw(down_threshold_hotplug_freq5);
+define_one_global_rw(block_down_multiplier_hotplug5);
+#endif
+#if (MAX_CORES == 8)
+define_one_global_rw(down_threshold_hotplug6);
+define_one_global_rw(down_threshold_hotplug_freq6);
+define_one_global_rw(block_down_multiplier_hotplug6);
+define_one_global_rw(down_threshold_hotplug7);
+define_one_global_rw(down_threshold_hotplug_freq7);
+define_one_global_rw(block_down_multiplier_hotplug7);
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+define_one_global_rw(ignore_nice_load);
+define_one_global_rw(smooth_up);
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+define_one_global_rw(smooth_up_sleep);
+#ifdef ENABLE_HOTPLUGGING
+define_one_global_rw(hotplug_sleep);
+#endif /* ENABLE_HOTPLUGGING */
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+define_one_global_rw(freq_limit);
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+define_one_global_rw(freq_limit_sleep);
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+define_one_global_rw(fast_scaling_up);
+define_one_global_rw(fast_scaling_down);
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+define_one_global_rw(fast_scaling_sleep_up);
+define_one_global_rw(fast_scaling_sleep_down);
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+define_one_global_rw(afs_threshold1);
+define_one_global_rw(afs_threshold2);
+define_one_global_rw(afs_threshold3);
+define_one_global_rw(afs_threshold4);
+define_one_global_rw(grad_up_threshold);
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+define_one_global_rw(grad_up_threshold_sleep);
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+define_one_global_rw(early_demand);
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+define_one_global_rw(early_demand_sleep);
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+define_one_global_rw(disable_hotplug);
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+define_one_global_rw(disable_hotplug_sleep);
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+define_one_global_rw(hotplug_block_up_cycles);
+define_one_global_rw(hotplug_block_down_cycles);
+define_one_global_rw(hotplug_stagger_up);
+define_one_global_rw(hotplug_stagger_down);
+define_one_global_rw(hotplug_idle_threshold);
+define_one_global_rw(hotplug_idle_freq);
+define_one_global_rw(hotplug_engage_freq);
+define_one_global_rw(hotplug_max_limit);
+define_one_global_rw(hotplug_min_limit);
+define_one_global_rw(hotplug_lock);
+#endif /* ENABLE_HOTPLUGGING */
+define_one_global_rw(scaling_block_threshold);
+define_one_global_rw(scaling_block_cycles);
+define_one_global_rw(scaling_up_block_cycles);
+define_one_global_rw(scaling_up_block_freq);
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+define_one_global_rw(scaling_block_temp);
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+define_one_global_rw(scaling_trip_temp);
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+define_one_global_rw(scaling_block_freq);
+define_one_global_rw(scaling_block_force_down);
+define_one_global_rw(scaling_fastdown_freq);
+define_one_global_rw(scaling_fastdown_up_threshold);
+define_one_global_rw(scaling_fastdown_down_threshold);
+define_one_global_rw(scaling_responsiveness_freq);
+define_one_global_rw(scaling_responsiveness_up_threshold);
+define_one_global_rw(scaling_proportional);
+#ifdef ENABLE_INPUTBOOSTER
+// ff: Input Booster
+define_one_global_rw(inputboost_cycles);
+define_one_global_rw(inputboost_up_threshold);
+define_one_global_rw(inputboost_punch_cycles);
+define_one_global_rw(inputboost_punch_freq);
+define_one_global_rw(inputboost_punch_on_fingerdown);
+define_one_global_rw(inputboost_punch_on_fingermove);
+define_one_global_rw(inputboost_punch_on_epenmove);
+define_one_global_rw(inputboost_typingbooster_up_threshold);
+define_one_global_rw(inputboost_typingbooster_cores);
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_MUSIC_LIMITS
+// ff: Music Detection
+define_one_global_rw(music_max_freq);
+define_one_global_rw(music_min_freq);
+#ifdef ENABLE_HOTPLUGGING
+define_one_global_rw(music_min_cores);
+#endif /* ENABLE_HOTPLUGGING */
+define_one_global_rw(music_state);
+#endif /* ENABLE_MUSIC_LIMITS */
+// Yank: version info tunable
+static ssize_t show_version(struct device *dev, struct device_attribute *attr, char *buf)
+{
+    return sprintf(buf, "%s\n", ZZMOOVE_VERSION);
+}
+
+static DEVICE_ATTR(version, S_IRUGO , show_version, NULL);
+
+#ifdef ENABLE_PROFILES_SUPPORT
+// ZZ: profiles version info tunable
+static ssize_t show_version_profiles(struct device *dev, struct device_attribute *attr, char *buf)
+{
+    return sprintf(buf, "%s\n", profiles_file_version);
+}
+
+static DEVICE_ATTR(version_profiles, S_IRUGO , show_version_profiles, NULL);
+
+// ZZ: print out all available profiles
+static ssize_t show_profile_list(struct device *dev, struct device_attribute *attr, char *buf)
+{
+    int i = 0, c = 0;
+    char profiles[512];
+
+    for (i = 0; (zzmoove_profiles[i].profile_number != PROFILE_TABLE_END); i++) {
+	c += sprintf(profiles+c, "profile: %d " "name: %s\n", zzmoove_profiles[i].profile_number,
+	zzmoove_profiles[i].profile_name);
+    }
+    return sprintf(buf, profiles);
+}
+
+static DEVICE_ATTR(profile_list, S_IRUGO , show_profile_list, NULL);
+#endif /* ENABLE_PROFILES_SUPPORT */
+
+#ifdef ZZMOOVE_DEBUG
+// Yank: debug info
+static ssize_t show_zzmoove_debug(struct device *dev, struct device_attribute *attr, char *buf)
+{
+    return sprintf(buf, "available cores                : %d\n"
+#if (MAX_CORES == 2 || MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+			"core 0 online                  : %d\n"
+			"core 1 online                  : %d\n"
+#endif /* (MAX_CORES == 2 || MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+			"core 2 online                  : %d\n"
+			"core 3 online                  : %d\n"
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+			"core 4 online                  : %d\n"
+			"core 5 online                  : %d\n"
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+			"core 6 online                  : %d\n"
+			"core 7 online                  : %d\n"
+#endif /* (MAX_CORES == 8) */
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+			"current cpu temp               : %d C\n"
+			"scaling block temp             : %d C\n"
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+			"current cpu temp               : %d C\n"
+			"thermal trip temp              : %d C\n"
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+			"running gov instances          : %d\n"
+#ifdef ENABLE_PROFILES_SUPPORT
+			"loaded profile                 : %s\n"
+			"loaded profile number          : %d\n"
+#endif /* ENABLE_PROFILES_SUPPORT */
+			"max freq                       : %d\n"
+			"min freq                       : %d\n"
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+			"auto adjust step               : %d\n"
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+			"current load                   : %d\n"
+			"current frequency              : %d\n"
+			"current sampling rate          : %u\n"
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+			"freq limit awake               : %u\n"
+			"freq limit asleep              : %u\n"
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND) */
+			"freq table in desc order       : %d\n"
+			"freq table size                : %u\n"
+			"limit table start              : %u\n"
+			"max scaling freq hard          : %u\n"
+			"max scaling freq soft          : %u\n"
+			"freq init count                : %u\n"
+			"scaling boost                  : %d\n"
+			"scaling cancel up              : %d\n"
+			"scaling mode up                : %d\n"
+			"scaling force down             : %d\n"
+			"scaling mode down              : %d\n"
+			"scaling up threshold           : %d\n"
+#ifdef ENABLE_INPUTBOOSTER
+			"inputboost cycles              : %d\n"
+			"inputboost up threshold        : %d\n"
+			"inputboost punch cycles        : %u\n"
+			"inputboost punch freq          : %d\n"
+			"inputboost punch on finger down: %d\n"
+			"inputboost punch on finger move: %d\n"
+			"inputboost punch on epenmove   : %d\n"
+			"inputboost typing up threshold : %d\n"
+			"inputboost typing cores        : %d\n"
+#endif /* ENABLE_INPUTBOOSTER */
+			"current sampling down factor   : %d\n"
+			"current max momentum           : %d\n"
+#ifdef ENABLE_MUSIC_LIMITS
+			"music max freq                 : %d\n"
+			"music min freq                 : %d\n"
+			"music state                    : %d\n"
+#endif /* ENABLE_MUSIC_LIMITS */
+#ifdef ENABLE_HOTPLUGGING
+			"scaling down threshold         : %d\n"
+			"hotplug block up cycles        : %d\n"
+			"hotplug block down cycles      : %d\n"
+			"hotplug up threshold1          : %d\n"
+			"hotplug up threshold2          : %d\n"
+			"hotplug up threshold3          : %d\n"
+			"hotplug up threshold1 freq     : %d\n"
+			"hotplug up threshold2 freq     : %d\n"
+			"hotplug up threshold3 freq     : %d\n"
+			"hotplug down threshold1        : %d\n"
+			"hotplug down threshold2        : %d\n"
+			"hotplug down threshold3        : %d\n"
+			"hotplug down threshold1 freq   : %d\n"
+			"hotplug down threshold2 freq   : %d\n"
+			"hotplug down threshold3 freq   : %d\n",
+#else
+			"scaling down threshold         : %d\n",
+#endif /* ENABLE_HOTPLUGGING */
+			possible_cpus,
+#if (MAX_CORES == 2 || MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+			cpu_online(0),
+			cpu_online(1),
+#endif
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+			cpu_online(2),
+			cpu_online(3),
+#endif
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+			cpu_online(4),
+			cpu_online(5),
+#endif
+#if (MAX_CORES == 8)
+			cpu_online(6),
+			cpu_online(7),
+#endif
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+			cpu_temp,
+			dbs_tuners_ins.scaling_block_temp,
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+			tmu_temp_cpu,
+			dbs_tuners_ins.scaling_trip_temp,
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+			dbs_enable,
+#ifdef ENABLE_PROFILES_SUPPORT
+			dbs_tuners_ins.profile,
+			dbs_tuners_ins.profile_number,
+#endif /* PROFILES_SUPPORT */
+			pol_max,
+			pol_min,
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+			pol_step,
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+			cur_load,
+			cur_freq,
+			dbs_tuners_ins.sampling_rate_current,
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+			freq_limit_awake,
+			freq_limit_asleep,
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND) */
+			freq_table_desc,
+			freq_table_size,
+			limit_table_start,
+			max_scaling_freq_hard,
+			max_scaling_freq_soft,
+			freq_init_count,
+			boost_freq,
+			cancel_up_scaling,
+			scaling_mode_up,
+			force_down_scaling,
+			scaling_mode_down,
+			scaling_up_threshold,
+#ifdef ENABLE_INPUTBOOSTER
+			dbs_tuners_ins.inputboost_cycles,
+			dbs_tuners_ins.inputboost_up_threshold,
+			dbs_tuners_ins.inputboost_punch_cycles,
+			dbs_tuners_ins.inputboost_punch_freq,
+			dbs_tuners_ins.inputboost_punch_on_fingerdown,
+			dbs_tuners_ins.inputboost_punch_on_fingermove,
+			dbs_tuners_ins.inputboost_punch_on_epenmove,
+			dbs_tuners_ins.inputboost_typingbooster_up_threshold,
+			dbs_tuners_ins.inputboost_typingbooster_cores,
+#endif /* ENABLE_INPUTBOOSTER */
+			zz_sampling_down_factor,
+			zz_sampling_down_max_mom,
+#ifdef ENABLE_MUSIC_LIMITS
+			dbs_tuners_ins.music_max_freq,
+			dbs_tuners_ins.music_min_freq,
+			dbs_tuners_ins.music_state,
+#endif /* ENABLE_MUSIC_LIMITS */
+#ifdef ENABLE_HOTPLUGGING
+			scaling_down_threshold,
+			zz_hotplug_block_up_cycles,
+			zz_hotplug_block_down_cycles,
+			hotplug_thresholds[0][0],
+			hotplug_thresholds[0][1],
+			hotplug_thresholds[0][2],
+			hotplug_thresholds_freq[0][0],
+			hotplug_thresholds_freq[0][1],
+			hotplug_thresholds_freq[0][2],
+			hotplug_thresholds[1][0],
+			hotplug_thresholds[1][1],
+			hotplug_thresholds[1][2],
+			hotplug_thresholds_freq[1][0],
+			hotplug_thresholds_freq[1][1],
+			hotplug_thresholds_freq[1][2]);
+#else
+			scaling_down_threshold);
+#endif /* ENABLE_HOTPLUGGING */
+}
+
+static DEVICE_ATTR(zzmoove_debug, S_IRUGO , show_zzmoove_debug, NULL);
+#endif /* ZZMOOVE_DEBUG */
+
+static struct attribute *dbs_attributes[] = {
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&sampling_rate_current.attr,
+	&sampling_rate_idle_threshold.attr,
+	&sampling_rate_idle.attr,
+	&sampling_rate_idle_delay.attr,
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	&disable_sleep_mode.attr,
+	&sampling_rate_sleep_multiplier.attr,
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	&sampling_down_factor.attr,
+	&sampling_down_max_momentum.attr,
+	&sampling_down_momentum_sensitivity.attr,
+	&up_threshold.attr,
+	&down_threshold.attr,
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	&up_threshold_sleep.attr,
+	&down_threshold_sleep.attr,
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	&ignore_nice_load.attr,
+	&smooth_up.attr,
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	&smooth_up_sleep.attr,
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	&freq_limit.attr,
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	&freq_limit_sleep.attr,
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	&fast_scaling_up.attr,
+	&fast_scaling_down.attr,
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	&fast_scaling_sleep_up.attr,
+	&fast_scaling_sleep_down.attr,
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	&afs_threshold1.attr,
+	&afs_threshold2.attr,
+	&afs_threshold3.attr,
+	&afs_threshold4.attr,
+	&grad_up_threshold.attr,
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	&grad_up_threshold_sleep.attr,
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	&early_demand.attr,
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	&early_demand_sleep.attr,
+#ifdef ENABLE_HOTPLUGGING
+	&hotplug_sleep.attr,
+#endif /* ENABLE_HOTPLUGGING */
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+	&disable_hotplug.attr,
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+	&disable_hotplug_sleep.attr,
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	&hotplug_block_up_cycles.attr,
+	&hotplug_block_down_cycles.attr,
+	&hotplug_stagger_up.attr,
+	&hotplug_stagger_down.attr,
+	&hotplug_idle_threshold.attr,
+	&hotplug_idle_freq.attr,
+	&hotplug_engage_freq.attr,
+	&hotplug_max_limit.attr,
+	&hotplug_min_limit.attr,
+	&hotplug_lock.attr,
+#endif /* ENABLE_HOTPLUGGING */
+	&scaling_block_threshold.attr,
+	&scaling_block_cycles.attr,
+	&scaling_up_block_cycles.attr,
+	&scaling_up_block_freq.attr,
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+	&scaling_block_temp.attr,
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+	&scaling_trip_temp.attr,
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+	&scaling_block_freq.attr,
+	&scaling_block_force_down.attr,
+	&scaling_fastdown_freq.attr,
+	&scaling_fastdown_up_threshold.attr,
+	&scaling_fastdown_down_threshold.attr,
+	&scaling_responsiveness_freq.attr,
+	&scaling_responsiveness_up_threshold.attr,
+	&scaling_proportional.attr,
+#ifdef ENABLE_HOTPLUGGING
+	&up_threshold_hotplug1.attr,
+	&up_threshold_hotplug_freq1.attr,
+	&block_up_multiplier_hotplug1.attr,
+	&down_threshold_hotplug1.attr,
+	&down_threshold_hotplug_freq1.attr,
+	&block_down_multiplier_hotplug1.attr,
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+	&up_threshold_hotplug2.attr,
+	&up_threshold_hotplug_freq2.attr,
+	&block_up_multiplier_hotplug2.attr,
+	&down_threshold_hotplug2.attr,
+	&down_threshold_hotplug_freq2.attr,
+	&block_down_multiplier_hotplug2.attr,
+	&up_threshold_hotplug3.attr,
+	&up_threshold_hotplug_freq3.attr,
+	&block_up_multiplier_hotplug3.attr,
+	&down_threshold_hotplug3.attr,
+	&down_threshold_hotplug_freq3.attr,
+	&block_down_multiplier_hotplug3.attr,
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+	&up_threshold_hotplug4.attr,
+	&up_threshold_hotplug_freq4.attr,
+	&block_up_multiplier_hotplug4.attr,
+	&down_threshold_hotplug4.attr,
+	&down_threshold_hotplug_freq4.attr,
+	&block_down_multiplier_hotplug4.attr,
+	&up_threshold_hotplug5.attr,
+	&up_threshold_hotplug_freq5.attr,
+	&block_up_multiplier_hotplug5.attr,
+	&down_threshold_hotplug5.attr,
+	&down_threshold_hotplug_freq5.attr,
+	&block_down_multiplier_hotplug5.attr,
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+	&up_threshold_hotplug6.attr,
+	&up_threshold_hotplug_freq6.attr,
+	&block_up_multiplier_hotplug6.attr,
+	&down_threshold_hotplug6.attr,
+	&down_threshold_hotplug_freq6.attr,
+	&block_down_multiplier_hotplug6.attr,
+	&up_threshold_hotplug7.attr,
+	&up_threshold_hotplug_freq7.attr,
+	&block_up_multiplier_hotplug7.attr,
+	&down_threshold_hotplug7.attr,
+	&down_threshold_hotplug_freq7.attr,
+	&block_down_multiplier_hotplug7.attr,
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#ifdef ENABLE_INPUTBOOSTER
+	&inputboost_cycles.attr,
+	&inputboost_up_threshold.attr,
+	&inputboost_punch_cycles.attr,
+	&inputboost_punch_freq.attr,
+	&inputboost_punch_on_fingerdown.attr,
+	&inputboost_punch_on_fingermove.attr,
+	&inputboost_punch_on_epenmove.attr,
+	&inputboost_typingbooster_up_threshold.attr,
+	&inputboost_typingbooster_cores.attr,
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_MUSIC_LIMITS
+	&music_max_freq.attr,
+	&music_min_freq.attr,
+#ifdef ENABLE_HOTPLUGGING
+	&music_min_cores.attr,
+#endif /* ENABLE_HOTPLUGGING */
+	&music_state.attr,
+#endif /* ENABLE_MUSIC_LIMITS */
+	&dev_attr_version.attr,
+#ifdef ENABLE_PROFILES_SUPPORT
+	&dev_attr_version_profiles.attr,
+	&dev_attr_profile_list.attr,
+	&profile.attr,
+	&profile_number.attr,
+	&profile_sticky_mode.attr,
+#endif /* ENABLE_PROFILES_SUPPORT */
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+	&auto_adjust_freq_thresholds.attr,
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+#ifdef ZZMOOVE_DEBUG
+	&dev_attr_zzmoove_debug.attr,
+#endif /* ZZMOOVE_DEBUG */
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "zzmoove",
+};
+
+/************************** sysfs end **************************/
+
+static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
+{
+	unsigned int j, load = 0, max_load = 0;
+	struct cpufreq_policy *policy;
+#if defined(ZZMOOVE_DEBUG) && defined(ENABLE_INPUTBOOSTER)
+	struct timeval time_now;
+#endif /* ZZMOOVE_DEBUG */
+	unsigned int down_threshold_override = 0;
+#ifdef ENABLE_HOTPLUGGING
+	unsigned int num_online_cpus;
+
+	boost_hotplug = false;					// ZZ: reset early demand boost hotplug flag
+#endif /* ENABLE_HOTPLUGGING */
+	boost_freq = false;					// ZZ: reset early demand boost freq flag
+	force_down_scaling = false;				// ZZ: reset force down scaling flag
+	cancel_up_scaling = false;				// ZZ: reset cancel up scaling flag
+
+	policy = this_dbs_info->cur_policy;
+	on_cpu = policy->cpu;
+
+	if (on_cpu == 0)
+	    cur_freq = policy->cur;				// Yank: store current frequency for hotplugging frequency thresholds
+
+#ifdef ENABLE_INPUTBOOSTER
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+	if (tmu_throttle_steps < 1) {
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+		if (flg_ctr_cpuboost > 0) {
+#ifdef ZZMOOVE_DEBUG
+			pr_info("[zzmoove/dbs_check_cpu] manual max-boost call! boosting to: %d mhz %d more times, cur: %d\n", policy->max, flg_ctr_cpuboost, policy->cur);
+#endif /* ZZMOOVE_DEBUG */
+			if (policy->cur < policy->max) {
+				__cpufreq_driver_target(policy, policy->max, CPUFREQ_RELATION_H);
+			}
+
+			flg_ctr_cpuboost--;
+			flg_ctr_inputboost_punch--;
+			scaling_up_block_cycles_count = 0;
+			return;
+		}
+
+		if (flg_ctr_inputboost_punch > 0) {
+
+			// decrement now, and boost to inputboost_punch_freq later on.
+			// ctr should have anticipated the lost final cycle by padding +1.
+
+			flg_ctr_inputboost_punch--;
+
+			if (!flg_ctr_inputboost_punch) {
+#ifdef ZZMOOVE_DEBUG
+				pr_info("[zzmoove/dbs_check_cpu] inputboost - punch expired\n");
+#endif /* ZZMOOVE_DEBUG */
+			} else {
+
+				if (policy->cur < dbs_tuners_ins.inputboost_punch_freq) {
+#ifdef ZZMOOVE_DEBUG
+					// ZZ: save current time
+					do_gettimeofday(&time_now);
+
+					// ZZ: get time difference
+					time_since_touchbooster_lastrun = (time_now.tv_sec - time_touchbooster_lastrun.tv_sec) * MSEC_PER_SEC +
+										(time_now.tv_usec - time_touchbooster_lastrun.tv_usec) / USEC_PER_MSEC;
+
+					pr_info("[zzmoove/dbs_check_cpu] time since touch: %d ms\n", time_since_touchbooster_lastrun);
+					pr_info("[zzmoove/dbs_check_cpu] freq: %d too low, punching to %d immediately\n", policy->cur, dbs_tuners_ins.inputboost_punch_freq);
+#endif /* ZZMOOVE_DEBUG */
+					__cpufreq_driver_target(policy, dbs_tuners_ins.inputboost_punch_freq, CPUFREQ_RELATION_H);
+				}
+			}
+		}
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+	} else {
+		// ff: reset all boosts
+		flg_ctr_cpuboost = 0;
+		flg_ctr_inputboost = 0;
+		flg_ctr_inputboost_punch = 0;
+		flg_ctr_inputbooster_typingbooster = 0;
+
+		// ff: check for cores that need to come down
+#ifdef ENABLE_HOTPLUGGING
+		if (on_cpu == 0 && !dbs_tuners_ins.disable_hotplug && num_online_cpus() != 1)
+		    queue_work_on(0, dbs_wq, &hotplug_offline_work);
+#endif /* ENABLE_HOTPLUGGING */
+	}
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+#endif /* ENABLE_INPUTBOOSTER */
+	/*
+	 * Every sampling_rate, we check, if current idle time is less than 20%
+	 * (default), then we try to increase frequency. Every sampling_rate *
+	 * sampling_down_factor, we check, if current idle time is more than 80%
+	 * (default), then we try to decrease frequency.
+	 */
+
+	/*
+	 * ZZ: Get absolute load and make calcualtions for early demand, auto fast
+	 * scaling and scaling block functionality
+	 */
+	for_each_cpu(j, policy->cpus) {
+		struct cpu_dbs_info_s *j_dbs_info;
+		u64 cur_wall_time, cur_idle_time;
+		unsigned int idle_time, wall_time;
+
+		j_dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+
+		cur_idle_time = get_cpu_idle_time(j,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,10,0) || defined(CPU_IDLE_TIME_IN_CPUFREQ)	/* overrule for sources with backported cpufreq implementation */
+		     &cur_wall_time, 0);
+#else
+		     &cur_wall_time);
+#endif /* LINUX_VERSION_CODE... */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+		wall_time = (unsigned int)
+				(cur_wall_time - j_dbs_info->prev_cpu_wall);
+#else
+		wall_time = (unsigned int) cputime64_sub(cur_wall_time,
+				j_dbs_info->prev_cpu_wall);
+#endif /* LINUX_VERSION_CODE... */
+		j_dbs_info->prev_cpu_wall = cur_wall_time;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+		idle_time = (unsigned int)
+
+		(cur_idle_time - j_dbs_info->prev_cpu_idle);
+		j_dbs_info->prev_cpu_idle = cur_idle_time;
+#else
+		idle_time = (unsigned int) cputime64_sub(cur_idle_time,
+				j_dbs_info->prev_cpu_idle);
+		j_dbs_info->prev_cpu_idle = cur_idle_time;
+#endif /* LINUX_VERSION_CODE... */
+		if (dbs_tuners_ins.ignore_nice) {
+		    u64 cur_nice;
+		    unsigned long cur_nice_jiffies;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+		    cur_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE] -
+				 j_dbs_info->prev_cpu_nice;
+#else
+		    cur_nice = cputime64_sub(kstat_cpu(j).cpustat.nice,
+				 j_dbs_info->prev_cpu_nice);
+#endif /* LINUX_VERSION_CODE... */
+		    /*
+		     * Assumption: nice time between sampling periods will
+		     * be less than 2^32 jiffies for 32 bit sys
+		     */
+		    cur_nice_jiffies = (unsigned long)
+		    cputime64_to_jiffies64(cur_nice);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+		    j_dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+#else
+		    j_dbs_info->prev_cpu_nice = kstat_cpu(j).cpustat.nice;
+#endif /* LINUX_VERSION_CODE... */
+		    idle_time += jiffies_to_usecs(cur_nice_jiffies);
+		}
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+		    continue;
+
+		load = 100 * (wall_time - idle_time) / wall_time;
+#if defined(CONFIG_ARCH_EXYNOS4)
+		if (load > max_load)
+#endif /* CONFIG_ARCH_EXYNOS4 */
+		    cur_load = max_load = load;		// ZZ: added static cur_load for hotplugging functions
+
+		/*
+		 * ZZ: Early demand by Stratosk
+		 * Calculate the gradient of load. If it is too steep we assume
+		 * that the load will go over up_threshold in next iteration(s) and
+		 * we increase the frequency immediately
+		 *
+		 * At suspend:
+		 * Seperate early demand for suspend to be able to adjust scaling behaving at screen off and therefore to be
+		 * able to react problems which can occur because of too strictly suspend settings
+		 * So this will: boost freq and switch to fast scaling mode 2 at the same time if load is steep enough
+		 * (the value in grad_up_threshold_sleep) and in addition will lower the sleep multiplier to 2
+		 * (if it was set higher) when load goes above the value in grad_up_threshold_sleep
+		 */
+
+		if (dbs_tuners_ins.early_demand && !suspend_flag) {
+
+		    // ZZ: early demand at awake
+		    if (max_load > this_dbs_info->prev_load && max_load - this_dbs_info->prev_load
+			> dbs_tuners_ins.grad_up_threshold)
+			boost_freq = true;
+#ifdef ENABLE_HOTPLUGGING
+			boost_hotplug = true;
+#endif /* ENABLE_HOTPLUGGING */
+		// ZZ: early demand at suspend
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		} else if (dbs_tuners_ins.early_demand_sleep && suspend_flag) {
+
+			    // ZZ: check if we are over sleep threshold
+			    if (max_load > dbs_tuners_ins.grad_up_threshold_sleep
+				&& dbs_tuners_ins.sampling_rate_sleep_multiplier > 2)
+				dbs_tuners_ins.sampling_rate_current = dbs_tuners_ins.sampling_rate_idle * 2;	// ZZ: lower sleep multiplier
+			    else
+			        dbs_tuners_ins.sampling_rate_current = dbs_tuners_ins.sampling_rate_idle
+			        * dbs_tuners_ins.sampling_rate_sleep_multiplier;				// ZZ: restore sleep multiplier
+
+			    // ZZ: if load is steep enough enable freq boost and fast up scaling
+			    if (max_load > this_dbs_info->prev_load && max_load - this_dbs_info->prev_load
+			        > dbs_tuners_ins.grad_up_threshold_sleep) {
+			        boost_freq = true;								// ZZ: boost frequency
+			        scaling_mode_up = 2;								// ZZ: enable fast scaling up mode 2
+			    } else {
+			        scaling_mode_up = 0;								// ZZ: disable fast scaling again
+			    }
+		}
+#else
+		}
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND) */
+
+		/*
+		 * ZZ/Yank: Auto fast scaling mode
+		 * Switch to all 4 fast scaling modes depending on load gradient
+		 * the mode will start switching at given afs threshold load changes in both directions
+		 */
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		if ((dbs_tuners_ins.fast_scaling_up       > 4 && !suspend_flag) ||
+		    (dbs_tuners_ins.fast_scaling_sleep_up > 4 &&  suspend_flag)    ) {
+#else
+		if (dbs_tuners_ins.fast_scaling_up       > 4 && !suspend_flag) {
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND) */
+		    if (max_load > this_dbs_info->prev_load && max_load - this_dbs_info->prev_load <= dbs_tuners_ins.afs_threshold1) {
+				scaling_mode_up = 0;
+		    } else if (max_load - this_dbs_info->prev_load <= dbs_tuners_ins.afs_threshold2) {
+				scaling_mode_up = 1;
+		    } else if (max_load - this_dbs_info->prev_load <= dbs_tuners_ins.afs_threshold3) {
+				scaling_mode_up = 2;
+		    } else if (max_load - this_dbs_info->prev_load <= dbs_tuners_ins.afs_threshold4) {
+				scaling_mode_up = 3;
+		    } else {
+				scaling_mode_up = 4;
+		    }
+		}
+
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		if ((dbs_tuners_ins.fast_scaling_down       > 4 && !suspend_flag) ||
+		    (dbs_tuners_ins.fast_scaling_sleep_down > 4 &&  suspend_flag)    ) {
+#else
+		if (dbs_tuners_ins.fast_scaling_down       > 4 && !suspend_flag) {
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND) */
+		  if (max_load < this_dbs_info->prev_load && this_dbs_info->prev_load - max_load <= dbs_tuners_ins.afs_threshold1) {
+				scaling_mode_down = 0;
+		    } else if (this_dbs_info->prev_load - max_load <= dbs_tuners_ins.afs_threshold2) {
+				scaling_mode_down = 1;
+		    } else if (this_dbs_info->prev_load - max_load <= dbs_tuners_ins.afs_threshold3) {
+				scaling_mode_down = 2;
+		    } else if (this_dbs_info->prev_load - max_load <= dbs_tuners_ins.afs_threshold4) {
+				scaling_mode_down = 3;
+		    } else {
+				scaling_mode_down = 4;
+		    }
+		}
+
+		/*
+		 * ZZ: Scaling block for reducing up scaling 'overhead'
+		 *
+		 * If the given freq threshold is reached do following:
+		 * Calculate the gradient of load in both directions count them every time they are under the load threshold
+		 * and block up scaling during that time. If max count of cycles (and therefore threshold hits) are reached
+		 * switch to 'force down mode' which lowers the freq the next given block cycles. By all that we can avoid
+		 * 'sticking' on max or relatively high frequency (caused by the very fast scaling behaving of zzmoove)
+		 * when load is constantly on mid to higher load during a 'longer' peroid.
+		 *
+		 * Or if exynos4 CPU temperature reading is enabled below do following:
+		 * Use current CPU temperature as a blocking threshold to lower the frequency and therefore keep the CPU cooler.
+		 * so in particular this will lower the frequency to the frequency set in 'scaling_block_freq' and hold it
+		 * there till the temperature goes under the temperature threshold again.
+		 *
+		 * u can choose here to use either fixed blocking cycles or the temperature threshold. using fixed blocking cycles disables
+		 * temperature depending blocking. in case of temperature depending blocks u must set a target freq in scaling_block_freq
+		 * tuneable. fixed block cycle feature can still be used optional without a frequency as 'starting threshold' like before
+		 */
+
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+		if (dbs_tuners_ins.scaling_block_cycles == 0 && dbs_tuners_ins.scaling_block_freq != 0
+		    && cpu_temp >= dbs_tuners_ins.scaling_block_temp && !suspend_flag) {
+		    if (policy->cur == dbs_tuners_ins.scaling_block_freq) {
+			cancel_up_scaling = true;
+#ifdef ENABLE_HOTPLUGGING
+			hotplug_up_temp_block = true;
+#endif /* ENABLE_HOTPLUGGING */
+		    }
+		    if (policy->cur > dbs_tuners_ins.scaling_block_freq || policy->cur == policy->max) {
+			scaling_mode_down = 0;	// ZZ: if fast scaling down was enabled disable it to be sure that block freq will be met
+			force_down_scaling = true;
+		    }
+		}
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+		// ZZ: start blocking if we are not at suspend freq threshold is reached and max load is not at maximum
+		if (dbs_tuners_ins.scaling_block_cycles != 0 && policy->cur >= dbs_tuners_ins.scaling_block_freq
+		    && !suspend_flag && max_load != 100) {
+
+		    // ZZ: depending on load threshold count the gradients and block up scaling till max cycles are reached
+		    if ((scaling_block_cycles_count <= dbs_tuners_ins.scaling_block_cycles && max_load > this_dbs_info->prev_load
+			&& max_load - this_dbs_info->prev_load >= dbs_tuners_ins.scaling_block_threshold) ||
+			(scaling_block_cycles_count <= dbs_tuners_ins.scaling_block_cycles && max_load < this_dbs_info->prev_load
+			&& this_dbs_info->prev_load - max_load >= dbs_tuners_ins.scaling_block_threshold) ||
+			dbs_tuners_ins.scaling_block_threshold == 0) {
+			scaling_block_cycles_count++;							// ZZ: count gradients
+			cancel_up_scaling = true;							// ZZ: block scaling up at the same time
+		    }
+
+		    // ZZ: then switch to 'force down mode'
+		    if (scaling_block_cycles_count == dbs_tuners_ins.scaling_block_cycles) {		// ZZ: amount of cycles is reached
+			if (dbs_tuners_ins.scaling_block_force_down != 0)
+			    scaling_block_cycles_count = dbs_tuners_ins.scaling_block_cycles		// ZZ: switch to force down mode if enabled
+			    * dbs_tuners_ins.scaling_block_force_down;
+		        else
+			    scaling_block_cycles_count = 0;						// ZZ: down force disabled start from scratch
+		    }
+
+		    // ZZ: and force down scaling during next given bock cycles
+		    if (scaling_block_cycles_count > dbs_tuners_ins.scaling_block_cycles) {
+			if (unlikely(--scaling_block_cycles_count > dbs_tuners_ins.scaling_block_cycles))
+			    force_down_scaling = true;							// ZZ: force down scaling
+			else
+			    scaling_block_cycles_count = 0;						// ZZ: done -> reset counter
+		    }
+
+		}
+
+		// ZZ: used for gradient load calculation in fast scaling, scaling block and early demand
+		if (dbs_tuners_ins.early_demand || dbs_tuners_ins.scaling_block_cycles != 0
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		  || dbs_tuners_ins.fast_scaling_up > 4 || dbs_tuners_ins.fast_scaling_down > 4 || (dbs_tuners_ins.early_demand_sleep && !suspend_flag))
+		    this_dbs_info->prev_load = max_load;
+#else
+		  || dbs_tuners_ins.fast_scaling_up > 4 || dbs_tuners_ins.fast_scaling_down > 4)
+		    this_dbs_info->prev_load = max_load;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+	}
+
+#ifdef ENABLE_HOTPLUGGING
+	// ZZ: if hotplug idle threshold is reached and cpu frequency is at its minimum disable hotplug
+	if (policy->cur < dbs_tuners_ins.hotplug_idle_freq && max_load < dbs_tuners_ins.hotplug_idle_threshold
+	    && dbs_tuners_ins.hotplug_idle_threshold != 0 && !suspend_flag)
+	    hotplug_idle_flag = true;
+	else
+	    hotplug_idle_flag = false;
+
+	num_online_cpus = num_online_cpus();
+
+	// ff: calculate hotplug block multipliers
+	if (num_online_cpus == 1) {
+		// ff: only main core is online, so apply block_up for core #2 (aka 1)
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] 1 core\n");
+#endif /* ZZMOOVE_DEBUG */
+		zz_hotplug_block_up_cycles = dbs_tuners_ins.hotplug_block_up_cycles * dbs_tuners_ins.block_up_multiplier_hotplug1;
+		zz_hotplug_block_down_cycles = 0; // ff: if 1 core is online, we can't go below that, so it's a moot setting
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] 1 core - block up: %d, block down: %d\n", zz_hotplug_block_up_cycles, zz_hotplug_block_down_cycles);
+#endif /* ZZMOOVE_DEBUG */
+#if (MAX_CORES == 2 || MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+	} else if (num_online_cpus == 2) {
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] 2 cores\n");
+#endif /* ZZMOOVE_DEBUG */
+#if (MAX_CORES == 2)
+		    zz_hotplug_block_up_cycles = 0; // ff: if all cores are online, we can't go above that, so it's a moot setting
+#else
+		    zz_hotplug_block_up_cycles = dbs_tuners_ins.hotplug_block_up_cycles * dbs_tuners_ins.block_up_multiplier_hotplug2;
+		    zz_hotplug_block_down_cycles = dbs_tuners_ins.hotplug_block_down_cycles * dbs_tuners_ins.block_down_multiplier_hotplug1;
+#endif /* #if (MAX_CORES == 2) */
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] 2 cores - block up: %d, block down: %d\n", zz_hotplug_block_up_cycles, zz_hotplug_block_down_cycles);
+#endif /* ZZMOOVE_DEBUG */
+#endif /* (MAX_CORES == 2 || MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+	} else if (num_online_cpus == 3) {
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] 3 cores\n");
+#endif /* ZZMOOVE_DEBUG */
+		zz_hotplug_block_up_cycles = dbs_tuners_ins.hotplug_block_up_cycles * dbs_tuners_ins.block_up_multiplier_hotplug3;
+		zz_hotplug_block_down_cycles = dbs_tuners_ins.hotplug_block_down_cycles * dbs_tuners_ins.block_down_multiplier_hotplug2;
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] 3 cores - block up: %d, block down: %d\n", zz_hotplug_block_up_cycles, zz_hotplug_block_down_cycles);
+#endif /* ZZMOOVE_DEBUG */
+	} else if (num_online_cpus == 4) {
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] 4 cores\n");
+#endif /* ZZMOOVE_DEBUG */
+#if (MAX_CORES == 4)
+		    zz_hotplug_block_up_cycles = 0; // ff: if all cores are online, we can't go above that, so it's a moot setting
+#else
+		    zz_hotplug_block_up_cycles = dbs_tuners_ins.hotplug_block_up_cycles * dbs_tuners_ins.block_up_multiplier_hotplug4;
+		    zz_hotplug_block_down_cycles = dbs_tuners_ins.hotplug_block_down_cycles * dbs_tuners_ins.block_down_multiplier_hotplug3;
+#endif /* (MAX_CORES == 4) */
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] 4 cores - block up: %d, block down: %d\n", zz_hotplug_block_up_cycles, zz_hotplug_block_down_cycles);
+#endif /* ZZMOOVE_DEBUG */
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+	} else if (num_online_cpus == 5) {
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] 5 cores\n");
+#endif /* ZZMOOVE_DEBUG */
+		zz_hotplug_block_up_cycles = dbs_tuners_ins.hotplug_block_up_cycles * dbs_tuners_ins.block_up_multiplier_hotplug5;
+		zz_hotplug_block_down_cycles = dbs_tuners_ins.hotplug_block_down_cycles * dbs_tuners_ins.block_down_multiplier_hotplug4;
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] 5 cores - block up: %d, block down: %d\n", zz_hotplug_block_up_cycles, zz_hotplug_block_down_cycles);
+#endif /* ZZMOOVE_DEBUG */
+	} else if (num_online_cpus == 6) {
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] 6 cores\n");
+#endif /* ZZMOOVE_DEBUG */
+#if (MAX_CORES == 6)
+		    zz_hotplug_block_up_cycles = 0; // ff: if all cores are online, we can't go above that, so it's a moot setting
+#else
+		    zz_hotplug_block_up_cycles = dbs_tuners_ins.hotplug_block_up_cycles * dbs_tuners_ins.block_up_multiplier_hotplug6;
+		    zz_hotplug_block_down_cycles = dbs_tuners_ins.hotplug_block_down_cycles * dbs_tuners_ins.block_down_multiplier_hotplug5;
+#endif /* (MAX_CORES == 6) */
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] 6 cores - block up: %d, block down: %d\n", zz_hotplug_block_up_cycles, zz_hotplug_block_down_cycles);
+#endif /* ZZMOOVE_DEBUG */
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+	} else if (num_online_cpus == 7) {
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] 7 cores\n");
+#endif /* ZZMOOVE_DEBUG */
+		zz_hotplug_block_up_cycles = dbs_tuners_ins.hotplug_block_up_cycles * dbs_tuners_ins.block_up_multiplier_hotplug7;
+		zz_hotplug_block_down_cycles = dbs_tuners_ins.hotplug_block_down_cycles * dbs_tuners_ins.block_down_multiplier_hotplug6;
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] 7 cores - block up: %d, block down: %d\n", zz_hotplug_block_up_cycles, zz_hotplug_block_down_cycles);
+#endif /* ZZMOOVE_DEBUG */
+	} else if (num_online_cpus == 8) {
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] 8 cores\n");
+#endif /* ZZMOOVE_DEBUG */
+		zz_hotplug_block_up_cycles = 0; // ff: if all cores are online, we can't go above that, so it's a moot setting
+		zz_hotplug_block_down_cycles = dbs_tuners_ins.hotplug_block_down_cycles * dbs_tuners_ins.block_down_multiplier_hotplug7;
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] 8 cores - block up: %d, block down: %d\n", zz_hotplug_block_up_cycles, zz_hotplug_block_down_cycles);
+#endif /* ZZMOOVE_DEBUG */
+#endif /* (MAX_CORES == 8) */
+	}
+
+	// ff: make sure counters are synced
+	if (num_online_cpus != num_online_cpus_last) {
+		// ff: cores have been changed, counters invalid
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] reset block counters\n");
+#endif /* ZZMOOVE_DEBUG */
+		hplg_up_block_cycles = 0;
+		hplg_down_block_cycles = 0;
+	}
+
+	// ff: save how many cores were on so we'll know if they changed
+	num_online_cpus_last = num_online_cpus;
+
+	// ZZ: block cycles to be able to slow down hotplugging - added hotplug enagage freq (ffolkes)
+	// ff: also added a check to see if hotplug_max_limit is requesting only 1 core - if so, no sense in wasting time with hotplugging work
+	// ff: also added a check for hotplug_lock - if it's enabled, don't hotplug.
+	if (on_cpu == 0) {
+	    if (((!dbs_tuners_ins.disable_hotplug && num_online_cpus() != possible_cpus) || hotplug_idle_flag)
+		&& (!dbs_tuners_ins.hotplug_engage_freq || policy->cur >= dbs_tuners_ins.hotplug_engage_freq)
+		&& (!dbs_tuners_ins.hotplug_max_limit || dbs_tuners_ins.hotplug_max_limit > 1)
+		&& (!dbs_tuners_ins.hotplug_lock || num_online_cpus() > dbs_tuners_ins.hotplug_lock)) {
+		if (hplg_up_block_cycles > zz_hotplug_block_up_cycles
+		    || (!hotplug_up_in_progress && zz_hotplug_block_up_cycles == 0)) {
+			queue_work_on(0, dbs_wq, &hotplug_online_work);
+		    if (zz_hotplug_block_up_cycles != 0)
+			hplg_up_block_cycles = 0;
+		}
+		if (zz_hotplug_block_up_cycles != 0)
+		    hplg_up_block_cycles++;
+	    }
+	}
+#endif /* ENABLE_HOTPLUGGING */
+
+	// ZZ: Sampling rate idle
+	if (dbs_tuners_ins.sampling_rate_idle != dbs_tuners_ins.sampling_rate
+	    && max_load > dbs_tuners_ins.sampling_rate_idle_threshold
+	    && !suspend_flag && dbs_tuners_ins.sampling_rate_current != dbs_tuners_ins.sampling_rate) {
+	    if (sampling_rate_step_up_delay >= dbs_tuners_ins.sampling_rate_idle_delay) {
+	        dbs_tuners_ins.sampling_rate_current = dbs_tuners_ins.sampling_rate;
+	        if (dbs_tuners_ins.sampling_rate_idle_delay != 0)
+		    sampling_rate_step_up_delay = 0;
+	    }
+	    if (dbs_tuners_ins.sampling_rate_idle_delay != 0)
+	        sampling_rate_step_up_delay++;
+	}
+
+	// ZZ: Scaling fastdown and responsiveness thresholds (ffolkes)
+	if (!suspend_flag && dbs_tuners_ins.scaling_fastdown_freq && policy->cur > dbs_tuners_ins.scaling_fastdown_freq) {
+	    scaling_up_threshold = dbs_tuners_ins.scaling_fastdown_up_threshold;
+	} else if (!suspend_flag && dbs_tuners_ins.scaling_responsiveness_freq && policy->cur < dbs_tuners_ins.scaling_responsiveness_freq) {
+	    scaling_up_threshold = dbs_tuners_ins.scaling_responsiveness_up_threshold;
+	} else {
+	    scaling_up_threshold = dbs_tuners_ins.up_threshold;
+	}
+
+#ifdef ENABLE_INPUTBOOSTER
+	// ff: apply inputboost up threshold(s)
+	if (flg_ctr_inputboost > 0 && !suspend_flag) {
+		if (flg_ctr_inputbooster_typingbooster > 0) {
+			// ff: override normal inputboost_up_threshold
+			scaling_up_threshold = dbs_tuners_ins.inputboost_typingbooster_up_threshold;
+#ifdef ENABLE_HOTPLUGGING
+			if (num_online_cpus < dbs_tuners_ins.inputboost_typingbooster_cores) {
+				// ff: bring core(s) online
+				if (on_cpu == 0)
+				    queue_work_on(0, dbs_wq, &hotplug_online_work);
+			}
+#endif /* ENABLE_HOTPLUGGING */
+			flg_ctr_inputbooster_typingbooster--;
+		} else {
+			if (dbs_tuners_ins.inputboost_up_threshold) {
+				// ff: in the future there may be other boost options,
+				//     so be prepared for this one to be 0
+#ifdef ZZMOOVE_DEBUG
+				pr_info("[zzmoove] inputboost - boosting up threshold to: %d, from: %d, %d more times\n", dbs_tuners_ins.inputboost_up_threshold, scaling_up_threshold, flg_ctr_inputboost);
+#endif /* ZZMOOVE_DEBUG */
+				scaling_up_threshold = dbs_tuners_ins.inputboost_up_threshold;
+			}
+		}
+		if (scaling_up_threshold <= dbs_tuners_ins.down_threshold) {
+			// ff: we need to adjust the down_threshold. also, don't go too low
+			down_threshold_override = max(11, (int)(scaling_up_threshold - 5));
+		}
+		flg_ctr_inputboost--;
+#ifdef ZZMOOVE_DEBUG
+		if (flg_ctr_inputboost < 1) {
+			pr_info("[zzmoove/dbs_check_cpu] inputboost event expired\n");
+		}
+#endif /* ZZMOOVE_DEBUG */
+	}
+
+#ifdef ZZMOOVE_DEBUG
+	pr_info("[zzmoove/dbs_check_cpu] up_threshold: %d\n", scaling_up_threshold);
+#endif /* ZZMOOVE_DEBUG */
+#endif /* ENABLE_INPUTBOOSTER */
+	// Check for frequency increase
+	if ((max_load >= scaling_up_threshold || boost_freq) // ZZ: boost switch for early demand and scaling block switches added
+	    && !cancel_up_scaling && !force_down_scaling) {
+
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+		if (tmu_throttle_steps > 0) {
+			// ff: thermal throttling is in effect, disregard all other functions
+
+			this_dbs_info->requested_freq = zz_get_next_freq(policy->cur, 1, max_load);
+#ifdef ZZMOOVE_DEBUG
+			pr_info("[zzmoove/dbs_check_cpu] thermal throttling - cur freq: %d, max: %d, new freq: %d\n", policy->cur, policy->max, this_dbs_info->requested_freq);
+#endif /* ZZMOOVE_DEBUG */
+			__cpufreq_driver_target(policy, this_dbs_info->requested_freq,
+									CPUFREQ_RELATION_H);
+			return;
+		}
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+
+	    // ZZ: Sampling down momentum - if momentum is inactive switch to 'down_skip' method
+	    if (zz_sampling_down_max_mom == 0 && zz_sampling_down_factor > 1)
+		this_dbs_info->down_skip = 0;
+
+		// ZZ: Frequency Limit: if we are at freq_limit break out early
+		if (dbs_tuners_ins.freq_limit != 0
+			&& policy->cur == dbs_tuners_ins.freq_limit) {
+#ifdef ENABLE_MUSIC_LIMITS
+			// ff: but what if the music max freq wants to take over?
+			if (suspend_flag && dbs_tuners_ins.music_max_freq && dbs_tuners_ins.music_state && policy->cur < dbs_tuners_ins.music_max_freq) {
+				// ff: this is ugly, but this IF is so much easier like this.
+			} else {
+				return;
+			}
+#else
+				return;
+#endif /* ENABLE_MUSIC_LIMITS */
+		}
+
+	    // if we are already at full speed then break out early but not if freq limit is set
+	    if (policy->cur == policy->max && dbs_tuners_ins.freq_limit == 0)	// ZZ: changed check from reqested_freq to current freq (DerTeufel1980)
+		return;
+
+	    // ZZ: Sampling down momentum - if momentum is active and we are switching to max speed, apply sampling_down_factor
+	    if (zz_sampling_down_max_mom != 0 && policy->cur < policy->max)
+		this_dbs_info->rate_mult = zz_sampling_down_factor;
+
+		this_dbs_info->requested_freq = zz_get_next_freq(policy->cur, 1, max_load);
+
+		if (dbs_tuners_ins.freq_limit != 0
+			&& this_dbs_info->requested_freq > dbs_tuners_ins.freq_limit) {
+#ifdef ENABLE_MUSIC_LIMITS
+			// ff: right now we normally would let the freq_limit snub this, but we have to see if music needs to take over
+			if (suspend_flag && dbs_tuners_ins.music_max_freq && dbs_tuners_ins.music_state) {
+				// ff: screen is off, music freq is set, and music is playing.
+
+				// ff: make sure we haven't exceeded the music freq.
+				if (this_dbs_info->requested_freq > dbs_tuners_ins.music_max_freq) {
+					this_dbs_info->requested_freq = dbs_tuners_ins.music_max_freq;
+				}
+
+			} else {
+				this_dbs_info->requested_freq = dbs_tuners_ins.freq_limit;
+			}
+#else
+				this_dbs_info->requested_freq = dbs_tuners_ins.freq_limit;
+#endif /* ENABLE_MUSIC_LIMITS */
+		}
+
+#ifdef ENABLE_INPUTBOOSTER
+		if (flg_ctr_inputboost_punch > 0 && this_dbs_info->requested_freq < dbs_tuners_ins.inputboost_punch_freq) {
+			// ff: inputbooster punch is active and the the target freq needs to be at least that high
+#ifdef ZZMOOVE_DEBUG
+			pr_info("[zzmoove/dbs_check_cpu] inputboost - UP too low - repunched freq to %d, from %d\n", dbs_tuners_ins.inputboost_punch_freq, this_dbs_info->requested_freq);
+#endif /* ZZMOOVE_DEBUG */
+			this_dbs_info->requested_freq = dbs_tuners_ins.inputboost_punch_freq;
+		}
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_INPUTBOOSTER
+		// ff: check to see if we need to block up cycles, only do it if the screen is on, and typingbooster off
+		if (dbs_tuners_ins.scaling_up_block_cycles && !suspend_flag && flg_ctr_inputbooster_typingbooster < 1) {
+#else
+		// ff: check to see if we need to block up cycles, only do it if the screen is on, and typingbooster off
+		if (dbs_tuners_ins.scaling_up_block_cycles && !suspend_flag) {
+#endif /* ENABLE_INPUTBOOSTER */
+			// ff: if we're at or beyond the threshold frequency
+			if (policy->cur >= dbs_tuners_ins.scaling_up_block_freq) {
+
+				if (scaling_up_block_cycles_count < dbs_tuners_ins.scaling_up_block_cycles) {
+					scaling_up_block_cycles_count++;
+#ifdef ZZMOOVE_DEBUG
+					pr_info("[zzmoove/dbs_check_cpu] scaling up BLOCKED #%d - cur freq: %d, target freq: %d\n", scaling_up_block_cycles_count, policy->cur, this_dbs_info->requested_freq);
+#endif /* ZZMOOVE_DEBUG */
+					return;
+				} else {
+					scaling_up_block_cycles_count = 0;
+				}
+
+			} else {
+
+				if (policy->cur < dbs_tuners_ins.scaling_up_block_freq) {
+#ifdef ZZMOOVE_DEBUG
+					pr_info("[zzmoove/dbs_check_cpu] scaling up RESET #%d - cur freq: %d, target freq: %d\n", scaling_up_block_cycles_count, policy->cur, this_dbs_info->requested_freq);
+#endif /* ZZMOOVE_DEBUG */
+					scaling_up_block_cycles_count = 0;
+				}
+			}
+		}
+
+#ifdef ENABLE_MUSIC_LIMITS
+		if (dbs_tuners_ins.music_min_freq
+			&& this_dbs_info->requested_freq <= dbs_tuners_ins.music_min_freq
+			&& dbs_tuners_ins.music_state
+			) {
+			// ff: music is playing and there is a min set. ignore the screen-off max and set this then
+			this_dbs_info->requested_freq = dbs_tuners_ins.music_min_freq;
+		}
+#endif /* ENABLE_MUSIC_LIMITS */
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] scaling up SET - cur freq: %d, max: %d, new freq: %d\n", policy->cur, policy->max,  this_dbs_info->requested_freq);
+#endif /* ZZMOOVE_DEBUG */
+		__cpufreq_driver_target(policy, this_dbs_info->requested_freq,
+			    CPUFREQ_RELATION_H);
+
+	    // ZZ: Sampling down momentum - calculate momentum and update sampling down factor
+	    if (zz_sampling_down_max_mom != 0 && this_dbs_info->momentum_adder
+		< dbs_tuners_ins.sampling_down_mom_sens) {
+		this_dbs_info->momentum_adder++;
+		dbs_tuners_ins.sampling_down_momentum = (this_dbs_info->momentum_adder
+		* zz_sampling_down_max_mom) / dbs_tuners_ins.sampling_down_mom_sens;
+		zz_sampling_down_factor = orig_sampling_down_factor
+		+ dbs_tuners_ins.sampling_down_momentum;
+	    }
+	    return;
+	}
+
+#ifdef ENABLE_HOTPLUGGING
+	// ZZ: block cycles to be able to slow down hotplugging
+	if (on_cpu == 0) {
+	    if (!dbs_tuners_ins.disable_hotplug && num_online_cpus() != 1 && !hotplug_idle_flag) {
+		    if (unlikely(hplg_down_block_cycles >= zz_hotplug_block_down_cycles)
+			    || (!hotplug_down_in_progress && zz_hotplug_block_down_cycles == 0)) {
+#ifdef ZZMOOVE_DEBUG
+				    pr_info("[zzmoove] offline_work - %d / %d\n", hplg_down_block_cycles, zz_hotplug_block_down_cycles);
+#endif /* ZZMOOVE_DEBUG */
+					queue_work_on(0, dbs_wq, &hotplug_offline_work);
+
+					hplg_down_block_cycles = 0;
+		    }
+		    if (zz_hotplug_block_down_cycles != 0) {
+#ifdef ZZMOOVE_DEBUG
+			    pr_info("[zzmoove] skipped offline_work - %d / %d\n", hplg_down_block_cycles, zz_hotplug_block_down_cycles);
+#endif /* ZZMOOVE_DEBUG */
+			    hplg_down_block_cycles++;
+		    }
+	    }
+	}
+#endif /* ENABLE_HOTPLUGGING */
+
+	// ZZ: Sampling down momentum - if momentum is inactive switch to down skip method and if sampling_down_factor is active break out early
+	if (zz_sampling_down_max_mom == 0 && zz_sampling_down_factor > 1) {
+	    if (++this_dbs_info->down_skip < zz_sampling_down_factor)
+		return;
+	    this_dbs_info->down_skip = 0;
+	}
+
+	// ZZ: Sampling down momentum - calculate momentum and update sampling down factor
+	if (zz_sampling_down_max_mom != 0 && this_dbs_info->momentum_adder > 1) {
+	    this_dbs_info->momentum_adder -= 2;
+	    dbs_tuners_ins.sampling_down_momentum = (this_dbs_info->momentum_adder
+	    * zz_sampling_down_max_mom) / dbs_tuners_ins.sampling_down_mom_sens;
+	    zz_sampling_down_factor = orig_sampling_down_factor
+	    + dbs_tuners_ins.sampling_down_momentum;
+	}
+
+	// ZZ: Sampling rate idle
+	if (dbs_tuners_ins.sampling_rate_idle != dbs_tuners_ins.sampling_rate
+	    && max_load < dbs_tuners_ins.sampling_rate_idle_threshold && !suspend_flag
+	    && dbs_tuners_ins.sampling_rate_current != dbs_tuners_ins.sampling_rate_idle) {
+	    if (sampling_rate_step_down_delay >= dbs_tuners_ins.sampling_rate_idle_delay) {
+		dbs_tuners_ins.sampling_rate_current = dbs_tuners_ins.sampling_rate_idle;
+		if (dbs_tuners_ins.sampling_rate_idle_delay != 0)
+		    sampling_rate_step_down_delay = 0;
+	    }
+	    if (dbs_tuners_ins.sampling_rate_idle_delay != 0)
+		sampling_rate_step_down_delay++;
+	}
+
+	// ZZ: Scaling fastdown threshold (ffolkes)
+	if (!suspend_flag && dbs_tuners_ins.scaling_fastdown_freq != 0 && policy->cur > dbs_tuners_ins.scaling_fastdown_freq)
+	    scaling_down_threshold = dbs_tuners_ins.scaling_fastdown_down_threshold;
+	else
+	    scaling_down_threshold = dbs_tuners_ins.down_threshold;
+
+
+	// ff: if the up_threshold was boosted, we need to adjust this, too
+	if (down_threshold_override && down_threshold_override < scaling_down_threshold) {
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/dbs_check_cpu] DOWN - down_threshold override (from: %d, to: %d)\n", scaling_down_threshold, down_threshold_override);
+#endif /* ZZMOOVE_DEBUG */
+		scaling_down_threshold = down_threshold_override;
+	}
+
+	// Check for frequency decrease
+	if (max_load < scaling_down_threshold || force_down_scaling) {				// ZZ: added force down switch
+
+		// ZZ: Sampling down momentum - no longer fully busy, reset rate_mult
+		this_dbs_info->rate_mult = 1;
+
+		// if we cannot reduce the frequency anymore, break out early
+#ifdef ENABLE_MUSIC_LIMITS
+		if (policy->cur == policy->min || (dbs_tuners_ins.music_min_freq && dbs_tuners_ins.music_state && policy->cur == dbs_tuners_ins.music_min_freq))
+			return;
+#else
+		if (policy->cur == policy->min)
+			return;
+#endif /* ENABLE_MUSIC_LIMITS */
+		this_dbs_info->requested_freq = zz_get_next_freq(policy->cur, 2, max_load);
+
+#ifdef ENABLE_INPUTBOOSTER
+		if (flg_ctr_inputboost_punch > 0 && this_dbs_info->requested_freq < dbs_tuners_ins.inputboost_punch_freq) {
+			// ff: inputbooster punch is active and the the target freq needs to be at least that high
+#ifdef ZZMOOVE_DEBUG
+			pr_info("[zzmoove/dbs_check_cpu] inputboost - DOWN too low - repunched freq to %d, from %d\n", dbs_tuners_ins.inputboost_punch_freq, this_dbs_info->requested_freq);
+#endif /* ZZMOOVE_DEBUG */
+			this_dbs_info->requested_freq = dbs_tuners_ins.inputboost_punch_freq;
+		}
+#endif /* ENABLE_INPUTBOOSTER */
+		if (dbs_tuners_ins.freq_limit != 0 && this_dbs_info->requested_freq
+		    > dbs_tuners_ins.freq_limit)
+		    this_dbs_info->requested_freq = dbs_tuners_ins.freq_limit;
+
+#ifdef ENABLE_MUSIC_LIMITS
+		if (dbs_tuners_ins.music_min_freq
+			&& this_dbs_info->requested_freq <= dbs_tuners_ins.music_min_freq
+			&& dbs_tuners_ins.music_state
+			) {
+
+			this_dbs_info->requested_freq = dbs_tuners_ins.music_min_freq;
+		}
+#endif /* ENABLE_MUSIC_LIMITS */
+		__cpufreq_driver_target(policy, this_dbs_info->requested_freq,
+			CPUFREQ_RELATION_L);							// ZZ: changed to relation low
+		return;
+	}
+}
+
+// ZZ/ff: inputbooster work
+#ifdef ENABLE_WORK_RESTARTLOOP
+static void zz_restartloop_work(struct work_struct *work)
+{
+	struct cpu_dbs_info_s *dbs_info =
+		container_of(work, struct cpu_dbs_info_s, work.work);
+	unsigned int cpu = dbs_info->cpu;
+	work_restartloop_in_progress = true;
+#ifdef ZZMOOVE_DEBUG
+	pr_info("[zzmoove/zz_restartloop_work] restarting cycle\n");
+#endif /* ZZMOOVE_DEBUG */
+	cancel_delayed_work_sync(&dbs_info->work);
+	flush_workqueue(dbs_wq);
+	mod_delayed_work_on(cpu, dbs_wq, &dbs_info->work, 0);
+	work_restartloop_in_progress = false;
+}
+#endif /* ENABLE_WORK_RESTARTLOOP */
+
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+static void tt_reset(void)
+{
+	flg_ctr_tmu_overheating = 0;
+	tmu_throttle_steps = 0;
+	ctr_tmu_neutral = 0;
+	ctr_tmu_falling = 0;
+}
+
+// ff: Snapdragon thermal tripping
+static void tmu_check_work(struct work_struct * work_tmu_check)
+{
+	struct tsens_device tsens_dev;
+	long temp = 0;
+	int tmu_temp_delta = 0;
+	int tmu_temp_eventdelta = 0;
+
+	// ff: get temp
+	tsens_dev.sensor_num = 1;
+	tsens_get_temp(&tsens_dev, &temp);
+#ifdef ZZMOOVE_DEBUG
+	pr_info("[zzmoove/thermal] sensor: %d, value: %ld\n", tsens_dev.sensor_num, temp);
+#endif /* ZZMOOVE_DEBUG */
+	tmu_temp_cpu = temp;
+
+	// ff: check this first, since 99% of the time we'll stop here
+	if (tmu_temp_cpu < dbs_tuners_ins.scaling_trip_temp) {
+		flg_ctr_tmu_overheating = 0;
+		tt_reset();
+		tmu_temp_cpu_last = temp;
+		return;
+	}
+
+	if (tmu_temp_cpu >= 75) {
+		// ff: emergency mode, drop to min freq
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/thermal] ALERT! high temp: %ld, step throttle: %d (%d mhz)\n", temp, tmu_throttle_steps, system_freq_table[max_scaling_freq_soft].frequency);
+#endif /* ZZMOOVE_DEBUG */
+		tmu_throttle_steps = max_scaling_freq_soft;
+		return;
+
+	} else if (tmu_temp_cpu >= 70) {
+		// ff: emergency mode, drop to min freq + 4
+#ifdef ZZMOOVE_DEBUG
+		pr_info("[zzmoove/thermal] ALERT! high temp: %ld, step throttle: %d (%d mhz)\n", temp, tmu_throttle_steps, system_freq_table[max_scaling_freq_soft - 4].frequency);
+#endif /* ZZMOOVE_DEBUG */
+		tmu_throttle_steps = (max_scaling_freq_soft - 4);
+		return;
+	}
+
+	if (flg_ctr_tmu_overheating < 1) {
+		// ff: first run, not overheating
+
+		if (temp >= dbs_tuners_ins.scaling_trip_temp) {
+			flg_ctr_tmu_overheating = 1;
+			tmu_throttle_steps = 1;
+			ctr_tmu_falling = 0;
+			ctr_tmu_neutral = 0;
+#ifdef ZZMOOVE_DEBUG
+			pr_info("[zzmoove/thermal] TRIPPED - was: %d, now: %d - step throttle: %d (%d mhz)\n",
+			tmu_temp_cpu_last, tmu_temp_cpu, tmu_throttle_steps, system_freq_table[tmu_throttle_steps].frequency);
+#endif /* ZZMOOVE_DEBUG */
+		}
+
+	} else {
+		// ff: another run of overheating
+		tmu_temp_delta = (tmu_temp_cpu - tmu_temp_cpu_last);
+		tmu_temp_eventdelta = (tmu_temp_cpu - dbs_tuners_ins.scaling_trip_temp);
+
+		// ff: determine direction
+		if (tmu_temp_delta > 0) {
+
+			// ff: ascending
+			flg_ctr_tmu_overheating++;
+
+			tmu_throttle_steps = flg_ctr_tmu_overheating;
+#ifdef ZZMOOVE_DEBUG
+			pr_info("[zzmoove/thermal] RISING - was: %d, now: %d (delta: %d, eventdelta: %d) - step throttle: %d (%d mhz)\n",
+			tmu_temp_cpu_last, tmu_temp_cpu, tmu_temp_delta, tmu_temp_eventdelta, tmu_throttle_steps, system_freq_table[tmu_throttle_steps].frequency);
+#endif /* ZZMOOVE_DEBUG */
+			ctr_tmu_falling = 0;
+			ctr_tmu_neutral = 0;
+
+		} else if (tmu_temp_delta < 0) {
+
+			// ff: descending
+			ctr_tmu_falling++;
+			ctr_tmu_neutral = 0;
+
+			if (ctr_tmu_falling > 1 && tmu_temp_cpu <= (dbs_tuners_ins.scaling_trip_temp + 2)) {
+
+				ctr_tmu_falling = 0;
+
+				// ff: don't go too low
+				if (flg_ctr_tmu_overheating > max_scaling_freq_soft) {
+#ifdef ZZMOOVE_DEBUG
+					pr_info("[zzmoove/thermal] fell too low - was: %d, now; %d\n", flg_ctr_tmu_overheating, max_scaling_freq_soft);
+#endif /* ZZMOOVE_DEBUG */
+					flg_ctr_tmu_overheating = max_scaling_freq_soft;
+				}
+
+				flg_ctr_tmu_overheating--;
+
+				if (flg_ctr_tmu_overheating < 0) {
+					flg_ctr_tmu_overheating = 1;
+				}
+
+				tmu_throttle_steps = flg_ctr_tmu_overheating;
+#ifdef ZZMOOVE_DEBUG
+				pr_info("[zzmoove/thermal] FALLING - was: %d, now: %d (delta: %d, eventdelta: %d) - step throttle: %d (%d mhz)\n",
+						tmu_temp_cpu_last, tmu_temp_cpu, tmu_temp_delta, tmu_temp_eventdelta, tmu_throttle_steps, system_freq_table[tmu_throttle_steps].frequency);
+
+			} else {
+
+				pr_info("[zzmoove/thermal] FALLING - IGNORING - was: %d, now: %d (delta: %d, eventdelta: %d) - step throttle: %d (%d mhz)\n",
+						tmu_temp_cpu_last, tmu_temp_cpu, tmu_temp_delta, tmu_temp_eventdelta, tmu_throttle_steps, system_freq_table[tmu_throttle_steps].frequency);
+#endif /* ZZMOOVE_DEBUG */
+			}
+
+		} else {
+			// ff: neutral
+			ctr_tmu_neutral++;
+
+			//ctr_tmu_falling = 0;
+
+			if (ctr_tmu_neutral > 2 && tmu_temp_cpu >= (dbs_tuners_ins.scaling_trip_temp + 5)) {
+				// ff: if it has remained neutral for too long, throttle more
+
+				ctr_tmu_neutral = 0;
+
+				flg_ctr_tmu_overheating++;
+
+				tmu_throttle_steps = flg_ctr_tmu_overheating;
+#ifdef ZZMOOVE_DEBUG
+				pr_info("[zzmoove/thermal] STEADY - now: %d - step throttle: %d (%d mhz)\n", tmu_temp_cpu, tmu_throttle_steps, system_freq_table[tmu_throttle_steps].frequency);
+
+			} else {
+				pr_info("[zzmoove/thermal] STEADY - IGNORING - was: %d, now: %d (delta: %d, eventdelta: %d) - step throttle: %d (%d mhz)\n",
+						tmu_temp_cpu_last, tmu_temp_cpu, tmu_temp_delta, tmu_temp_eventdelta, tmu_throttle_steps, system_freq_table[tmu_throttle_steps].frequency);
+#endif /* ZZMOOVE_DEBUG */
+			}
+		}
+	}
+
+	// ff: save the temp for the next loop.
+	tmu_temp_cpu_last = temp;
+}
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+
+// ZZ: function for hotplug down work
+#ifdef ENABLE_HOTPLUGGING
+static void __cpuinit hotplug_offline_work_fn(struct work_struct *work)
+{
+	int cpu;	// ZZ: for hotplug down loop
+
+	hotplug_down_in_progress = true;
+
+	if (dbs_tuners_ins.hotplug_lock > 0)
+	    disable_cores = 1;
+
+	if (disable_cores > 0) {
+	    enable_disable_cores();
+	    hotplug_down_in_progress = false;
+	    return;
+	}
+
+	// Yank: added frequency thresholds
+	for_each_online_cpu(cpu) {
+		if (likely(cpu_online(cpu) && (cpu)) && cpu != 0
+			&& cur_load <= hotplug_thresholds[1][cpu-1]
+			&& (!dbs_tuners_ins.hotplug_min_limit || cpu >= dbs_tuners_ins.hotplug_min_limit)
+#ifdef ENABLE_MUSIC_LIMITS
+			&& (!dbs_tuners_ins.music_state || (dbs_tuners_ins.music_state
+			&& (!dbs_tuners_ins.music_min_cores || cpu >= dbs_tuners_ins.music_min_cores)))
+#endif /* ENABLE_MUSIC_LIMITS */
+#ifdef ENABLE_INPUTBOOSTER
+			&& (!dbs_tuners_ins.hotplug_min_limit_touchbooster || cpu >= dbs_tuners_ins.hotplug_min_limit_touchbooster)
+#endif /* ENABLE_INPUTBOOSTER */
+			&& (hotplug_thresholds_freq[1][cpu-1] == 0
+				|| cur_freq <= hotplug_thresholds_freq[1][cpu-1]
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+				|| hotplug_freq_threshold_out_of_range[1][cpu-1])
+#else
+			)
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+			) {
+#ifdef ZZMOOVE_DEBUG
+#if defined(ENABLE_MUSIC_LIMITS) && defined(ENABLE_AUTO_ADJUST_FREQ)
+			pr_info("[zzmoove/hotplug_offline_work] turning off cpu: %d, load: %d / %d, min_limit: %d music_min: %d (saved: %d), min_touchbooster: %d, freq: %d / %d, mftl: %d\n",
+				cpu, cur_load, hotplug_thresholds[1][cpu-1], dbs_tuners_ins.hotplug_min_limit, dbs_tuners_ins.music_min_cores, dbs_tuners_ins.hotplug_min_limit_saved,
+				dbs_tuners_ins.hotplug_min_limit_touchbooster, cur_freq, hotplug_thresholds_freq[1][cpu-1], hotplug_freq_threshold_out_of_range[1][cpu-1]);
+#elif defined(ENABLE_MUSIC_LIMITS) && !defined(ENABLE_AUTO_ADJUST_FREQ)
+			pr_info("[zzmoove/hotplug_offline_work] turning off cpu: %d, load: %d / %d, min_limit: %d music_min: %d (saved: %d), min_touchbooster: %d, freq: %d, mftl: %d\n",
+				cpu, cur_load, hotplug_thresholds[1][cpu-1], dbs_tuners_ins.hotplug_min_limit, dbs_tuners_ins.music_min_cores, dbs_tuners_ins.hotplug_min_limit_saved,
+				dbs_tuners_ins.hotplug_min_limit_touchbooster, cur_freq, hotplug_thresholds_freq[1][cpu-1]);
+#elif !defined(ENABLE_MUSIC_LIMITS) && defined(ENABLE_AUTO_ADJUST_FREQ)
+			pr_info("[zzmoove/hotplug_offline_work] turning off cpu: %d, load: %d / %d, min_limit: %d (saved: %d), min_touchbooster: %d, freq: %d / %d, mftl: %d\n",
+				cpu, cur_load, hotplug_thresholds[1][cpu-1], dbs_tuners_ins.hotplug_min_limit, dbs_tuners_ins.hotplug_min_limit_saved,
+				dbs_tuners_ins.hotplug_min_limit_touchbooster, cur_freq, hotplug_thresholds_freq[1][cpu-1], hotplug_freq_threshold_out_of_range[1][cpu-1]);
+#elif !defined(ENABLE_MUSIC_LIMITS) && !defined(ENABLE_AUTO_ADJUST_FREQ)
+			pr_info("[zzmoove/hotplug_offline_work] turning off cpu: %d, load: %d / %d, min_limit: %d (saved: %d), min_touchbooster: %d, freq: %d, mftl: %d\n",
+				cpu, cur_load, hotplug_thresholds[1][cpu-1], dbs_tuners_ins.hotplug_min_limit, dbs_tuners_ins.hotplug_min_limit_saved,
+				dbs_tuners_ins.hotplug_min_limit_touchbooster, cur_freq, hotplug_thresholds_freq[1][cpu-1]);
+#endif /* defined(ENABLE_MUSIC_LIMITS)... */
+#endif /* ZZMOOVE_DEBUG */
+
+#ifdef ENABLE_INPUTBOOSTER
+			// ff: don't take this cpu offline if it is less than what the typingbooster set
+			if (flg_ctr_inputbooster_typingbooster > 0 && cpu < dbs_tuners_ins.inputboost_typingbooster_cores) {
+#ifdef ZZMOOVE_DEBUG
+				pr_info("[zzmoove/hotplug_offline_work] typing booster requested cpu%d on, trying next cpu...\n", cpu);
+#endif /* ZZMOOVE_DEBUG */
+				continue;
+			}
+#endif /* ENABLE_INPUTBOOSTER */
+			if (dbs_tuners_ins.hotplug_stagger_down) {
+
+				// ff: stagger and remove core incrementally
+				if (cpu < (possible_cpus - 1) && cpu_online(cpu + 1)) {
+#ifdef ZZMOOVE_DEBUG
+					pr_info("[zzmoove/hotplug_offline_work] higher cpu (%d) is still online, trying next cpu...\n", cpu + 1);
+#endif /* ZZMOOVE_DEBUG */
+					continue;
+				}
+#ifdef ZZMOOVE_DEBUG
+				pr_info("[zzmoove/hotplug_offline_work] CPU %d OFF\n", cpu);
+#endif /* ZZMOOVE_DEBUG */
+				cpu_down(cpu);
+
+				// ff: break after a core removed
+				hotplug_down_in_progress = false;
+				return;
+
+			} else {
+				// ff: remove core normally
+#ifdef ZZMOOVE_DEBUG
+				pr_info("[zzmoove/hotplug_offline_work] CPU %d OFF\n", cpu);
+#endif /* ZZMOOVE_DEBUG */
+				cpu_down(cpu);
+			}
+		}
+	}
+	hotplug_down_in_progress = false;
+}
+
+// ZZ: function for hotplug up work
+static void __cpuinit hotplug_online_work_fn(struct work_struct *work)
+{
+	int i = 0;	// ZZ: for hotplug up loop
+
+	hotplug_up_in_progress = true;
+
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+	if (hotplug_up_temp_block) {
+	    hotplug_up_temp_block = false;
+	    hotplug_up_in_progress = false;
+	    return;
+	}
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+	if (dbs_tuners_ins.hotplug_lock > 0)
+	    disable_cores = 3;
+
+	/*
+	 * ZZ: hotplug idle flag to enable offline cores on idle to avoid higher/achieve balanced cpu load at idle
+	 * and enable cores flag to enable offline cores on governor stop and at late resume
+	 */
+	if (unlikely(hotplug_idle_flag || enable_cores > 0)){
+	    enable_disable_cores();
+	    hotplug_up_in_progress = false;
+	    return;
+	}
+#ifdef ENABLE_INPUTBOOSTER
+	if (flg_ctr_inputbooster_typingbooster > 0 && num_online_cpus() < dbs_tuners_ins.inputboost_typingbooster_cores) {
+		for (i = 1; i < num_possible_cpus(); i++) {
+			if (!cpu_online(i) && i < dbs_tuners_ins.inputboost_typingbooster_cores) {
+				cpu_up(i);
+#ifdef ZZMOOVE_DEBUG
+				pr_info("[zzmoove/hotplug_online_work_fn/typingbooster] cpu%d forced online\n", i);
+#endif /* ZZMOOVE_DEBUG */
+			}
+		}
+	}
+#endif /* ENABLE_INPUTBOOSTER */
+	// Yank: added frequency thresholds
+	for (i = 1; likely(i < possible_cpus); i++) {
+		if (!cpu_online(i) && hotplug_thresholds[0][i-1] != 0 && cur_load >= hotplug_thresholds[0][i-1]
+			&& (!dbs_tuners_ins.hotplug_max_limit || i < dbs_tuners_ins.hotplug_max_limit)
+			&& (hotplug_thresholds_freq[0][i-1] == 0 || cur_freq >= hotplug_thresholds_freq[0][i-1]
+			|| boost_hotplug
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+			|| hotplug_freq_threshold_out_of_range[0][i-1])
+#else
+			)
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+			) {
+#ifdef ZZMOOVE_DEBUG
+#if defined(ENABLE_MUSIC_LIMITS) && defined(ENABLE_AUTO_ADJUST_FREQ)
+			pr_info("[zzmoove/hotplug_online_work] turning on cpu: %d, load: %d / %d, min_limit: %d music_min: %d (saved: %d), min_touchbooster: %d, freq: %d / %d, mftl: %d\n",
+				i, cur_load, hotplug_thresholds[0][i-1], dbs_tuners_ins.hotplug_min_limit, dbs_tuners_ins.music_min_cores, dbs_tuners_ins.hotplug_min_limit_saved,
+				dbs_tuners_ins.hotplug_min_limit_touchbooster, cur_freq, hotplug_thresholds_freq[0][i-1], hotplug_freq_threshold_out_of_range[0][i-1]);
+#elif defined(ENABLE_MUSIC_LIMITS) && !defined(ENABLE_AUTO_ADJUST_FREQ)
+			pr_info("[zzmoove/hotplug_online_work] turning on cpu: %d, load: %d / %d, min_limit: %d music_min: %d (saved: %d), min_touchbooster: %d, freq: %d, mftl: %d\n",
+				i, cur_load, hotplug_thresholds[0][i-1], dbs_tuners_ins.hotplug_min_limit, dbs_tuners_ins.music_min_cores, dbs_tuners_ins.hotplug_min_limit_saved,
+				dbs_tuners_ins.hotplug_min_limit_touchbooster, cur_freq, hotplug_thresholds_freq[0][i-1]);
+#elif !defined(ENABLE_MUSIC_LIMITS) && defined(ENABLE_AUTO_ADJUST_FREQ)
+			pr_info("[zzmoove/hotplug_online_work] turning on cpu: %d, load: %d / %d, min_limit: %d (saved: %d), min_touchbooster: %d, freq: %d / %d, mftl: %d\n",
+				i, cur_load, hotplug_thresholds[0][i-1], dbs_tuners_ins.hotplug_min_limit, dbs_tuners_ins.hotplug_min_limit_saved,
+				dbs_tuners_ins.hotplug_min_limit_touchbooster, cur_freq, hotplug_thresholds_freq[0][i-1], hotplug_freq_threshold_out_of_range[0][i-1]);
+#elif !defined(ENABLE_MUSIC_LIMITS) && !defined(ENABLE_AUTO_ADJUST_FREQ)
+			pr_info("[zzmoove/hotplug_online_work] turning on cpu: %d, load: %d / %d, min_limit: %d (saved: %d), min_touchbooster: %d, freq: %d, mftl: %d\n",
+				i, cur_load, hotplug_thresholds[0][i-1], dbs_tuners_ins.hotplug_min_limit, dbs_tuners_ins.hotplug_min_limit_saved,
+				dbs_tuners_ins.hotplug_min_limit_touchbooster, cur_freq, hotplug_thresholds_freq[0][i-1]);
+#endif /* defined(ENABLE_MUSIC_LIMITS)... */
+#endif /* ZZMOOVE_DEBUG */
+			// ff: stagger and add core incrementally
+			if (dbs_tuners_ins.hotplug_stagger_up) {
+
+				if (i > 1 && !cpu_online(i - 1)) {
+#ifdef ZZMOOVE_DEBUG
+					pr_info("[zzmoove/hotplug_online_work] previous cpu (%d) was not online, aborting work\n", i - 1);
+#endif /* ZZMOOVE_DEBUG */
+					hotplug_up_in_progress = false;
+					return;
+				}
+
+#ifdef ZZMOOVE_DEBUG
+				pr_info("[zzmoove/hotplug_online_work] CPU %d ON\n", i);
+#endif /* ZZMOOVE_DEBUG */
+				cpu_up(i);
+
+				// ff: break after a core added
+				hotplug_up_in_progress = false;
+				return;
+
+			} else {
+				// ff: add core normally
+#ifdef ZZMOOVE_DEBUG
+				pr_info("[zzmoove/hotplug_online_work] CPU %d ON\n", i);
+#endif /* ZZMOOVE_DEBUG */
+				cpu_up(i);
+			}
+		}
+	}
+	hotplug_up_in_progress = false;
+}
+#endif /* ENABLE_HOTPLUGGING */
+
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+// ZZ: function for exynos4 CPU temperature reading
+static void tmu_read_temperature(struct work_struct * tmu_read_work)
+{
+	cpu_temp = get_exynos4_temperature();
+	return;
+}
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+
+static void do_dbs_timer(struct work_struct *work)
+{
+	struct cpu_dbs_info_s *dbs_info =
+		container_of(work, struct cpu_dbs_info_s, work.work);
+	unsigned int cpu = dbs_info->cpu;
+
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+	unsigned int tmu_check_delay = 0;
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+	// We want all CPUs to do sampling nearly on same jiffy
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate_current * dbs_info->rate_mult); // ZZ: Sampling down momentum - added multiplier
+
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+	// ZZ: start reading of temperature from exynos4 thermal management driver but disable it at suspend
+	if (dbs_tuners_ins.scaling_block_temp != 0) {						// ZZ: only if it is enabled and we are not at suspend
+	    if (!suspend_flag) {
+		schedule_delayed_work(&tmu_read_work, msecs_to_jiffies(DEF_TMU_READ_DELAY));	// ZZ: start work
+		temp_reading_started = true;							// ZZ: set work started flag
+		cancel_temp_reading = false;							// ZZ: reset cancel flag
+	    } else {
+		cancel_temp_reading = true;							// ZZ: else set cancel flag
+	    }
+
+	    if (temp_reading_started && cancel_temp_reading) {					// ZZ: if work was started and cancel flag was set
+		cancel_delayed_work(&tmu_read_work);						// ZZ: cancel work
+		cancel_temp_reading = false;							// ZZ: reset cancel flag
+		temp_reading_started = false;							// ZZ: reset started flag
+	    }
+	}
+
+	if (dbs_tuners_ins.scaling_block_temp == 0 && temp_reading_started)			// ZZ: if temp reading was disabled via sysfs and work was started
+	    cancel_delayed_work(&tmu_read_work);						// ZZ: cancel work
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+	if (dbs_tuners_ins.scaling_trip_temp > 0) {
+		if (!suspend_flag) {
+			tmu_check_delay = DEF_TMU_CHECK_DELAY;
+		} else {
+			tmu_check_delay = DEF_TMU_CHECK_DELAY_SLEEP;
+			if (cpu == 0)								// ZZ: only start temp reading work if we are on core 0 to avoid re-scheduling on every gov reload during hotplugging
+				schedule_delayed_work(&work_tmu_check, msecs_to_jiffies(tmu_check_delay));
+		}
+	} else {
+		tt_reset();
+	}
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+	delay -= jiffies % delay;
+
+	mutex_lock(&dbs_info->timer_mutex);
+
+	dbs_check_cpu(dbs_info);
+
+	mod_delayed_work_on(cpu, dbs_wq, &dbs_info->work, delay);
+
+	mutex_unlock(&dbs_info->timer_mutex);
+}
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
+{
+	// We want all CPUs to do sampling nearly on same jiffy
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate_current);
+	delay -= jiffies % delay;
+
+	dbs_info->enable = 1;
+
+	dbs_info_enabled = true;
+
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,7,0)
+	INIT_DEFERRABLE_WORK(&dbs_info->work, do_dbs_timer);
+#else
+	INIT_DELAYED_WORK_DEFERRABLE(&dbs_info->work, do_dbs_timer);
+#endif /* LINUX_VERSION_CODE... */
+	mod_delayed_work_on(dbs_info->cpu, dbs_wq, &dbs_info->work, delay);
+}
+
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
+{
+	dbs_info->enable = 0;
+
+	dbs_info_enabled = false;
+
+	cancel_delayed_work_sync(&dbs_info->work);
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+	cancel_delayed_work(&tmu_read_work);					// ZZ: cancel cpu temperature reading
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+}
+
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+// raise sampling rate to SR*multiplier and adjust sampling rate/thresholds/hotplug/scaling/freq limit/freq step on blank screen
+#if defined(CONFIG_HAS_EARLYSUSPEND) && !defined(USE_LCD_NOTIFIER)
+static void __cpuinit powersave_early_suspend(struct early_suspend *handler)
+#elif defined(CONFIG_POWERSUSPEND) && !defined(USE_LCD_NOTIFIER) || defined(CONFIG_POWERSUSPEND) && defined(USE_LCD_NOTIFIER)
+static void __cpuinit powersave_suspend(struct power_suspend *handler)
+#elif defined(USE_LCD_NOTIFIER)
+void zzmoove_suspend(void)
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+{
+	if (dbs_tuners_ins.disable_sleep_mode)					// ZZ: exit if sleep mode is disabled
+	    return;
+
+	if (!freq_table_desc && limit_table_start != 0)				// ZZ: asc: when entering suspend reset freq table start point to full range in case it
+	    limit_table_start = 0;						//     was changed for example because of pol min boosts - this is important otherwise
+										//     freq will stuck at soft limit and wont go below anymore!
+
+	suspend_flag = true;							// ZZ: we want to know if we are at suspend because of things that shouldn't be executed at suspend
+	sampling_rate_awake = dbs_tuners_ins.sampling_rate_current;		// ZZ: save current sampling rate for restore on awake
+	up_threshold_awake = dbs_tuners_ins.up_threshold;			// ZZ: save up threshold for restore on awake
+	down_threshold_awake = dbs_tuners_ins.down_threshold;			// ZZ: save down threhold for restore on awake
+	zz_sampling_down_max_mom = 0;						// ZZ: sampling down momentum - disabled at suspend
+	smooth_up_awake = dbs_tuners_ins.smooth_up;				// ZZ: save smooth up value for restore on awake
+	fast_scaling_up_awake = dbs_tuners_ins.fast_scaling_up;			// Yank: save scaling setting for restore on awake for upscaling
+	fast_scaling_down_awake = dbs_tuners_ins.fast_scaling_down;		// Yank: save scaling setting for restore on awake for downscaling
+#ifdef ENABLE_HOTPLUGGING
+	disable_hotplug_awake = dbs_tuners_ins.disable_hotplug;			// ZZ: save hotplug switch state for restore on awake
+	hotplug1_awake = dbs_tuners_ins.up_threshold_hotplug1;			// ZZ: save hotplug1 value for restore on awake
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+	hotplug2_awake = dbs_tuners_ins.up_threshold_hotplug2;			// ZZ: save hotplug2 value for restore on awake
+	hotplug3_awake = dbs_tuners_ins.up_threshold_hotplug3;			// ZZ: save hotplug3 value for restore on awake
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+	hotplug4_awake = dbs_tuners_ins.up_threshold_hotplug4;			// ZZ: save hotplug4 value for restore on awake
+	hotplug5_awake = dbs_tuners_ins.up_threshold_hotplug5;			// ZZ: save hotplug5 value for restore on awake
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+	hotplug6_awake = dbs_tuners_ins.up_threshold_hotplug6;			// ZZ: save hotplug6 value for restore on awake
+	hotplug7_awake = dbs_tuners_ins.up_threshold_hotplug7;			// ZZ: save hotplug7 value for restore on awake
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+	sampling_rate_asleep = dbs_tuners_ins.sampling_rate_sleep_multiplier;	// ZZ: save sleep multiplier for sleep
+	up_threshold_asleep = dbs_tuners_ins.up_threshold_sleep;		// ZZ: save up threshold for sleep
+	down_threshold_asleep = dbs_tuners_ins.down_threshold_sleep;		// ZZ: save down threshold for sleep
+	smooth_up_asleep = dbs_tuners_ins.smooth_up_sleep;			// ZZ: save smooth up for sleep
+	fast_scaling_up_asleep = dbs_tuners_ins.fast_scaling_sleep_up;		// Yank: save fast scaling for sleep for upscaling
+	fast_scaling_down_asleep = dbs_tuners_ins.fast_scaling_sleep_down;	// Yank: save fast scaling for sleep for downscaling
+#ifdef ENABLE_HOTPLUGGING
+	disable_hotplug_asleep = dbs_tuners_ins.disable_hotplug_sleep;		// ZZ: save disable hotplug switch for sleep
+#endif /* ENABLE_HOTPLUGGING */
+	dbs_tuners_ins.sampling_rate_current = dbs_tuners_ins.sampling_rate_idle
+	* sampling_rate_asleep;							// ZZ: set sampling rate for sleep
+	dbs_tuners_ins.up_threshold = up_threshold_asleep;			// ZZ: set up threshold for sleep
+	dbs_tuners_ins.down_threshold = down_threshold_asleep;			// ZZ: set down threshold for sleep
+	dbs_tuners_ins.smooth_up = smooth_up_asleep;				// ZZ: set smooth up for for sleep
+	dbs_tuners_ins.freq_limit = freq_limit_asleep;				// ZZ: set freqency limit for sleep
+	dbs_tuners_ins.fast_scaling_up = fast_scaling_up_asleep;		// Yank: set fast scaling for sleep for upscaling
+	dbs_tuners_ins.fast_scaling_down = fast_scaling_down_asleep;		// Yank: set fast scaling for sleep for downscaling
+#ifdef ENABLE_HOTPLUGGING
+	dbs_tuners_ins.disable_hotplug = disable_hotplug_asleep;		// ZZ: set hotplug switch for sleep
+#endif /* ENABLE_HOTPLUGGING */
+	evaluate_scaling_order_limit_range(0, 0, suspend_flag, 0, 0);		// ZZ: table order detection and limit optimizations
+#ifdef ENABLE_HOTPLUGGING
+	if (dbs_tuners_ins.disable_hotplug_sleep == 1) {			// ZZ: enable all cores at suspend if disable hotplug sleep is set
+		enable_cores = 1;
+		queue_work_on(0, dbs_wq, &hotplug_online_work);
+	}
+#endif /* ENABLE_HOTPLUGGING */
+	if (dbs_tuners_ins.fast_scaling_up > 4)					// Yank: set scaling mode
+	    scaling_mode_up   = 0;						// ZZ: auto fast scaling
+	else
+	    scaling_mode_up   = dbs_tuners_ins.fast_scaling_up;			// Yank: fast scaling up only
+
+	if (dbs_tuners_ins.fast_scaling_down > 4)				// Yank: set scaling mode
+	    scaling_mode_down = 0;						// ZZ: auto fast scaling
+	else
+	    scaling_mode_down = dbs_tuners_ins.fast_scaling_down;		// Yank: fast scaling up only
+#ifdef ENABLE_HOTPLUGGING
+	if (likely(dbs_tuners_ins.hotplug_sleep != 0)
+#ifdef ENABLE_MUSIC_LIMITS
+	    && !dbs_tuners_ins.music_state) {					// ZZ: if set to 0 or music state is enabled do not touch hotplugging values
+#else
+	    ) {
+#endif /* ENABLE_MUSIC_LIMITS */
+	    if (dbs_tuners_ins.hotplug_sleep == 1) {
+		dbs_tuners_ins.up_threshold_hotplug1 = 0;			// ZZ: set to one core
+		hotplug_thresholds[0][0] = 0;
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		dbs_tuners_ins.up_threshold_hotplug2 = 0;			// ZZ: set to one core
+		hotplug_thresholds[0][1] = 0;
+		dbs_tuners_ins.up_threshold_hotplug3 = 0;			// ZZ: set to one core
+		hotplug_thresholds[0][2] = 0;
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		dbs_tuners_ins.up_threshold_hotplug4 = 0;			// ZZ: set to one core
+		hotplug_thresholds[0][3] = 0;
+		dbs_tuners_ins.up_threshold_hotplug5 = 0;			// ZZ: set to one core
+		hotplug_thresholds[0][4] = 0;
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		dbs_tuners_ins.up_threshold_hotplug6 = 0;			// ZZ: set to one core
+		hotplug_thresholds[0][5] = 0;
+		dbs_tuners_ins.up_threshold_hotplug7 = 0;			// ZZ: set to one core
+		hotplug_thresholds[0][6] = 0;
+#endif /* (MAX_CORES == 8) */
+	    }
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+	    if (dbs_tuners_ins.hotplug_sleep == 2) {
+		dbs_tuners_ins.up_threshold_hotplug2 = 0;			// ZZ: set to two cores
+		hotplug_thresholds[0][1] = 0;
+		dbs_tuners_ins.up_threshold_hotplug3 = 0;			// ZZ: set to two cores
+		hotplug_thresholds[0][2] = 0;
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		dbs_tuners_ins.up_threshold_hotplug4 = 0;			// ZZ: set to two cores
+		hotplug_thresholds[0][3] = 0;
+		dbs_tuners_ins.up_threshold_hotplug5 = 0;			// ZZ: set to two cores
+		hotplug_thresholds[0][4] = 0;
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		dbs_tuners_ins.up_threshold_hotplug6 = 0;			// ZZ: set to two cores
+		hotplug_thresholds[0][5] = 0;
+		dbs_tuners_ins.up_threshold_hotplug7 = 0;			// ZZ: set to two cores
+		hotplug_thresholds[0][6] = 0;
+#endif /* (MAX_CORES == 8) */
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+	    }
+	    if (dbs_tuners_ins.hotplug_sleep == 3) {
+		dbs_tuners_ins.up_threshold_hotplug3 = 0;			// ZZ: set to three cores
+	        hotplug_thresholds[0][2] = 0;
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		dbs_tuners_ins.up_threshold_hotplug4 = 0;			// ZZ: set to three cores
+		hotplug_thresholds[0][3] = 0;
+		dbs_tuners_ins.up_threshold_hotplug5 = 0;			// ZZ: set to three cores
+		hotplug_thresholds[0][4] = 0;
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		dbs_tuners_ins.up_threshold_hotplug6 = 0;			// ZZ: set to three cores
+		hotplug_thresholds[0][5] = 0;
+		dbs_tuners_ins.up_threshold_hotplug7 = 0;			// ZZ: set to three cores
+		hotplug_thresholds[0][6] = 0;
+#endif /* (MAX_CORES == 8) */
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+	    }
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+	    if (dbs_tuners_ins.hotplug_sleep == 4) {
+		dbs_tuners_ins.up_threshold_hotplug4 = 0;			// ZZ: set to four cores
+		hotplug_thresholds[0][3] = 0;
+		dbs_tuners_ins.up_threshold_hotplug5 = 0;			// ZZ: set to four cores
+		hotplug_thresholds[0][4] = 0;
+		dbs_tuners_ins.up_threshold_hotplug6 = 0;			// ZZ: set to four cores
+		hotplug_thresholds[0][5] = 0;
+		dbs_tuners_ins.up_threshold_hotplug7 = 0;			// ZZ: set to four cores
+		hotplug_thresholds[0][6] = 0;
+	    }
+
+	    if (dbs_tuners_ins.hotplug_sleep == 5) {
+		dbs_tuners_ins.up_threshold_hotplug5 = 0;			// ZZ: set to five cores
+		hotplug_thresholds[0][4] = 0;
+		dbs_tuners_ins.up_threshold_hotplug6 = 0;			// ZZ: set to five cores
+		hotplug_thresholds[0][5] = 0;
+		dbs_tuners_ins.up_threshold_hotplug7 = 0;			// ZZ: set to five cores
+		hotplug_thresholds[0][6] = 0;
+	    }
+
+	    if (dbs_tuners_ins.hotplug_sleep == 6) {
+		dbs_tuners_ins.up_threshold_hotplug6 = 0;			// ZZ: set to six cores
+		hotplug_thresholds[0][5] = 0;
+		dbs_tuners_ins.up_threshold_hotplug7 = 0;			// ZZ: set to six cores
+		hotplug_thresholds[0][6] = 0;
+	    }
+
+	    if (dbs_tuners_ins.hotplug_sleep == 7) {
+		dbs_tuners_ins.up_threshold_hotplug7 = 0;			// ZZ: set to seven cores
+		hotplug_thresholds[0][6] = 0;
+	    }
+#endif /* (MAX_CORES == 8) */
+	}
+#endif /* ENABLE_HOTPLUGGING */
+#ifdef ENABLE_INPUTBOOSTER
+	// ff: reset some stuff.
+	flg_ctr_cpuboost = 0;
+	flg_ctr_inputboost = 0;
+	flg_ctr_inputboost_punch = 0;
+	flg_ctr_inputbooster_typingbooster = 0;
+#endif /* ENABLE_INPUTBOOSTER */
+	scaling_up_block_cycles_count = 0;
+#ifdef ZZMOOVE_DEBUG
+	pr_info("[zzmoove/lcd_notifier] Suspend function executed.\n");
+#endif /* ZZMOOVE_DEBUG */
+}
+
+#if defined(CONFIG_HAS_EARLYSUSPEND) && !defined(USE_LCD_NOTIFIER)
+static void __cpuinit powersave_late_resume(struct early_suspend *handler)
+#elif defined(CONFIG_POWERSUSPEND) && !defined(USE_LCD_NOTIFIER) || defined(CONFIG_POWERSUSPEND) && defined(USE_LCD_NOTIFIER)
+static void __cpuinit powersave_resume(struct power_suspend *handler)
+#elif defined(USE_LCD_NOTIFIER)
+void zzmoove_resume(void)
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+{
+	if (dbs_tuners_ins.disable_sleep_mode)					// ZZ: exit if sleep mode is disabled
+	    return;
+
+	suspend_flag = false;							// ZZ: we are resuming so reset supend flag
+	scaling_mode_up = 4;							// ZZ: scale up as fast as possibe
+	boost_freq = true;							// ZZ: and boost freq in addition
+
+#ifdef ENABLE_HOTPLUGGING
+	if (dbs_tuners_ins.disable_hotplug_sleep == 0) {
+	    enable_cores = 1;
+	    queue_work_on(0, dbs_wq, &hotplug_online_work); // ZZ: enable offline cores to avoid stuttering after resume if hotplugging limit was active
+	}
+#endif /* ENABLE_HOTPLUGGING */
+
+#ifdef ENABLE_INPUTBOOSTER
+	if (flg_ctr_cpuboost < 5)
+		flg_ctr_cpuboost = 5;
+#endif /* ENABLE_INPUTBOOSTER */
+
+#ifdef ENABLE_WORK_RESTARTLOOP
+	// ff: immediately call the dbs loop to apply the boost
+	if (!work_restartloop_in_progress)
+		queue_work_on(0, dbs_aux_wq, &work_restartloop);
+#endif /* ENABLE_WORK_RESTARTLOOP */
+
+#ifdef ENABLE_HOTPLUGGING
+	if (likely(dbs_tuners_ins.hotplug_sleep != 0)
+#ifdef ENABLE_MUSIC_LIMITS
+	    && !dbs_tuners_ins.music_state) {
+#else
+	    ) {
+#endif /* ENABLE_MUSIC_LIMITS */
+	    dbs_tuners_ins.up_threshold_hotplug1 = hotplug1_awake;		// ZZ: restore previous settings
+	    hotplug_thresholds[0][0] = hotplug1_awake;
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+	    dbs_tuners_ins.up_threshold_hotplug2 = hotplug2_awake;		// ZZ: restore previous settings
+	    hotplug_thresholds[0][1] = hotplug2_awake;
+	    dbs_tuners_ins.up_threshold_hotplug3 = hotplug3_awake;		// ZZ: restore previous settings
+	    hotplug_thresholds[0][2] = hotplug3_awake;
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+	    dbs_tuners_ins.up_threshold_hotplug4 = hotplug4_awake;		// ZZ: restore previous settings
+	    hotplug_thresholds[0][3] = hotplug4_awake;
+	    dbs_tuners_ins.up_threshold_hotplug5 = hotplug5_awake;		// ZZ: restore previous settings
+	    hotplug_thresholds[0][4] = hotplug5_awake;
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+	    dbs_tuners_ins.up_threshold_hotplug6 = hotplug6_awake;		// ZZ: restore previous settings
+	    hotplug_thresholds[0][5] = hotplug6_awake;
+	    dbs_tuners_ins.up_threshold_hotplug7 = hotplug7_awake;		// ZZ: restore previous settings
+	    hotplug_thresholds[0][6] = hotplug7_awake;
+#endif /* (MAX_CORES == 8) */
+	}
+#endif /* ENABLE_HOTPLUGGING */
+	zz_sampling_down_max_mom = orig_sampling_down_max_mom;			// ZZ: Sampling down momentum - restore max value
+	dbs_tuners_ins.sampling_rate_current = sampling_rate_awake;		// ZZ: restore previous settings
+	dbs_tuners_ins.up_threshold = up_threshold_awake;			// ZZ: restore previous settings
+	dbs_tuners_ins.down_threshold = down_threshold_awake;			// ZZ: restore previous settings
+	dbs_tuners_ins.smooth_up = smooth_up_awake;				// ZZ: restore previous settings
+	dbs_tuners_ins.freq_limit = freq_limit_awake;				// ZZ: restore previous settings
+	dbs_tuners_ins.fast_scaling_up   = fast_scaling_up_awake;		// Yank: restore previous settings for upscaling
+	dbs_tuners_ins.fast_scaling_down = fast_scaling_down_awake;		// Yank: restore previous settings for downscaling
+#ifdef ENABLE_HOTPLUGGING
+	dbs_tuners_ins.disable_hotplug = disable_hotplug_awake;			// ZZ: restore previous settings
+#endif /* ENABLE_HOTPLUGGING */
+	evaluate_scaling_order_limit_range(0, 0, suspend_flag, 0, 0);		// ZZ: table order detection and limit optimizations
+
+	if (dbs_tuners_ins.fast_scaling_up > 4)					// Yank: set scaling mode
+	    scaling_mode_up   = 0;						// ZZ: auto fast scaling
+	else
+	    scaling_mode_up   = dbs_tuners_ins.fast_scaling_up;			// Yank: fast scaling up only
+
+	if (dbs_tuners_ins.fast_scaling_down > 4)				// Yank: set scaling mode
+	    scaling_mode_down = 0;						// ZZ: auto fast scaling
+	else
+	    scaling_mode_down = dbs_tuners_ins.fast_scaling_down;		// Yank: fast scaling up only
+
+#ifdef ZZMOOVE_DEBUG
+	pr_info("[zzmoove/lcd_notifier] Resume function executed.\n");
+#endif /* ZZMOOVE_DEBUG */
+}
+
+#if defined(CONFIG_HAS_EARLYSUSPEND) && !defined(USE_LCD_NOTIFIER) && !defined(DISABLE_POWER_MANAGEMENT)
+static struct early_suspend __refdata _powersave_early_suspend = {
+  .suspend = powersave_early_suspend,
+  .resume = powersave_late_resume,
+  .level = EARLY_SUSPEND_LEVEL_BLANK_SCREEN,
+};
+#elif defined(CONFIG_POWERSUSPEND) && defined(USE_LCD_NOTIFIER) && !defined (DISABLE_POWER_MANAGEMENT) || defined(CONFIG_POWERSUSPEND) && !defined(USE_LCD_NOTIFIER) && !defined (DISABLE_POWER_MANAGEMENT)
+static struct power_suspend __refdata powersave_powersuspend = {
+  .suspend = powersave_suspend,
+  .resume = powersave_resume,
+};
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int rc;
+#if defined(ENABLE_HOTPLUGGING) && !defined(SNAP_NATIVE_HOTPLUGGING)
+	int i = 0;
+#endif /* ENABLE_HOTPLUGGING */
+	this_dbs_info = &per_cpu(cs_cpu_dbs_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy->cur))
+		    return -EINVAL;
+		mutex_lock(&dbs_mutex);
+
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			j_dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,10,0) || defined(CPU_IDLE_TIME_IN_CPUFREQ)	/* ZZ: overrule for sources with backported cpufreq implementation */
+			&j_dbs_info->prev_cpu_wall, 0);
+#else
+			&j_dbs_info->prev_cpu_wall);
+#endif /* LINUX_VERSION_CODE... */
+			if (dbs_tuners_ins.ignore_nice) {
+			    j_dbs_info->prev_cpu_nice =
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+			    kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+#else
+			    kstat_cpu(j).cpustat.nice;
+#endif /* LINUX_VERSION_CODE... */
+			}
+			j_dbs_info->time_in_idle = get_cpu_idle_time_us(cpu, &j_dbs_info->idle_exit_time); // ZZ: idle exit time handling
+		}
+		this_dbs_info->cpu = cpu;					// ZZ: initialise the cpu field during governor start
+		this_dbs_info->rate_mult = 1;					// ZZ: sampling down momentum - reset multiplier
+		this_dbs_info->momentum_adder = 0;				// ZZ: sampling down momentum - reset momentum adder
+		this_dbs_info->down_skip = 0;					// ZZ: sampling down - reset down_skip
+		this_dbs_info->requested_freq = policy->cur;
+
+		// ZZ: get freq table, available cpus for hotplugging and optimize/detect scaling range
+#ifdef ENABLE_HOTPLUGGING
+		possible_cpus = num_possible_cpus();
+#endif /* ENABLE_HOTPLUGGING */
+		if (cpu == 0) {
+		    freq_init_count = 0;					// ZZ: reset init flag for governor reload
+		    system_freq_table = cpufreq_frequency_get_table(0);		// ZZ: update static system frequency table
+		    evaluate_scaling_order_limit_range(1, 0, 0, policy->min, policy->max);	// ZZ: table order detection and limit optimizations
+		}
+#if defined(ENABLE_HOTPLUGGING) && !defined(SNAP_NATIVE_HOTPLUGGING)
+		// ZZ: save default values in threshold array
+		for (i = 0; i < possible_cpus; i++) {
+		    hotplug_thresholds[0][i] = DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG;
+		    hotplug_thresholds[1][i] = DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG;
+		}
+#endif /* ENABLE_HOTPLUGGING */
+		mutex_init(&this_dbs_info->timer_mutex);
+		dbs_enable++;
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 1) {
+		    unsigned int latency;
+		    // policy latency is in nS. Convert it to uS first
+		    latency = policy->cpuinfo.transition_latency / 1000;
+		    if (latency == 0)
+			latency = 1;
+
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&dbs_attr_group);
+			if (rc) {
+			    mutex_unlock(&dbs_mutex);
+			    return rc;
+			}
+
+			/*
+			 * conservative does not implement micro like ondemand
+			 * governor, thus we are bound to jiffes/HZ
+			 */
+			min_sampling_rate =
+				MIN_SAMPLING_RATE_RATIO * jiffies_to_usecs(3);
+			// Bring kernel and HW constraints together
+			min_sampling_rate = max(min_sampling_rate,
+					MIN_LATENCY_MULTIPLIER * latency);
+			dbs_tuners_ins.sampling_rate_current =
+				max(min_sampling_rate,
+				    latency * LATENCY_MULTIPLIER);
+#ifdef ENABLE_PROFILES_SUPPORT
+#if (DEF_PROFILE_NUMBER > 0)
+			set_profile(DEF_PROFILE_NUMBER);
+#endif /* (DEF_PROFILE_NUMBER > 0) */
+#endif /* ENABLE_PROFILES_SUPPORT */
+			// ZZ: Sampling down momentum - set down factor and max momentum
+			orig_sampling_down_factor = zz_sampling_down_factor;
+			orig_sampling_down_max_mom = zz_sampling_down_max_mom;
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+			sampling_rate_awake = dbs_tuners_ins.sampling_rate
+			= dbs_tuners_ins.sampling_rate_current;
+#else
+			dbs_tuners_ins.sampling_rate
+			= dbs_tuners_ins.sampling_rate_current;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+			up_threshold_awake = dbs_tuners_ins.up_threshold;
+			down_threshold_awake = dbs_tuners_ins.down_threshold;
+			smooth_up_awake = dbs_tuners_ins.smooth_up;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+			// ZZ: switch to proportional scaling if we didn't get system freq table
+			if (!system_freq_table) {
+			    printk(KERN_ERR "[zzmoove] Failed to get system freq table! falling back to proportional scaling!\n");
+			    dbs_tuners_ins.scaling_proportional = 2;
+			}
+
+			cpufreq_register_notifier(
+					&dbs_cpufreq_notifier_block,
+					CPUFREQ_TRANSITION_NOTIFIER);
+#ifdef ENABLE_INPUTBOOST
+			if (dbs_tuners_ins.inputboost_cycles) {
+				rc = input_register_handler(&interactive_input_handler);
+				if (!rc)
+					pr_info("[zzmoove/store_inputboost_cycles] inputbooster - registered\n");
+				else
+					pr_info("[zzmoove/store_inputboost_cycles] inputbooster - register FAILED\n");
+				rc = 0;
+			}
+#endif /* ENABLE_INPUTBOOST */
+		}
+		mutex_unlock(&dbs_mutex);
+		dbs_timer_init(this_dbs_info);
+#if defined(CONFIG_HAS_EARLYSUSPEND) && !defined(USE_LCD_NOTIFIER) && !defined(DISABLE_POWER_MANAGEMENT)
+		register_early_suspend(&_powersave_early_suspend);
+#elif defined(CONFIG_POWERSUSPEND) && !defined(USE_LCD_NOTIFIER) && !defined(DISABLE_POWER_MANAGEMENT) || defined(CONFIG_POWERSUSPEND) && defined(USE_LCD_NOTIFIER) && !defined(DISABLE_POWER_MANAGEMENT)
+		if (cpu == 0)
+		    register_power_suspend(&powersave_powersuspend);
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		/*
+		 * ZZ: enable all cores to avoid cores staying in offline state
+		 * when changing to a non-hotplugging-able governor
+		 */
+#if defined(ENABLE_HOTPLUGGING) && !defined(SNAP_NATIVE_HOTPLUGGING)
+		if (cpu == 0) {
+		    enable_cores = 1;
+		    queue_work_on(0, dbs_wq, &hotplug_online_work);			// ZZ: enable offline cores
+		}
+#endif /* defined(ENABLE_HOTPLUGGING)... */
+		dbs_timer_exit(this_dbs_info);
+
+		this_dbs_info->idle_exit_time = 0;					// ZZ: idle exit time handling
+
+		mutex_lock(&dbs_mutex);
+		dbs_enable--;
+		mutex_destroy(&this_dbs_info->timer_mutex);
+		/*
+		 * Stop the timerschedule work, unregister input handler and reset various tuneables when this governor
+		 * is used for the last time
+		 */
+		if (dbs_enable == 0) {
+#ifdef ENABLE_INPUTBOOST
+		    if (!policy->cpu && dbs_tuners_ins.inputboost_cycles)
+			input_unregister_handler(&interactive_input_handler);
+#endif /* ENABLE_INPUTBOOST */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+		    cancel_delayed_work(&work_tmu_check);				// ZZ: cancel cpu temperature reading when leaving the governor
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+#if (defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) && !defined(DISABLE_POWER_MANAGEMENT)) || defined(USE_LCD_NOTIFIER)
+		    dbs_tuners_ins.disable_sleep_mode = DEF_DISABLE_SLEEP_MODE;
+#endif /* (defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_MUSIC_LIMITS
+		    dbs_tuners_ins.music_state = 0;
+#endif /* ENABLE_MUSIC_LIMITS */
+		    cpufreq_unregister_notifier(
+		    &dbs_cpufreq_notifier_block,
+		    CPUFREQ_TRANSITION_NOTIFIER);
+		}
+
+		mutex_unlock(&dbs_mutex);
+		if (!dbs_enable)
+		    sysfs_remove_group(cpufreq_global_kobject,
+		   &dbs_attr_group);
+#if defined(CONFIG_HAS_EARLYSUSPEND) && !defined(USE_LCD_NOTIFIER) && !defined(DISABLE_POWER_MANAGEMENT)
+		unregister_early_suspend(&_powersave_early_suspend);
+#elif defined(CONFIG_POWERSUSPEND) && !defined(USE_LCD_NOTIFIER) && !defined(DISABLE_POWER_MANAGEMENT) || defined(CONFIG_POWERSUSPEND) && defined(USE_LCD_NOTIFIER) && !defined(DISABLE_POWER_MANAGEMENT)
+		if (cpu == 0)
+		    unregister_power_suspend(&powersave_powersuspend);
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+
+		// ZZ: save min/max policy only once from core 0 for freq thresholds ajustment
+		if (cpu == 0) {
+		    pol_max = policy->max;
+		    pol_min = policy->min;
+		}
+
+		mutex_lock(&this_dbs_info->timer_mutex);
+		    __cpufreq_driver_target(this_dbs_info->cur_policy,
+				policy->cur, CPUFREQ_RELATION_L);
+		dbs_check_cpu(this_dbs_info);
+		mutex_unlock(&this_dbs_info->timer_mutex);
+
+		/*
+		 * ZZ: here again table order detection and limit optimizations
+		 * in case max freq has changed after gov start and before
+		 * Limit case due to apply timing issues. now we should be able to
+		 * catch all freq max changes during start of the governor
+		 */
+		if (cpu == 0)
+		    evaluate_scaling_order_limit_range(0, 1, suspend_flag, policy->min, policy->max);
+
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+		// ZZ: do this only on core 0
+		if (cpu == 0) {
+		    if (old_pol_max == 0)						// ZZ: initialize var if we start the first time
+			old_pol_max = policy->max;
+
+		    if (dbs_tuners_ins.auto_adjust_freq_thresholds != 0) {
+			if (old_pol_max != policy->max) {
+			    pol_step = (old_pol_max / 100000) - (policy->max / 100000);	// ZZ: truncate and calculate step
+			    pol_step *= 100000;						// ZZ: bring it back to kHz
+			    pol_step *= -1;						// ZZ: invert for proper addition
+			} else {
+			    pol_step = 0;
+			}
+			old_pol_max = policy->max;
+		    }
+
+		adjust_freq_thresholds(pol_step);					// ZZ: adjust thresholds
+		}
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+		this_dbs_info->time_in_idle
+		= get_cpu_idle_time_us(cpu, &this_dbs_info->idle_exit_time);		// ZZ: idle exit time handling
+		break;
+	}
+	return 0;
+}
+
+#if (defined(USE_LCD_NOTIFIER) && !defined(CONFIG_POWERSUSPEND))
+// AP: callback handler for lcd notifier
+static int zzmoove_lcd_notifier_callback(struct notifier_block *this,
+								unsigned long event, void *data)
+{
+	switch (event)
+	{
+		case LCD_EVENT_OFF_END:
+
+			if (!suspend_flag)
+			    zzmoove_suspend();
+#ifdef ZZMOOVE_DEBUG
+			pr_info("[zzmoove/lcd_notifier] Screen switched off.\n");
+#endif /* ZZMOOVE_DEBUG */
+			break;
+
+		case LCD_EVENT_ON_START:
+
+			if (suspend_flag)
+			    zzmoove_resume();
+#ifdef ZZMOOVE_DEBUG
+			pr_info("[zzmoove/lcd_notifier] Screen switched on.\n");
+#endif /* ZZMOOVE_DEBUG */
+			break;
+
+		default:
+			break;
+	}
+return 0;
+}
+#endif /* (defined(USE_LCD_NOTIFIER) */
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_ZZMOOVE
+static
+#endif /* CONFIG_CPU_FREQ_DEFAULT_GOV_ZZMOOVE */
+struct cpufreq_governor cpufreq_gov_zzmoove = {
+	.name			= "zzmoove",
+	.governor		= cpufreq_governor_dbs,
+	.max_transition_latency	= TRANSITION_LATENCY_LIMIT,
+	.owner			= THIS_MODULE,
+};
+
+static int __init cpufreq_gov_dbs_init(void)						// ZZ: idle exit time handling
+{
+    unsigned int i;
+    struct cpu_dbs_info_s *this_dbs_info;
+    // Initalize per-cpu data:
+    for_each_possible_cpu(i) {
+	this_dbs_info = &per_cpu(cs_cpu_dbs_info, i);
+	this_dbs_info->time_in_idle = 0;
+	this_dbs_info->idle_exit_time = 0;
+    }
+
+    dbs_wq = alloc_workqueue("zzmoove_dbs_wq", WQ_HIGHPRI, 0);
+#ifdef ENABLE_WORK_RESTARTLOOP
+    dbs_aux_wq = alloc_workqueue("zzmoove_dbs_aux_wq", WQ_HIGHPRI, 0);
+
+    if (!dbs_aux_wq) {
+	printk(KERN_ERR "[zzmoove] Failed to create zzmoove_dbs_aux_wq workqueue!\n");
+	return -EFAULT;
+    }
+#endif /* ENABLE_WORK_RESTARTLOOP */
+    if (!dbs_wq) {
+	printk(KERN_ERR "[zzmoove] Failed to create zzmoove_dbs_wq workqueue!\n");
+	return -EFAULT;
+    }
+
+#ifdef ENABLE_WORK_RESTARTLOOP
+    INIT_WORK(&work_restartloop, zz_restartloop_work);
+#endif /* ENABLE_WORK_RESTARTLOOP */
+
+#ifdef ENABLE_HOTPLUGGING
+    INIT_WORK(&hotplug_offline_work, hotplug_offline_work_fn);				// ZZ: init hotplug offline work
+    INIT_WORK(&hotplug_online_work, hotplug_online_work_fn);				// ZZ: init hotplug online work
+#endif /* ENABLE_HOTPLUGGING */
+
+#if (defined(USE_LCD_NOTIFIER) && !defined(CONFIG_POWERSUSPEND))
+	// AP: register callback handler for lcd notifier
+	zzmoove_lcd_notif.notifier_call = zzmoove_lcd_notifier_callback;
+	if (lcd_register_client(&zzmoove_lcd_notif) != 0) {
+		pr_err("%s: Failed to register lcd callback\n", __func__);
+		return -EFAULT;
+	}
+#endif /* (defined(USE_LCD_NOTIFIER)... */
+	return cpufreq_register_governor(&cpufreq_gov_zzmoove);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_zzmoove);
+	destroy_workqueue(dbs_wq);
+#ifdef ENABLE_WORK_RESTARTLOOP
+	destroy_workqueue(dbs_aux_wq);
+#endif /* ENABLE_WORK_RESTARTLOOP */
+
+#if (defined(USE_LCD_NOTIFIER) && !defined(CONFIG_POWERSUSPEND))
+	lcd_unregister_client(&zzmoove_lcd_notif);
+#endif /* (defined(USE_LCD_NOTIFIER)... */
+}
+
+MODULE_AUTHOR("Zane Zaminsky <cyxman@yahoo.com>");
+MODULE_DESCRIPTION("'cpufreq_zzmoove' - A dynamic cpufreq governor based "
+		"on smoove governor from Michael Weingaertner which was originally based on "
+		"cpufreq_conservative from Alexander Clouter. Optimized for use with Samsung I9300 "
+		"using a fast scaling and CPU hotplug logic - ported/modified/optimized for I9300 "
+		"since November 2012 and further improved for exynos and snapdragon platform "
+		"by ZaneZam,Yank555 and ffolkes in 2013/14/15");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_ZZMOOVE
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif /* CONFIG_CPU_FREQ_DEFAULT_GOV_ZZMOOVE */
+module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_zzmoove_profiles.h b/drivers/cpufreq/cpufreq_zzmoove_profiles.h
new file mode 100755
index 0000000..345adba
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_zzmoove_profiles.h
@@ -0,0 +1,2332 @@
+/*
+ * drivers/cpufreq/cpufreq_zzmoove_profiles.h - Profiles
+ *
+ * Copyright (C)  2013 Jean-Pierre Rasquin <yank555.lu@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * ZZMoove Governor profiles header file modified by Zane Zaminsky 2013/14
+ *
+ * currently available profiles by ZaneZam and Yank555:
+ * ------------------------------------------------------------------------------------------------------------------------------------------
+ * -  (1)'def'    -> Default              -> will set governor defaults                                                                     -
+ * ------------------------------------------------------------------------------------------------------------------------------------------
+ * -  (2)'ybat    -> Yank Battery         -> a very good battery/performance balanced setting                                               -
+ * -                                         DEV-NOTE: highly recommended!                                                                  -
+ * ------------------------------------------------------------------------------------------------------------------------------------------
+ * -  (3)'ybatext'-> Yank Battery Extreme -> like yank battery but focus on battery saving                                                  -
+ * ------------------------------------------------------------------------------------------------------------------------------------------
+ * -  (4)'zzbat'  -> ZaneZam Battery      -> a more 'harsh' setting strictly focused on battery saving                                      -
+ * -                                         DEV-NOTE: might give some lags!                                                                -
+ * ------------------------------------------------------------------------------------------------------------------------------------------
+ * -  (5)'zzbatp' -> ZaneZam Battery Plus -> NEW! reworked 'faster' battery setting                                                         -
+ * -                                         DEV-NOTE: recommended too!:)                                                                   -
+ * ------------------------------------------------------------------------------------------------------------------------------------------
+ * -  (6)'zzopt'  -> ZaneZam Optimized    -> balanced setting with no focus in any direction                                                -
+ * -                                         DEV-NOTE: relict from back in the days, even though some people still like it!                 -
+ * ------------------------------------------------------------------------------------------------------------------------------------------
+ * -  (7)'zzmod'  -> ZaneZam Moderate     -> NEW! setting based on 'zzopt' which has mainly (but not strictly only!) 2 cores online         -
+ * ------------------------------------------------------------------------------------------------------------------------------------------
+ * -  (8)'zzperf' -> ZaneZam Performance  -> all you can get from zzmoove in terms of performance but still has the fast                    -
+ * -                                         down scaling/hotplugging behaving                                                              -
+ * ------------------------------------------------------------------------------------------------------------------------------------------
+ * -  (9)'zzinz'  -> ZaneZam InZane       -> NEW! based on performance with new insane scaling active. a new experience!                    -
+ * ------------------------------------------------------------------------------------------------------------------------------------------
+ * - (10)'zzgame' -> ZaneZam Gaming       -> NEW! based on performance with scaling block enabled to avoid cpu overheating during gameplay  -
+ * ------------------------------------------------------------------------------------------------------------------------------------------
+ * - (11)'zzrelax'-> ZaneZam Relax        -> NEW! based on moderate (except hotplug settings) with relaxed sleep settings                   -
+ * ------------------------------------------------------------------------------------------------------------------------------------------
+ *
+ * NOTE: be aware when setting tuneables which have a 'should' in comments below that giving
+ *       them 'wrong' values can lead to odd hotplug behaving!
+ */
+
+// NOTE: profile values in this version are mainly for Snapdragon devices!
+static char profiles_file_version[20] = "develop-24.09.15";
+#define PROFILE_TABLE_END ~1
+#define END_OF_PROFILES "end"
+#define PROFILE_MAX_FREQ (2880000)	// ZZ: max possible freq in system table for freq adaption (possible OC frequencies inclusive)
+
+struct zzmoove_profile {
+	unsigned int profile_number;
+	char         profile_name[20];
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+	unsigned int auto_adjust_freq_thresholds;
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+#ifdef ENABLE_HOTPLUGGING
+	unsigned int disable_hotplug;
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+	unsigned int disable_hotplug_sleep;
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#endif /* ENABLE_HOTPLUGGING */
+	unsigned int down_threshold;
+#ifdef ENABLE_HOTPLUGGING
+	unsigned int down_threshold_hotplug1;
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+	unsigned int down_threshold_hotplug2;
+	unsigned int down_threshold_hotplug3;
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+	unsigned int down_threshold_hotplug4;
+	unsigned int down_threshold_hotplug5;
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+	unsigned int down_threshold_hotplug6;
+	unsigned int down_threshold_hotplug7;
+#endif /* (MAX_CORES == 8) */
+	unsigned int down_threshold_hotplug_freq1;
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+	unsigned int down_threshold_hotplug_freq2;
+	unsigned int down_threshold_hotplug_freq3;
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+	unsigned int down_threshold_hotplug_freq4;
+	unsigned int down_threshold_hotplug_freq5;
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+	unsigned int down_threshold_hotplug_freq6;
+	unsigned int down_threshold_hotplug_freq7;
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+	unsigned int down_threshold_sleep;
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	unsigned int early_demand;
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+	unsigned int early_demand_sleep;
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	unsigned int fast_scaling_up;
+	unsigned int fast_scaling_down;
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+	unsigned int fast_scaling_sleep_up;
+	unsigned int fast_scaling_sleep_down;
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	unsigned int afs_threshold1;
+	unsigned int afs_threshold2;
+	unsigned int afs_threshold3;
+	unsigned int afs_threshold4;
+	unsigned int freq_limit;
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+	unsigned int freq_limit_sleep;
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	unsigned int grad_up_threshold;
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+	unsigned int grad_up_threshold_sleep;
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+	unsigned int hotplug_block_up_cycles;
+	unsigned int block_up_multiplier_hotplug1;
+	unsigned int block_up_multiplier_hotplug2;
+	unsigned int block_up_multiplier_hotplug3;
+	unsigned int block_up_multiplier_hotplug4;
+	unsigned int block_up_multiplier_hotplug5;
+	unsigned int block_up_multiplier_hotplug6;
+	unsigned int block_up_multiplier_hotplug7;
+	unsigned int hotplug_block_down_cycles;
+	unsigned int block_down_multiplier_hotplug1;
+	unsigned int block_down_multiplier_hotplug2;
+	unsigned int block_down_multiplier_hotplug3;
+	unsigned int block_down_multiplier_hotplug4;
+	unsigned int block_down_multiplier_hotplug5;
+	unsigned int block_down_multiplier_hotplug6;
+	unsigned int block_down_multiplier_hotplug7;
+	unsigned int hotplug_stagger_up;
+	unsigned int hotplug_stagger_down;
+	unsigned int hotplug_idle_threshold;
+	unsigned int hotplug_idle_freq;
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+	unsigned int hotplug_sleep;
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	unsigned int hotplug_engage_freq;
+	unsigned int hotplug_max_limit;
+	unsigned int hotplug_min_limit;
+	unsigned int hotplug_lock;
+#endif /* ENABLE_HOTPLUGGING */
+	unsigned int ignore_nice_load;
+	unsigned int sampling_down_factor;
+	unsigned int sampling_down_max_momentum;
+	unsigned int sampling_down_momentum_sensitivity;
+	unsigned int sampling_rate;
+	unsigned int sampling_rate_idle;
+	unsigned int sampling_rate_idle_delay;
+	unsigned int sampling_rate_idle_threshold;
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+	unsigned int sampling_rate_sleep_multiplier;
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	unsigned int scaling_block_cycles;
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+	unsigned int scaling_block_temp;
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+	unsigned int scaling_trip_temp;
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+	unsigned int scaling_block_freq;
+	unsigned int scaling_block_threshold;
+	unsigned int scaling_block_force_down;
+	unsigned int scaling_fastdown_freq;
+	unsigned int scaling_fastdown_up_threshold;
+	unsigned int scaling_fastdown_down_threshold;
+	unsigned int scaling_responsiveness_freq;
+	unsigned int scaling_responsiveness_up_threshold;
+	unsigned int scaling_proportional;
+#ifdef ENABLE_INPUTBOOSTER
+	unsigned int inputboost_cycles;
+	unsigned int inputboost_up_threshold;
+	unsigned int inputboost_punch_cycles;
+	unsigned int inputboost_punch_freq;
+	unsigned int inputboost_punch_on_fingerdown;
+	unsigned int inputboost_punch_on_fingermove;
+	unsigned int inputboost_punch_on_epenmove;
+	unsigned int inputboost_typingbooster_up_threshold;
+	unsigned int inputboost_typingbooster_cores;
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_MUSIC_LIMITS
+	unsigned int music_max_freq;
+	unsigned int music_min_freq;
+	unsigned int music_min_cores;
+#endif /* ENABLE_MUSIC_LIMITS */
+	unsigned int smooth_up;
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+	unsigned int smooth_up_sleep;
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	unsigned int up_threshold;
+#ifdef ENABLE_HOTPLUGGING
+	unsigned int up_threshold_hotplug1;
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+	unsigned int up_threshold_hotplug2;
+	unsigned int up_threshold_hotplug3;
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+	unsigned int up_threshold_hotplug4;
+	unsigned int up_threshold_hotplug5;
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+	unsigned int up_threshold_hotplug6;
+	unsigned int up_threshold_hotplug7;
+#endif /* (MAX_CORES == 8) */
+	unsigned int up_threshold_hotplug_freq1;
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+	unsigned int up_threshold_hotplug_freq2;
+	unsigned int up_threshold_hotplug_freq3;
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+	unsigned int up_threshold_hotplug_freq4;
+	unsigned int up_threshold_hotplug_freq5;
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+	unsigned int up_threshold_hotplug_freq6;
+	unsigned int up_threshold_hotplug_freq7;
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+	unsigned int up_threshold_sleep;
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+};
+
+struct zzmoove_profile zzmoove_profiles[] = {
+	{
+		1,		// Default Profile
+		"def",		// default settings as hardcoded in the governor (please don't remove this profile)
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+		0,		// auto_adjust_freq_thresholds (any value=enable, 0=disable)
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+#ifdef ENABLE_HOTPLUGGING
+		1,		// disable_hotplug (1=disable hotplugging, 0=enable hotplugging)
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// disable_hotplug_sleep (1=disable hotplugging, 0=enable hotplugging)
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#endif /* ENABLE_HOTPLUGGING */
+		52,		// down_threshold (range from 11 to 100 and must be lower than up_threshold)
+#ifdef ENABLE_HOTPLUGGING
+		55,		// down_threshold_hotplug1 (range from 1 to 100 and should be lower than up_threshold_hotplug1)
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		55,		// down_threshold_hotplug2 (range from 1 to 100 and should be lower than up_threshold_hotplug2)
+		55,		// down_threshold_hotplug3 (range from 1 to 100 and should be lower than up_threshold_hotplug3)
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		55,		// down_threshold_hotplug4 (range from 1 to 100 and should be lower than up_threshold_hotplug4)
+		55,		// down_threshold_hotplug5 (range from 1 to 100 and should be lower than up_threshold_hotplug5)
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		55,		// down_threshold_hotplug6 (range from 1 to 100 and should be lower than up_threshold_hotplug6)
+		55,		// down_threshold_hotplug7 (range from 1 to 100 and should be lower than up_threshold_hotplug7)
+#endif /* (MAX_CORES == 8) */
+		0,		// down_threshold_hotplug_freq1 (range from 0 to scaling max and should be lower than up_threshold_hotplug_freq1)
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq2 (range from 0 to scaling max and should be lower than up_threshold_hotplug_freq2)
+		0,		// down_threshold_hotplug_freq3 (range from 0 to scaling max and should be lower than up_threshold_hotplug_freq3)
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq4 (range from 0 to scaling max and should be lower than up_threshold_hotplug_freq4)
+		0,		// down_threshold_hotplug_freq5 (range from 0 to scaling max and should be lower than up_threshold_hotplug_freq5)
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq6 (range from 0 to scaling max and should be lower than up_threshold_hotplug_freq6)
+		0,		// down_threshold_hotplug_freq7 (range from 0 to scaling max and should be lower than up_threshold_hotplug_freq7)
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		44,		// down_threshold_sleep (range from 11 to 100 and must be lower than up_threshold_sleep)
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// early_demand (any value=enable, 0=disable)
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// early_demand_sleep (any value=enable, 0=disable)
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// fast_scaling_up (range from 0 to 4)
+		0,		// fast_scaling_down (range from 0 to 4)
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// fast_scaling_sleep_up (range from 0 to 4)
+		0,		// fast_scaling_sleep_down (range from 0 to 4)
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		25,		// auto fast scaling step one (range from 1 to 100)
+		50,		// auto fast scaling step two (range from 1 to 100)
+		75,		// auto fast scaling step three (range from 1 to 100)
+		90,		// auto fast scaling step four (range from 1 to 100)
+		0,		// freq_limit (0=disable, range in system table from freq->min to freq->max in khz)
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// freq_limit_sleep (0=disable, range in system table from freq->min to freq->max in khz)
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		25,		// grad_up_threshold (range from 1 to 100)
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		28,		// grad_up_threshold_sleep (range from 1 to 100)
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+		2,		// hotplug_block_up_cycles (0=disable, any value above 0)
+		1,		// block_up_multiplier_hotplug1 (1=disable hotplug up block cycles for 2nd core, 2 to 25x)
+		1,		// block_up_multiplier_hotplug2 (1=disable hotplug up block cycles for 3rd core, 2 to 25x)
+		1,		// block_up_multiplier_hotplug3 (1=disable hotplug up block cycles for 4th core, 2 to 25x)
+		1,		// block_up_multiplier_hotplug4 (1=disable hotplug up block cycles for 2nd core, 2 to 25x)
+		1,		// block_up_multiplier_hotplug5 (1=disable hotplug up block cycles for 3rd core, 2 to 25x)
+		1,		// block_up_multiplier_hotplug6 (1=disable hotplug up block cycles for 4th core, 2 to 25x)
+		1,		// block_up_multiplier_hotplug7 (1=disable hotplug up block cycles for 4th core, 2 to 25x)
+		20,		// hotplug_block_down_cycles (0=disable, any value above 0)
+		1,		// block_down_multiplier_hotplug1 (1=disable hotplug down block cycles for 2nd core, 2 to 25x)
+		1,		// block_down_multiplier_hotplug2 (1=disable hotplug down block cycles for 3rd core, 2 to 25x)
+		1,		// block_down_multiplier_hotplug3 (1=disable hotplug down block cycles for 4th core, 2 to 25x)
+		1,		// block_down_multiplier_hotplug4 (1=disable hotplug down block cycles for 2nd core, 2 to 25x)
+		1,		// block_down_multiplier_hotplug5 (1=disable hotplug down block cycles for 3rd core, 2 to 25x)
+		1,		// block_down_multiplier_hotplug6 (1=disable hotplug down block cycles for 4th core, 2 to 25x)
+		1,		// block_down_multiplier_hotplug7 (1=disable hotplug down block cycles for 4th core, 2 to 25x)
+		0,		// hotplug_stagger_up (0=disable, any value above 0)
+		0,		// hotplug_stagger_down (0=disable, any value above 0)
+		0,		// hotplug_idle_threshold (0=disable, range from 1 to 100)
+		0,		// hotplug_idle_freq (0=disable, range in system table from freq->min to freq->max in khz)
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// hotplug_sleep (0=all cores enabled, range 1 to MAX_CORES - 1)
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// hotplug_engage_freq (0=disable, range in system table from freq->min to freq->max in khz)
+		0,		// hotplug_max_limit (0=disable, range from 1 to MAX_CORES)
+		0,		// hotplug_min_limit (0=disable, range from 1 to MAX_CORES)
+		0,		// hotplug_lock (0=disable, range from 1 to MAX_CORES)
+#endif /* ENABLE_HOTPLUGGING */
+		0,		// ignore_nice_load (0=disable, 1=enable)
+		1,		// sampling_down_factor (1=disable, range from 2 to MAX_SAMPLING_DOWN_FACTOR)
+		0,		// sampling_down_max_momentum (0=disable, range from 1 to MAX_SAMPLING_DOWN_FACTOR)
+		50,		// sampling_down_momentum_sensitivity (range from 1 to MAX_SAMPLING_DOWN_SENSITIVITY)
+		100000,		// sampling_rate (range from MIN_SAMPLING_RATE to any value)
+		180000,		// sampling_rate_idle (range from MIN_SAMPLING_RATE to any value)
+		0,		// sampling_rate_idle_delay (0=disable, any value above 0)
+		40,		// sampling_rate_idle_threshold (range from 1 to 100)
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		2,		// sampling_rate_sleep_multiplier (range from 1 to 4)
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// scaling_block_cycles (0=disable, any value above 0)
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+		0,		// scaling_block_temp (0=disable, range from 30C to 80C)
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+		0,		// scaling_trip_temp (0=disable, range from 40C to 69C)
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+		1728000,	// scaling_block_freq (all valid system frequencies)
+		10,		// scaling_block_threshold (0=disable, range from 1 to 100)
+		2,		// scaling_block_force_down (0=disable, range from 2 to any value)
+		0,		// scaling_fastdown_freq (0=disable, range in system table from freq->min to freq->max in khz)
+		95,		// scaling_fastdown_up_threshold (range from over scaling_fastdown_down_threshold to 100)
+		90,		// scaling_fastdown_down_threshold (range from 11 to under scaling_fastdown_up_threshold)
+		0,		// scaling_responsiveness_freq (0=disable, range in system table from freq->min to freq->max in khz)
+		30,		// scaling_responsiveness_up_threshold (0=disable, range from 11 to 100)
+		0,		// scaling_proportional (0=disable, range from 1 to 3)
+#ifdef ENABLE_INPUTBOOSTER
+		0,		// inputboost_cycles (0=disable, range from 0 to 1000)
+		80,		// inputboost_up_threshold (0=disable, range from 0 to 100)
+		20,		// inputboost_punch_cycles (0= disable, range form 0 to 500)
+		1728000,	// inputboost_punch_freq (0=disable, range from 0 to freq->max in khz)
+		1,		// inputboost_punch_on_fingerdown (0=disable, any value above 0)
+		0,		// inputboost_punch_on_fingermove (0=disable, any value above 0)
+		0,		// inputboost_punch_on_epenmove (0=disable, any value above 0)
+		40,		// inputboost_typingbooster_up_threshold (0=disable, range from 0 to 100)
+		3,		// inputboost_typingbooster_cores (0=disable, range from 1 to MAX_CORES)
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_MUSIC_LIMITS
+		1497600,	// music_max_freq (0=disable, range from 0 to freq->max in khz)
+		422400,		// music_min_freq (0=disable, range from 0 to freq->min in khz)
+		2,		// music_min_cores (0=disable, range from 1 to MAX_CORES)
+#endif /* ENABLE_MUSIC_LIMITS */
+		75,		// smooth_up (range from 1 to 100)
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		100,		// smooth_up_sleep (range from 1 to 100)
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		70,		// up_threshold (range 1 to 100 and must be higher than down_threshold)
+#ifdef ENABLE_HOTPLUGGING
+		68,		// up_threshold_hotplug1 (range 1 to 100 and should be higher than down_threshold_hotplug1)
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		68,		// up_threshold_hotplug2 (range 1 to 100 and should be higher than down_threshold_hotplug2)
+		68,		// up_threshold_hotplug3 (range 1 to 100 and should be higher than down_threshold_hotplug3)
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		68,		// up_threshold_hotplug4 (range 1 to 100 and should be higher than down_threshold_hotplug4)
+		68,		// up_threshold_hotplug5 (range 1 to 100 and should be higher than down_threshold_hotplug5)
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		68,		// up_threshold_hotplug6 (range 1 to 100 and should be higher than down_threshold_hotplug6)
+		68,		// up_threshold_hotplug7 (range 1 to 100 and should be higher than down_threshold_hotplug7)
+#endif /* (MAX_CORES == 8) */
+		0,		// up_threshold_hotplug_freq1 (0 to disable core, range from 1 to scaling max and should be higher than down_threshold_hotplug_freq1)
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq2 (0 to disable core, range from 1 to scaling max and should be higher than down_threshold_hotplug_freq2)
+		0,		// up_threshold_hotplug_freq3 (0 to disable core, range from 1 to scaling max and should be higher than down_threshold_hotplug_freq3)
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq4 (0 to disable core, range from 1 to scaling max and should be higher than down_threshold_hotplug_freq4)
+		0,		// up_threshold_hotplug_freq5 (0 to disable core, range from 1 to scaling max and should be higher than down_threshold_hotplug_freq5)
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq6 (0 to disable core, range from 1 to scaling max and should be higher than down_threshold_hotplug_freq6)
+		0,		// up_threshold_hotplug_freq7 (0 to disable core, range from 1 to scaling max and should be higher than down_threshold_hotplug_freq7)
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		90		// up_threshold_sleep (range from above down_threshold_sleep to 100)
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	},
+	{
+		2,
+		"ybat",		// Yank555.lu Battery Profile (please don't remove this profile)
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+		0,		// auto_adjust_freq_thresholds
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+#ifdef ENABLE_HOTPLUGGING
+		1,		// disable_hotplug
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// disable_hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#endif /* ENABLE_HOTPLUGGING */
+		40,		// down_threshold
+#ifdef ENABLE_HOTPLUGGING
+		65,		// down_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		75,		// down_threshold_hotplug2
+		85,		// down_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		55,		// down_threshold_hotplug4
+		55,		// down_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		55,		// down_threshold_hotplug6
+		55,		// down_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		960000,		// down_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		1267200,	// down_threshold_hotplug_freq2
+		1728000,	// down_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq4
+		0,		// down_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq6
+		0,		// down_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		75,		// down_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// early_demand
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// early_demand_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		5,		// fast_scaling_up
+		2,		// fast_scaling_down
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// fast_scaling_sleep_up
+		0,		// fast_scaling_sleep_down
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		30,		// afs_threshold1
+		50,		// afs_threshold2
+		70,		// afs_threshold3
+		90,		// afs_threshold4
+		0,		// freq_limit
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		729600,		// freq_limit_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		50,		// grad_up_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		28,		// grad_up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+		2,		// hotplug_block_up_cycles
+		1,		// block_up_multiplier_hotplug1
+		1,		// block_up_multiplier_hotplug2
+		1,		// block_up_multiplier_hotplug3
+		1,		// block_up_multiplier_hotplug4
+		1,		// block_up_multiplier_hotplug5
+		1,		// block_up_multiplier_hotplug6
+		1,		// block_up_multiplier_hotplug7
+		20,		// hotplug_block_down_cycles
+		1,		// block_down_multiplier_hotplug1
+		1,		// block_down_multiplier_hotplug2
+		1,		// block_down_multiplier_hotplug3
+		1,		// block_down_multiplier_hotplug4
+		1,		// block_down_multiplier_hotplug5
+		1,		// block_down_multiplier_hotplug6
+		1,		// block_down_multiplier_hotplug7
+		0,		// hotplug_stagger_up
+		0,		// hotplug_stagger_down
+		0,		// hotplug_idle_threshold
+		0,		// hotplug_idle_freq
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// hotplug_engage_freq
+		0,		// hotplug_max_limit
+		0,		// hotplug_min_limit
+		0,		// hotplug_lock
+#endif /* ENABLE_HOTPLUGGING */
+		0,		// ignore_nice_load
+		1,		// sampling_down_factor
+		0,		// sampling_down_max_momentum
+		50,		// sampling_down_momentum_sensitivity
+		75000,		// sampling_rate
+		180000,		// sampling_rate_idle
+		0,		// sampling_rate_idle_delay
+		40,		// sampling_rate_idle_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		4,		// sampling_rate_sleep_multiplier
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// scaling_block_cycles
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+		0,		// scaling_block_temp
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+		0,		// scaling_trip_temp
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+		0,		// scaling_block_freq
+		0,		// scaling_block_threshold
+		2,		// scaling_block_force_down
+		1958400,	// scaling_fastdown_freq
+		95,		// scaling_fastdown_up_threshold
+		90,		// scaling_fastdown_down_threshold
+		652800,		// scaling_responsiveness_freq
+		20,		// scaling_responsiveness_up_threshold
+		1,		// scaling_proportional
+#ifdef ENABLE_INPUTBOOSTER
+		0,		// inputboost_cycles
+		80,		// inputboost_up_threshold
+		20,		// inputboost_punch_cycles
+		1728000,	// inputboost_punch_freq
+		1,		// inputboost_punch_on_fingerdown
+		0,		// inputboost_punch_on_fingermove
+		0,		// inputboost_punch_on_epenmove
+		40,		// inputboost_typingbooster_up_threshold
+		3,		// inputboost_typingbooster_cores
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_MUSIC_LIMITS
+		1497600,	// music_max_freq
+		422400,		// music_min_freq
+		2,		// music_min_cores
+#endif /* ENABLE_MUSIC_LIMITS */
+		95,		// smooth_up
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		90,		// smooth_up_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		60,		// up_threshold
+#ifdef ENABLE_HOTPLUGGING
+		85,		// up_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		90,		// up_threshold_hotplug2
+		98,		// up_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		68,		// up_threshold_hotplug4
+		68,		// up_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		68,		// up_threshold_hotplug6
+		68,		// up_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		1036800,	// up_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		1497600,	// up_threshold_hotplug_freq2
+		2265600,	// up_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq4
+		0,		// up_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq6
+		0,		// up_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		85		// up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	},
+	{
+		3,
+		"ybatext",	// Yank555.lu Battery Extreme Profile (please don't remove this profile)
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+		0,		// auto_adjust_freq_thresholds
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+#ifdef ENABLE_HOTPLUGGING
+		1,		// disable_hotplug
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// disable_hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#endif /* ENABLE_HOTPLUGGING */
+		50,		// down_threshold
+#ifdef ENABLE_HOTPLUGGING
+		70,		// down_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		80,		// down_threshold_hotplug2
+		90,		// down_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		55,		// down_threshold_hotplug4
+		55,		// down_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		55,		// down_threshold_hotplug6
+		55,		// down_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		883200,		// down_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		1190400,	// down_threshold_hotplug_freq2
+		1574400,	// down_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq4
+		0,		// down_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq6
+		0,		// down_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		75,		// down_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// early_demand
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// early_demand_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		5,		// fast_scaling_up
+		3,		// fast_scaling_down
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// fast_scaling_sleep_up
+		0,		// fast_scaling_sleep_down
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		30,		// afs_threshold1
+		50,		// afs_threshold2
+		70,		// afs_threshold3
+		90,		// afs_threshold4
+		0,		// freq_limit
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		729600,		// freq_limit_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		50,		// grad_up_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		28,		// grad_up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+		2,		// hotplug_block_up_cycles
+		1,		// block_up_multiplier_hotplug1
+		1,		// block_up_multiplier_hotplug2
+		1,		// block_up_multiplier_hotplug3
+		1,		// block_up_multiplier_hotplug4
+		1,		// block_up_multiplier_hotplug5
+		1,		// block_up_multiplier_hotplug6
+		1,		// block_up_multiplier_hotplug7
+		20,		// hotplug_block_down_cycles
+		1,		// block_down_multiplier_hotplug1
+		1,		// block_down_multiplier_hotplug2
+		1,		// block_down_multiplier_hotplug3
+		1,		// block_down_multiplier_hotplug4
+		1,		// block_down_multiplier_hotplug5
+		1,		// block_down_multiplier_hotplug6
+		1,		// block_down_multiplier_hotplug7
+		0,		// hotplug_stagger_up
+		0,		// hotplug_stagger_down
+		0,		// hotplug_idle_threshold
+		0,		// hotplug_idle_freq
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// hotplug_engage_freq
+		0,		// hotplug_max_limit
+		0,		// hotplug_min_limit
+		0,		// hotplug_lock
+#endif /* ENABLE_HOTPLUGGING */
+		0,		// ignore_nice_load
+		1,		// sampling_down_factor
+		0,		// sampling_down_max_momentum
+		50,		// sampling_down_momentum_sensitivity
+		70000,		// sampling_rate
+		180000,		// sampling_rate_idle
+		0,		// sampling_rate_idle_delay
+		40,		// sampling_rate_idle_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		4,		// sampling_rate_sleep_multiplier
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// scaling_block_cycles
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+		0,		// scaling_block_temp
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+		0,		// scaling_trip_temp
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+		0,		// scaling_block_freq
+		0,		// scaling_block_threshold
+		2,		// scaling_block_force_down
+		1958400,	// scaling_fastdown_freq
+		95,		// scaling_fastdown_up_threshold
+		90,		// scaling_fastdown_down_threshold
+		652800,		// scaling_responsiveness_freq
+		20,		// scaling_responsiveness_up_threshold
+		1,		// scaling_proportional
+#ifdef ENABLE_INPUTBOOSTER
+		0,		// inputboost_cycles
+		80,		// inputboost_up_threshold
+		20,		// inputboost_punch_cycles
+		1728000,	// inputboost_punch_freq
+		1,		// inputboost_punch_on_fingerdown
+		0,		// inputboost_punch_on_fingermove
+		0,		// inputboost_punch_on_epenmove
+		40,		// inputboost_typingbooster_up_threshold
+		3,		// inputboost_typingbooster_cores
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_MUSIC_LIMITS
+		1497600,	// music_max_freq
+		422400,		// music_min_freq
+		2,		// music_min_cores
+#endif /* ENABLE_MUSIC_LIMITS */
+		95,		// smooth_up
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		90,		// smooth_up_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		70,		// up_threshold
+#ifdef ENABLE_HOTPLUGGING
+		90,		// up_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		95,		// up_threshold_hotplug2
+		98,		// up_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		68,		// up_threshold_hotplug4
+		68,		// up_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		68,		// up_threshold_hotplug6
+		68,		// up_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		1190400,	// up_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		1574400,	// up_threshold_hotplug_freq2
+		2265600,	// up_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq4
+		0,		// up_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq6
+		0,		// up_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		85		// up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	},
+	{
+		4,
+		"zzbat",	// ZaneZam Battery Profile (please don't remove this profile)
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+		0,		// auto_adjust_freq_thresholds
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+#ifdef ENABLE_HOTPLUGGING
+		1,		// disable_hotplug
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// disable_hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#endif /* ENABLE_HOTPLUGGING */
+		40,		// down_threshold
+#ifdef ENABLE_HOTPLUGGING
+		45,		// down_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		55,		// down_threshold_hotplug2
+		65,		// down_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		55,		// down_threshold_hotplug4
+		55,		// down_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		55,		// down_threshold_hotplug6
+		55,		// down_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		652800,		// down_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		960000,		// down_threshold_hotplug_freq2
+		1267200,	// down_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq4
+		0,		// down_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq6
+		0,		// down_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		60,		// down_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// early_demand
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// early_demand_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// fast_scaling_up
+		0,		// fast_scaling_down
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// fast_scaling_sleep_up
+		0,		// fast_scaling_sleep_down
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		30,		// afs_threshold1
+		50,		// afs_threshold2
+		70,		// afs_threshold3
+		90,		// afs_threshold4
+		0,		// freq_limit
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		729600,		// freq_limit_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		50,		// grad_up_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		28,		// grad_up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+		2,		// hotplug_block_up_cycles
+		1,		// block_up_multiplier_hotplug1
+		1,		// block_up_multiplier_hotplug2
+		1,		// block_up_multiplier_hotplug3
+		1,		// block_up_multiplier_hotplug4
+		1,		// block_up_multiplier_hotplug5
+		1,		// block_up_multiplier_hotplug6
+		1,		// block_up_multiplier_hotplug7
+		20,		// hotplug_block_down_cycles
+		1,		// block_down_multiplier_hotplug1
+		1,		// block_down_multiplier_hotplug2
+		1,		// block_down_multiplier_hotplug3
+		1,		// block_down_multiplier_hotplug4
+		1,		// block_down_multiplier_hotplug5
+		1,		// block_down_multiplier_hotplug6
+		1,		// block_down_multiplier_hotplug7
+		0,		// hotplug_stagger_up
+		0,		// hotplug_stagger_down
+		0,		// hotplug_idle_threshold
+		0,		// hotplug_idle_freq
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// hotplug_engage_freq
+		0,		// hotplug_max_limit
+		0,		// hotplug_min_limit
+		0,		// hotplug_lock
+#endif /* ENABLE_HOTPLUGGING */
+		0,		// ignore_nice_load
+		1,		// sampling_down_factor
+		0,		// sampling_down_max_momentum
+		50,		// sampling_down_momentum_sensitivity
+		100000,		// sampling_rate
+		180000,		// sampling_rate_idle
+		0,		// sampling_rate_idle_delay
+		40,		// sampling_rate_idle_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		4,		// sampling_rate_sleep_multiplier
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// scaling_block_cycles
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+		0,		// scaling_block_temp
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+		0,		// scaling_trip_temp
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+		0,		// scaling_block_freq
+		0,		// scaling_block_threshold
+		2,		// scaling_block_force_down
+		1958400,	// scaling_fastdown_freq
+		95,		// scaling_fastdown_up_threshold
+		90,		// scaling_fastdown_down_threshold
+		652800,		// scaling_responsiveness_freq
+		20,		// scaling_responsiveness_up_threshold
+		1,		// scaling_proportional
+#ifdef ENABLE_INPUTBOOSTER
+		0,		// inputboost_cycles
+		80,		// inputboost_up_threshold
+		20,		// inputboost_punch_cycles
+		1728000,	// inputboost_punch_freq
+		1,		// inputboost_punch_on_fingerdown
+		0,		// inputboost_punch_on_fingermove
+		0,		// inputboost_punch_on_epenmove
+		40,		// inputboost_typingbooster_up_threshold
+		3,		// inputboost_typingbooster_cores
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_MUSIC_LIMITS
+		1497600,	// music_max_freq
+		422400,		// music_min_freq
+		2,		// music_min_cores
+#endif /* ENABLE_MUSIC_LIMITS */
+		75,		// smooth_up
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		100,		// smooth_up_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		95,		// up_threshold
+#ifdef ENABLE_HOTPLUGGING
+		60,		// up_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		80,		// up_threshold_hotplug2
+		98,		// up_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		68,		// up_threshold_hotplug4
+		68,		// up_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		68,		// up_threshold_hotplug6
+		68,		// up_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		729600,		// up_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		1190400,	// up_threshold_hotplug_freq2
+		1574400,	// up_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq4
+		0,		// up_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq6
+		0,		// up_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		100		// up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	},
+	{
+		5,
+		"zzbatp",	// ZaneZam Battery Plus Profile (please don't remove this profile)
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+		0,		// auto_adjust_freq_thresholds
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+#ifdef ENABLE_HOTPLUGGING
+		1,		// disable_hotplug
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// disable_hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#endif /* ENABLE_HOTPLUGGING */
+		70,		// down_threshold
+#ifdef ENABLE_HOTPLUGGING
+		20,		// down_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		70,		// down_threshold_hotplug2
+		80,		// down_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		55,		// down_threshold_hotplug4
+		55,		// down_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		55,		// down_threshold_hotplug6
+		55,		// down_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		422400,		// down_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		652800,		// down_threshold_hotplug_freq2
+		1574400,	// down_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq4
+		0,		// down_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq6
+		0,		// down_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		65,		// down_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		1,		// early_demand
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// early_demand_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// fast_scaling_up
+		0,		// fast_scaling_down
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// fast_scaling_sleep_up
+		0,		// fast_scaling_sleep_down
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		30,		// afs_threshold1
+		50,		// afs_threshold2
+		70,		// afs_threshold3
+		90,		// afs_threshold4
+		0,		// freq_limit
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		729600,		// freq_limit_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		60,		// grad_up_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		28,		// grad_up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+		2,		// hotplug_block_up_cycles
+		1,		// block_up_multiplier_hotplug1
+		1,		// block_up_multiplier_hotplug2
+		1,		// block_up_multiplier_hotplug3
+		1,		// block_up_multiplier_hotplug4
+		1,		// block_up_multiplier_hotplug5
+		1,		// block_up_multiplier_hotplug6
+		1,		// block_up_multiplier_hotplug7
+		20,		// hotplug_block_down_cycles
+		1,		// block_down_multiplier_hotplug1
+		1,		// block_down_multiplier_hotplug2
+		1,		// block_down_multiplier_hotplug3
+		1,		// block_down_multiplier_hotplug4
+		1,		// block_down_multiplier_hotplug5
+		1,		// block_down_multiplier_hotplug6
+		1,		// block_down_multiplier_hotplug7
+		0,		// hotplug_stagger_up
+		0,		// hotplug_stagger_down
+		0,		// hotplug_idle_threshold
+		0,		// hotplug_idle_freq
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// hotplug_engage_freq
+		0,		// hotplug_max_limit
+		0,		// hotplug_min_limit
+		0,		// hotplug_lock
+#endif /* ENABLE_HOTPLUGGING */
+		0,		// ignore_nice_load
+		1,		// sampling_down_factor
+		0,		// sampling_down_max_momentum
+		50,		// sampling_down_momentum_sensitivity
+		120000,		// sampling_rate
+		200000,		// sampling_rate_idle
+		5,		// sampling_rate_idle_delay
+		40,		// sampling_rate_idle_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		4,		// sampling_rate_sleep_multiplier
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// scaling_block_cycles
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+		0,		// scaling_block_temp
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+		0,		// scaling_trip_temp
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+		0,		// scaling_block_freq
+		0,		// scaling_block_threshold
+		2,		// scaling_block_force_down
+		1958400,	// scaling_fastdown_freq
+		95,		// scaling_fastdown_up_threshold
+		90,		// scaling_fastdown_down_threshold
+		652800,		// scaling_responsiveness_freq
+		20,		// scaling_responsiveness_up_threshold
+		1,		// scaling_proportional
+#ifdef ENABLE_INPUTBOOSTER
+		0,		// inputboost_cycles
+		80,		// inputboost_up_threshold
+		20,		// inputboost_punch_cycles
+		1728000,	// inputboost_punch_freq
+		1,		// inputboost_punch_on_fingerdown
+		0,		// inputboost_punch_on_fingermove
+		0,		// inputboost_punch_on_epenmove
+		40,		// inputboost_typingbooster_up_threshold
+		3,		// inputboost_typingbooster_cores
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_MUSIC_LIMITS
+		1497600,	// music_max_freq
+		422400,		// music_min_freq
+		2,		// music_min_cores
+#endif /* ENABLE_MUSIC_LIMITS */
+		80,		// smooth_up
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		100,		// smooth_up_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		75,		// up_threshold
+#ifdef ENABLE_HOTPLUGGING
+		20,		// up_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		50,		// up_threshold_hotplug2
+		90,		// up_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		68,		// up_threshold_hotplug4
+		68,		// up_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		68,		// up_threshold_hotplug6
+		68,		// up_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		422400,		// up_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		1497600,	// up_threshold_hotplug_freq2
+		1728000,	// up_threshold_hotplug_freq3
+
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq4
+		0,		// up_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq6
+		0,		// up_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		100		// up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	},
+	{
+		6,
+		"zzopt",	// ZaneZam Optimized Profile (please don't remove this profile)
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+		0,		// auto_adjust_freq_thresholds
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+#ifdef ENABLE_HOTPLUGGING
+		1,		// disable_hotplug
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// disable_hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#endif /* ENABLE_HOTPLUGGING */
+		52,		// down_threshold
+#ifdef ENABLE_HOTPLUGGING
+		45,		// down_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		55,		// down_threshold_hotplug2
+		65,		// down_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		55,		// down_threshold_hotplug4
+		55,		// down_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		55,		// down_threshold_hotplug6
+		55,		// down_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		422400,		// down_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		883200,		// down_threshold_hotplug_freq2
+		1190400,	// down_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq4
+		0,		// down_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq6
+		0,		// down_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		60,		// down_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		1,		// early_demand
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// early_demand_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		1,		// fast_scaling_up
+		0,		// fast_scaling_down
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		2,		// fast_scaling_sleep_up
+		0,		// fast_scaling_sleep_down
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		30,		// afs_threshold1
+		50,		// afs_threshold2
+		70,		// afs_threshold3
+		90,		// afs_threshold4
+		0,		// freq_limit
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		729600,		// freq_limit_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		35,		// grad_up_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		28,		// grad_up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+		2,		// hotplug_block_up_cycles
+		1,		// block_up_multiplier_hotplug1
+		1,		// block_up_multiplier_hotplug2
+		1,		// block_up_multiplier_hotplug3
+		1,		// block_up_multiplier_hotplug4
+		1,		// block_up_multiplier_hotplug5
+		1,		// block_up_multiplier_hotplug6
+		1,		// block_up_multiplier_hotplug7
+		20,		// hotplug_block_down_cycles
+		1,		// block_down_multiplier_hotplug1
+		1,		// block_down_multiplier_hotplug2
+		1,		// block_down_multiplier_hotplug3
+		1,		// block_down_multiplier_hotplug4
+		1,		// block_down_multiplier_hotplug5
+		1,		// block_down_multiplier_hotplug6
+		1,		// block_down_multiplier_hotplug7
+		0,		// hotplug_stagger_up
+		0,		// hotplug_stagger_down
+		0,		// hotplug_idle_threshold
+		0,		// hotplug_idle_freq
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// hotplug_engage_freq
+		0,		// hotplug_max_limit
+		0,		// hotplug_min_limit
+		0,		// hotplug_lock
+#endif /* ENABLE_HOTPLUGGING */
+		0,		// ignore_nice_load
+		4,		// sampling_down_factor
+		20,		// sampling_down_max_momentum
+		50,		// sampling_down_momentum_sensitivity
+		70000,		// sampling_rate
+		100000,		// sampling_rate_idle
+		0,		// sampling_rate_idle_delay
+		40,		// sampling_rate_idle_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		4,		// sampling_rate_sleep_multiplier
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// scaling_block_cycles
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+		0,		// scaling_block_temp
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+		0,		// scaling_trip_temp
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+		0,		// scaling_block_freq
+		0,		// scaling_block_threshold
+		2,		// scaling_block_force_down
+		0,		// scaling_fastdown_freq
+		95,		// scaling_fastdown_up_threshold
+		90,		// scaling_fastdown_down_threshold
+		422400,		// scaling_responsiveness_freq
+		0,		// scaling_responsiveness_up_threshold
+		0,		// scaling_proportional
+#ifdef ENABLE_INPUTBOOSTER
+		0,		// inputboost_cycles
+		80,		// inputboost_up_threshold
+		20,		// inputboost_punch_cycles
+		1728000,	// inputboost_punch_freq
+		1,		// inputboost_punch_on_fingerdown
+		0,		// inputboost_punch_on_fingermove
+		0,		// inputboost_punch_on_epenmove
+		40,		// inputboost_typingbooster_up_threshold
+		3,		// inputboost_typingbooster_cores
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_MUSIC_LIMITS
+		1497600,	// music_max_freq
+		422400,		// music_min_freq
+		2,		// music_min_cores
+#endif /* ENABLE_MUSIC_LIMITS */
+		75,		// smooth_up
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		100,		// smooth_up_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		67,		// up_threshold
+#ifdef ENABLE_HOTPLUGGING
+		68,		// up_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		78,		// up_threshold_hotplug2
+		88,		// up_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		68,		// up_threshold_hotplug4
+		68,		// up_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		68,		// up_threshold_hotplug6
+		68,		// up_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		652800,		// up_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		1036800,	// up_threshold_hotplug_freq2
+		1574400,	// up_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq4
+		0,		// up_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq6
+		0,		// up_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		100		// up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	},
+	{
+		7,
+		"zzmod",	// ZaneZam Moderate Profile (please don't remove this profile)
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+		0,		// auto_adjust_freq_thresholds
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+#ifdef ENABLE_HOTPLUGGING
+		1,		// disable_hotplug
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// disable_hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#endif /* ENABLE_HOTPLUGGING */
+		52,		// down_threshold
+#ifdef ENABLE_HOTPLUGGING
+		30,		// down_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		60,		// down_threshold_hotplug2
+		70,		// down_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		55,		// down_threshold_hotplug4
+		55,		// down_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		55,		// down_threshold_hotplug6
+		55,		// down_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		300000,		// down_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq2
+		0,		// down_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq4
+		0,		// down_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq6
+		0,		// down_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		55,		// down_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		1,		// early_demand
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// early_demand_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		3,		// fast_scaling_up
+		0,		// fast_scaling_down
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// fast_scaling_sleep_up
+		0,		// fast_scaling_sleep_down
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		30,		// afs_threshold1
+		50,		// afs_threshold2
+		70,		// afs_threshold3
+		90,		// afs_threshold4
+		0,		// freq_limit
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		729600,		// freq_limit_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		40,		// grad_up_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		28,		// grad_up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+		2,		// hotplug_block_up_cycles
+		1,		// block_up_multiplier_hotplug1
+		1,		// block_up_multiplier_hotplug2
+		1,		// block_up_multiplier_hotplug3
+		1,		// block_up_multiplier_hotplug4
+		1,		// block_up_multiplier_hotplug5
+		1,		// block_up_multiplier_hotplug6
+		1,		// block_up_multiplier_hotplug7
+		20,		// hotplug_block_down_cycles
+		1,		// block_down_multiplier_hotplug1
+		1,		// block_down_multiplier_hotplug2
+		1,		// block_down_multiplier_hotplug3
+		1,		// block_down_multiplier_hotplug4
+		1,		// block_down_multiplier_hotplug5
+		1,		// block_down_multiplier_hotplug6
+		1,		// block_down_multiplier_hotplug7
+		0,		// hotplug_stagger_up
+		0,		// hotplug_stagger_down
+		0,		// hotplug_idle_threshold
+		0,		// hotplug_idle_freq
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// hotplug_engage_freq
+		0,		// hotplug_max_limit
+		0,		// hotplug_min_limit
+		0,		// hotplug_lock
+#endif /* ENABLE_HOTPLUGGING */
+		0,		// ignore_nice_load
+		4,		// sampling_down_factor
+		20,		// sampling_down_max_momentum
+		50,		// sampling_down_momentum_sensitivity
+		70000,		// sampling_rate
+		100000,		// sampling_rate_idle
+		0,		// sampling_rate_idle_delay
+		40,		// sampling_rate_idle_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		4,		// sampling_rate_sleep_multiplier
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// scaling_block_cycles
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+		0,		// scaling_block_temp
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+		0,		// scaling_trip_temp
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+		0,		// scaling_block_freq
+		0,		// scaling_block_threshold
+		2,		// scaling_block_force_down
+		1958400,	// scaling_fastdown_freq
+		95,		// scaling_fastdown_up_threshold
+		90,		// scaling_fastdown_down_threshold
+		652800,		// scaling_responsiveness_freq
+		20,		// scaling_responsiveness_up_threshold
+		1,		// scaling_proportional
+#ifdef ENABLE_INPUTBOOSTER
+		0,		// inputboost_cycles
+		80,		// inputboost_up_threshold
+		20,		// inputboost_punch_cycles
+		1728000,	// inputboost_punch_freq
+		1,		// inputboost_punch_on_fingerdown
+		0,		// inputboost_punch_on_fingermove
+		0,		// inputboost_punch_on_epenmove
+		40,		// inputboost_typingbooster_up_threshold
+		3,		// inputboost_typingbooster_cores
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_MUSIC_LIMITS
+		1497600,	// music_max_freq
+		422400,		// music_min_freq
+		2,		// music_min_cores
+#endif /* ENABLE_MUSIC_LIMITS */
+		68,		// smooth_up
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		100,		// smooth_up_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		60,		// up_threshold
+#ifdef ENABLE_HOTPLUGGING
+		20,		// up_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		100,		// up_threshold_hotplug2
+		100,		// up_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		68,		// up_threshold_hotplug4
+		68,		// up_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		68,		// up_threshold_hotplug6
+		68,		// up_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		422400,		// up_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq2
+		0,		// up_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq4
+		0,		// up_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq6
+		0,		// up_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		100		// up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	},
+	{
+		8,
+		"zzperf",	// ZaneZam Performance Profile (please don't remove this profile)
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+		0,		// auto_adjust_freq_thresholds
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+#ifdef ENABLE_HOTPLUGGING
+		1,		// disable_hotplug
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// disable_hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#endif /* ENABLE_HOTPLUGGING */
+		20,		// down_threshold
+#ifdef ENABLE_HOTPLUGGING
+		25,		// down_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		35,		// down_threshold_hotplug2
+		45,		// down_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		55,		// down_threshold_hotplug4
+		55,		// down_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		55,		// down_threshold_hotplug6
+		55,		// down_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		300000,		// down_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		1190400,	// down_threshold_hotplug_freq2
+		1574400,	// down_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq4
+		0,		// down_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq6
+		0,		// down_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		60,		// down_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		1,		// early_demand
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// early_demand_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		1,		// fast_scaling_up
+		1,		// fast_scaling_down
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		2,		// fast_scaling_sleep_up
+		0,		// fast_scaling_sleep_down
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		30,		// afs_threshold1
+		50,		// afs_threshold2
+		70,		// afs_threshold3
+		90,		// afs_threshold4
+		0,		// freq_limit
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		729600,		// freq_limit_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		25,		// grad_up_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		28,		// grad_up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+		2,		// hotplug_block_up_cycles
+		1,		// block_up_multiplier_hotplug1
+		1,		// block_up_multiplier_hotplug2
+		1,		// block_up_multiplier_hotplug3
+		1,		// block_up_multiplier_hotplug4
+		1,		// block_up_multiplier_hotplug5
+		1,		// block_up_multiplier_hotplug6
+		1,		// block_up_multiplier_hotplug7
+		20,		// hotplug_block_down_cycles
+		1,		// block_down_multiplier_hotplug1
+		1,		// block_down_multiplier_hotplug2
+		1,		// block_down_multiplier_hotplug3
+		1,		// block_down_multiplier_hotplug4
+		1,		// block_down_multiplier_hotplug5
+		1,		// block_down_multiplier_hotplug6
+		1,		// block_down_multiplier_hotplug7
+		0,		// hotplug_stagger_up
+		0,		// hotplug_stagger_down
+		0,		// hotplug_idle_threshold
+		0,		// hotplug_idle_freq
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// hotplug_engage_freq
+		0,		// hotplug_max_limit
+		0,		// hotplug_min_limit
+		0,		// hotplug_lock
+#endif /* ENABLE_HOTPLUGGING */
+		0,		// ignore_nice_load
+		4,		// sampling_down_factor
+		50,		// sampling_down_max_momentum
+		25,		// sampling_down_momentum_sensitivity
+		60000,		// sampling_rate
+		100000,		// sampling_rate_idle
+		0,		// sampling_rate_idle_delay
+		40,		// sampling_rate_idle_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		4,		// sampling_rate_sleep_multiplier
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// scaling_block_cycles
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+		0,		// scaling_block_temp
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+		0,		// scaling_trip_temp
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+		0,		// scaling_block_freq
+		0,		// scaling_block_threshold
+		2,		// scaling_block_force_down
+		0,		// scaling_fastdown_freq
+		95,		// scaling_fastdown_up_threshold
+		90,		// scaling_fastdown_down_threshold
+		0,		// scaling_responsiveness_freq
+		0,		// scaling_responsiveness_up_threshold
+		0,		// scaling_proportional
+#ifdef ENABLE_INPUTBOOSTER
+		0,		// inputboost_cycles
+		80,		// inputboost_up_threshold
+		20,		// inputboost_punch_cycles
+		1728000,	// inputboost_punch_freq
+		1,		// inputboost_punch_on_fingerdown
+		0,		// inputboost_punch_on_fingermove
+		0,		// inputboost_punch_on_epenmove
+		40,		// inputboost_typingbooster_up_threshold
+		3,		// inputboost_typingbooster_cores
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_MUSIC_LIMITS
+		1497600,	// music_max_freq
+		422400,		// music_min_freq
+		2,		// music_min_cores
+#endif /* ENABLE_MUSIC_LIMITS */
+		70,		// smooth_up
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		100,		// smooth_up_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		60,		// up_threshold
+#ifdef ENABLE_HOTPLUGGING
+		65,		// up_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		75,		// up_threshold_hotplug2
+		85,		// up_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		68,		// up_threshold_hotplug4
+		68,		// up_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		68,		// up_threshold_hotplug6
+		68,		// up_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		422400,		// up_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		1267200,	// up_threshold_hotplug_freq2
+		1728000,	// up_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq4
+		0,		// up_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq6
+		0,		// up_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		100		// up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	},
+	{
+		9,
+		"zzinz",	// ZaneZam InZane Profile (please don't remove this profile)
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+		0,		// auto_adjust_freq_thresholds
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+#ifdef ENABLE_HOTPLUGGING
+		1,		// disable_hotplug
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// disable_hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#endif /* ENABLE_HOTPLUGGING */
+		20,		// down_threshold
+#ifdef ENABLE_HOTPLUGGING
+		25,		// down_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		35,		// down_threshold_hotplug2
+		45,		// down_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		55,		// down_threshold_hotplug4
+		55,		// down_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		55,		// down_threshold_hotplug6
+		55,		// down_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		300000,		// down_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		883200,		// down_threshold_hotplug_freq2
+		1036800,	// down_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq4
+		0,		// down_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq6
+		0,		// down_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		60,		// down_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		1,		// early_demand
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// early_demand_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		5,		// fast_scaling_up
+		5,		// fast_scaling_down
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		2,		// fast_scaling_sleep_up
+		0,		// fast_scaling_sleep_down
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		30,		// afs_threshold1
+		50,		// afs_threshold2
+		70,		// afs_threshold3
+		90,		// afs_threshold4
+		0,		// freq_limit
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		729600,		// freq_limit_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		25,		// grad_up_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		28,		// grad_up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+		2,		// hotplug_block_up_cycles
+		1,		// block_up_multiplier_hotplug1
+		1,		// block_up_multiplier_hotplug2
+		1,		// block_up_multiplier_hotplug3
+		1,		// block_up_multiplier_hotplug4
+		1,		// block_up_multiplier_hotplug5
+		1,		// block_up_multiplier_hotplug6
+		1,		// block_up_multiplier_hotplug7
+		20,		// hotplug_block_down_cycles
+		1,		// block_down_multiplier_hotplug1
+		1,		// block_down_multiplier_hotplug2
+		1,		// block_down_multiplier_hotplug3
+		1,		// block_down_multiplier_hotplug4
+		1,		// block_down_multiplier_hotplug5
+		1,		// block_down_multiplier_hotplug6
+		1,		// block_down_multiplier_hotplug7
+		0,		// hotplug_stagger_up
+		0,		// hotplug_stagger_down
+		0,		// hotplug_idle_threshold
+		0,		// hotplug_idle_freq
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// hotplug_engage_freq
+		0,		// hotplug_max_limit
+		0,		// hotplug_min_limit
+		0,		// hotplug_lock
+#endif /* ENABLE_HOTPLUGGING */
+		0,		// ignore_nice_load
+		4,		// sampling_down_factor
+		80,		// sampling_down_max_momentum
+		15,		// sampling_down_momentum_sensitivity
+		60000,		// sampling_rate
+		100000,		// sampling_rate_idle
+		0,		// sampling_rate_idle_delay
+		40,		// sampling_rate_idle_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		4,		// sampling_rate_sleep_multiplier
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// scaling_block_cycles
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+		0,		// scaling_block_temp
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+		0,		// scaling_trip_temp
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+		0,		// scaling_block_freq
+		0,		// scaling_block_threshold
+		2,		// scaling_block_force_down
+		0,		// scaling_fastdown_freq
+		95,		// scaling_fastdown_up_threshold
+		90,		// scaling_fastdown_down_threshold
+		0,		// scaling_responsiveness_freq
+		0,		// scaling_responsiveness_up_threshold
+		0,		// scaling_proportional
+#ifdef ENABLE_INPUTBOOSTER
+		0,		// inputboost_cycles
+		80,		// inputboost_up_threshold
+		20,		// inputboost_punch_cycles
+		1728000,	// inputboost_punch_freq
+		1,		// inputboost_punch_on_fingerdown
+		0,		// inputboost_punch_on_fingermove
+		0,		// inputboost_punch_on_epenmove
+		40,		// inputboost_typingbooster_up_threshold
+		3,		// inputboost_typingbooster_cores
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_MUSIC_LIMITS
+		1497600,	// music_max_freq
+		422400,		// music_min_freq
+		2,		// music_min_cores
+#endif /* ENABLE_MUSIC_LIMITS */
+		60,		// smooth_up
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		100,		// smooth_up_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		50,		// up_threshold
+#ifdef ENABLE_HOTPLUGGING
+		60,		// up_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		70,		// up_threshold_hotplug2
+		80,		// up_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		68,		// up_threshold_hotplug4
+		68,		// up_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		68,		// up_threshold_hotplug6
+		68,		// up_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		422400,		// up_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		1267200,	// up_threshold_hotplug_freq2
+		1728000,	// up_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq4
+		0,		// up_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq6
+		0,		// up_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		100		// up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	},
+	{
+		10,
+		"zzgame",	// ZaneZam Game Profile (please don't remove this profile)
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+		0,		// auto_adjust_freq_thresholds
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+#ifdef ENABLE_HOTPLUGGING
+		1,		// disable_hotplug
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// disable_hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#endif /* ENABLE_HOTPLUGGING */
+		20,		// down_threshold
+#ifdef ENABLE_HOTPLUGGING
+		25,		// down_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		35,		// down_threshold_hotplug2
+		45,		// down_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		55,		// down_threshold_hotplug4
+		55,		// down_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		55,		// down_threshold_hotplug6
+		55,		// down_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		422400,		// down_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		1190400,	// down_threshold_hotplug_freq2
+		1574400,	// down_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq4
+		0,		// down_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq6
+		0,		// down_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		60,		// down_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		1,		// early_demand
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// early_demand_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// fast_scaling_up
+		0,		// fast_scaling_down
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		2,		// fast_scaling_sleep_up
+		0,		// fast_scaling_sleep_down
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		30,		// afs_threshold1
+		50,		// afs_threshold2
+		70,		// afs_threshold3
+		90,		// afs_threshold4
+		0,		// freq_limit
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		729600,		// freq_limit_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		25,		// grad_up_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		28,		// grad_up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+		2,		// hotplug_block_up_cycles
+		1,		// block_up_multiplier_hotplug1
+		1,		// block_up_multiplier_hotplug2
+		1,		// block_up_multiplier_hotplug3
+		1,		// block_up_multiplier_hotplug4
+		1,		// block_up_multiplier_hotplug5
+		1,		// block_up_multiplier_hotplug6
+		1,		// block_up_multiplier_hotplug7
+		20,		// hotplug_block_down_cycles
+		1,		// block_down_multiplier_hotplug1
+		1,		// block_down_multiplier_hotplug2
+		1,		// block_down_multiplier_hotplug3
+		1,		// block_down_multiplier_hotplug4
+		1,		// block_down_multiplier_hotplug5
+		1,		// block_down_multiplier_hotplug6
+		1,		// block_down_multiplier_hotplug7
+		0,		// hotplug_stagger_up
+		0,		// hotplug_stagger_down
+		0,		// hotplug_idle_threshold
+		0,		// hotplug_idle_freq
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// hotplug_engage_freq
+		0,		// hotplug_max_limit
+		0,		// hotplug_min_limit
+		0,		// hotplug_lock
+#endif /* ENABLE_HOTPLUGGING */
+		0,		// ignore_nice_load
+		4,		// sampling_down_factor
+		60,		// sampling_down_max_momentum
+		20,		// sampling_down_momentum_sensitivity
+		60000,		// sampling_rate
+		100000,		// sampling_rate_idle
+		0,		// sampling_rate_idle_delay
+		40,		// sampling_rate_idle_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		4,		// sampling_rate_sleep_multiplier
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+		0,		// scaling_block_cycles
+		65,		// scaling_block_temp
+#else
+		15,		// scaling_block_cycles
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+		0,		// scaling_trip_temp
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+		1574400,	// scaling_block_freq
+		5,		// scaling_block_threshold
+		3,		// scaling_block_force_down
+		0,		// scaling_fastdown_freq
+		95,		// scaling_fastdown_up_threshold
+		90,		// scaling_fastdown_down_threshold
+		0,		// scaling_responsiveness_freq
+		0,		// scaling_responsiveness_up_threshold
+		1,		// scaling_proportional
+#ifdef ENABLE_INPUTBOOSTER
+		0,		// inputboost_cycles
+		80,		// inputboost_up_threshold
+		20,		// inputboost_punch_cycles
+		1728000,	// inputboost_punch_freq
+		1,		// inputboost_punch_on_fingerdown
+		0,		// inputboost_punch_on_fingermove
+		0,		// inputboost_punch_on_epenmove
+		40,		// inputboost_typingbooster_up_threshold
+		3,		// inputboost_typingbooster_cores
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_MUSIC_LIMITS
+		1497600,	// music_max_freq
+		422400,		// music_min_freq
+		2,		// music_min_cores
+#endif /* ENABLE_MUSIC_LIMITS */
+		70,		// smooth_up
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		100,		// smooth_up_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		60,		// up_threshold
+#ifdef ENABLE_HOTPLUGGING
+		65,		// up_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		75,		// up_threshold_hotplug2
+		85,		// up_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		68,		// up_threshold_hotplug4
+		68,		// up_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		68,		// up_threshold_hotplug6
+		68,		// up_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		652800,		// up_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		1267200,	// up_threshold_hotplug_freq2
+		1958400,	// up_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq4
+		0,		// up_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq6
+		0,		// up_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		100		// up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	},
+	{
+		11,
+		"zzrelax",	// ZaneZam Relax Profile (please don't remove this profile)
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+		0,		// auto_adjust_freq_thresholds
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+#ifdef ENABLE_HOTPLUGGING
+		1,		// disable_hotplug
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// disable_hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#endif /* ENABLE_HOTPLUGGING */
+		52,		// down_threshold
+#ifdef ENABLE_HOTPLUGGING
+		30,		// down_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		60,		// down_threshold_hotplug2
+		70,		// down_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		55,		// down_threshold_hotplug4
+		55,		// down_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		55,		// down_threshold_hotplug6
+		55,		// down_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		300000,		// down_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		883200,		// down_threshold_hotplug_freq2
+		1190400,	// down_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq4
+		0,		// down_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq6
+		0,		// down_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		59,		// down_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		1,		// early_demand
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		1,		// early_demand_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		3,		// fast_scaling_up
+		0,		// fast_scaling_down
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		2,		// fast_scaling_sleep_up
+		0,		// fast_scaling_sleep_down
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		30,		// afs_threshold1
+		50,		// afs_threshold2
+		70,		// afs_threshold3
+		90,		// afs_threshold4
+		0,		// freq_limit
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// freq_limit_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		40,		// grad_up_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		28,		// grad_up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+		2,		// hotplug_block_up_cycles
+		1,		// block_up_multiplier_hotplug1
+		1,		// block_up_multiplier_hotplug2
+		1,		// block_up_multiplier_hotplug3
+		1,		// block_up_multiplier_hotplug4
+		1,		// block_up_multiplier_hotplug5
+		1,		// block_up_multiplier_hotplug6
+		1,		// block_up_multiplier_hotplug7
+		20,		// hotplug_block_down_cycles
+		1,		// block_down_multiplier_hotplug1
+		1,		// block_down_multiplier_hotplug2
+		1,		// block_down_multiplier_hotplug3
+		1,		// block_down_multiplier_hotplug4
+		1,		// block_down_multiplier_hotplug5
+		1,		// block_down_multiplier_hotplug6
+		1,		// block_down_multiplier_hotplug7
+		0,		// hotplug_stagger_up
+		0,		// hotplug_stagger_down
+		0,		// hotplug_idle_threshold
+		0,		// hotplug_idle_freq
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// hotplug_engage_freq
+		0,		// hotplug_max_limit
+		2,		// hotplug_min_limit
+		0,		// hotplug_lock
+#endif /* ENABLE_HOTPLUGGING */
+		0,		// ignore_nice_load
+		4,		// sampling_down_factor
+		20,		// sampling_down_max_momentum
+		50,		// sampling_down_momentum_sensitivity
+		70000,		// sampling_rate
+		100000,		// sampling_rate_idle
+		0,		// sampling_rate_idle_delay
+		40,		// sampling_rate_idle_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		2,		// sampling_rate_sleep_multiplier
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// scaling_block_cycles
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+		0,		// scaling_block_temp
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+		0,		// scaling_trip_temp
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+		0,		// scaling_block_freq
+		0,		// scaling_block_threshold
+		2,		// scaling_block_force_down
+		1958400,	// scaling_fastdown_freq
+		95,		// scaling_fastdown_up_threshold
+		90,		// scaling_fastdown_down_threshold
+		652800,		// scaling_responsiveness_freq
+		20,		// scaling_responsiveness_up_threshold
+		1,		// scaling_proportional
+#ifdef ENABLE_INPUTBOOSTER
+		0,		// inputboost_cycles
+		80,		// inputboost_up_threshold
+		20,		// inputboost_punch_cycles
+		1728000,	// inputboost_punch_freq
+		1,		// inputboost_punch_on_fingerdown
+		0,		// inputboost_punch_on_fingermove
+		0,		// inputboost_punch_on_epenmove
+		40,		// inputboost_typingbooster_up_threshold
+		3,		// inputboost_typingbooster_cores
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_MUSIC_LIMITS
+		1497600,	// music_max_freq
+		422400,		// music_min_freq
+		2,		// music_min_cores
+#endif /* ENABLE_MUSIC_LIMITS */
+		68,		// smooth_up
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		68,		// smooth_up_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		60,		// up_threshold
+#ifdef ENABLE_HOTPLUGGING
+		68,		// up_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		78,		// up_threshold_hotplug2
+		88,		// up_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		68,		// up_threshold_hotplug4
+		68,		// up_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		68,		// up_threshold_hotplug6
+		68,		// up_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		422400,		// up_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		1036800,	// up_threshold_hotplug_freq2
+		1574400,	// up_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq4
+		0,		// up_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq6
+		0,		// up_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		70		// up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	},
+	{
+		PROFILE_TABLE_END,
+		END_OF_PROFILES,// End of table entry (DON'T REMOVE THIS PROFILE !!!)
+#ifdef ENABLE_AUTO_ADJUST_FREQ
+		0,		// auto_adjust_freq_thresholds
+#endif /* ENABLE_AUTO_ADJUST_FREQ */
+#ifdef ENABLE_HOTPLUGGING
+		0,		// disable_hotplug
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// disable_hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#endif /* ENABLE_HOTPLUGGING */
+		0,		// down_threshold
+#ifdef ENABLE_HOTPLUGGING
+		0,		// down_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// down_threshold_hotplug2
+		0,		// down_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// down_threshold_hotplug4
+		0,		// down_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// down_threshold_hotplug6
+		0,		// down_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		0,		// down_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq2
+		0,		// down_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq4
+		0,		// down_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// down_threshold_hotplug_freq6
+		0,		// down_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// down_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// early_demand
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// early_demand_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// fast_scaling_up
+		0,		// fast_scaling_down
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// fast_scaling_sleep_up
+		0,		// fast_scaling_sleep_down
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// afs_threshold1
+		0,		// afs_threshold2
+		0,		// afs_threshold3
+		0,		// afs_threshold4
+		0,		// freq_limit
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// freq_limit_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// grad_up_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// grad_up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+#ifdef ENABLE_HOTPLUGGING
+		0,		// hotplug_block_up_cycles
+		0,		// block_up_multiplier_hotplug1
+		0,		// block_up_multiplier_hotplug2
+		0,		// block_up_multiplier_hotplug3
+		0,		// block_up_multiplier_hotplug4
+		0,		// block_up_multiplier_hotplug5
+		0,		// block_up_multiplier_hotplug6
+		0,		// block_up_multiplier_hotplug7
+		0,		// hotplug_block_down_cycles
+		0,		// block_down_multiplier_hotplug1
+		0,		// block_down_multiplier_hotplug2
+		0,		// block_down_multiplier_hotplug3
+		0,		// block_down_multiplier_hotplug4
+		0,		// block_down_multiplier_hotplug5
+		0,		// block_down_multiplier_hotplug6
+		0,		// block_down_multiplier_hotplug7
+		0,		// hotplug_stagger_up
+		0,		// hotplug_stagger_down
+		0,		// hotplug_idle_threshold
+		0,		// hotplug_idle_freq
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// hotplug_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// hotplug_engage_freq
+		0,		// hotplug_max_limit
+		0,		// hotplug_min_limit
+		0,		// hotplug_lock
+#endif /* ENABLE_HOTPLUGGING */
+		0,		// ignore_nice_load
+		0,		// sampling_down_factor
+		0,		// sampling_down_max_momentum
+		0,		// sampling_down_momentum_sensitivity
+		0,		// sampling_rate
+		0,		// sampling_rate_idle
+		0,		// sampling_rate_idle_delay
+		0,		// sampling_rate_idle_threshold
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// sampling_rate_sleep_multiplier
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// scaling_block_cycles
+#ifdef CONFIG_EXYNOS4_EXPORT_TEMP
+		0,		// hotplug_block_temp
+#endif /* CONFIG_EXYNOS4_EXPORT_TEMP */
+#ifdef ENABLE_SNAP_THERMAL_SUPPORT
+		0,		// scaling_trip_temp
+#endif /* ENABLE_SNAP_THERMAL_SUPPORT */
+		0,		// scaling_block_freq
+		0,		// scaling_block_threshold
+		0,		// scaling_block_force_down
+		0,		// scaling_fastdown_freq
+		0,		// scaling_fastdown_up_threshold
+		0,		// scaling_fastdown_down_threshold
+		0,		// scaling_responsiveness_freq
+		0,		// scaling_responsiveness_up_threshold
+		0,		// scaling_proportional
+#ifdef ENABLE_INPUTBOOSTER
+		0,		// inputboost_cycles
+		0,		// inputboost_up_threshold
+		0,		// inputboost_punch_cycles
+		0,		// inputboost_punch_freq
+		0,		// inputboost_punch_on_fingerdown
+		0,		// inputboost_punch_on_fingermove
+		0,		// inputboost_punch_on_epenmove
+		0,		// inputboost_typingbooster_up_threshold
+		0,		// inputboost_typingbooster_cores
+#endif /* ENABLE_INPUTBOOSTER */
+#ifdef ENABLE_MUSIC_LIMITS
+		0,		// music_max_freq
+		0,		// music_min_freq
+		0,		// music_min_cores
+#endif /* ENABLE_MUSIC_LIMITS */
+		0,		// smooth_up
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0,		// smooth_up_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+		0,		// up_threshold
+#ifdef ENABLE_HOTPLUGGING
+		0,		// up_threshold_hotplug1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// up_threshold_hotplug2
+		0,		// up_threshold_hotplug3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// up_threshold_hotplug4
+		0,		// up_threshold_hotplug5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// up_threshold_hotplug6
+		0,		// up_threshold_hotplug7
+#endif /* (MAX_CORES == 8) */
+		0,		// up_threshold_hotplug_freq1
+#if (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq2
+		0,		// up_threshold_hotplug_freq3
+#endif /* (MAX_CORES == 4 || MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 6 || MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq4
+		0,		// up_threshold_hotplug_freq5
+#endif /* (MAX_CORES == 6 || MAX_CORES == 8) */
+#if (MAX_CORES == 8)
+		0,		// up_threshold_hotplug_freq6
+		0,		// up_threshold_hotplug_freq7
+#endif /* (MAX_CORES == 8) */
+#endif /* ENABLE_HOTPLUGGING */
+#if defined(CONFIG_HAS_EARLYSUSPEND) || defined(CONFIG_POWERSUSPEND) || defined(USE_LCD_NOTIFIER)
+		0		// up_threshold_sleep
+#endif /* defined(CONFIG_HAS_EARLYSUSPEND)... */
+	}
+};
diff --git a/drivers/gpu/msm/kgsl_pwrctrl.c b/drivers/gpu/msm/kgsl_pwrctrl.c
index 2c224db..374fac3 100755
--- a/drivers/gpu/msm/kgsl_pwrctrl.c
+++ b/drivers/gpu/msm/kgsl_pwrctrl.c
@@ -44,6 +44,14 @@
 #define INIT_UDELAY		200
 #define MAX_UDELAY		2000
 
+#ifdef CONFIG_CPU_FREQ_GOV_ELECTROACTIVE
+int graphics_boost_electroactive = 6;
+#endif
+
+#ifdef CONFIG_CPU_FREQ_GOV_ELEMENTALX
+int graphics_boost_elementalx = 4;
+#endif
+
 struct clk_pair {
 	const char *name;
 	uint map;
@@ -178,6 +186,14 @@ void kgsl_pwrctrl_pwrlevel_change(struct kgsl_device *device,
 
 
 	trace_kgsl_pwrlevel(device, pwr->active_pwrlevel, pwrlevel->gpu_freq);
+
+#ifdef CONFIG_CPU_FREQ_GOV_ELECTROACTIVE
+        graphics_boost_electroactive = pwr->active_pwrlevel;
+#endif
+
+#ifdef CONFIG_CPU_FREQ_GOV_ELEMENTALX
+        graphics_boost_elementalx = pwr->active_pwrlevel;
+#endif
 }
 
 EXPORT_SYMBOL(kgsl_pwrctrl_pwrlevel_change);
diff --git a/include/linux/cpufreq.h b/include/linux/cpufreq.h
index 71b049b..c660747 100755
--- a/include/linux/cpufreq.h
+++ b/include/linux/cpufreq.h
@@ -75,7 +75,7 @@ struct cpufreq_policy {
 	unsigned int		max;    /* in kHz */
 	unsigned int		cur;    /* in kHz, only needed if cpufreq
 					 * governors are used */
-	unsigned int		util;  /* CPU utilization at max frequency */
+	unsigned int		util;	/* CPU utilization at max frequency */
 	unsigned int		load_at_max;  /* CPU utilization at max frequency */
 
 	unsigned int		policy; /* see above */
@@ -267,6 +267,8 @@ struct cpufreq_driver {
 	unsigned int	(*get)	(unsigned int cpu);
 
 	/* optional */
+	unsigned int (*getavg)	(struct cpufreq_policy *policy,
+				 unsigned int cpu);
 	int	(*bios_limit)	(int cpu, unsigned int *limit);
 
 	int	(*exit)		(struct cpufreq_policy *policy);
@@ -334,6 +336,7 @@ cpufreq_verify_within_cpu_limits(struct cpufreq_policy *policy)
 
 #define CPUFREQ_TRANSITION_NOTIFIER	(0)
 #define CPUFREQ_POLICY_NOTIFIER		(1)
+#define CPUFREQ_GOVINFO_NOTIFIER	(2)
 
 /* Transition notifiers */
 #define CPUFREQ_PRECHANGE		(0)
@@ -350,12 +353,28 @@ cpufreq_verify_within_cpu_limits(struct cpufreq_policy *policy)
 #define CPUFREQ_CREATE_POLICY		(5)
 #define CPUFREQ_REMOVE_POLICY		(6)
 
+/* Govinfo Notifiers */
+#define CPUFREQ_LOAD_CHANGE		(0)
+
 #ifdef CONFIG_CPU_FREQ
 int cpufreq_register_notifier(struct notifier_block *nb, unsigned int list);
 int cpufreq_unregister_notifier(struct notifier_block *nb, unsigned int list);
 
 void cpufreq_notify_transition(struct cpufreq_policy *policy,
 		struct cpufreq_freqs *freqs, unsigned int state);
+void cpufreq_notify_utilization(struct cpufreq_policy *policy,
+		unsigned int load);
+
+/*
+ * Governor specific info that can be passed to modules that subscribe
+ * to CPUFREQ_GOVINFO_NOTIFIER
+ */
+struct cpufreq_govinfo {
+	unsigned int cpu;
+	unsigned int load;
+	unsigned int sampling_rate_us;
+};
+extern struct atomic_notifier_head cpufreq_govinfo_notifier_list;
 
 #else /* CONFIG_CPU_FREQ */
 static inline int cpufreq_register_notifier(struct notifier_block *nb,
@@ -437,6 +456,8 @@ int cpufreq_driver_target(struct cpufreq_policy *policy,
 int __cpufreq_driver_target(struct cpufreq_policy *policy,
 				   unsigned int target_freq,
 				   unsigned int relation);
+extern int __cpufreq_driver_getavg(struct cpufreq_policy *policy,
+				   unsigned int cpu);
 int cpufreq_register_governor(struct cpufreq_governor *governor);
 void cpufreq_unregister_governor(struct cpufreq_governor *governor);
 
@@ -448,6 +469,10 @@ void cpufreq_unregister_governor(struct cpufreq_governor *governor);
 #ifdef CONFIG_CPU_FREQ_GOV_PERFORMANCE
 extern struct cpufreq_governor cpufreq_gov_performance;
 #endif
+#ifdef CONFIG_CPU_FREQ_GOV_CAFACTIVE
+extern unsigned int cpufreq_cafactive_get_hispeed_freq(int cpu);
+extern void cafactive_boost_ondemand(int cpu, s64 miliseconds, bool static_switch);
+#endif
 #ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE
 #define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_performance)
 #elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_POWERSAVE)
@@ -465,6 +490,99 @@ extern struct cpufreq_governor cpufreq_gov_conservative;
 #elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE)
 extern struct cpufreq_governor cpufreq_gov_interactive;
 #define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_interactive)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_ZZMOOVE)
+extern struct cpufreq_governor cpufreq_gov_zzmoove;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_zzmoove)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_IMPULSE)
+extern struct cpufreq_governor cpufreq_gov_impulse;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_impulse)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_BARRY_ALLEN)
+extern struct cpufreq_governor cpufreq_gov_barry_allen;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_barry_allen)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE)
+extern struct cpufreq_governor cpufreq_gov_nightmare;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_nightmare)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_DARKNESS)
+extern struct cpufreq_governor cpufreq_gov_darkness;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_darkness)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ)
+extern struct cpufreq_governor cpufreq_gov_pegasusq;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_pegasusq)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_YANKACTIVE)
+extern struct cpufreq_governor cpufreq_gov_yankactive;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_yankactive)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE)
+extern struct cpufreq_governor cpufreq_gov_intelliactive;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_intelliactive)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS)
+extern struct cpufreq_governor cpufreq_gov_ondemandplus;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_ondemandplus)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE_PRO)
+extern struct cpufreq_governor cpufreq_gov_interactive_pro;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_interactive_pro)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIMM)
+extern struct cpufreq_governor cpufreq_gov_intellimm;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_intellimm)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_UMBRELLA_CORE)
+extern struct cpufreq_governor cpufreq_gov_umbrella_core;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_umbrella_core)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_BIOSHOCK)
+extern struct cpufreq_governor cpufreq_gov_bioshock;
+#define CPUFREQ_DEFAULT_GOVERNOR        (&cpufreq_gov_bioshock)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_RACCOON_CITY)
+extern struct cpufreq_governor cpufreq_gov_raccoon_city;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_raccoon_city)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_LIONHEART)
+extern struct cpufreq_governor cpufreq_gov_lionheart;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_lionheart)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX)
+extern struct cpufreq_governor cpufreq_gov_smartmax;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_smartmax)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2)
+extern struct cpufreq_governor cpufreq_gov_smartass2;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_smartass2)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_IRONACTIVE)
+extern struct cpufreq_governor cpufreq_gov_ironactive;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_ironactive)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX_EPS)
+extern struct cpufreq_governor cpufreq_gov_smartmax_eps;
+#define CPUFREQ_DEFAULT_GOVERNOR        (&cpufreq_gov_smartmax_eps)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_ALUCARD)
+extern struct cpufreq_governor cpufreq_gov_alucard;
+#define CPUFREQ_DEFAULT_GOVERNOR        (&cpufreq_gov_alucard)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_ELEMENTALX)
+extern struct cpufreq_governor cpufreq_gov_elementalx;
+#define CPUFREQ_DEFAULT_GOVERNOR        (&cpufreq_gov_elementalx)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_BLU_ACTIVE)
+extern struct cpufreq_governor cpufreq_gov_blu_active;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_blu_active)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_CAFACTIVE)
+extern struct cpufreq_governor cpufreq_gov_cafactive;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_cafactive)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_ELECTROACTIVE)
+extern struct cpufreq_governor cpufreq_gov_electroactive;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_electroactive)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_ELECTRODEMAND)
+extern struct cpufreq_governor cpufreq_gov_electrodemand;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_electrodemand)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND_X)
+extern struct cpufreq_governor cpufreq_gov_ondemand_x;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_ondemand_x)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND)
+extern struct cpufreq_governor cpufreq_gov_intellidemand;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_intellidemand)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_DESPAIR)
+extern struct cpufreq_governor cpufreq_gov_despair;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_despair)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_HELLSACTIVE)
+extern struct cpufreq_governor cpufreq_gov_hellsactive;
+#define CPUFREQ_DEFAULT_GOVERNOR        (&cpufreq_gov_hellsactive)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE_X)
+extern struct cpufreq_governor cpufreq_gov_interactive_x;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_interactive_x)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE_X)
+extern struct cpufreq_governor cpufreq_gov_conservative_x;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_conservative_x)
 #elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_ARTERACTIVE)
 extern struct cpufreq_governor cpufreq_gov_arteractive;
 #define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_arteractive)
diff --git a/include/linux/msm_kgsl.h b/include/linux/msm_kgsl.h
index 437bf5c..c0428e4 100755
--- a/include/linux/msm_kgsl.h
+++ b/include/linux/msm_kgsl.h
@@ -18,6 +18,14 @@
 #define KGSL_3D0_SHADER_MEMORY	"kgsl_3d0_shader_memory"
 #define KGSL_3D0_IRQ		"kgsl_3d0_irq"
 
+#ifdef CONFIG_CPU_FREQ_GOV_ELEMENTALX
+extern int graphics_boost_elementalx;
+#endif
+
+#ifdef CONFIG_CPU_FREQ_GOV_ELECTROACTIVE
+extern int graphics_boost_electroactive;
+#endif
+
 enum kgsl_iommu_context_id {
 	KGSL_IOMMU_CONTEXT_USER = 0,
 	KGSL_IOMMU_CONTEXT_PRIV = 1,
diff --git a/include/trace/events/cpufreq_barry_allen.h b/include/trace/events/cpufreq_barry_allen.h
new file mode 100644
index 0000000..422333c
--- /dev/null
+++ b/include/trace/events/cpufreq_barry_allen.h
@@ -0,0 +1,150 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM cpufreq_barry_allen
+
+#if !defined(_TRACE_CPUFREQ_BARRY_ALLEN_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_CPUFREQ_BARRY_ALLEN_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_EVENT_CLASS(set,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	         unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq),
+
+	TP_STRUCT__entry(
+	    __field(          u32, cpu_id    )
+	    __field(unsigned long, targfreq   )
+	    __field(unsigned long, actualfreq )
+	   ),
+
+	TP_fast_assign(
+	    __entry->cpu_id = (u32) cpu_id;
+	    __entry->targfreq = targfreq;
+	    __entry->actualfreq = actualfreq;
+	),
+
+	TP_printk("cpu=%u targ=%lu actual=%lu",
+	      __entry->cpu_id, __entry->targfreq,
+	      __entry->actualfreq)
+);
+
+DEFINE_EVENT(set, cpufreq_barry_allen_setspeed,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	     unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq)
+);
+
+DECLARE_EVENT_CLASS(loadeval,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+		    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg),
+
+	    TP_STRUCT__entry(
+		    __field(unsigned long, cpu_id    )
+		    __field(unsigned long, load      )
+		    __field(unsigned long, curtarg   )
+		    __field(unsigned long, curactual )
+		    __field(unsigned long, newtarg   )
+	    ),
+
+	    TP_fast_assign(
+		    __entry->cpu_id = cpu_id;
+		    __entry->load = load;
+		    __entry->curtarg = curtarg;
+		    __entry->curactual = curactual;
+		    __entry->newtarg = newtarg;
+	    ),
+
+	    TP_printk("cpu=%lu load=%lu cur=%lu actual=%lu targ=%lu",
+		      __entry->cpu_id, __entry->load, __entry->curtarg,
+		      __entry->curactual, __entry->newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_barry_allen_target,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_barry_allen_already,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_barry_allen_notyet,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DECLARE_EVENT_CLASS(modeeval,
+	    TP_PROTO(unsigned long cpu_id, unsigned long total_load,
+		     unsigned long single_enter, unsigned long multi_enter,
+		     unsigned long single_exit, unsigned long multi_exit, unsigned long mode),
+		    TP_ARGS(cpu_id, total_load, single_enter, multi_enter, single_exit, multi_exit, mode),
+
+	    TP_STRUCT__entry(
+		    __field(unsigned long, cpu_id      )
+		    __field(unsigned long, total_load  )
+		    __field(unsigned long, single_enter)
+		    __field(unsigned long, multi_enter )
+		    __field(unsigned long, single_exit )
+		    __field(unsigned long, multi_exit  )
+		    __field(unsigned long, mode)
+	    ),
+
+	    TP_fast_assign(
+		    __entry->cpu_id = cpu_id;
+		    __entry->total_load = total_load;
+		    __entry->single_enter = single_enter;
+		    __entry->multi_enter = multi_enter;
+		    __entry->single_exit = single_exit;
+		    __entry->multi_exit = multi_exit;
+		    __entry->mode = mode ;
+	    ),
+
+	    TP_printk("cpu=%lu load=%3lu s_en=%6lu m_en=%6lu s_ex=%6lu m_ex=%6lu ret=%lu",
+		      __entry->cpu_id, __entry->total_load, __entry->single_enter,
+		      __entry->multi_enter, __entry->single_exit, __entry->multi_exit, __entry->mode)
+);
+
+DEFINE_EVENT(modeeval, cpufreq_barry_allen_mode,
+	    TP_PROTO(unsigned long cpu_id, unsigned long total_load,
+		     unsigned long single_enter, unsigned long multi_enter,
+		     unsigned long single_exit, unsigned long multi_exit, unsigned long mode),
+	    TP_ARGS(cpu_id, total_load, single_enter, multi_enter, single_exit, multi_exit, mode)
+);
+
+TRACE_EVENT(cpufreq_barry_allen_boost,
+	    TP_PROTO(const char *s),
+	    TP_ARGS(s),
+	    TP_STRUCT__entry(
+		    __string(s, s)
+	    ),
+	    TP_fast_assign(
+		    __assign_str(s, s);
+	    ),
+	    TP_printk("%s", __get_str(s))
+);
+
+TRACE_EVENT(cpufreq_barry_allen_unboost,
+	    TP_PROTO(const char *s),
+	    TP_ARGS(s),
+	    TP_STRUCT__entry(
+		    __string(s, s)
+	    ),
+	    TP_fast_assign(
+		    __assign_str(s, s);
+	    ),
+	    TP_printk("%s", __get_str(s))
+);
+
+#endif /* _TRACE_CPUFREQ_BARRY_ALLEN_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/include/trace/events/cpufreq_cafactive.h b/include/trace/events/cpufreq_cafactive.h
new file mode 100644
index 0000000..b052c3e
--- /dev/null
+++ b/include/trace/events/cpufreq_cafactive.h
@@ -0,0 +1,124 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM cpufreq_cafactive
+
+#if !defined(_TRACE_CPUFREQ_CAFACTIVE_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_CPUFREQ_CAFACTIVE_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_EVENT_CLASS(set,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	         unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq),
+
+	TP_STRUCT__entry(
+	    __field(          u32, cpu_id    )
+	    __field(unsigned long, targfreq   )
+	    __field(unsigned long, actualfreq )
+	   ),
+
+	TP_fast_assign(
+	    __entry->cpu_id = (u32) cpu_id;
+	    __entry->targfreq = targfreq;
+	    __entry->actualfreq = actualfreq;
+	),
+
+	TP_printk("cpu=%u targ=%lu actual=%lu",
+	      __entry->cpu_id, __entry->targfreq,
+	      __entry->actualfreq)
+);
+
+DEFINE_EVENT(set, cpufreq_cafactive_setspeed,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	     unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq)
+);
+
+DECLARE_EVENT_CLASS(loadeval,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+		    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg),
+
+	    TP_STRUCT__entry(
+		    __field(unsigned long, cpu_id    )
+		    __field(unsigned long, load      )
+		    __field(unsigned long, curtarg   )
+		    __field(unsigned long, curactual )
+		    __field(unsigned long, newtarg   )
+	    ),
+
+	    TP_fast_assign(
+		    __entry->cpu_id = cpu_id;
+		    __entry->load = load;
+		    __entry->curtarg = curtarg;
+		    __entry->curactual = curactual;
+		    __entry->newtarg = newtarg;
+	    ),
+
+	    TP_printk("cpu=%lu load=%lu cur=%lu actual=%lu targ=%lu",
+		      __entry->cpu_id, __entry->load, __entry->curtarg,
+		      __entry->curactual, __entry->newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_cafactive_target,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_cafactive_already,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_cafactive_notyet,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+TRACE_EVENT(cpufreq_cafactive_boost,
+	    TP_PROTO(const char *s),
+	    TP_ARGS(s),
+	    TP_STRUCT__entry(
+		    __string(s, s)
+	    ),
+	    TP_fast_assign(
+		    __assign_str(s, s);
+	    ),
+	    TP_printk("%s", __get_str(s))
+);
+
+TRACE_EVENT(cpufreq_cafactive_unboost,
+	    TP_PROTO(const char *s),
+	    TP_ARGS(s),
+	    TP_STRUCT__entry(
+		    __string(s, s)
+	    ),
+	    TP_fast_assign(
+		    __assign_str(s, s);
+	    ),
+	    TP_printk("%s", __get_str(s))
+);
+
+TRACE_EVENT(cpufreq_cafactive_load_change,
+	    TP_PROTO(unsigned long cpu_id),
+	    TP_ARGS(cpu_id),
+	    TP_STRUCT__entry(
+		__field(unsigned long, cpu_id)
+	    ),
+	    TP_fast_assign(
+		__entry->cpu_id = cpu_id;
+	    ),
+	    TP_printk("re-evaluate for cpu=%lu", __entry->cpu_id)
+);
+
+#endif /* _TRACE_CPUFREQ_CAFACTIVE_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/include/trace/events/cpufreq_electroactive.h b/include/trace/events/cpufreq_electroactive.h
new file mode 100644
index 0000000..29c96de
--- /dev/null
+++ b/include/trace/events/cpufreq_electroactive.h
@@ -0,0 +1,86 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM cpufreq_electroactive
+
+#if !defined(_TRACE_CPUFREQ_INTERACTIVE_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_CPUFREQ_INTERACTIVE_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_EVENT_CLASS(set,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	         unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq),
+
+	TP_STRUCT__entry(
+	    __field(          u32, cpu_id    )
+	    __field(unsigned long, targfreq   )
+	    __field(unsigned long, actualfreq )
+	   ),
+
+	TP_fast_assign(
+	    __entry->cpu_id = (u32) cpu_id;
+	    __entry->targfreq = targfreq;
+	    __entry->actualfreq = actualfreq;
+	),
+
+	TP_printk("cpu=%u targ=%lu actual=%lu",
+	      __entry->cpu_id, __entry->targfreq,
+	      __entry->actualfreq)
+);
+
+DEFINE_EVENT(set, cpufreq_electroactive_up,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+		unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq)
+);
+
+DEFINE_EVENT(set, cpufreq_electroactive_down,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+		unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq)
+);
+
+DECLARE_EVENT_CLASS(loadeval,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+		    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg),
+
+	    TP_STRUCT__entry(
+		    __field(unsigned long, cpu_id    )
+		    __field(unsigned long, load      )
+		    __field(unsigned long, curtarg   )
+		    __field(unsigned long, curactual )
+		    __field(unsigned long, newtarg   )
+	    ),
+
+	    TP_fast_assign(
+		    __entry->cpu_id = cpu_id;
+		    __entry->load = load;
+		    __entry->curtarg = curtarg;
+		    __entry->curactual = curactual;
+		    __entry->newtarg = newtarg;
+	    ),
+
+	    TP_printk("cpu=%lu load=%lu cur=%lu actual=%lu targ=%lu",
+		      __entry->cpu_id, __entry->load, __entry->curtarg,
+		      __entry->curactual, __entry->newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_electroactive_target,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_electroactive_already,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+#endif /* _TRACE_CPUFREQ_INTERACTIVE_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/include/trace/events/cpufreq_interactive.h b/include/trace/events/cpufreq_interactive.h
index 4b14801..6c0d9e5 100755
--- a/include/trace/events/cpufreq_interactive.h
+++ b/include/trace/events/cpufreq_interactive.h
@@ -34,6 +34,20 @@ DEFINE_EVENT(set, cpufreq_interactive_setspeed,
 	TP_ARGS(cpu_id, targfreq, actualfreq)
 );
 
+#ifdef CONFIG_CPU_FREQ_GOV_ELECTROACTIVE
+DEFINE_EVENT(set, cpufreq_interactive_up,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	     unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq)
+);
+
+DEFINE_EVENT(set, cpufreq_interactive_down,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	     unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq)
+);
+#endif
+
 DECLARE_EVENT_CLASS(loadeval,
 	    TP_PROTO(unsigned long cpu_id, unsigned long load,
 		     unsigned long curtarg, unsigned long curactual,
diff --git a/include/trace/events/cpufreq_interactive_x.h b/include/trace/events/cpufreq_interactive_x.h
new file mode 100755
index 0000000..464a46c
--- /dev/null
+++ b/include/trace/events/cpufreq_interactive_x.h
@@ -0,0 +1,113 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM cpufreq_interactive_x
+
+#if !defined(_TRACE_CPUFREQ_INTERACTIVE_X_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_CPUFREQ_INTERACTIVE_X_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_EVENT_CLASS(set,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	         unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq),
+
+	TP_STRUCT__entry(
+	    __field(          u32, cpu_id    )
+	    __field(unsigned long, targfreq   )
+	    __field(unsigned long, actualfreq )
+	   ),
+
+	TP_fast_assign(
+	    __entry->cpu_id = (u32) cpu_id;
+	    __entry->targfreq = targfreq;
+	    __entry->actualfreq = actualfreq;
+	),
+
+	TP_printk("cpu=%u targ=%lu actual=%lu",
+	      __entry->cpu_id, __entry->targfreq,
+	      __entry->actualfreq)
+);
+
+DEFINE_EVENT(set, cpufreq_interactive_x_setspeed,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	     unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq)
+);
+
+DECLARE_EVENT_CLASS(loadeval,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+		    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg),
+
+	    TP_STRUCT__entry(
+		    __field(unsigned long, cpu_id    )
+		    __field(unsigned long, load      )
+		    __field(unsigned long, curtarg   )
+		    __field(unsigned long, curactual )
+		    __field(unsigned long, newtarg   )
+	    ),
+
+	    TP_fast_assign(
+		    __entry->cpu_id = cpu_id;
+		    __entry->load = load;
+		    __entry->curtarg = curtarg;
+		    __entry->curactual = curactual;
+		    __entry->newtarg = newtarg;
+	    ),
+
+	    TP_printk("cpu=%lu load=%lu cur=%lu actual=%lu targ=%lu",
+		      __entry->cpu_id, __entry->load, __entry->curtarg,
+		      __entry->curactual, __entry->newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_interactive_x_target,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_interactive_x_already,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_interactive_x_notyet,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+TRACE_EVENT(cpufreq_interactive_x_boost,
+	    TP_PROTO(const char *s),
+	    TP_ARGS(s),
+	    TP_STRUCT__entry(
+		    __string(s, s)
+	    ),
+	    TP_fast_assign(
+		    __assign_str(s, s);
+	    ),
+	    TP_printk("%s", __get_str(s))
+);
+
+TRACE_EVENT(cpufreq_interactive_x_unboost,
+	    TP_PROTO(const char *s),
+	    TP_ARGS(s),
+	    TP_STRUCT__entry(
+		    __string(s, s)
+	    ),
+	    TP_fast_assign(
+		    __assign_str(s, s);
+	    ),
+	    TP_printk("%s", __get_str(s))
+);
+
+#endif /* _TRACE_CPUFREQ_INTERACTIVE_X_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
+
diff --git a/include/trace/events/cpufreq_ironactive.h b/include/trace/events/cpufreq_ironactive.h
new file mode 100755
index 0000000..bfdd8aa
--- /dev/null
+++ b/include/trace/events/cpufreq_ironactive.h
@@ -0,0 +1,138 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM cpufreq_ironactive
+
+#if !defined(_TRACE_CPUFREQ_IRONACTIVE_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_CPUFREQ_IRONACTIVE_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_EVENT_CLASS(set,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	         unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq),
+
+	TP_STRUCT__entry(
+	    __field(          u32, cpu_id    )
+	    __field(unsigned long, targfreq   )
+	    __field(unsigned long, actualfreq )
+	   ),
+
+	TP_fast_assign(
+	    __entry->cpu_id = (u32) cpu_id;
+	    __entry->targfreq = targfreq;
+	    __entry->actualfreq = actualfreq;
+	),
+
+	TP_printk("cpu=%u targ=%lu actual=%lu",
+	      __entry->cpu_id, __entry->targfreq,
+	      __entry->actualfreq)
+);
+
+DEFINE_EVENT(set, cpufreq_ironactive_setspeed,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	     unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq)
+);
+
+DECLARE_EVENT_CLASS(loadeval,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+		    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg),
+
+	    TP_STRUCT__entry(
+		    __field(unsigned long, cpu_id    )
+		    __field(unsigned long, load      )
+		    __field(unsigned long, curtarg   )
+		    __field(unsigned long, curactual )
+		    __field(unsigned long, newtarg   )
+	    ),
+
+	    TP_fast_assign(
+		    __entry->cpu_id = cpu_id;
+		    __entry->load = load;
+		    __entry->curtarg = curtarg;
+		    __entry->curactual = curactual;
+		    __entry->newtarg = newtarg;
+	    ),
+
+	    TP_printk("cpu=%lu load=%lu cur=%lu actual=%lu targ=%lu",
+		      __entry->cpu_id, __entry->load, __entry->curtarg,
+		      __entry->curactual, __entry->newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_ironactive_target,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_ironactive_already,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_ironactive_notyet,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+TRACE_EVENT(cpufreq_ironactive_boost,
+	    TP_PROTO(const char *s),
+	    TP_ARGS(s),
+	    TP_STRUCT__entry(
+		    __string(s, s)
+	    ),
+	    TP_fast_assign(
+		    __assign_str(s, s);
+	    ),
+	    TP_printk("%s", __get_str(s))
+);
+
+TRACE_EVENT(cpufreq_ironactive_unboost,
+	    TP_PROTO(const char *s),
+	    TP_ARGS(s),
+	    TP_STRUCT__entry(
+		    __string(s, s)
+	    ),
+	    TP_fast_assign(
+		    __assign_str(s, s);
+	    ),
+	    TP_printk("%s", __get_str(s))
+);
+
+TRACE_EVENT(cpufreq_ironactive_load_change,
+	    TP_PROTO(unsigned long cpu_id),
+	    TP_ARGS(cpu_id),
+	    TP_STRUCT__entry(
+		__field(unsigned long, cpu_id)
+	    ),
+	    TP_fast_assign(
+		__entry->cpu_id = cpu_id;
+	    ),
+	    TP_printk("re-evaluate for cpu=%lu", __entry->cpu_id)
+);
+
+TRACE_EVENT(cpufreq_ironactive_cpuload,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load),
+	    TP_ARGS(cpu_id, load),
+	    TP_STRUCT__entry(
+		__field(unsigned long, cpu_id)
+		__field(unsigned long, load)
+	    ),
+	    TP_fast_assign(
+		__entry->cpu_id = cpu_id;
+		__entry->load = load;
+	    ),
+	    TP_printk("cpu=%lu load=%lu", __entry->cpu_id, __entry->load)
+);
+
+#endif /* _TRACE_CPUFREQ_IRONACTIVE_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/include/trace/events/cpufreq_ondemandplus.h b/include/trace/events/cpufreq_ondemandplus.h
new file mode 100755
index 0000000..d178e5b
--- /dev/null
+++ b/include/trace/events/cpufreq_ondemandplus.h
@@ -0,0 +1,77 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM cpufreq_ondemandplus
+
+#if !defined(_TRACE_CPUFREQ_ONDEMANDPLUS_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_CPUFREQ_ONDEMANDPLUS_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_EVENT_CLASS(set,
+        TP_PROTO(u32 cpu_id, unsigned long targfreq,
+                 unsigned long actualfreq),
+        TP_ARGS(cpu_id, targfreq, actualfreq),
+
+        TP_STRUCT__entry(
+            __field(          u32, cpu_id    )
+            __field(unsigned long, targfreq   )
+            __field(unsigned long, actualfreq )
+           ),
+
+        TP_fast_assign(
+            __entry->cpu_id = (u32) cpu_id;
+            __entry->targfreq = targfreq;
+            __entry->actualfreq = actualfreq;
+        ),
+
+        TP_printk("cpu=%u targ=%lu actual=%lu",
+              __entry->cpu_id, __entry->targfreq,
+              __entry->actualfreq)
+);
+
+DEFINE_EVENT(set, cpufreq_ondemandplus_setspeed,
+        TP_PROTO(u32 cpu_id, unsigned long targfreq,
+             unsigned long actualfreq),
+        TP_ARGS(cpu_id, targfreq, actualfreq)
+);
+
+DECLARE_EVENT_CLASS(loadeval,
+            TP_PROTO(unsigned long cpu_id, unsigned long load,
+                     unsigned long curfreq, unsigned long targfreq),
+            TP_ARGS(cpu_id, load, curfreq, targfreq),
+
+            TP_STRUCT__entry(
+                    __field(unsigned long, cpu_id    )
+                    __field(unsigned long, load      )
+                    __field(unsigned long, curfreq   )
+                    __field(unsigned long, targfreq  )
+            ),
+
+            TP_fast_assign(
+                    __entry->cpu_id = cpu_id;
+                    __entry->load = load;
+                    __entry->curfreq = curfreq;
+                    __entry->targfreq = targfreq;
+            ),
+
+            TP_printk("cpu=%lu load=%lu cur=%lu targ=%lu",
+                      __entry->cpu_id, __entry->load, __entry->curfreq,
+                      __entry->targfreq)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_ondemandplus_target,
+            TP_PROTO(unsigned long cpu_id, unsigned long load,
+                     unsigned long curfreq, unsigned long targfreq),
+            TP_ARGS(cpu_id, load, curfreq, targfreq)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_ondemandplus_already,
+            TP_PROTO(unsigned long cpu_id, unsigned long load,
+                     unsigned long curfreq, unsigned long targfreq),
+            TP_ARGS(cpu_id, load, curfreq, targfreq)
+);
+
+#endif /* _TRACE_CPUFREQ_ONDEMANDPLUS_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
+
diff --git a/include/trace/events/cpufreq_umbrella_core.h b/include/trace/events/cpufreq_umbrella_core.h
new file mode 100755
index 0000000..7e71763
--- /dev/null
+++ b/include/trace/events/cpufreq_umbrella_core.h
@@ -0,0 +1,150 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM cpufreq_umbrella_core
+
+#if !defined(_TRACE_CPUFREQ_UMBRELLA_CORE_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_CPUFREQ_UMBRELLA_CORE_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_EVENT_CLASS(set,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	         unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq),
+
+	TP_STRUCT__entry(
+	    __field(          u32, cpu_id    )
+	    __field(unsigned long, targfreq   )
+	    __field(unsigned long, actualfreq )
+	   ),
+
+	TP_fast_assign(
+	    __entry->cpu_id = (u32) cpu_id;
+	    __entry->targfreq = targfreq;
+	    __entry->actualfreq = actualfreq;
+	),
+
+	TP_printk("cpu=%u targ=%lu actual=%lu",
+	      __entry->cpu_id, __entry->targfreq,
+	      __entry->actualfreq)
+);
+
+DEFINE_EVENT(set, cpufreq_umbrella_core_setspeed,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	     unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq)
+);
+
+DECLARE_EVENT_CLASS(loadeval,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+		    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg),
+
+	    TP_STRUCT__entry(
+		    __field(unsigned long, cpu_id    )
+		    __field(unsigned long, load      )
+		    __field(unsigned long, curtarg   )
+		    __field(unsigned long, curactual )
+		    __field(unsigned long, newtarg   )
+	    ),
+
+	    TP_fast_assign(
+		    __entry->cpu_id = cpu_id;
+		    __entry->load = load;
+		    __entry->curtarg = curtarg;
+		    __entry->curactual = curactual;
+		    __entry->newtarg = newtarg;
+	    ),
+
+	    TP_printk("cpu=%lu load=%lu cur=%lu actual=%lu targ=%lu",
+		      __entry->cpu_id, __entry->load, __entry->curtarg,
+		      __entry->curactual, __entry->newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_umbrella_core_target,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_umbrella_core_already,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_umbrella_core_notyet,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DECLARE_EVENT_CLASS(modeeval,
+	    TP_PROTO(unsigned long cpu_id, unsigned long total_load,
+		     unsigned long single_enter, unsigned long multi_enter,
+		     unsigned long single_exit, unsigned long multi_exit, unsigned long mode),
+		    TP_ARGS(cpu_id, total_load, single_enter, multi_enter, single_exit, multi_exit, mode),
+
+	    TP_STRUCT__entry(
+		    __field(unsigned long, cpu_id      )
+		    __field(unsigned long, total_load  )
+		    __field(unsigned long, single_enter)
+		    __field(unsigned long, multi_enter )
+		    __field(unsigned long, single_exit )
+		    __field(unsigned long, multi_exit  )
+		    __field(unsigned long, mode)
+	    ),
+
+	    TP_fast_assign(
+		    __entry->cpu_id = cpu_id;
+		    __entry->total_load = total_load;
+		    __entry->single_enter = single_enter;
+		    __entry->multi_enter = multi_enter;
+		    __entry->single_exit = single_exit;
+		    __entry->multi_exit = multi_exit;
+		    __entry->mode = mode ;
+	    ),
+
+	    TP_printk("cpu=%lu load=%3lu s_en=%6lu m_en=%6lu s_ex=%6lu m_ex=%6lu ret=%lu",
+		      __entry->cpu_id, __entry->total_load, __entry->single_enter,
+		      __entry->multi_enter, __entry->single_exit, __entry->multi_exit, __entry->mode)
+);
+
+DEFINE_EVENT(modeeval, cpufreq_umbrella_core_mode,
+	    TP_PROTO(unsigned long cpu_id, unsigned long total_load,
+		     unsigned long single_enter, unsigned long multi_enter,
+		     unsigned long single_exit, unsigned long multi_exit, unsigned long mode),
+	    TP_ARGS(cpu_id, total_load, single_enter, multi_enter, single_exit, multi_exit, mode)
+);
+
+TRACE_EVENT(cpufreq_umbrella_core_boost,
+	    TP_PROTO(const char *s),
+	    TP_ARGS(s),
+	    TP_STRUCT__entry(
+		    __string(s, s)
+	    ),
+	    TP_fast_assign(
+		    __assign_str(s, s);
+	    ),
+	    TP_printk("%s", __get_str(s))
+);
+
+TRACE_EVENT(cpufreq_umbrella_core_unboost,
+	    TP_PROTO(const char *s),
+	    TP_ARGS(s),
+	    TP_STRUCT__entry(
+		    __string(s, s)
+	    ),
+	    TP_fast_assign(
+		    __assign_str(s, s);
+	    ),
+	    TP_printk("%s", __get_str(s))
+);
+
+#endif /* _TRACE_CPUFREQ_UMBRELLA_CORE_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/include/trace/events/cpufreq_yankactive.h b/include/trace/events/cpufreq_yankactive.h
new file mode 100644
index 0000000..47d4938
--- /dev/null
+++ b/include/trace/events/cpufreq_yankactive.h
@@ -0,0 +1,112 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM cpufreq_yankactive
+
+#if !defined(_TRACE_CPUFREQ_YANKACTIVE_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_CPUFREQ_YANKACTIVE_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_EVENT_CLASS(set,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	         unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq),
+
+	TP_STRUCT__entry(
+	    __field(          u32, cpu_id    )
+	    __field(unsigned long, targfreq   )
+	    __field(unsigned long, actualfreq )
+	   ),
+
+	TP_fast_assign(
+	    __entry->cpu_id = (u32) cpu_id;
+	    __entry->targfreq = targfreq;
+	    __entry->actualfreq = actualfreq;
+	),
+
+	TP_printk("cpu=%u targ=%lu actual=%lu",
+	      __entry->cpu_id, __entry->targfreq,
+	      __entry->actualfreq)
+);
+
+DEFINE_EVENT(set, cpufreq_yankactive_setspeed,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	     unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq)
+);
+
+DECLARE_EVENT_CLASS(loadeval,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+		    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg),
+
+	    TP_STRUCT__entry(
+		    __field(unsigned long, cpu_id    )
+		    __field(unsigned long, load      )
+		    __field(unsigned long, curtarg   )
+		    __field(unsigned long, curactual )
+		    __field(unsigned long, newtarg   )
+	    ),
+
+	    TP_fast_assign(
+		    __entry->cpu_id = cpu_id;
+		    __entry->load = load;
+		    __entry->curtarg = curtarg;
+		    __entry->curactual = curactual;
+		    __entry->newtarg = newtarg;
+	    ),
+
+	    TP_printk("cpu=%lu load=%lu cur=%lu actual=%lu targ=%lu",
+		      __entry->cpu_id, __entry->load, __entry->curtarg,
+		      __entry->curactual, __entry->newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_yankactive_target,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_yankactive_already,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_yankactive_notyet,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+TRACE_EVENT(cpufreq_yankactive_boost,
+	    TP_PROTO(const char *s),
+	    TP_ARGS(s),
+	    TP_STRUCT__entry(
+		    __string(s, s)
+	    ),
+	    TP_fast_assign(
+		    __assign_str(s, s);
+	    ),
+	    TP_printk("%s", __get_str(s))
+);
+
+TRACE_EVENT(cpufreq_yankactive_unboost,
+	    TP_PROTO(const char *s),
+	    TP_ARGS(s),
+	    TP_STRUCT__entry(
+		    __string(s, s)
+	    ),
+	    TP_fast_assign(
+		    __assign_str(s, s);
+	    ),
+	    TP_printk("%s", __get_str(s))
+);
+
+#endif /* _TRACE_CPUFREQ_YANKACTIVE_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
